\section{Results}
\label{sec:results}

\subsection{Alignment Shifts Text Distributions (Experiment 1)}
\label{sec:results_exp1}

\Tabref{tab:effect_sizes} presents Cohen's $d$ effect sizes for distributional feature differences between base and aligned models within each family.
Two patterns are consistent across all three families:

\begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
    \item \textbf{Alignment reduces sentence-length variability.}
    The standard deviation of sentence length decreases significantly with alignment in all three families ($d = -0.66$ for Mistral, $d = -0.64$ for MPT, $d = -0.45$ for Cohere; all $p < 0.001$ after Bonferroni correction).
    \item \textbf{Alignment produces shorter text.}
    Token count decreases across all families ($d = -1.57$ for Mistral, $d = -2.25$ for MPT, $d = -0.22$ for Cohere).
\end{itemize}

Lexical diversity measures show mixed patterns.
Alignment \emph{increases} distinct $n$-gram ratios for Mistral ($d = +0.92$ for distinct 2-grams) and MPT ($d = +1.13$), but \emph{decreases} them for Cohere ($d = -0.28$).
This suggests that the universal alignment watermark lies in \emph{structural regularity}, not necessarily vocabulary narrowing.

Comparing aligned text to human text reveals the same signature: aligned text has significantly lower sentence-length variation than human text ($d = -0.65$, $p < 10^{-144}$), is shorter ($d = -1.29$, $p < 10^{-137}$), and has higher type-token ratio ($d = +0.66$, $p < 10^{-33}$).
Base model text is closer to the human distribution on most metrics.

\begin{table}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Feature} & \textsc{Mistral} & \textsc{MPT} & \textsc{Cohere} & \textbf{Direction} \\
        \midrule
        Type-token ratio & $+0.57$ & $+0.64$ & $-0.29$ & Mixed \\
        Distinct 2-gram & $+0.92$ & $+1.13$ & $-0.28$ & Mixed \\
        Distinct 3-gram & $+1.02$ & $+1.21$ & $-0.26$ & Mixed \\
        Mean sent.\ length & $-0.11$ & $-0.52$ & $+0.08$ & Mixed \\
        Std sent.\ length & $\mathbf{-0.66}$ & $\mathbf{-0.64}$ & $\mathbf{-0.45}$ & Consistent $\downarrow$ \\
        Mean word length & $+0.56$ & $+0.01$ & $+0.19$ & Mostly $\uparrow$ \\
        Hapax ratio & $+0.68$ & $+0.75$ & $-0.27$ & Mixed \\
        Num tokens & $\mathbf{-1.57}$ & $\mathbf{-2.25}$ & $\mathbf{-0.22}$ & Consistent $\downarrow$ \\
        \bottomrule
    \end{tabular}
    }
    \caption{Cohen's $d$ effect sizes for base vs.\ aligned distributional features within each model family. Positive values indicate aligned $>$ base. Bold entries mark features that shift consistently across all families. Alignment consistently reduces sentence-length variability and text length.}
    \label{tab:effect_sizes}
\end{table}

\subsection{Alignment Dramatically Increases Statistical Detectability (Experiment 2)}
\label{sec:results_exp2}

\Tabref{tab:auroc} presents AUROC values for distinguishing human text from each AI source using three zero-shot detection signals.
The results strongly support the alignment watermark hypothesis.

\para{Base models approach undetectability.}
Base \mistral and \mpt have AUROC values near 0.50 for mean log-probability (0.514 and 0.504, respectively) and mean log-rank (0.535 and 0.506).
These models are statistically indistinguishable from human text by these metrics, consistent with the prediction that a model trained to match the data distribution should approximate the human distribution.

\para{Alignment dramatically increases detectability.}
Aligned variants reach AUROC 0.71--0.88 for the same metrics, representing increases of 10--35 percentage points (\tabref{tab:auroc}).
The largest effect is for \mistralchat on mean log-rank: AUROC jumps from 0.535 (base) to \textbf{0.881} (aligned), a $\Delta$ of +0.346.
Even \cohere, which has higher baseline detectability (AUROC 0.84--0.86), shows a further 9--11 point increase with alignment.

\para{Entropy is the weakest signal.}
Mean entropy shows smaller and less consistent alignment effects ($\Delta$ ranges from $-0.014$ to $+0.110$), suggesting that the alignment watermark is primarily about token-level predictability (log-probability, log-rank) rather than overall uncertainty.

\begin{table}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llccc@{}}
        \toprule
        \textbf{Family} & \textbf{Signal} & \textbf{Base AUROC} & \textbf{Aligned AUROC} & $\boldsymbol{\Delta}$ \\
        \midrule
        \multirow{3}{*}{\textsc{Mistral}} & Mean log-prob & 0.514 & \textbf{0.831} & +0.317 \\
        & Mean log-rank & 0.535 & \textbf{0.881} & \textbf{+0.346} \\
        & Mean entropy & 0.635 & 0.724 & +0.088 \\
        \midrule
        \multirow{3}{*}{\textsc{MPT}} & Mean log-prob & 0.504 & \textbf{0.711} & +0.206 \\
        & Mean log-rank & 0.506 & \textbf{0.740} & +0.234 \\
        & Mean entropy & 0.587 & 0.573 & $-0.014$ \\
        \midrule
        \multirow{3}{*}{\textsc{Cohere}} & Mean log-prob & 0.861 & \textbf{0.954} & +0.092 \\
        & Mean log-rank & 0.840 & \textbf{0.949} & +0.109 \\
        & Mean entropy & 0.620 & 0.731 & +0.110 \\
        \bottomrule
    \end{tabular}
    }
    \caption{AUROC for distinguishing human text from AI text using zero-shot statistical detection signals. Aligned models are substantially more detectable than base models across all families and primary signals. Best results per row in \textbf{bold}.}
    \label{tab:auroc}
\end{table}

\subsection{LLMs Detect Aligned Text Near-Perfectly (Experiment 3)}
\label{sec:results_exp3}

\Tabref{tab:llm_detector} presents \gptfour detection rates for each text category.
Aligned model text is detected with near-perfect accuracy: \gptfour achieves 100\% TPR on aligned Mistral and MPT text, and 97.5\% on aligned Cohere text.
Base model text is also highly detectable (85--96.3\% TPR), but aligned text consistently reaches higher rates.

The improvement is statistically significant for Mistral ($\Delta = +6.5$ pp, $p = 0.021$) and MPT ($\Delta = +15.0$ pp, $p = 0.0003$).
Cohere shows a smaller, non-significant improvement ($\Delta = +1.3$ pp, $p = 0.650$), likely because its base model is already near-ceiling.

\para{Human text is often misclassified.}
\gptfour achieves only 56.2\% TNR on human text, frequently labeling human scientific abstracts as AI-generated.
This reflects the inherent formulaic nature of scientific writing and suggests that the AUROC of $\sim$0.77--0.80 is driven more by confidence calibration than perfect binary classification.

\begin{table}[t]
    \centering
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}lcccccc@{}}
        \toprule
        & \multicolumn{2}{c}{\textsc{Mistral}} & \multicolumn{2}{c}{\textsc{MPT}} & \multicolumn{2}{c}{\textsc{Cohere}} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-7}
        & Base & Aligned & Base & Aligned & Base & Aligned \\
        \midrule
        TPR & 0.935 & \textbf{1.000} & 0.850 & \textbf{1.000} & 0.963 & \textbf{0.975} \\
        Mean AI score & 0.896 & \textbf{0.936} & 0.833 & \textbf{0.943} & 0.899 & \textbf{0.919} \\
        \midrule
        $\Delta$ TPR & \multicolumn{2}{c}{$+0.065$ ($p=0.021$)} & \multicolumn{2}{c}{$+0.150$ ($p=0.0003$)} & \multicolumn{2}{c}{$+0.013$ ($p=0.650$)} \\
        \bottomrule
    \end{tabular}
    }
    \caption{\gptfour detection performance by model family. TPR is the rate at which AI text is correctly classified as AI. Human text TNR is 56.2\%. Aligned models are detected at higher rates than base models, significantly so for Mistral and MPT. Best per-family results in \textbf{bold}.}
    \label{tab:llm_detector}
\end{table}

\subsection{The Alignment Watermark Generalizes Across Families (Experiment 4)}
\label{sec:results_exp4}

Across the 12 family$\times$metric comparisons, aligned models are more detectable than base models in 11 cases.
The single exception is MPT's mean entropy ($\Delta = -0.014$), which is negligible.
A binomial sign test confirms that this consistency significantly exceeds the null expectation of 50\% ($p = 0.006$).

\Tabref{tab:consistency} summarizes the cross-family results.
The alignment watermark is not an artifact of one model architecture or one detection method---it is a cross-family, cross-paradigm phenomenon.

\begin{table}[t]
    \centering
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Signal} & \textsc{Mistral} & \textsc{MPT} & \textsc{Cohere} & \textbf{Consistent?} \\
        \midrule
        Mean log-prob & \checkmark & \checkmark & \checkmark & Yes \\
        Mean log-rank & \checkmark & \checkmark & \checkmark & Yes \\
        Mean entropy & \checkmark & $\times$ & \checkmark & No \\
        \gptfour TPR & \checkmark & \checkmark & \checkmark & Yes \\
        \bottomrule
    \end{tabular}
    \caption{Cross-family consistency of the alignment watermark. \checkmark\ indicates aligned AUROC/TPR $>$ base. 11 of 12 comparisons show aligned $>$ base (sign test $p = 0.006$).}
    \label{tab:consistency}
\end{table}
