[
  {
    "id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
    "adv_source_id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
    "source_id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging",
    "prompt": null,
    "generation": "The recent advancements in artificial intelligence (AI) combined with the\nextensive amount of data generated by today's clinical systems, has led to the\ndevelopment of imaging AI solutions across the whole value chain of medical\nimaging, including image reconstruction, medical image segmentation,\nimage-based diagnosis and treatment planning. Notwithstanding the successes and\nfuture potential of AI in medical imaging, many stakeholders are concerned of\nthe potential risks and ethical implications of imaging AI solutions, which are\nperceived as complex, opaque, and difficult to comprehend, utilise, and trust\nin critical clinical applications. Despite these concerns and risks, there are\ncurrently no concrete guidelines and best practices for guiding future AI\ndevelopments in medical imaging towards increased trust, safety and adoption.\nTo bridge this gap, this paper introduces a careful selection of guiding\nprinciples drawn from the accumulated experiences, consensus, and best\npractices from five large European projects on AI in Health Imaging. These\nguiding principles are named FUTURE-AI and its building blocks consist of (i)\nFairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness\nand (vi) Explainability. In a step-by-step approach, these guidelines are\nfurther translated into a framework of concrete recommendations for specifying,\ndeveloping, evaluating, and deploying technically, clinically and ethically\ntrustworthy AI solutions into clinical practice."
  },
  {
    "id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
    "adv_source_id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
    "source_id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow",
    "prompt": null,
    "generation": "High-quality training data play a key role in image segmentation tasks.\nUsually, pixel-level annotations are expensive, laborious and time-consuming\nfor the large volume of training data. To reduce labelling cost and improve\nsegmentation quality, interactive segmentation methods have been proposed,\nwhich provide the result with just a few clicks. However, their performance\ndoes not meet the requirements of practical segmentation tasks in terms of\nspeed and accuracy. In this work, we propose EdgeFlow, a novel architecture\nthat fully utilizes interactive information of user clicks with edge-guided\nflow. Our method achieves state-of-the-art performance without any\npost-processing or iterative optimization scheme. Comprehensive experiments on\nbenchmarks also demonstrate the superiority of our method. In addition, with\nthe proposed method, we develop an efficient interactive segmentation tool for\npractical data annotation tasks. The source code and tool is avaliable at\nhttps://github.com/PaddlePaddle/PaddleSeg."
  },
  {
    "id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
    "adv_source_id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
    "source_id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation",
    "prompt": null,
    "generation": "The success of deep learning methods in medical image segmentation tasks\nheavily depends on a large amount of labeled data to supervise the training. On\nthe other hand, the annotation of biomedical images requires domain knowledge\nand can be laborious. Recently, contrastive learning has demonstrated great\npotential in learning latent representation of images even without any label.\nExisting works have explored its application to biomedical image segmentation\nwhere only a small portion of data is labeled, through a pre-training phase\nbased on self-supervised contrastive learning without using any labels followed\nby a supervised fine-tuning phase on the labeled portion of data only. In this\npaper, we establish that by including the limited label in formation in the\npre-training phase, it is possible to boost the performance of contrastive\nlearning. We propose a supervised local contrastive loss that leverages limited\npixel-wise annotation to force pixels with the same label to gather around in\nthe embedding space. Such loss needs pixel-wise computation which can be\nexpensive for large images, and we further propose two strategies, downsampling\nand block division, to address the issue. We evaluate our methods on two public\nbiomedical image datasets of different modalities. With different amounts of\nlabeled data, our methods consistently outperform the state-of-the-art\ncontrast-based methods and other semi-supervised learning techniques."
  },
  {
    "id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
    "adv_source_id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
    "source_id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation",
    "prompt": null,
    "generation": "Simultaneous segmentation of multiple organs from different medical imaging\nmodalities is a crucial task as it can be utilized for computer-aided\ndiagnosis, computer-assisted surgery, and therapy planning. Thanks to the\nrecent advances in deep learning, several deep neural networks for medical\nimage segmentation have been introduced successfully for this purpose. In this\npaper, we focus on learning a deep multi-organ segmentation network that labels\nvoxels. In particular, we examine the critical choice of a loss function in\norder to handle the notorious imbalance problem that plagues both the input and\noutput of a learning model. The input imbalance refers to the class-imbalance\nin the input training samples (i.e., small foreground objects embedded in an\nabundance of background voxels, as well as organs of varying sizes). The output\nimbalance refers to the imbalance between the false positives and false\nnegatives of the inference model. In order to tackle both types of imbalance\nduring training and inference, we introduce a new curriculum learning based\nloss function. Specifically, we leverage Dice similarity coefficient to deter\nmodel parameters from being held at bad local minima and at the same time\ngradually learn better model parameters by penalizing for false\npositives/negatives using a cross entropy term. We evaluated the proposed loss\nfunction on three datasets: whole body positron emission tomography (PET) scans\nwith 5 target organs, magnetic resonance imaging (MRI) prostate scans, and\nultrasound echocardigraphy images with a single target organ i.e., left\nventricular. We show that a simple network architecture with the proposed\nintegrative loss function can outperform state-of-the-art methods and results\nof the competing methods can be improved when our proposed loss is used."
  },
  {
    "id": "72c41b8d-0069-4886-b734-a4000ffca286",
    "adv_source_id": "72c41b8d-0069-4886-b734-a4000ffca286",
    "source_id": "72c41b8d-0069-4886-b734-a4000ffca286",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels",
    "prompt": null,
    "generation": "Detection faults in seismic data is a crucial step for seismic structural\ninterpretation, reservoir characterization and well placement. Some recent\nworks regard it as an image segmentation task. The task of image segmentation\nrequires huge labels, especially 3D seismic data, which has a complex structure\nand lots of noise. Therefore, its annotation requires expert experience and a\nhuge workload. In this study, we present lambda-BCE and lambda-smooth L1loss to\neffectively train 3D-CNN by some slices from 3D seismic data, so that the model\ncan learn the segmentation of 3D seismic data from a few 2D slices. In order to\nfully extract information from limited data and suppress seismic noise, we\npropose an attention module that can be used for active supervision training\nand embedded in the network. The attention heatmap label is generated by the\noriginal label, and letting it supervise the attention module using the\nlambda-smooth L1loss. The experiment demonstrates the effectiveness of our loss\nfunction, the method can extract 3D seismic features from a few 2D slice\nlabels. And it also shows the advanced performance of the attention module,\nwhich can significantly suppress the noise in the seismic data while increasing\nthe model's sensitivity to the foreground. Finally, on the public test set, we\nonly use the 2D slice labels training that accounts for 3.3% of the 3D volume\nlabel, and achieve similar performance to the 3D volume label training."
  },
  {
    "id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
    "adv_source_id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
    "source_id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Segmenter: Transformer for Semantic Segmentation",
    "prompt": null,
    "generation": "Image segmentation is often ambiguous at the level of individual image\npatches and requires contextual information to reach label consensus. In this\npaper we introduce Segmenter, a transformer model for semantic segmentation. In\ncontrast to convolution-based methods, our approach allows to model global\ncontext already at the first layer and throughout the network. We build on the\nrecent Vision Transformer (ViT) and extend it to semantic segmentation. To do\nso, we rely on the output embeddings corresponding to image patches and obtain\nclass labels from these embeddings with a point-wise linear decoder or a mask\ntransformer decoder. We leverage models pre-trained for image classification\nand show that we can fine-tune them on moderate sized datasets available for\nsemantic segmentation. The linear decoder allows to obtain excellent results\nalready, but the performance can be further improved by a mask transformer\ngenerating class masks. We conduct an extensive ablation study to show the\nimpact of the different parameters, in particular the performance is better for\nlarge models and small patch sizes. Segmenter attains excellent results for\nsemantic segmentation. It outperforms the state of the art on both ADE20K and\nPascal Context datasets and is competitive on Cityscapes."
  },
  {
    "id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
    "adv_source_id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
    "source_id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Mining Contextual Information Beyond Image for Semantic Segmentation",
    "prompt": null,
    "generation": "This paper studies the context aggregation problem in semantic image\nsegmentation. The existing researches focus on improving the pixel\nrepresentations by aggregating the contextual information within individual\nimages. Though impressive, these methods neglect the significance of the\nrepresentations of the pixels of the corresponding class beyond the input\nimage. To address this, this paper proposes to mine the contextual information\nbeyond individual images to further augment the pixel representations. We first\nset up a feature memory module, which is updated dynamically during training,\nto store the dataset-level representations of various categories. Then, we\nlearn class probability distribution of each pixel representation under the\nsupervision of the ground-truth segmentation. At last, the representation of\neach pixel is augmented by aggregating the dataset-level representations based\non the corresponding class probability distribution. Furthermore, by utilizing\nthe stored dataset-level representations, we also propose a representation\nconsistent learning strategy to make the classification head better address\nintra-class compactness and inter-class dispersion. The proposed method could\nbe effortlessly incorporated into existing segmentation frameworks (e.g., FCN,\nPSPNet, OCRNet and DeepLabV3) and brings consistent performance improvements.\nMining contextual information beyond image allows us to report state-of-the-art\nperformance on various benchmarks: ADE20K, LIP, Cityscapes and COCO-Stuff."
  },
  {
    "id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
    "adv_source_id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
    "source_id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
    "prompt": null,
    "generation": "We investigate Referring Image Segmentation (RIS), which outputs a\nsegmentation map corresponding to the given natural language description. To\nsolve RIS efficiently, we need to understand each word's relationship with\nother words, each region in the image to other regions, and cross-modal\nalignment between linguistic and visual domains. We argue that one of the\nlimiting factors in the recent methods is that they do not handle these\ninteractions simultaneously. To this end, we propose a novel architecture\ncalled JRNet, which uses a Joint Reasoning Module(JRM) to concurrently capture\nthe inter-modal and intra-modal interactions. The output of JRM is passed\nthrough a novel Cross-Modal Multi-Level Fusion (CMMLF) module which further\nrefines the segmentation masks by exchanging contextual information across\nvisual hierarchy through linguistic features acting as a bridge. We present\nthorough ablation studies and validate our approach's performance on four\nbenchmark datasets, showing considerable performance gains over the existing\nstate-of-the-art methods."
  },
  {
    "id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
    "adv_source_id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
    "source_id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Few-Shot Segmentation with Global and Local Contrastive Learning",
    "prompt": null,
    "generation": "In this work, we address the challenging task of few-shot segmentation.\nPrevious few-shot segmentation methods mainly employ the information of support\nimages as guidance for query image segmentation. Although some works propose to\nbuild cross-reference between support and query images, their extraction of\nquery information still depends on the support images. We here propose to\nextract the information from the query itself independently to benefit the\nfew-shot segmentation task. To this end, we first propose a prior extractor to\nlearn the query information from the unlabeled images with our proposed\nglobal-local contrastive learning. Then, we extract a set of predetermined\npriors via this prior extractor. With the obtained priors, we generate the\nprior region maps for query images, which locate the objects, as guidance to\nperform cross interaction with support features. In such a way, the extraction\nof query information is detached from the support branch, overcoming the\nlimitation by support, and could obtain more informative query clues to achieve\nbetter interaction. Without bells and whistles, the proposed approach achieves\nnew state-of-the-art performance for the few-shot segmentation task on\nPASCAL-5$^{i}$ and COCO datasets."
  },
  {
    "id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
    "adv_source_id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
    "source_id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
    "model": "human",
    "decoding": null,
    "repetition_penalty": null,
    "attack": "none",
    "domain": "abstracts",
    "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images",
    "prompt": null,
    "generation": "Semantic segmentation of medical images is an essential first step in\ncomputer-aided diagnosis systems for many applications. However, given many\ndisparate imaging modalities and inherent variations in the patient data, it is\ndifficult to consistently achieve high accuracy using modern deep neural\nnetworks (DNNs). This has led researchers to propose interactive image\nsegmentation techniques where a medical expert can interactively correct the\noutput of a DNN to the desired accuracy. However, these techniques often need\nseparate training data with the associated human interactions, and do not\ngeneralize to various diseases, and types of medical images. In this paper, we\nsuggest a novel conditional inference technique for DNNs which takes the\nintervention by a medical expert as test time constraints and performs\ninference conditioned upon these constraints. Our technique is generic can be\nused for medical images from any modality. Unlike other methods, our approach\ncan correct multiple structures simultaneously and add structures missed at\ninitial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and\n12.4 times in user annotation time than full human annotation for the nucleus,\nmultiple cells, liver and tumor, organ, and brain segmentation respectively. We\nreport a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other\ninteractive segmentation techniques. Our method can be useful to clinicians for\ndiagnosis and post-surgical follow-up with minimal intervention from the\nmedical expert. The source-code and the detailed results are available here\n[1]."
  }
]