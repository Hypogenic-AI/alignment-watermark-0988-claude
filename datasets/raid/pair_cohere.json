{
  "base": [
    {
      "id": "344faff9-ada7-4526-88da-6938d7e6f60d",
      "adv_source_id": "344faff9-ada7-4526-88da-6938d7e6f60d",
      "source_id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\" from arxiv.org:",
      "generation": " Abstract: Artificial intelligence (AI) has the potential to revolutionise medical imaging, but also carries the risk of perpetuating and even increasing existing health inequalities. An international collaboration of experts from diverse fields has come together to consider how to ensure that AI in medical imaging is developed and deployed to maximise the benefits and minimise the harms. Following a two-stage online consultation, we used a modified Delphi method to achieve consensus on key issues, and we provide here a summary of our discussions and recommendations for future AI development and deployment. Combining the principles of trustworthy AI with consideration of fairness and equity, we propose a framework to inform the development and deployment of AI in medical imaging. To ensure the trustworthy and fair use of AI in medical imaging, developers, clinicians and researchers should consider the following: promoting health equality and equity; upholding data privacy and security; maintaining technical performance; supporting clinical performance and decision making; and being responsive to patients and the public. In this report, we outline a consensus view on the key issues that need to be addressed, and make recommendations for the future development and deployment of AI in medical imaging."
    },
    {
      "id": "fab0b2f0-6221-4146-b308-11ae060c156d",
      "adv_source_id": "fab0b2f0-6221-4146-b308-11ae060c156d",
      "source_id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\" from arxiv.org:",
      "generation": " Abstract: Medical imaging today is deeply intertwined with artificial intelligence (AI). However, the frequent, invisible, and often automatic deployment of AI in medical imaging, coupled with its rapid evolution, raises concerns about the technology's impact on radiologists and patients alike. In this paper, we present results from a multidisciplinary expert workshop, where we collectively formulated overarching principles and specific actionable recommendations to guide the development and deployment of trustworthy AI for medical imaging, including fairness, transparency, robustness, and reproducibility. We discuss the need for (1) transparency in the entire AI workflow, (2) robust and reproducible AI systems with comprehensive documentation, (3) well-defined clinical problem formulation and clear specification of the desired AI solution, and (4) fairness in the AI development process with reduced bias. We propose a comprehensive, stakeholder-engaged roadmap with actionable recommendations for each phase of the AI lifecycle to maximize the benefit and minimize the risk of AI in medical imaging. By tangible actions today, we can foster trustworthy AI for better health outcomes in the future."
    },
    {
      "id": "9e61cf10-790e-4e46-b058-187eeee3ac0e",
      "adv_source_id": "9e61cf10-790e-4e46-b058-187eeee3ac0e",
      "source_id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow",
      "prompt": "The following is the full text of the abstract for a research paper titled \"EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow\" from arxiv.org:",
      "generation": " Recent progress in interactive segmentation has primarily focused on extracting object boundaries accurately rather than the entire object region. While boundary-based methods can achieve better performance, they often require a time-consuming initial segmentation step to generate the boundary predictions. Moreover, interactions for these methods become complicated as they involve modifying the boundary rather than the entire object region. In this paper, we present EdgeFlow, a framework for achieving practical interactive segmentation with edge-guided flow. It leverages a boundary-based approach but removes the initial segmentation step, making the interaction process as intuitive as the recent region-based methods. Specifically, given a preliminary segmentation result, EdgeFlow directly optimizes the segmentation by introducing a novel edge-guided flow, which can effectively propagate the user's modifications to the entire object region while maintaining the boundary details. We build a theoretical guarantee for the flow's convergence, and the flow's implementation is efficiently accelerated via graph cuts. Our method can handle both binary and pixel-wise segmentation with or without explicit edge information. We demonstrate the effectiveness of EdgeFlow on both benchmark datasets and a new dataset that contains complex object instances and illustrations. When compared to state-of-the-art methods on the latter, EdgeFlow improves the efficiency of interaction by at least one order of magnitude while maintaining comparable performance."
    },
    {
      "id": "ae176895-5402-419c-8d87-d15f54ddfb2d",
      "adv_source_id": "ae176895-5402-419c-8d87-d15f54ddfb2d",
      "source_id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow",
      "prompt": "The following is the full text of the abstract for a research paper titled \"EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow\" from arxiv.org:",
      "generation": " Recent progress in interactive segmentation has largely focused on integrating edge information into the segmentation framework. However, existing methods treat the edge information as a separate module, resulting in discontinuities between the edge and segmentation outputs. We propose EdgeFlow, a framework that leverages a conditional edge-guided flow field to achieve seamless integration of edges and segments. EdgeFlow estimates the flow field between the initial and final segment maps, with the flow constrained by the underlying image edges. This edge-constrained flow provides an effective bridge between the initial and final outputs, yielding a smooth transition that preserves important structures. We build a principled optimizer for the flow field by formulating the problem as a conditional Monge-Kantorovich mass transport problem with edge constraints. This allows us to design an efficient algorithm that can handle the large flow magnitude caused by initial-final segment discrepancies. We demonstrate the effectiveness of EdgeFlow on both standalone segmentation and interactive segmentation benchmarks, showing significant improvements over the state of the art in terms of precision and efficiency."
    },
    {
      "id": "34698cbd-1ee3-4d4d-92ad-7624792f11ab",
      "adv_source_id": "34698cbd-1ee3-4d4d-92ad-7624792f11ab",
      "source_id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in medical image segmentation has largely been driven by supervised learning, but the availability of large-scale manually annotated datasets remains a bottleneck. In this paper, we tackle the problem of medical image segmentation with a semi-supervised learning framework that leverages only a small amount of labeled data. Our approach contrasts an encoder's representations of image patches with different labels, and uses the contrastive loss to regularize the encoder's parameters. We demonstrate the efficacy of our approach on three public segmentation benchmarks (MRBrain, ISLES, and LiTS), where we achieve competitive performance compared to fully-supervised methods, while using only a fraction of the labeled data. In particular, on the MRBrain dataset with only 200 labeled images (out of ~1000), our approach achieves a Dice score of 0.8314, outperforming the fully supervised state-of-the-art by ~3%."
    },
    {
      "id": "a5e537ea-3f67-4141-9f8d-198987e65e34",
      "adv_source_id": "a5e537ea-3f67-4141-9f8d-198987e65e34",
      "source_id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in medical imaging have led to a wealth of data for training deep learning models for various applications. However, annotating medical images is expensive and time-consuming, which restricts the availability of fully labeled datasets. In this paper, we tackle the problem of medical image segmentation with a semi-supervised learning (SSL) approach, where only a small fraction of the images in a dataset are labeled. Our method leverages the contrastive learning framework, which enables the model to learn effectively from a variety of unlabeled data. We propose a novel SSL approach that combines contrastive learning with self-supervised and supervised learning. Our method utilizes a self-supervised objective that enforces the agreement of embeddings learned from unlabeled and labeled data, and a supervised objective that optimizes the segmentation performance on labeled data. We demonstrate the effectiveness of our method on three public datasets, showing that our approach achieves state-of-the-art performance in medical image segmentation with significantly fewer labels."
    },
    {
      "id": "a92c4074-40b1-4bd7-8df5-321b980422a9",
      "adv_source_id": "a92c4074-40b1-4bd7-8df5-321b980422a9",
      "source_id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation\" from arxiv.org:",
      "generation": " Recent progress in segmentation of multiple organs in medical images has resulted in algorithms that are often trained and evaluated on heavily imbalanced datasets, where certain classes may have far fewer training examples than others. This can lead to overfitting on the majority class and poor performance on the minority classes. We find that the commonly used class weighting scheme, in which the loss is weighted by the inverse frequency of the class in the training set, may not be sufficient to handle this imbalance. In this paper, we investigate the use of two other classes of techniques, including (1) sampling strategies that result in more balanced datasets and (2) loss functions that are designed to handle imbalance. We find that both approaches can significantly improve performance on minority classes, but that they work in different ways and are most effective when used together. To facilitate future research on multi-organ segmentation, we make our datasets, implementations, and results publicly available."
    },
    {
      "id": "c2bf6a20-d26f-4784-89ba-40fd02da1605",
      "adv_source_id": "c2bf6a20-d26f-4784-89ba-40fd02da1605",
      "source_id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation\" from arxiv.org:",
      "generation": " Recent progresses in semantic segmentation mainly focus on image datasets with a fine balance between the positive and negative samples. However, in many real-world applications, the dataset is often skewed, containing significantly more positive samples than negative ones. This imbalance brings problems for the training of deep neural networks, since they tend to memorize the positive samples and fail to generalize well to the negative samples. In this paper, we argue that the degradation of performance for segmenting organs is mainly due to the existence of other organs around, and a segmentator firstly suffers a \u201ccombo loss\u201d due to the imbalanced data, and then an \u201corgan loss\u201d due to the complexity of positive samples. To address these two types of loss, we propose a Novel Training Strategy via Combo Deformation and Augmentation (NTSCADA), which contains two components: a combo deformation and an augmentation strategy. Specifically, we propose to use a deformation field to map the positive samples to locations with fewer samples, and an augmentation strategy to resize the positive samples for better generalization. We evaluate our method on the IASGD and DDRD datasets for multi-organ segmentation, and compared to the state-of-the-art methods, our approach achieves a much better performance in terms of both metrics, intersection of union (IoU) and pixel accuracy (PA)."
    },
    {
      "id": "745f28d5-5ae6-4f28-a941-aa1288eabd29",
      "adv_source_id": "745f28d5-5ae6-4f28-a941-aa1288eabd29",
      "source_id": "72c41b8d-0069-4886-b734-a4000ffca286",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels\" from arxiv.org:",
      "generation": " We present a novel approach for training a 3D seismic fault segmentation model using only a few 2D slice labels. The model is based on attention mechanisms, which allow it to focus on the most relevant features in the input data. We show that our approach can achieve high performance on 3D seismic fault segmentation tasks, even when trained with only a small number of 2D slice labels. Our approach is fast and efficient, and can be used to segment faults in large 3D seismic volumes. We demonstrate the effectiveness of our approach on a public dataset, and show that it outperforms the state-of-the-art in 3D seismic fault segmentation. Code is available at https://github.com/yzslab/SeisFaultSeg."
    },
    {
      "id": "845b214e-c2f2-40cb-89f4-2c0f4e72953a",
      "adv_source_id": "845b214e-c2f2-40cb-89f4-2c0f4e72953a",
      "source_id": "72c41b8d-0069-4886-b734-a4000ffca286",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels\" from arxiv.org:",
      "generation": " We present a novel approach for training a 3D seismic fault segmentation model using only a few 2D slice labels. Our method uses attention mechanisms to learn a 3D model from 2D annotations, requiring fewer labels than traditional 3D convolutional neural network (3D-CNN) models. We introduce a 2D-to-3D attention model that uses 2D slice labels to compute attention weights for each slice in the input volume. These attention weights are then used to generate 3D feature maps, which are fed into a 3D segmentation module. Our approach is able to learn a 3D model that performs well on 3D seismic fault segmentation tasks, outperforming traditional 3D-CNN models trained with the same number of 2D slice labels. We demonstrate the effectiveness of our approach on the SEG-D format geologic fault dataset, where we achieve 76.8% mean accuracy on the validation set, outperforming the state-of-the-art 3D-CNN models. Our method also shows good generalization performance on the SEG-Y format dataset, where we achieve 84.4% mean accuracy on the validation set."
    },
    {
      "id": "33f8c750-472f-4e63-8d30-630f86b195e7",
      "adv_source_id": "33f8c750-472f-4e63-8d30-630f86b195e7",
      "source_id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmenter: Transformer for Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Segmenter: Transformer for Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent works tend to complicate segmentators for better performance, making them inaccessible to low-power platforms like IoT devices, and impractical for real-time applications. In this work, we aim to simplify semantic segmentation, while maintaining high performance. We propose Segmenter, a transformer-based segmentation model that is extremely lightweight, fast, and parameter-efficient. It achieves this with a simple transformer architecture, and an innovative relative positional encoding strategy, which allows the model to easily run on devices with memory or computational constraints. We conduct extensive experiments on ACDC and Cityscapes datasets, and show that our model achieves 79.3% and 91.3% mIOU on ACDC-Char and Cityscapes-Char, while being up to 22 times faster, and up to 4 times lighter compared to the state-of-the-art."
    },
    {
      "id": "3c7e9307-debc-42ff-a80e-1f40d9f41016",
      "adv_source_id": "3c7e9307-debc-42ff-a80e-1f40d9f41016",
      "source_id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmenter: Transformer for Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Segmenter: Transformer for Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent works have shown that transformers are a powerful tool for image classification tasks, achieving excellent results. However, in semantic segmentation, transformers have only been applied to the decoding/classification step, while majority of the recent works still use convolutional layers for the encoder and feature extraction steps. In this work, we argue that transformers can be naturally applied to perform these steps as well, and can replace fully connected CRFs for pixel-level labeling. To demonstrate this, we construct Segmenter, a state-of-the-art semantic segmentation architecture that uses transformers for both the encoder and decoding steps. Our encoder-decoder model, termed Segmenter Dual Transformer (SDT), achieves promising results on ADE20K and Cityscapes, and shows advantages over the recent convolutional-based models. We further introduce a simple yet effective locality-aware attention module to improve performance. Finally, we conduct extensive experiments to study the effects of different design choices in our SDT model."
    },
    {
      "id": "0a1c3391-e440-47de-87cc-a4c35f8f5c2d",
      "adv_source_id": "0a1c3391-e440-47de-87cc-a4c35f8f5c2d",
      "source_id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mining Contextual Information Beyond Image for Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mining Contextual Information Beyond Image for Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent progress in semantic segmentation mainly focuses on designing models that compile images and their annotations into compact representations, in order to exploit the strong correlations among pixels. These models however ignore the fact that the annotations themselves contain valuable information, which could be used to improve the segmentation. In this paper, we follow a different approach, and attempt to exploit the contextual information in the image captions. We design a simple yet effective approach to mine the contextual information, and integrate it into the recently proposed Mask R-CNN. Specifically, we propose to use a graph neural network to build a context graph for each word in the caption, and then we compose these context graphs to obtain a global context graph for the whole caption. Finally, the global context graph is exploited in Mask R-CNN to refine the segmentation results. We perform comprehensive experiments on the Cityscapes and COCO datasets, and the results demonstrate the effectiveness of the proposed approach."
    },
    {
      "id": "2364d6be-fa1e-4226-be77-b7444805aa39",
      "adv_source_id": "2364d6be-fa1e-4226-be77-b7444805aa39",
      "source_id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mining Contextual Information Beyond Image for Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mining Contextual Information Beyond Image for Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning enables training models to solve a broad range of tasks on images, alongside the tremendous success of semantic segmentation, which aims to assign a semantic label to each pixel in an image. However, the state-of-the-art performance is heavily relying on a large amount of labeled images with fine-grained and exhaustive annotations, which are difficult to obtain. To overcome this bottleneck, in this paper, we aim to mine contextual information beyond the image for semantic segmentation. We find that the power of language provides an effective means to describe and convey the subtle contexts, thus advancing the performance of semantic segmentation. Specifically, we propose a novel Adapter Module that bridges the image and language domains to harness the semantic contexts in an adaptive way. It forms a module within the state-of-the-art architectures for semantic segmentation, yielding substantial gains over the strong baselines on four challenging benchmarks, including Cityscapes, ACDC, and Mapillary Vistas."
    },
    {
      "id": "fc235692-7fbe-47a7-9249-bf2d0b3e1857",
      "adv_source_id": "fc235692-7fbe-47a7-9249-bf2d0b3e1857",
      "source_id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Comprehensive Multi-Modal Interactions for Referring Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has advanced referring expression comprehension, i.e., the understanding of sparse referring expressions that refer to objects or regions in images or videos, from image regions identification to pixel-wise segmentation. However, existing works mainly focus on 2D referring expression comprehension disregarding the inherent 3D structure of the real world. In this paper, we present a comprehensive multi-modal framework to tackle referring image segmentation, which takes advantages of both referring expressions and geometric constraints, to achieve referring expression comprehension with 3D awareness. Specifically, we first devise a new geometric constraint that leverages the 3D scene structure to obtain initial segmentations. Then, we formulate referring expression comprehension as a maximum a posteriori estimation problem that integrates the referring expression predictions and the geometric constraint. Furthermore, we explore a progressive refinement approach to iteratively refine the segmentation results. We validate the effectiveness of our method on the ReferItSeg dataset, which is a large-scale dataset for referring image segmentation, and achieve state-of-the-art performance."
    },
    {
      "id": "81877ab0-b8eb-412a-834f-a765c7729fc5",
      "adv_source_id": "81877ab0-b8eb-412a-834f-a765c7729fc5",
      "source_id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Comprehensive Multi-Modal Interactions for Referring Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has advanced referring expression comprehension considerably, yet effective comprehension systems still require extensive training data and rely on heuristic post-processing steps that juggle different prediction scores. In this work, we propose a comprehensive multi-modal interaction architecture that combines referring expression comprehension with semantic segmentation. Our method interacts complementary information from referring expressions and the underlying visual features to guide the segmentation process. We eliminate the need for heuristic post-processing steps and control the trade-off between the referring expression and visual information intuitively. Furthermore, our method reduces the dependence on abundant training data because the visual and linguistic information are mutually beneficial. We validate our approach on the 2014 ReferIt datasets where our method achieves 72.2% mean F1-score on the test set, a +4.2% improvement compared to the next best method."
    },
    {
      "id": "5da91c91-30b6-434d-8cba-d42d564bd3e6",
      "adv_source_id": "5da91c91-30b6-434d-8cba-d42d564bd3e6",
      "source_id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-Shot Segmentation with Global and Local Contrastive Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Few-Shot Segmentation with Global and Local Contrastive Learning\" from arxiv.org:",
      "generation": " Few-shot segmentation with global and local contrastive learning. This paper explores the connection between few-shot segmentation and contrastive learning. Given a fixed network fine-tuned on a large dataset, we show that training the network to classify segments in a small number of classes (e.g., 5) is highly ineffective, as the network's capacity to memorize incorrect labels is surprisingly high. To overcome this, we propose a two-step training approach. First, we train a global contrastive learning (GCL) model by minimizing the mutual information between segment predictions and labels on the training set. This is followed by a localization step that iteratively refines the segmentation map within the support regions of each class. We show that this approach effectively combines the global and local characteristics of few-shot segmentation, improving accuracy by large margins over the state-of-the-art on multiple benchmarks."
    },
    {
      "id": "8f81c785-d446-46bc-841d-7dd5ef1f4662",
      "adv_source_id": "8f81c785-d446-46bc-841d-7dd5ef1f4662",
      "source_id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-Shot Segmentation with Global and Local Contrastive Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Few-Shot Segmentation with Global and Local Contrastive Learning\" from arxiv.org:",
      "generation": " Few-shot segmentation with global and local contrastive learning. This paper explores the boundary of few-shot segmentation with a global and local contrastive learning (GLCL) approach. We propose a novel few-shot segmentation framework that integrates both global and local contrastive learning strategies to leverage the advantages of both approaches. In particular, we propose to use a global contrastive loss that optimizes the consistency of the feature representations of the support and query sets, and a local contrastive loss that optimizes the similarity of the features within each class. Furthermore, we introduce a novel meta-testing process that dynamically selects the appropriate number of shots based on the available data. Extensive experimental evaluations on popular few-shot segmentation benchmarks, including PASCAL VOC 2012, Cityscapes, and LIP, demonstrate the effectiveness of our approach compared to the state-of-the-art. The code is made available at https://github.com/HM-Smart/FewShotSegmentation."
    },
    {
      "id": "1c547d40-f89b-409f-be87-a4e14a393870",
      "adv_source_id": "1c547d40-f89b-409f-be87-a4e14a393870",
      "source_id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images\" from arxiv.org:",
      "generation": " Interactive segmentation is an important technique for correcting mispredictions in medical images, but existing methods can be time-consuming and require significant user interaction. This paper presents a efficient and generic interactive segmentation framework that reduces the amount of user interaction required while maintaining high accuracy. The framework utilizes a fully-automatic segmentation algorithm as a starting point, and then allows the user to quickly and easily correct any mispredictions using a novel interactive segmentation tool. The tool enables the user to quickly select the correct segmentation boundary, and the framework then automatically refines the boundary based on the user's input. The framework is evaluated on a variety of medical image datasets, and results show that it significantly reduces the amount of user interaction required while maintaining high accuracy. Furthermore, the framework is designed to be generic, and can be easily adapted to segment different anatomical structures in various medical image modalities."
    },
    {
      "id": "b9156caf-48e9-468f-9de1-799870accb63",
      "adv_source_id": "b9156caf-48e9-468f-9de1-799870accb63",
      "source_id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images\" from arxiv.org:",
      "generation": " In this paper, we present an efficient and generic interactive segmentation framework for corrective segmentation of medical images. Our method accurately extracts the ground truth region of interest (ROI) from a mispredicted lesion mask in a semi-automatic fashion, without any shape restriction, and typically in less than one minute. Our framework leverages a deep fully convolutional neural network (FCN) as the main segmentation engine in an interactive segmentation setup, where the user can correct the mispredictions of the network by simply drawing bounding boxes around the true ROI on a limited number of key frames. We employ a graphical interface that enables the user to provide minimal user interactions in the form of bounding boxes on the mispredicted frames. Our in-house implementation of the FCN network is designed to output a segmentation mask together with confidence scores, which are then used to weigh the influence of the user's bounding boxes on the final segmentation result. We evaluate our method on volumetric medical images of lung CT scans and positron emission tomography (PET) brain scans, and demonstrate that our approach is capable of producing very accurate segmentations in a fraction of the time compared to the state-of-the-art interactive segmentation methods."
    },
    {
      "id": "e978325c-780a-4fb2-84d7-e4bc7c3fe7d6",
      "adv_source_id": "e978325c-780a-4fb2-84d7-e4bc7c3fe7d6",
      "source_id": "ddcb207c-a790-4e16-a053-4aced58d7c15",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Distribution-Free, Risk-Controlling Prediction Sets",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Distribution-Free, Risk-Controlling Prediction Sets\" from arxiv.org:",
      "generation": " We provide a general framework for constructing prediction sets, i.e., sets that predict the values of an uncertain quantity, with probability 1. In addition to simplicity and accuracy, our framework controls risk by leveraging the order structure of the variable of interest. We provide both a distribution-free and a frequentist approach, with the latter enabling formal statistical inference for the prediction set. We illustrate the approaches through two real data examples: the first concerns the number of tropical cyclones in the Atlantic ocean, and the second the number of Ph.D. graduates in US mathematics departments. Compared to other prediction sets, ours achieve much better accuracy and smaller expected prediction errors."
    },
    {
      "id": "a04e44fe-da3f-4285-9af1-dc3858ad0629",
      "adv_source_id": "a04e44fe-da3f-4285-9af1-dc3858ad0629",
      "source_id": "ddcb207c-a790-4e16-a053-4aced58d7c15",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Distribution-Free, Risk-Controlling Prediction Sets",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Distribution-Free, Risk-Controlling Prediction Sets\" from arxiv.org:",
      "generation": " We propose Distribution-Free Prediction Sets (DPS), a new method for predictive distribution construction that is both distribution-free and produces prediction sets that control the worst-case risk. DPS leverages the representation of uncertainty provided by the set-based prediction schemes, which give a set of predictions from which a random variable can be determined. We enforce control of worst-case risk on the tails by limiting the prediction set sizes above and below a threshold, which results in a \u201cskinny\u201d tube-shaped set in the tail regions. This tail constraint enables use of DPS in risk management and regulation. We show that DPS reproduce the frequentist coverage achieved by a classical predictive distribution, thus providing a frequentist motivation and explanation for the tail constraints. We establish finite-sample guarantees for the frequentist coverage of DPS in terms of the constrained sample risks and show that DPS achieve the desired coverage level asymptotically as the sample size increases. An attractive feature of DPS is that the amount of tail risk can be quantified and controlled at prediction time, without needing to specify a tail risk measure or estimate the tails of the unknown distribution ahead of time. We illustrate the use of DPS in two real data examples and show that the proposed method can be used to construct prediction sets that achieve the desired coverage levels."
    },
    {
      "id": "0653b63a-a24f-4f8d-8f62-a58b33f20a48",
      "adv_source_id": "0653b63a-a24f-4f8d-8f62-a58b33f20a48",
      "source_id": "b00bf7dc-4de9-4ab4-9962-a16e0b5f4628",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation\" from arxiv.org:",
      "generation": " Deep learning has recently become popular for sonar image segmentation, but the majority of research has focused on supervised learning approaches that require large amounts of manually labeled training data. This can be a time-consuming and costly process, particularly for underwater sonar images where manually labeling images is slow and difficult. In this paper, we present an iterative, unsupervised synthetic aperture sonar image segmentation framework that is both deep and learns from sparse labels. Our approach uses a U-Net architecture and an iterative self-supervised learning strategy that progressively refines the segmentation results without any user interaction. We demonstrate the efficacy of our approach on a synthetic aperture sonar dataset and show that our method can achieve high segmentation performance with only a small number of labeled training examples. Our code and data are available at https://github.com/ARL-RED-Team/idus."
    },
    {
      "id": "521cfac7-d54e-416c-b19a-cf723a5ee939",
      "adv_source_id": "521cfac7-d54e-416c-b19a-cf723a5ee939",
      "source_id": "b00bf7dc-4de9-4ab4-9962-a16e0b5f4628",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation\" from arxiv.org:",
      "generation": " Recent results in deep learning have enabled the development of an effective end-to-end trainable sonar image segmentation network. However, segmentation networks are data-intensive and require large amounts of manually labeled images, which can be difficult to obtain in the sonar imaging context. We present an unsupervised iterative segmentation network that is trained entirely on synthetic images, without the need for any manually labeled data. Our method uses a generative adversarial network to produce a series of segmentations, which are then used to refine the segmentation network. This iterative process is repeated until convergence, resulting in a highly accurate segmentation network. We demonstrate the effectiveness of our method on a large dataset of sonar images, and show that it achieves state-of-the-art performance compared to other unsupervised and semi-supervised segmentation methods."
    },
    {
      "id": "eefaa971-d2ce-4082-8d54-1c4da6860b2e",
      "adv_source_id": "eefaa971-d2ce-4082-8d54-1c4da6860b2e",
      "source_id": "04d3809c-0abe-4bee-b1d2-9787af95362f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP\" from arxiv.org:",
      "generation": " Recent progress in image synthesis through the use of GANs provides a new opportunity for image understanding. In particular, we show that the image representation learned by GANs can be used to produce state-of-the-art performance on semantic image segmentation, a task that is fundamental to many applications in computer vision. We achieve this by proposing a new framework that leverages the rich structural information encoded in the generative models of GANs. Our approach, which we term \"Segmentation in Style\", combines an adversarial reconstruction loss with a discriminator that performs segmentation. This framework is applicable to any GAN architecture and we demonstrate its effectiveness on top of StyleGAN2 and CLIP, a recently proposed GAN-based image representation. Our method achieves state-of-the-art performance on the ACDC and Cityscapes segmentation benchmarks without requiring any labeled data for training, but rather only a few images for initialization."
    },
    {
      "id": "5e5105d4-f802-40cf-bc92-40bdcd4675fd",
      "adv_source_id": "5e5105d4-f802-40cf-bc92-40bdcd4675fd",
      "source_id": "04d3809c-0abe-4bee-b1d2-9787af95362f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP\" from arxiv.org:",
      "generation": " Recent progress in image synthesis through the use of GANs provides an opportunity for new methods of image understanding. In particular, the style latent vector of StyleGAN refers to an average of many latent codes that correspond to image regions with similar styles, i.e. similar appearance. We propose a new method for semantic image segmentation based on predicting style codes of images using CLIP, and a segmentation mask is obtained by an additional segmentation module. Our method is trained without requiring any segmentation labels, and it is able to automatically discover a hierarchy of image regions that corresponds to object structures in the image. We show that when trained on COCO images, our method discovers the major objects in the image as well as the background, and it outperforms existing state-of-the-art methods on the NYTech segmentation benchmark."
    },
    {
      "id": "f9946da5-3929-4b38-a907-ed476b0158d3",
      "adv_source_id": "f9946da5-3929-4b38-a907-ed476b0158d3",
      "source_id": "06bffeb2-bea0-4b0b-b60d-767ba9b660a7",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Double Similarity Distillation for Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Double Similarity Distillation for Semantic Image Segmentation\" from arxiv.org:",
      "generation": " Semantic image segmentation, which aims to assign semantic labels to each pixel in an image, is a fundamental problem in computer vision. Recent progress in this field is mainly driven by deep learning models, which can leverage large-scale labeled images to learn robust representations. However, training such models requires a significant amount of labeled data, which is difficult and expensive to obtain.\n\nTo overcome this challenge, we propose a novel approach for semantic image segmentation based on double similarity distillation (DSD). Our method exploits the intrinsic hierarchical structure of segmentation models to effectively reduce the amount of labeled data required for training. Specifically, we introduce a teacher-student paradigm, where a teacher model is trained on a small amount of labeled data and then used to distill knowledge to a student model trained on a larger dataset. By leveraging the similarity between the teacher and student models, we can significantly improve the performance of the student model while keeping the amount of labeled data required for training low.\n\nWe evaluate our method on the challenging PASCAL VOC 2012 segmentation benchmark, and achieve state-of-the-art performance with only a fraction of the labeled data required by previous approaches. Our results demonstrate the effectiveness of our approach in reducing the amount of labeled data needed for training robust segmentation models.\n\nOur code and models are available at: https://github.com/Cartucho/DSD"
    },
    {
      "id": "ed103e84-86ee-4ea9-b80b-8233648d6304",
      "adv_source_id": "ed103e84-86ee-4ea9-b80b-8233648d6304",
      "source_id": "06bffeb2-bea0-4b0b-b60d-767ba9b660a7",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Double Similarity Distillation for Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Double Similarity Distillation for Semantic Image Segmentation\" from arxiv.org:",
      "generation": " Semantic image segmentation, which aims to predict a label for each pixel in an image, is a fundamental problem in computer vision. Recent progress in this area has been driven by deep learning models, such as convolutional neural networks (CNNs). However, these models are still not perfect, and they can be fooled by images that are similar but have different labels. This paper introduces a new model called Double Similarity Distillation (DSD), which is designed to improve the performance of semantic image segmentation models by making them more robust to these types of images.\n\nDSD is based on the observation that images that are similar in terms of their low-level features, such as color and texture, but have different labels, often have similar high-level features as well. For example, two images of mountains may have similar color and texture characteristics, but one may be labeled as a forest and the other as a mountain. To address this issue, DSD learns a joint distribution of feature maps and labels from a training dataset, which helps to align the distributions of similar images and improve the segmentation performance.\n\nThe experimental results show that DSD significantly improves the performance of semantic image segmentation models on a variety of datasets, including the Cityscapes, CamVid, and Mapillary datasets. The improvements are particularly significant for images that are similar but have different labels, which demonstrates the effectiveness of the proposed approach.\n\nOverall, this paper introduces a new model for semantic image segmentation that is designed to improve the performance of deep learning models by making them more robust to images that are similar but have different labels. The experimental results show that the proposed approach is effective and can significantly improve the segmentation performance."
    },
    {
      "id": "63eab06f-f3ee-4192-bd7d-88446df492d9",
      "adv_source_id": "63eab06f-f3ee-4192-bd7d-88446df492d9",
      "source_id": "5eb88a59-eb5a-49ea-8304-f67efe338921",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in supervised learning for medical image segmentation have benefited from large datasets and powerful models, but training data with ground truth segmentations remain scarce for many applications. In this work, we tackle the issue of limited training data for medical image segmentation by developing a spatial guided self-supervised clustering (SGSC) network. Our approach leverages the spatial consistency of medical images to learn meaningful representations for segmentation without relying on ground truth segmentations. We introduce a novel self-supervised loss function that encourages spatial coherence among cluster assignments, and combine it with an attention module to focus on informative regions. We demonstrate the effectiveness of our method on three different medical image segmentation tasks: fetal brain, abdominal aortic aneurysm, and prostate, showing that our approach produces segmentations comparable to state-of-the-art supervised methods while using only unlabeled data."
    },
    {
      "id": "74bafe91-e69f-43b9-91ef-0a07298fb3a7",
      "adv_source_id": "74bafe91-e69f-43b9-91ef-0a07298fb3a7",
      "source_id": "5eb88a59-eb5a-49ea-8304-f67efe338921",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in contextual encoders has propelled clinical image segmentation, yet solely relies on pixel similarities or image-level annotations, which limits the model's ability to capture high-level contextual information. In this paper, we present Spatial-Guided Self-supervised Clustering Network (SGS-Net) for unsupervised medical image segmentation. SGS-Net introduces a spatial-guided constraint to the recently proposed Encoder-Assembly-Decoder framework, forming a clustering-based objective for unpaired and unstructured images. Without any labels, SGS-Net learns to discover underlying structures from raw image features, leading to robust segmentation. We evaluate SGS-Net on four publicly available kidney and pancreas segmentation benchmarks, where it achieves state-of-the-art performance in most cases. The code is made available at https://github.com/ externals/SGS-Net."
    },
    {
      "id": "0e8631ec-0aac-4d2b-832e-90c38d2e08d7",
      "adv_source_id": "0e8631ec-0aac-4d2b-832e-90c38d2e08d7",
      "source_id": "1389aa64-25fb-4e56-9358-ef34143bfea9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Robust General Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Towards Robust General Medical Image Segmentation\" from arxiv.org:",
      "generation": " General image segmentation is a fundamental task in medical imaging, and deep learning methods have demonstrated promising performance. However, these methods often require large amounts of manually annotated training data, which is difficult to obtain for medical images. In this paper, we present a new framework for general medical image segmentation that is based on a fully convolutional neural network (FCN) and a novel loss function that is tailored to medical images. Our framework is designed to be robust to variations in image quality, resolution, and noise, which are common in medical images. We demonstrate the performance of our method on a publicly available dataset of abdominal computed tomography (CT) scans, and show that our method achieves state-of-the-art performance. We also show that our method is general and can be applied to other medical image segmentation tasks, such as lung CT segmentation."
    },
    {
      "id": "b76bd950-6ca4-497f-8fa0-6a377da54b08",
      "adv_source_id": "b76bd950-6ca4-497f-8fa0-6a377da54b08",
      "source_id": "1389aa64-25fb-4e56-9358-ef34143bfea9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Robust General Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Towards Robust General Medical Image Segmentation\" from arxiv.org:",
      "generation": " General medical image segmentation is a fundamental but challenging task in radiology. Despite the recent advances in deep learning, the trained models are often overly sensitive to variations in image acquisition protocols, finding-characteristics, and noise, which limits their deployment in clinical practice. In this paper, we propose a simple yet effective approach to address the model's lack of robustness. Specifically, we train a segmentation model using a mixture of medical images with varying protocols. Then, during the testing process, we implicitly estimate the unknown protocol of the given test image via a canonical correlation analysis (CCA), and adapt the pre-trained model accordingly. The proposed approach is applicable to any pre-trained segmentation model and does not require any modification to the model architecture or the training process. We demonstrate the effectiveness of our approach on three publicly available MRI datasets, where our method achieves robustness against protocol variations with a negligible loss in segmentation accuracy."
    },
    {
      "id": "076fa8d7-156a-4ab9-af3a-ed85c81961a5",
      "adv_source_id": "076fa8d7-156a-4ab9-af3a-ed85c81961a5",
      "source_id": "d0064195-c22e-4550-a265-6b372deea3e0",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Left Atrium Segmentation with Mutual Consistency Training",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised Left Atrium Segmentation with Mutual Consistency Training\" from arxiv.org:",
      "generation": " Recent studies have demonstrated the potential of deep learning for medical image analysis. However, these methods usually require a large amount of manually annotated training data, which is difficult to obtain for medical images. In this paper, we present a semi-supervised learning method for left atrium segmentation in cardiac MR images. Our method leverages a large amount of unlabeled data and a small amount of manually annotated data to train a deep neural network. To deal with the class imbalance and the lack of annotated data, we use a mutual consistency training strategy, which ensures that the predictions from the model with semi-supervised labels are similar to the fully supervised labels. Our method achieves a Dice similarity coefficient of 0.8033 and a Hausdorff distance of 7.35 mm on a public dataset, which outperforms the state-of-the-art fully-supervised method by a large margin. Furthermore, our method only requires approximately 1/50 of the annotated training data."
    },
    {
      "id": "58260832-4e9e-4aae-b1d7-7d952c1d6850",
      "adv_source_id": "58260832-4e9e-4aae-b1d7-7d952c1d6850",
      "source_id": "d0064195-c22e-4550-a265-6b372deea3e0",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Left Atrium Segmentation with Mutual Consistency Training",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised Left Atrium Segmentation with Mutual Consistency Training\" from arxiv.org:",
      "generation": " Current manual segmentation of the left atrium (LA) is tedious and expensive. In this work, we aim to develop an automatic LA segmentation method to assist the diagnosis of heart diseases. Unlike previous works that treat it as a binary segmentation problem, we address the problem as a pixel-wise segmentation of the LA region from the whole heart, which is more challenging. To tackle the difficulty of the pixel-wise segmentation, we propose to use a fully convolutional network (FCN) with an atrous spatial pyramid pooling (ASPP) structure and incorporate a novel mutual consistency training (MCT) strategy to effectively train the network. In addition, we propose to use a fully supervised network to provide accurate pixel-wise predictions as ground truth for semi-supervised training. Specifically, we train our network in a semi-supervised manner, where we use the LA mask from a fully supervised network as the ground truth for training our network with the MCT strategy. We evaluate our method on 100 cardiac magnetic resonance images and compare it to state-of-the-art binary and pixel-wise segmentation methods. The results show that our method achieves a Dice similarity coefficient of 0.8821, which outperforms all compared methods."
    },
    {
      "id": "b77cb571-13dc-4245-8dbb-adb7ddeef116",
      "adv_source_id": "b77cb571-13dc-4245-8dbb-adb7ddeef116",
      "source_id": "417afaa2-2d21-4df1-953b-768647de9980",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search\" from arxiv.org:",
      "generation": " Recent advances in deep learning have propelled the development of visual tracking algorithms, enabling the adoption of off-the-shelf deep convolutional neural networks (CNNs) as features for correlation filtering-based methods. However, deep correlation filters still face challenges in handling target appearance variations, occlusions, and out-of-distribution (OOD) samples. To address these issues, we propose to learn a cell-level convolutional architecture for visual tracking (CHASE), which enables the model to adapt to different kinds of target appearance variations and occlusions. We first introduce a cell-level correlation filtering framework that operates on cell-generated features for efficient feature representation and localization. Then, we propose a differentiable neural architecture search (NAS) framework to optimize the cell-level architecture for accurate and robust tracking. Specifically, we design an innovative search space that explicitly considers the correlations between cells, such that the learned cell-level architecture can effectively capture the target structure and context. Moreover, to alleviate the OOD issues, we propose a robust tracking framework that can explicitly handle OOD samples via a robust loss function and an OOD detection module. Experimental results on multiple challenging datasets, including OOD scenarios, demonstrate the effectiveness of the proposed approach against state-of-the-art (SOTA) methods."
    },
    {
      "id": "cf4974ee-11d2-422f-84ee-59290acc9926",
      "adv_source_id": "cf4974ee-11d2-422f-84ee-59290acc9926",
      "source_id": "417afaa2-2d21-4df1-953b-768647de9980",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search\" from arxiv.org:",
      "generation": " Recent progress in visual object tracking is largely driven by deep learning. However, existing learning-based methods tend to overfit the specific datasets during training and fail to generalize to unseen datasets, especially for complex and challenging tracking scenarios. To address this issue, we propose to use neural architecture search (NAS) to find a robust tracking framework. Our approach first learns a configurable network skeleton (i.e., a cell-based architecture) for visual tracking, and then optimizes the network via a gradient-based NAS algorithm that searches over different cells (i.e., operations) to adapt to target appearance variations. In this way, our approach treats the tracking framework as a whole and learns a robust cell-level architecture via different operations that can be selected from a search space. Experiments on VOT-2018 and VOT-2019 datasets validate the efficacy of our approach compared to the state-of-the-art. Code is available at https://github.com/AI4FOLLOWER/CHASE."
    },
    {
      "id": "788fd6ec-bd96-46a4-9f28-c15d8f2bb00b",
      "adv_source_id": "788fd6ec-bd96-46a4-9f28-c15d8f2bb00b",
      "source_id": "ce898c28-428f-446f-975e-a1265942f2da",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Inter Extreme Points Geodesics for Weakly Supervised Segmentation\" from arxiv.org:",
      "generation": " We introduce a new weakly-supervised segmentation framework based on the geodesic active contour model (GAC). Unlike existing methods that use pairwise constraints learned from shape contexts, our approach utilizes a new energy term in the GAC model that favors contours that intersect a given set of extreme points. We show that the resulting optimization problem can be formulated as a geodesic flow on a Riemannian manifold and propose an efficient algorithm to solve it. We demonstrate the efficacy of our approach on the challenging task of annotating pixels in densely-packed objects (e.g., cells in tissues) in a weakly-supervised manner, where only the locations of a small subset of pixels inside the objects are given. In this setting, our approach achieves state-of-the-art performance on two benchmark datasets."
    },
    {
      "id": "c20a6982-dd5b-481f-b5ee-b6786113361b",
      "adv_source_id": "c20a6982-dd5b-481f-b5ee-b6786113361b",
      "source_id": "ce898c28-428f-446f-975e-a1265942f2da",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Inter Extreme Points Geodesics for Weakly Supervised Segmentation\" from arxiv.org:",
      "generation": " We introduce a new weakly-supervised segmentation framework that leverages geodesic inter-extreme points (GEIX) for object instance segmentation. Our approach extends the traditional geodesic ellipsoids to a new class of shapes that capture the relationships among multiple objects. This is achieved by optimizing over the manifold of the geometric shapes, referred to as geodesic inter-extreme variations, where the optimization objective is driven by the geometric properties and the label correlations among the objects. We show that the resulting optimization is efficiently solved as a linear programming (LP) problem on certain parametrization of the geodesic inter-extreme variations. In addition, to effectively solve the LP problem, we propose an algorithm that iteratively updates the shapes by performing a series of convex optimizations. We evaluate our approach on the cityscapes dataset, where our method outperforms the state-of-the-art approaches that rely on geodesics."
    },
    {
      "id": "602323cb-24f7-4bd2-bd0b-c46678737ecd",
      "adv_source_id": "602323cb-24f7-4bd2-bd0b-c46678737ecd",
      "source_id": "380cd71d-3300-422c-9cde-8a63e71f2797",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "K-Net: Towards Unified Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"K-Net: Towards Unified Image Segmentation\" from arxiv.org:",
      "generation": " K-Net: Towards Unified Image Segmentation\n\nWe present a simple, fast, and effective approach to image segmentation that achieves state-of-the-art performance on both integer and pixel-wise labeling. K-Net learns a linear classifier in a weakly-supervised manner by leveraging the power of conditional random fields and the structure of deep neural networks. This allows us to use a single network to segment images without needing to choose between the computational efficiency of integer-based methods and the accuracy of pixel-wise labeling methods. Our approach is fast enough for semi-interactive segmentation, while achieving strong performance on multiple benchmarks. Code is available at https://github.com/autonomousvision/K-Net."
    },
    {
      "id": "9b8502f6-0de2-4a7d-ad43-d71a0ba74df2",
      "adv_source_id": "9b8502f6-0de2-4a7d-ad43-d71a0ba74df2",
      "source_id": "380cd71d-3300-422c-9cde-8a63e71f2797",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "K-Net: Towards Unified Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"K-Net: Towards Unified Image Segmentation\" from arxiv.org:",
      "generation": " K-Net: Towards Unified Image Segmentation\n\nK-Net is a novel deep learning framework for effective image segmentation. It utilizes a unique combination of a U-Net architecture with a k-means clustering algorithm to achieve precise and coherent segmentation results. In this work, we introduce a novel architecture called the K-Net, which combines the strengths of both U-Net and k-means to deliver state-of-the-art performance in image segmentation. Our approach demonstrates superiority in terms of segmentation quality and running time, as evidenced by extensive experimental evaluations on a variety of benchmark datasets. The proposed method is versatile and can be applied to different imaging modalities and segmentation tasks."
    },
    {
      "id": "0590fd90-4dcd-4422-a30b-c5d87bcc282d",
      "adv_source_id": "0590fd90-4dcd-4422-a30b-c5d87bcc282d",
      "source_id": "c093400c-2bd2-4e0d-a732-f99d499d58a9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has propelled convolutional neural networks (CNNs) to advance the state-of-the-art in image segmentation. However, CNNs suffer from the lack of effective uncertainty measures, which hinders their applications in reliable and safety-critical environments. In this paper, we exploit Bayesian neural networks (BNNs) to address this issue, which have shown promising results in producing meaningful uncertainty measures. Nevertheless, BNNs usually demand a large number of parameters and thus are less compact compared to CNNs. To overcome this limitation, we propose ComBiNet, a novel BNN architecture that is specifically designed for image segmentation. It inherits the merits of both CNNs and BNNs, namely high performance and uncertainty estimation, while maintaining a compact architecture. We validate ComBiNet on multiple public datasets, including MIT-CBCL, NYU-VOC, and Cityscapes. Results show that our approach achieves promising performance with significant uncertainty estimation, and outperforms existing state-of-the-art methods."
    },
    {
      "id": "544df4e3-9f3a-4dd0-a8d2-40643145a543",
      "adv_source_id": "544df4e3-9f3a-4dd0-a8d2-40643145a543",
      "source_id": "c093400c-2bd2-4e0d-a732-f99d499d58a9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has propelled convolutional neural networks (CNNs) to advance the state-of-the-art in image segmentation. However, CNNs suffer from the lack of effective uncertainty measures, which hinders their applications in reliable image segmentation. In this paper, we exploit Bayesian neural networks (BNNs) and propose ComBiNet, a compact convolutional Bayesian neural network for image segmentation. We begin by analyzing the requirements for effective uncertainty measurement, and then present a factor graph formulation of a CNN, based on which we propose a series of novel architectural components, including scalable convolutions, gated convolutions, and gated pooling, to enable effective uncertainty measurement. We show that existing techniques in BNNs, such as uncertainty dropout and variational inference, can be naturally embedded in our proposed model. Furthermore, we present a series of optimizations to reduce the memory footprint of our model, which allows our model to achieve a better trade-off between performance and memory usage. We validate our method on four public segmentation benchmarks, and achieve state-of-the-art performance in terms of both segmentation quality and uncertainty measurement."
    },
    {
      "id": "a45261da-b438-4f75-8cbd-505ba0c13e4d",
      "adv_source_id": "a45261da-b438-4f75-8cbd-505ba0c13e4d",
      "source_id": "05f40b6d-67cf-4a6e-ad2f-cfe003b7917e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images\" from arxiv.org:",
      "generation": " We address the problem of semantic segmentation of very fine resolution urban scene images, where the ground truth pixel-level labels are available. We propose a bilateral awareness network (BAN) that utilizes a transformer encoder decoder to model the bilateral relations of objects in the scene, while also taking into account the global context. Our approach is based on the observation that fine-grained object segmentation of very high resolution (VH) urban scene images exhibits a strong bilateral constraint, i.e., the object appearance is influenced by its bilateral context, which we refer to as the object and its spatially-adjacent background. The BAN effectively captures the bilateral context by introducing a novel bilateral transformer encoder decoder architecture that learns to predict object masks and their corresponding background masks. It models the bilateral relations of objects with a self-supervised objective that does not require any additional annotation. We evaluate our approach on the Cityscapes dataset and show that it achieves state-of-the-art performance. Our code and models are available at: this https URL."
    },
    {
      "id": "bc328b67-1a50-4e2b-8c0b-2f6a5d3505b6",
      "adv_source_id": "bc328b67-1a50-4e2b-8c0b-2f6a5d3505b6",
      "source_id": "05f40b6d-67cf-4a6e-ad2f-cfe003b7917e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images\" from arxiv.org:",
      "generation": " We introduce a bilateral awareness network (BAN) that achieves state-of-the-art performance on semantic segmentation of very fine resolution urban scene images. The BAN utilizes a transformer encoder architecture to capture multi-scale context in parallel with an efficient convolutional decoder to refine the context into segmentation masks with precise localization. A novel bilateral attention module is introduced in the transformer encoder to selectively attend to distant spatial support and suppress noisy background details. Furthermore, the decoder utilizes a grouped convolution module that explicitly groups pixels along object boundaries to effectively exploit object shape information for accurate segmentation. To evaluate the BAN, we conduct extensive experiments on Cityscapes andecc UFC-SDK-03 datasets. Without using any extra training data or post-processing techniques, the BAN achieves 71.3% mIOU on the Cityscapes val and 73.3% mIOU on the UFC-SDK-03 val datasets, which significantly surpasses the current state-of-the-art."
    },
    {
      "id": "fd28b39a-ea8c-4891-8708-45609b32005c",
      "adv_source_id": "fd28b39a-ea8c-4891-8708-45609b32005c",
      "source_id": "c404dbd3-7cec-41f5-8b70-968e9212ed7c",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Large-scale image segmentation based on distributed clustering algorithms",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Large-scale image segmentation based on distributed clustering algorithms\" from arxiv.org:",
      "generation": " Recent developments in distributed computing and cluster computing technologies have made it possible to efficiently process huge datasets that cannot be handled by a single computer. In this paper, we present a framework for efficient processing of large-scale image segmentation problems based on distributed clustering algorithms. Our approach is based on dividing the input image into smaller regions, and processing each region in parallel on different computers. We propose a few distributed clustering algorithms that are suitable for image segmentation, and evaluate their performance on a large-scale image segmentation benchmark. Our experimental results show that the proposed approach can achieve significant speedup compared to a single-computer implementation, while maintaining a high quality of segmentation results."
    },
    {
      "id": "541a72c9-dbe4-413d-ade2-e09a71b51fb1",
      "adv_source_id": "541a72c9-dbe4-413d-ade2-e09a71b51fb1",
      "source_id": "c404dbd3-7cec-41f5-8b70-968e9212ed7c",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Large-scale image segmentation based on distributed clustering algorithms",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Large-scale image segmentation based on distributed clustering algorithms\" from arxiv.org:",
      "generation": " Abstract\u2014Image segmentation is cumbersome but crucial for many image processing tasks. Methods based on clustering, however, seem to lack the accuracy when the image size is large. Also, when the image size is huge, it becomes impossible to process it on a single machine. In this paper, we present a distributed image segmentation algorithm that can achieve high accuracy even on large-scale images. Our method to segment the image is based on the widely used K-means algorithm, where we have implemented a distributed version of the K-means algorithm using MapReduce. We evaluate our implementation on a large dataset using three benchmark measures, and the results show that our implementation achieves high accuracy even with a large number of clusters and is nearly linearly scalable with the number of machines used.\n\nIntroduction\u2014Image segmentation is the process of partitioning an image into multiple segments with respect to some criteria. It has been an active research area for the last four decades, and there are many algorithms that have been proposed for segmentation [1, 2]. Image segmentation serves as a fundamental tool in many applications of image processing, such as object extraction, content-based image retrieval, and image understanding. When the image size is large, segmentation through clustering becomes a cumbersome task. Clustering algorithms tend to produce poor quality results, as they require a large number of iterations to converge accurately. This is due to the fact that the computational complexity of these algorithms is proportional to the square of the number of pixels in the image. Though some algorithms that can handle large-scale images have been proposed, they are not scalable and are bound to work on a single machine. In this paper, we address the problem of segmenting large-scale images in a distributed environment. We implement a distributed version of the K-means algorithm using MapReduce [3], which is a programming model and execution framework for processing large data sets in a distributed environment. Our implementation is evaluated on a large dataset, and the results show that our implementation achieves high accuracy even with a large number of clusters and is nearly linearly scalable with the number of machines used.\n\nMethod\u2014Our method to segment images is based on the K-means algorithm. The K-means algorithm is a widely used clustering algorithm that can handle large amounts of data and can be implemented in a distributed way. Our implementation of the K-means algorithm is based on the MapReduce framework. We divide the image into a set of tiles, and each tile is processed by a different machine. The tiles are then merged together to form the final segmented image"
    },
    {
      "id": "5193dc54-19b9-4880-9053-9ca8e4b05388",
      "adv_source_id": "5193dc54-19b9-4880-9053-9ca8e4b05388",
      "source_id": "bc43fe33-d378-4bc3-b666-038cbacbdbc8",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Trilateral Attention Network for Real-time Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Trilateral Attention Network for Real-time Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a real-time algorithm for the segmentation of medical images based on a trilateral attention network. Our approach combines the use of a convolutional neural network (CNN) with a graph-based segmentation algorithm to achieve accurate and efficient segmentation. The CNN is used to extract features from the medical image, while the graph-based segmentation algorithm uses these features to segment the image. The trilateral attention network is designed to improve the segmentation accuracy by considering the relationships between the three primary anatomical structures in the image: the tumor, the healthy tissue, and the background. We evaluate our algorithm on a dataset of head computed tomography (CT) scans and show that it achieves a high degree of accuracy and is able to run in real-time.\n\nOur algorithm has three main components: the CNN, the graph-based segmentation algorithm, and the trilateral attention network. The CNN is used to extract features from the medical image, including the tumor, healthy tissue, and background. The graph-based segmentation algorithm uses these features to segment the image, using a shortest-path algorithm to find the boundaries between the tumor and healthy tissue. The trilateral attention network is designed to improve the segmentation accuracy by considering the relationships between the three primary anatomical structures in the image. It does this by combining the features from the CNN with the graph-based segmentation algorithm, using attention weights to determine the importance of each feature in the segmentation process.\n\nWe evaluate our algorithm on a dataset of head CT scans and show that it achieves a high degree of accuracy, with a mean absolute error of 0.69 mm and a mean relative error of 2.47%. Our algorithm is also able to run in real-time, with a processing time of 32.5 seconds. This is faster than other state-of-the-art medical image segmentation algorithms, which typically take several minutes to process a single image."
    },
    {
      "id": "7ee74ce6-4558-4e44-ae2c-30f02411b997",
      "adv_source_id": "7ee74ce6-4558-4e44-ae2c-30f02411b997",
      "source_id": "bc43fe33-d378-4bc3-b666-038cbacbdbc8",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Trilateral Attention Network for Real-time Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Trilateral Attention Network for Real-time Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a real-time algorithm for the segmentation of medical images based on a fully-connected trilateral attention network (TAN). This approach allows for accurate segmentation results even when the images are of low quality or highly ambiguous. Our method efficiently utilizes all available information within the image and does not require any manual initialization or post-processing. We evaluate our method on a public dataset and show that it achieves state-of-the-art performance in terms of speed and accuracy. We also demonstrate the effectiveness of our approach on challenging images that contain noise, low contrast, and other artifacts. \n\nOur TAN architecture consists of encoders and decoders that process the image in a hierarchical manner. The bottom-up encoder gradually extracts high-level features from the pixel data, while the top-down decoder generates the final prediction by iteratively refining the attention maps. The attention maps are obtained by aggregating the features from the encoder through a set of attention modules. Each module performs a weighted sum of the features based on the level of importance assigned to each feature by the decoder. \n\nWe introduce a novel trilateral attention module that utilizes three separate attention maps to capture different aspects of the image. These maps are combined using a weighted sum to create a single attention map that is then used by the decoder. This approach allows us to capture complex relationships between the features and improve the segmentation accuracy. \n\nOur method is fully convolutional and can be trained end-to-end on a single GPU. We utilize a PyTorch implementation and introduce a number of techniques to speed up the training process and improve memory efficiency. For example, we modify the sampling process of the decoder's attention maps to reduce the memory requirement. \n\nWe evaluate our method on the public ISBI 2017 Challenge Segmentation dataset. This dataset contains 300 abdominal CT scans with manual annotations for 20 organs and structures. We compare our method to other state-of-the-art approaches on this dataset and show that it achieves the best performance in terms of speed and accuracy. In particular, our method achieves a mean accuracy of 95.5% on the test set, which is higher than the second-best method by 2.1%. \n\nWe also demonstrate the effectiveness of our approach on challenging images that contain noise, low contrast, and other artifacts. Our method is able to produce high-quality segmentation results even in the presence of these issues. This is important for real-world applications, where the quality of the input images can vary significantly. \n\nIn summary, we"
    },
    {
      "id": "e696a94a-cb18-4917-94c6-5c0bd9658e90",
      "adv_source_id": "e696a94a-cb18-4917-94c6-5c0bd9658e90",
      "source_id": "eff4a017-daa1-4b5d-84ac-927333aaa900",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery\" from arxiv.org:",
      "generation": " We propose a new algorithm for real-time stereo matching, which can potentially be used in minimally invasive surgery. Our algorithm is based on the Bayesian dense inverse searching (BDIS) algorithm, which uses a probabilistic approach to find the best match between two sets of data. The BDIS algorithm is designed to be efficient and to handle large amounts of data, which makes it well-suited for real-time applications. We evaluate our algorithm on a set of stereo images from the MICCAI 2012 stereo matching challenge and compare its performance to that of other state-of-the-art algorithms. Our algorithm achieves competitive results while running at nearly real-time speeds. Furthermore, we modify the BDIS algorithm to incorporate a smoothness prior, in order to improve its performance on challenging regions of the images. This modification improves the accuracy of our algorithm, particularly in areas with low contrast and texture. We believe that our algorithm has the potential to be used in real-time stereo matching applications, such as minimally invasive surgery."
    },
    {
      "id": "64e34cf7-7ca1-4c95-be3c-25a7f536e05d",
      "adv_source_id": "64e34cf7-7ca1-4c95-be3c-25a7f536e05d",
      "source_id": "eff4a017-daa1-4b5d-84ac-927333aaa900",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery\" from arxiv.org:",
      "generation": " We propose a new algorithm for real-time stereo matching of surgical images using a Bayesian dense inverse search (BDIS) approach. Minimally invasive surgery (MIS) poses challenges for computer-assisted intervention (CAI) due to limited access, navigational difficulties, and the substantial anatomical variation between patients. We address these challenges by developing a generic texture-encoded dictionary learned from a large database of annotated surgical images (MISDOOP) representing a wide range of surgical scenarios and anatomical variations. Given a stereo pair of intra-operative images, we find the best match between the current view and the dictionary elements and use the coordinate transformation of the best-matching element to infer the 3D point cloud of the scene. We validate the approach on synthetic scenes and clinical data and show that our approach compares favorably to state-of-the-art algorithms on publicly available benchmarks while being fast enough for real-time applications."
    },
    {
      "id": "6c5fbdbc-602a-4bbf-9d43-ecb564470fc6",
      "adv_source_id": "6c5fbdbc-602a-4bbf-9d43-ecb564470fc6",
      "source_id": "92bcf142-f39d-48c7-a63e-0b770803fc81",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in connectomics have yielded high-quality images of neural tissue, but manual analysis still poses a substantial bottleneck. In this paper, we present RLCorrector, a reinforced proofreading system that assists a user in segmenting connectomics images. It is common for users to accidentally add or remove pixels from their segmentation, so we train a neural network to correct such errors. We train our network using a novel loss function that incentivizes it to make corrections resembling human corrections. We evaluate RLCorrector on both simulated and real world data, and show that it induces significant improvements in segmentation accuracy whilst requiring minimal user interaction."
    },
    {
      "id": "8fde8645-ec74-4a22-9be8-a1461b10cca0",
      "adv_source_id": "8fde8645-ec74-4a22-9be8-a1461b10cca0",
      "source_id": "92bcf142-f39d-48c7-a63e-0b770803fc81",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in connectomics have yielded high-resolution images of neural tissue, providing unprecedented opportunities for computational analysis of neuronal connectivity. However, segmenting cells in these images is a challenging problem, due to the size and complexity of the datasets, as well as the significant amount of noise and artifacts in the images.\n\nWe have developed a proofreading approach for segmentation of connectomics images, based on the concept of a supervisor neural network that provides weak and optionally randomized feedback to a worker neural network performing the segmentation. We introduce a new variant of the basic supervisor/worker algorithm that uses a Reinforcement Learning (RL) approach to reward or penalize the worker's segmentation output, and allows the supervisor to provide stronger feedback when it has higher confidence. We show that this approach, which we call RLCorrector, significantly improves segmentation accuracy in connectomics images, and is efficient enough to be applied to very large datasets. Our evaluation on a publicly available dataset shows that RLCorrector achieves a 3.7% absolute reduction in segmentation error relative to a state-of-the-art segmentation algorithm, and a 5.7% relative reduction over a multi-expert segmentation."
    },
    {
      "id": "7f72d539-5d7b-4f14-a2df-966a6a66a1ed",
      "adv_source_id": "7f72d539-5d7b-4f14-a2df-966a6a66a1ed",
      "source_id": "93029efb-6d75-45e7-bcfa-6ccd61690b3f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks\" from arxiv.org:",
      "generation": " We introduce Points2Polygons, an approach for training a geometric context-based segmentation model using only weak labels, i.e., point-level labels. Our model segments images based on the underlying geometric context of the weak labels. We introduce an encoder-decoder architecture that leverages the geometric consistency of object segments to refine initial predictions. We train our model using an adversarial loss that encourages the model to produce segmentations with geometric properties similar to the weak labels. Our method does not require any post-processing steps, such as graph cuts or conditional random fields. We demonstrate the effectiveness of our approach on the Cityscapes and KITTI datasets, where we achieve state-of-the-art performance on the weakly-supervised segmentation task.\n\nOur results show that our approach performs on par with fully-supervised segmentation methods in terms of accuracy, while being more efficient in terms of training time and the amount of labeled data required.\n\nOur main contributions can be summarised as follows:\n\nWe propose a new weakly-supervised segmentation approach that utilizes geometric context to segment images without any post-processing steps.\n\nWe introduce an encoder-decoder architecture that leverages the geometric consistency of object segments to refine initial predictions.\n\nWe train our model using an adversarial loss that encourages the model to produce segmentations with geometric properties similar to the weak labels.\n\nWe demonstrate the effectiveness of our approach on the Cityscapes and KITTI datasets, where we achieve state-of-the-art performance on the weakly-supervised segmentation task."
    },
    {
      "id": "8e9fa713-9f56-4e35-b318-1e258be11b82",
      "adv_source_id": "8e9fa713-9f56-4e35-b318-1e258be11b82",
      "source_id": "93029efb-6d75-45e7-bcfa-6ccd61690b3f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks\" from arxiv.org:",
      "generation": " We introduce Points2Polygons (P2P), a novel approach for instance-based segmentation of 3D point clouds that only requires weak annotation in the form of semantic labels. P2P utilizes a context-based approach, computing a sparse point-wise semantic affinity graph between input points and labeled points from a training dataset. Affinity graph construction is performed by an adversarial network that reasons about point context. We show that training this network is equivalently to learning a data-driven partitioning of the 3D space. Consequently, tested points are assigned to the nearest partition, which is in turn mapped to a semantic class. P2P doesnt require a separate post-processing step, as done by most existing weakly-supervised methods for point clouds. We demonstrate the efficacy of our approach on the task of segmenting indoor scenes, where only a coarse semantic occupancy grid is available as weak supervision. Our method achieves 68.4% mean accuracy on the Part segmentation benchmark, outperforming previous state-of-the-art methods that utilize substantially stronger weak supervision."
    },
    {
      "id": "e8053398-cf3d-4167-a6e1-b560c47000e8",
      "adv_source_id": "e8053398-cf3d-4167-a6e1-b560c47000e8",
      "source_id": "0818a486-cec9-4668-bef1-e22e2f92b912",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy\" from arxiv.org:",
      "generation": " We present a view of exact inference in graphs from the degree-4 sum-of-squares hierarchy (SoSH) that yields the currently fastest known algorithms for approximate counting, maximization, and minimization derived from a broad range of probability distributions. These algorithms have an approximation quality that is rarely worse than existing state-of-the-art algorithms for the corresponding problems, while having the advantage of yielding exact inference results in graphs under the SoSH relaxation. We present a general recipe for converting any problem in graphs that is solvable by belief propagation (BP) on a factor graph into a problem that can be solved by generalized belief propagation (BP^*) on a factor graph, and show that the resulting algorithm is always an approximation of the original problem with a quality that is rarely worse than previously known algorithms for the corresponding problem. This recipe enables us to obtain our fast algorithms for approximate counting, maximization, and minimization. We present experimental results that demonstrate the performance of our algorithms on a set of real-world benchmarks."
    },
    {
      "id": "11855bc2-e92c-4eb6-ab1c-89a6b6b6ecc1",
      "adv_source_id": "11855bc2-e92c-4eb6-ab1c-89a6b6b6ecc1",
      "source_id": "0818a486-cec9-4668-bef1-e22e2f92b912",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy\" from arxiv.org:",
      "generation": " We present a view of exact inference in graphs from the degree-4 sum-of-squares hierarchy (SoSH) that tightly integrates with algorithmic theory. This integration is achieved by viewing the solutions to the underlying linear programming problems as flowing through a dynamic programming grid indexed by the vertices and angles of the graph. This perspective allows for a simple proof of recent results on exclusion games, and more generally, for a simple proof that the deterministic annealing algorithm ofcryptographic interest for the degree-4 SoSH runs in polynomial time."
    },
    {
      "id": "f3442205-1161-4667-9680-6e3e3769e411",
      "adv_source_id": "f3442205-1161-4667-9680-6e3e3769e411",
      "source_id": "1e89cdc5-f7ba-4f0b-a2f3-22285fade7a5",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection\" from arxiv.org:",
      "generation": " In this paper, a method of high-resolution segmentation of tooth root fuzzy edges based on polynomial curve fitting with landmark detection is proposed. Specially, we establish a coordinate system of dental radiograph via four detected landmark points, and then fit the polynomial curve to the top and bottom of the tooth root. Next, we segment the tooth root fuzzy edges based on the fitting curves. The experimental results show that the proposed method can effectively extract the tooth root and its fuzzy edges, and has the advantages of high precision and good stability."
    },
    {
      "id": "dbb666d3-7405-4809-87d6-d4e2249e2ed0",
      "adv_source_id": "dbb666d3-7405-4809-87d6-d4e2249e2ed0",
      "source_id": "1e89cdc5-f7ba-4f0b-a2f3-22285fade7a5",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection\" from arxiv.org:",
      "generation": " In this paper, a method of segmentation of tooth root edges from dental digital images based on polynomial curve fitting is proposed. First, the edge of the tooth is detected by a Canny detector. Then, the elliptic approximation of the edge points is calculated based on the polynomial curve fitting algorithm given by least square method. Finally, the initial curve segments are got by landmark detection and the curve segments are fused to form the final tooth root edge. The experimental results show that the method presented in this paper can effectively extract the edge of tooth root, and has a certain ability of segmenting tooth roots in noisy images."
    },
    {
      "id": "4afe22cc-e465-4175-a307-376ef32bbfab",
      "adv_source_id": "4afe22cc-e465-4175-a307-376ef32bbfab",
      "source_id": "162114f5-d069-46bf-a0c6-b5d9cf465c61",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Active Contours Using Locally Controlled Distance Vector Flow",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Active Contours Using Locally Controlled Distance Vector Flow\" from arxiv.org:",
      "generation": " We present a new algorithm for deep active contours that combines the advantages of distance vector flow (DVF) and the recent progress in deep learning. Our approach uses a convolutional neural network (CNN) to learn a locally controlled distance function in a data-driven manner. This allows us to modify the behavior of the DVF according to the local characteristics of the image, resulting in a more flexible and accurate algorithm. We show that our approach can handle a variety of tasks, including image segmentation, object tracking, and pose estimation, and we demonstrate its performance on several challenging datasets. Our code and models are publicly available at https://github.com/CVMI-Lab/DAC-LCDVF."
    },
    {
      "id": "d209cbe6-eaf7-41de-9ecf-ef421ed3e3a5",
      "adv_source_id": "d209cbe6-eaf7-41de-9ecf-ef421ed3e3a5",
      "source_id": "162114f5-d069-46bf-a0c6-b5d9cf465c61",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Active Contours Using Locally Controlled Distance Vector Flow",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Active Contours Using Locally Controlled Distance Vector Flow\" from arxiv.org:",
      "generation": " We present a new algorithm for active contour modeling of objects in images based on a variational approach to minimize the energy of the active contour model. The proposed method, called Deep Active Contours using Locally Controlled Distance Vector Flow (DAC-LCDVF), combines ideas from adaptive curve evolution, level set methods, and deep learning. In particular, the spatial coherence of the contour evolution is enforced by a locally controlled distance vector field, computed with a deep neural network. The network is trained using a collection of image-based prototypes that cover the diversity of the foreground objects as well as the background. We demonstrate the efficacy of the proposed approach on a variety of image segmentation benchmarks, showing that it produces robust and accurate segmentations of objects in images, outperforming the state-of-the-art active contour models."
    },
    {
      "id": "7ae182ef-b929-4594-aec9-4ca0519ffbb0",
      "adv_source_id": "7ae182ef-b929-4594-aec9-4ca0519ffbb0",
      "source_id": "2b623feb-b86c-48f8-84df-83ac9d36359f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-Modal Progressive Comprehension for Referring Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Cross-Modal Progressive Comprehension for Referring Segmentation\" from arxiv.org:",
      "generation": " Most referring expressions in natural language are grounded in visual or spatial content, such as demonstratives (e.g., \u201chere\u201d, \u201cthere\u201d) or spatial prepositions (e.g., \u201cin\u201d, \u201con\u201d). Moreover, the successful deployment of artificial intelligence (AI) technologies often hinges on the interpretability of the model outputs, especially in safety-critical domains such as healthcare and autonomous driving. In this paper, we tackle the referring expression generation (REG) task with a progressive comprehension (PC) approach, where a referring expression is generated by gradually refining an initially vague description of the referent through interaction with a user. We propose a cross-modal PC framework that can not only reason about the visual and linguistic content together, but also explain how the final referring expression is derived. We evaluate our method on the ReferIt dataset, and show that our model can generate more accurate referring expressions compared to the state-of-the-art methods while providing interpretable referring segments."
    },
    {
      "id": "28ccaa96-75a0-46f0-81d6-de96ffd03aa9",
      "adv_source_id": "28ccaa96-75a0-46f0-81d6-de96ffd03aa9",
      "source_id": "2b623feb-b86c-48f8-84df-83ac9d36359f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-Modal Progressive Comprehension for Referring Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Cross-Modal Progressive Comprehension for Referring Segmentation\" from arxiv.org:",
      "generation": " Most referring expressions in natural language are not explicit but rather rely on the hearer\u2019s background knowledge, i.e., the semantic common ground between the speaker and the hearer. In this paper, we investigate the referring expression generation in a multi-modal dialogue setting, where the hearer can freely navigate the visual and the conversational modalities for referent recovery. We propose a modular multi-modal progressive comprehension model with weakly-supervised training. In particular, we identify referring segments as regions/objects that capture the referring expression in the conversational modality, and use a refer-expression dataset to train a referring segment classifier. We propose a cross-modal referring region retrieval module that highlights the regions in the image most relevant to the referring segment. Furthermore, we propose a progressive compressive module that reasons about the saliency of the referring segment in an iterative manner, and compresses the visual input to a more concise representation over time. We evaluate our model on a referring expression dataset, and show that our model can effectively leverage the strength of different modalities under the proposed weakly-supervised training framework."
    },
    {
      "id": "0c90f8a5-fd00-4d9a-8e61-afc4c21830a4",
      "adv_source_id": "0c90f8a5-fd00-4d9a-8e61-afc4c21830a4",
      "source_id": "ef61c066-14f3-413e-a266-afb9742903d9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Large-Scale Benchmark for Food Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Large-Scale Benchmark for Food Image Segmentation\" from arxiv.org:",
      "generation": " A Large-Scale Benchmark for Food Image Segmentation\n\nFood image segmentation is a fundamental task for enabling smarter tools in culinary applications. There have been recent advances in the field, but no standardized benchmark is available for the community to compare different approaches. We introduce a large-scale dataset of 5000 images, each of which is manually segmented into nine semantic classes by human experts. To the best of our knowledge, this is the largest dataset with pixel-level segmentation for food images. We further provide a benchmark of five state-of-the-art segmentation methods for this dataset. The evaluation results show the characteristics of different methods, and provide insights into how to improve segmentation performance.\n\nOverview of the Benchmark\n\nFood segmentation differs from general image segmentation in that the former often requires a fine-grained and per-class segmentation, while the latter usually focuses on global image segmentation into few classes (e.g., foreground vs. background). Some recent works on food image segmentation usually make an assumption that the food lies in the foreground, and segment the image into two classes: food vs. background. This assumption may not hold true in some situations. In this work, we define nine semantic classes, including seven food classes and two background classes. The seven food classes are \"Background\", \"Appetizer\", \"Beverage\", \"Dessert\", \"Main Course\", \"Sides\", and \"Utensils\". The two background classes are \"Tabletop\" and \"Plates\". An example is shown in Figure 1. We segment each image into these nine classes.\n\nWe search for existing food image segmentation datasets. There are some datasets with ground truth segmentation that are specific to certain cuisines (e.g., cakes, fast food), or specific applications (e.g., plate detection). However, these datasets are usually small, and thus are not suitable for training and evaluating segmentation models. There are also some datasets with bounding boxes of food regions, but no pixel-level segmentation. Since pixel-level segmentation is a prerequisite for many follow-up tasks (e.g., recognition, attribute prediction), we feel that a pixel-level segmentation dataset is more useful to the community.\n\nOur dataset is collected from Flickr with the help of a food image search engine. We manually segment each image into nine classes. To provide a fair comparison, we choose five state-of-the-art segmentation methods, including Graph Cuts, GrabCut, Conditional Random Fields (CRF), Expectation-Maximization (EM),"
    },
    {
      "id": "b7b4c4c3-fe28-4df9-89ae-9cdf11cbfed6",
      "adv_source_id": "b7b4c4c3-fe28-4df9-89ae-9cdf11cbfed6",
      "source_id": "ef61c066-14f3-413e-a266-afb9742903d9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Large-Scale Benchmark for Food Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Large-Scale Benchmark for Food Image Segmentation\" from arxiv.org:",
      "generation": " A Large-Scale Benchmark for Food Image Segmentation\n\nFood image segmentation is a fundamental task for enabling automated nutrition information extraction from meal images. However, existing datasets are small-scale, incomplete, and do not contain ground truth nutrition annotations, making it difficult to train and evaluate effective deep learning models. To address these limitations, we introduce a new large-scale benchmark, Food101, consisting of 101 categories, 27,491 training images, and a robust crowdsourcing pipeline for collecting ground truth segmentation and nutrition annotations. We propose a simple yet effective approach for segmenting food in images and releasing this benchmark as a resource for the community to enable future progress in this underserved area. Our segmentation model achieves a F1-score of 0.85 on average and outperforms the current state-of-the-art models.\n\nAvailability: The dataset, code, and trained models are available for download at http://ai.stanford.edu/Food101/."
    },
    {
      "id": "035e96eb-f80b-4e22-8f55-405d5d04a1db",
      "adv_source_id": "035e96eb-f80b-4e22-8f55-405d5d04a1db",
      "source_id": "27891e8c-5a91-4bbc-a1a8-1de8c44fe219",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian Logistic Shape Model Inference: application to cochlea image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bayesian Logistic Shape Model Inference: application to cochlea image segmentation\" from arxiv.org:",
      "generation": " We propose a Bayesian logistic shape model (LSM) for image segmentation based on a latent variable model that we refer to as the conical LSM. The conical LSM is a variation of the standard LSM in which each class is described by a conical region, thereby allowing for spatial variation in the classification likelihoods. We propose a practical inference algorithm for the conical LSM based on a reversible jump MCMC algorithm that allows us to borrow strength across images and models. We apply our method to the segmentation of cochlea images. Experimentally, we demonstrate that our method yields more accurate segmentations at a lower computational cost than state-of-the-art methods."
    },
    {
      "id": "540ad7eb-3859-4754-a60a-6280057055b0",
      "adv_source_id": "540ad7eb-3859-4754-a60a-6280057055b0",
      "source_id": "27891e8c-5a91-4bbc-a1a8-1de8c44fe219",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian Logistic Shape Model Inference: application to cochlea image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bayesian Logistic Shape Model Inference: application to cochlea image segmentation\" from arxiv.org:",
      "generation": " We propose a Bayesian approach for logistic shape model inference, characterized by a joint prior distribution on the shape parameters and the inclusion of a Baysian marginalization strategy in the inference algorithm. The proposed approach readily incorporates prior knowledge in the form of covariates, in this case anatomical knowledge of the cochlea, in the estimation process. This is a significant improvement on existing methods which mainly rely on a priori fixed covariates. We illustrate the approach in the context of segmenting the cochlea from MR images."
    },
    {
      "id": "3aa448f7-ec78-4935-aafc-6421fba0d2da",
      "adv_source_id": "3aa448f7-ec78-4935-aafc-6421fba0d2da",
      "source_id": "59635421-a84f-4293-be66-323c49fca652",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation\" from arxiv.org:",
      "generation": " We present a new active contour model for image segmentation based on a generalized asymmetric dual-front approach. In this model, the interface is described by two fronts, one for the exterior and one for the interior of the object, which move simultaneously toward each other or away from each other depending on the local gradient direction. The front that is inside the evolving contour is always faster than the outside front. This feature imposes a strong constraint on the contour evolution, resulting in a bias toward the formation of a global contour that is closed and does not leak out of the object boundary. An important advantage of the proposed method is that it is very fast and easy to implement because it only involves a simple calculation of the gradient of the image and a marching process in the gradient direction. Experimental results on several segmentation benchmarks show the efficiency of the proposed approach compared with state-of-the-art active contour models."
    },
    {
      "id": "0f06c4b7-8abe-4467-8d81-393186c7f5a4",
      "adv_source_id": "0f06c4b7-8abe-4467-8d81-393186c7f5a4",
      "source_id": "59635421-a84f-4293-be66-323c49fca652",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation\" from arxiv.org:",
      "generation": " We present a new active contour model for image segmentation, which incorporates a new energy term that variationally forces the contour toward a generalized asymmetric dual-front configuration. This model naturally handles occlusions and generates smooth contours, as demonstrated on several images. The computational scheme is also efficient, and based on a dynamic programming scheme for calculating the gradient of the energy function. The proposed method is compared to several state-of-the-art active contour models on a standard image segmentation benchmark, and experimental results show the advantages of our approach."
    },
    {
      "id": "9b6e09fd-2000-45bd-a763-dd3f522f36ea",
      "adv_source_id": "9b6e09fd-2000-45bd-a763-dd3f522f36ea",
      "source_id": "a957f241-6622-4b33-9d53-932c2c3ac2b6",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Rethinking BiSeNet For Real-time Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Rethinking BiSeNet For Real-time Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent works usually enhance the performance of semantic segmentation models mainly in two ways: (i) increase the complexity of the model, e.g., designing more powerful CNN architectures, or (ii) utilize more training data. However, these methods usually lead to a significantly increased computational cost which limits their applications on mobile devices or for real-time applications. In this work, we try to revisit the BiSeNet, a well-designed lightweight model for semantic segmentation, to make it better suit for real-time applications. Specifically, we propose a fast and lightweight semantic segmentation architecture by modifying the original BiSeNet in four major ways: (i) we remove the redundant skip connections in the original model to save computational cost, (ii) we introduce an adaptive instance normalization mechanism to adaptively adjust the channel numbers of the outputs of the Atrous Spatial Pyramid (ASP) blocks, (iii) we remove the ASP blocks in the original model and replace them with a single ASP block with a channel number determined by the adaptive instance normalization mechanism, and (iv) we remove the concatenation layers in the original model and directly stack the deep features and the shallow features as input of the final conv layers. The proposed method is thoroughly evaluated on four popular semantic segmentation benchmarks, i.e., NYUv2, NYUv2-360, Cityscapes and ACDC, and promising results are achieved."
    },
    {
      "id": "700b2059-5c06-4774-a552-7f35922c3723",
      "adv_source_id": "700b2059-5c06-4774-a552-7f35922c3723",
      "source_id": "a957f241-6622-4b33-9d53-932c2c3ac2b6",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Rethinking BiSeNet For Real-time Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Rethinking BiSeNet For Real-time Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent works usually enhance the performance of semantic segmentation models mainly in two ways: (i) increase the complexity of the model, e.g., designing more powerful CNN architectures, or (ii) utilize more training data. However, these methods usually lead to a significant increase in the model's complexity and computational cost, which hinders their applicability especially for real-time applications. In this paper, we try a different way to enhance the segmentation performance, namely, reforming the pre-trained network architecture by removing the redundant connections and optimizing the weights for the remaining connections. Specifically, we rethink the recently proposed BiSeNet, which originally consists of two streams: a base stream and a detail stream. We remove the redundant connections in the original BiSeNet and optimize the weights for the remaining connections. Experiments on the NYC-Depth dataset show that the proposed method can significantly improve the segmentation performance compared with the original BiSeNet model with much lower model complexity. In particular, our best model achieves a gain of ~5 FPS compared with the original BiSeNet model, while still preserving ~85% of the segmentation performance."
    },
    {
      "id": "5013e89f-b68b-4082-950a-7d4b078ceba4",
      "adv_source_id": "5013e89f-b68b-4082-950a-7d4b078ceba4",
      "source_id": "59abcb55-9a95-4a34-9479-70cc7198c288",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement",
      "prompt": "The following is the full text of the abstract for a research paper titled \"GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement\" from arxiv.org:",
      "generation": " Generative Models are typically trained under strong assumptions: 3D object categories are assumed to have a large number of object instances, and those instances are assumed to be ordered, e.g., sorted by their horizontal pose. We question these assumptions in the context of arranging object instances in a horizontal strip: How many instances can be recovered, if at all, when there are no ordering constraints? If ordering is imposed, how much does it help, and how much supervision is required? We investigate the limits of generative modeling under various supervisory signals, ranging from ordering constraints (e.g., horizontal pose) to no supervision at all. Our analysis reveals that while fully-supervised models perform well when the number of instances is large, they struggle when the number of instances is small or when the instances are even partially ordered. In contrast, our proposed method, GENESIS-V2, which does not rely on iterative refinement, performs consistently across all settings, often outperforming the state-of-the-art."
    },
    {
      "id": "1563c5d9-55bb-44d6-bc98-3307bb686b53",
      "adv_source_id": "1563c5d9-55bb-44d6-bc98-3307bb686b53",
      "source_id": "59abcb55-9a95-4a34-9479-70cc7198c288",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement",
      "prompt": "The following is the full text of the abstract for a research paper titled \"GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement\" from arxiv.org:",
      "generation": " Generative Modeling of Objects Sold on eBay has attracted increasing interest, with many datasets and models proposed for this task. There has been a surprising lack of evaluation of these methods on the task they are intended to solve - predicting the sale price of an object given its image. Furthermore, inference speed (time to generate a conditioning vector given an input image) and the size of the model are factors that would affect the usability of these methods in a retrieval-based indexing system. We propose a novel approach, Gen-V2, to the generation of fixed-length vectors for objects shown in images. When quantizing the output of a well-trained Gen-V2 network, we achieve a top-5 retrieval accuracy of 84.4%, with an inference speed of 4.4 seconds per object on a single CPU, and a model size of less than 1.7GB in RAM. When evaluating on a dataset of objects with associated sale prices from eBay, our model achieves a 0.49% increase in accuracy over the previous state-of-the-art."
    },
    {
      "id": "77723312-37de-4caf-b13c-dbc2e00844ba",
      "adv_source_id": "77723312-37de-4caf-b13c-dbc2e00844ba",
      "source_id": "bb5299ed-bebd-4210-b65b-6e466c875342",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization\" from arxiv.org:",
      "generation": " Recent works on semantic segmentation have achieved remarkable performance on standard benchmarks, but most lack the ability to generalize to unseen domains. In this paper, we investigate the use of generative models, namely Generative Adversarial Networks (GANs), for producing more robust feature representations for semantic segmentation. We propose a semi-supervised approach where a GAN is trained to synthesize training samples in the target domain, and show that this leads to improved performance and robustness. To effectively utilize the limited labeled data in each target domain, we propose a novel consistency loss that enforces the segmentation model to be robust to variations in the unlabeled images, as well as a domain-adversarial loss that explicitly trains the model to recognize differences between domains. When tested on the Cityscapes dataset, our approach yields meaningful improvements in segmentation quality and out-of-domain robustness, even with no target-domain labels. We further demonstrate the generalization capability of our approach on two different datasets: a synthetic dataset rendered from a 3D city model and a real-world dataset captured by an autonomous driving car."
    },
    {
      "id": "44dfc958-6398-45de-ad02-433c46f06913",
      "adv_source_id": "44dfc958-6398-45de-ad02-433c46f06913",
      "source_id": "bb5299ed-bebd-4210-b65b-6e466c875342",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization\" from arxiv.org:",
      "generation": " Recent works on semantic segmentation have achieved remarkable performance on standard benchmarks, but most methods struggle with strong out-of-domain generalization and require large amounts of labeled data. In this paper, we investigate the potential of generative models for achieving good performance with limited labeled data and strong generalization to unseen domains. We propose a semi-supervised approach for semantic segmentation that leverages generative models to learn a universal prior from a small labeled dataset, and harness the prior to boost segmentation performance on both the labeled and unlabeled data. This semi-supervised method is motivated by the observation that the generative models can capture high-level features in a supervised fashion, while learning a universal prior in an unsupervised fashion. We build our generative model using a Cycle Consistent Adversarial Network (CycleGAN) and show that it effectively learns a prior that helps segmentation across different domains with only a fraction of the labeled data. Our approach achieves 74.3% mean accuracy on ACDC, outperforming the state-of-the-art fully-supervised methods by 1.7%, and demonstrating strong generalization to five out-of-domain datasets."
    },
    {
      "id": "f5650705-d138-4fa2-ac31-eef4fa57ea27",
      "adv_source_id": "f5650705-d138-4fa2-ac31-eef4fa57ea27",
      "source_id": "2003dcc1-7c5b-4c86-9263-cdcda09a1655",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation\" from arxiv.org:",
      "generation": " Polyp localisation in Computed Tomography (CT) colonography is a challenging image processing task. In this paper, we propose a new approach for polyp localisation based on ensemble learning and particle swarm optimisation (PSO). The ensemble learning is based on classifier prediction confidence, and the PSO is used for comprehensive learning. The proposed approach is evaluated on a public dataset, and the results show that it outperforms the state-of-the-art approaches. The proposed approach is also shown to be robust to variations in the input data and the number of classes in the dataset."
    },
    {
      "id": "cb10b99e-86e5-4130-8a10-153d0d993c9a",
      "adv_source_id": "cb10b99e-86e5-4130-8a10-153d0d993c9a",
      "source_id": "2003dcc1-7c5b-4c86-9263-cdcda09a1655",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation\" from arxiv.org:",
      "generation": " Polyp localisation in Computed Tomography (CT) colonography is an active research field, aiming to assist physicians in the early detection of colorectal cancer. Various approaches have been proposed for polyp detection and localisation, with Deep Learning (DL) having shown promising results. However, existing DL-based methods can still be improved, particularly in terms of precision and robustness. In this paper, we propose a novel approach for polyp localisation based on Ensemble Learning (EL) and Comprehensive Learning Particle Swarm Optimisation (CLPSO). The proposed method utilizes a combination of classifier prediction confidence and EL to improve the accuracy of polyp detection and localisation. CLPSO is employed to optimize the hyperparameters of the EL framework, leading to improved performance. The experimental results show that the proposed method achieves state-of-the-art performance in terms of both Polyp Localisation Accuracy (PLA) and True Positive Rate (TPR) on the CRC-ID2017 dataset, marking a promising step towards more accurate and robust polyp detection and localisation in CT colonography."
    },
    {
      "id": "609fc53f-d226-4148-9588-1a22ee1357f8",
      "adv_source_id": "609fc53f-d226-4148-9588-1a22ee1357f8",
      "source_id": "764ed713-72bd-43c1-85da-9b71c62bd873",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax\" from arxiv.org:",
      "generation": " Abstract:\n\nPneumothorax is a common medical condition in which air leaks into the space between a patient's lungs and chest wall. It can be challenging to identify pneumothorax in medical images, as it can take on many shapes and sizes, and appear near other structures such as the heart and lungs. In this paper, we present a method for the automatic segmentation of pneumothorax in medical images using deep learning. Our method uses a U-Net architecture, trained on a large dataset of annotated images, to learn a robust feature representation for pneumothorax segmentation. We evaluate our method on a separate test set, and show that it outperforms existing methods in terms of accuracy and speed. Our method has the potential to be a valuable tool for the diagnosis and treatment of pneumothorax, and may also be useful for other medical image segmentation tasks.\n\nHere is a 250-word summary of the paper:\n\nThe paper presents a method for the automatic segmentation of pneumothorax, a medical condition in which air leaks into the space between the lungs and the chest wall, in medical images using deep learning. The goal of the paper is to develop a robust and accurate method for the segmentation of pneumothorax, which can assist in the diagnosis and treatment of the condition. \n\nThe method uses a U-Net architecture, which has been successfully used for other medical image segmentation tasks. The U-Net is trained on a large dataset of annotated images, which helps it to learn a robust feature representation for pneumothorax segmentation. The annotated images are collected from medical imaging databases and manually annotated by experts in the field.\n\nThe U-Net architecture consists of an encoder and decoder network, which are trained together. The encoder network extracts features from the input images, and the decoder network uses these features to predict the segmentation of the pneumothorax. The two networks are trained in a supervised manner, with the annotated images used as the training data.\n\nThe paper evaluates the method on a separate test set, and shows that it outperforms existing methods in terms of accuracy and speed. The evaluation is performed by comparing the method to other methods for pneumothorax segmentation, and measuring the accuracy and speed of the predictions. The paper also shows that the method is generalizable to different types of medical images, and can be used to segment pneumothorax in images from different patients and imaging modalities.\n\nThe potential applications of the method include the diagnosis"
    },
    {
      "id": "f2e30dab-3090-4b5b-9230-8ce1096eb157",
      "adv_source_id": "f2e30dab-3090-4b5b-9230-8ce1096eb157",
      "source_id": "764ed713-72bd-43c1-85da-9b71c62bd873",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax\" from arxiv.org:",
      "generation": " Abstract:\n\nPneumothorax is a common medical condition where air leaks into the space between a patient's lungs and chest wall. It can be challenging to identify this condition in its early stages, as the visual signs are often faint and easily missed by human experts. This work presents a novel method of using deep learning image segmentation to predict pneumothorax. Our method utilizes a fully-connected neural network coupled with a shallow feedforward neural network to produce a final segmentation. We show that our method is effective in detecting pneumothorax, even when the visual signs are faint and challenging to identify. Our method could be used as a tool to help human experts in the diagnosis of this condition, potentially leading to earlier detection and treatment.\n\nHighlights:\n\nOur method utilizes a novel combination of a fully-connected neural network and a shallow feedforward neural network for image segmentation.\nWe show that our method is effective in detecting pneumothorax, even when the visual signs are faint and easily missed by human experts.\nOur method could serve as a tool to help human experts in the diagnosis of this condition, potentially leading to earlier detection and treatment.\n\nhttps://www.mdpi.com/1999-4915/12/2/193"
    },
    {
      "id": "1e875df1-2b01-4322-b286-efa042783ce5",
      "adv_source_id": "1e875df1-2b01-4322-b286-efa042783ce5",
      "source_id": "d4f93d77-d6d9-4ee1-91d8-58ce8c7fe216",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in segmentation algorithms have shown that the use of attention mechanisms within convolutional neural networks (CNNs) can improve performance. In this work, we propose a novel architecture for biomedical image segmentation, called Feedback Attention Network (FANet). FANet incorporates an attention mechanism that utilizes both bottom-up and top-down attention, whilst also including a feedback loop from the attention module to the encoder. This unique combination improves performance by focusing on relevant image regions and leveraging global information for disambiguating between similar tissue types. We evaluate FANet on three publicly available datasets of different modalities (MRI, CT, and histology), in which we achieve state-of-the-art performance. Code is available at https://github.com/CCRALAB/fanet."
    },
    {
      "id": "3be0a18c-9453-4821-896a-8e54a173691a",
      "adv_source_id": "3be0a18c-9453-4821-896a-8e54a173691a",
      "source_id": "d4f93d77-d6d9-4ee1-91d8-58ce8c7fe216",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in biomedical imaging technologies have resulted in a flood of digital images, posing a significant challenge for accurate and efficient image analysis. This paper introduces a Feedback Attention Network (FANet), a novel deep learning architecture designed specifically for biomedical image segmentation. FANet incorporates attention mechanisms in a feedback loop, allowing the network to selectively focus on relevant information and adapt to varying image characteristics. This approach is particularly effective in tackling the challenges of morphological variability, intensity variations, and image noise commonly found in biomedical images. We demonstrate the efficacy of FANet on three different datasets: semantic segmentation of neuronal structures in electron microscopic images of the mouse brain, cell segmentation in fluorescent images of immunohistochemically stained human tissue samples, and tumor segmentation in CT scans of the lung. In all cases, FANet significantly improves upon the state-of-the-art."
    },
    {
      "id": "b86b9a6a-fd46-4a07-a53d-308921d61625",
      "adv_source_id": "b86b9a6a-fd46-4a07-a53d-308921d61625",
      "source_id": "e9899aaa-4e38-408c-bc2c-7f1fb3cf934c",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Self-Guided and Cross-Guided Learning for Few-Shot Segmentation\" from arxiv.org:",
      "generation": " Few-shot segmentation is a challenging problem in which the goal is to segment new classes given only a few training examples. In this paper, we propose a self-guided learning approach with an adaptive similarity metric learning module to address this problem. Specifically, we learn a similarity metric adapted to the few-shot segmentation by minimizing the divergence between the predicted and ground truth segmentations. Furthermore, we explore the connection between the few-shot segmentation and the well-studied image segmentation problem with a large number of labeled images and show that the latter can provide valuable information for the former. We propose a cross-guided learning approach to exploit the information from the well-studied problem and integrate it into the self-guided learning framework. The proposed methods are evaluated on the popular Cityscapes and Mapillary Vistas datasets, and the results demonstrate the effectiveness of the proposed approaches."
    },
    {
      "id": "5b89ddf2-6042-4be8-87ae-bec693f24e1a",
      "adv_source_id": "5b89ddf2-6042-4be8-87ae-bec693f24e1a",
      "source_id": "e9899aaa-4e38-408c-bc2c-7f1fb3cf934c",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Self-Guided and Cross-Guided Learning for Few-Shot Segmentation\" from arxiv.org:",
      "generation": " Few-shot segmentation is a challenging problem in which the goal is to segment new classes given only a few training examples. In this paper, we propose a self-guided learning approach to few-shot segmentation, where a teacher model is used to provide guidance to a student model for learning new classes. The teacher model is trained on a large dataset of images, and the student model is trained on a few examples of each new class. We show that the teacher model can provide effective guidance to the student model, improving the performance of the student model on few-shot segmentation. Our approach is based on a novel loss function that combines a cross-entropy loss with a consistency loss, which ensures that the student model learns to segment new classes in a consistent manner. We evaluate our approach on the VOC2012 and Cityscapes datasets, and show that it achieves state-of-the-art performance on few-shot segmentation."
    },
    {
      "id": "a409efef-5611-4ce6-ae45-d4016c882e02",
      "adv_source_id": "a409efef-5611-4ce6-ae45-d4016c882e02",
      "source_id": "24f2ca6c-1cb0-4ff0-a98f-e4c86496f942",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates\" from arxiv.org:",
      "generation": " We present a fully convolutional network that learns to segment objects in images, given only sparse scribbles as input. The network uses multi-scale adversarial attention gates to aggregate information provided by the scribbles across multiple scales, while focusing on the regions most relevant for segmentation. We introduce a novel loss function that trains the network to predict object masks directly, without the need for post-processing steps such as graph cuts. We demonstrate the effectiveness of our approach on the public Scribble Reconition Benchmark (SRB), achieving state-of-the-art performance. Code is available at https://github.com/ankitsharma008/scribble-segmentation."
    },
    {
      "id": "e718f506-add1-4e1e-851c-ef9f9b8cb5f8",
      "adv_source_id": "e718f506-add1-4e1e-851c-ef9f9b8cb5f8",
      "source_id": "24f2ca6c-1cb0-4ff0-a98f-e4c86496f942",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates\" from arxiv.org:",
      "generation": " We introduce a new weakly-supervised learning paradigm for segmentation. Given a set of image-level labels, our goal is to learn a segmentation model that predicts pixel-level labels. We propose to use scribbles from a labeler to create a weakly-supervised training signal, and show that an attention mechanism using multi-scale adversarial learning can use this signal to learn a dense segmentation model. We perform experiments on the VOC2012 and ACDC datasets, and show that our approach compares favorably with existing state-of-the-art methods that use substantially more labeled data."
    },
    {
      "id": "4d8f46e8-0b38-4469-8bdd-d762fe3c8217",
      "adv_source_id": "4d8f46e8-0b38-4469-8bdd-d762fe3c8217",
      "source_id": "a7ce4b85-f4ec-45a2-b1f4-5219923dbbd2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images\" from arxiv.org:",
      "generation": " Deep learning has become a popular method for image segmentation, but it suffers from a lack of understanding. We propose a novel approach to image segmentation using deep neural networks that learn meta-structures in the input image to aid in segmentation. Our approach is based on the observation that deep neural networks can learn to detect and utilize subtle patterns in the input image, which can be used for segmentation. We demonstrate the effectiveness of our approach on fluorescence microscopy images, which contain complex and subtle structures that are challenging to segment using traditional methods. Our results show that our approach can achieve state-of-the-art performance in image segmentation, while providing insights into the decision-making process of the neural network.\n\nOur approach can be useful for segmenting other types of images, such as medical images and satellite images, which often contain complex and subtle structures that are challenging to segment using traditional methods. Additionally, our approach can be used to understand the decision-making process of deep neural networks, which can help to improve their performance and interpretability."
    },
    {
      "id": "0cb338a3-e8fe-4f77-819e-a799f5c48978",
      "adv_source_id": "0cb338a3-e8fe-4f77-819e-a799f5c48978",
      "source_id": "a7ce4b85-f4ec-45a2-b1f4-5219923dbbd2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images\" from arxiv.org:",
      "generation": " Deep learning algorithms have enabled recent breakthroughs in image segmentation, but these successes have largely come at the cost of increased computational complexity and reliance on large amounts of labeled training data. In this paper, we study the problem of segmenting fluorescent microscopy images, which has been a mainstay of biological research for decades, and explore a data-efficient approach to address this problem. We find that state-of-the-art deep learning models are not suitable for segmenting such images due to the lack of training data and the highly specific nature of the problem, and that it is necessary to rely on domain expertise to gain an advantage in this task. To leverage this domain expertise, we propose a new approach for segmenting fluorescent microscopy images based on meta-learning, which enables the transfer of segmentation knowledge from one image to the next in an unsupervised manner. Our approach performs competitively with state-of-the-art methods while requiring significantly less computational resources. We demonstrate the effectiveness of our approach on both a public dataset and a private dataset of neuronal traces."
    },
    {
      "id": "0085d2e8-a7fc-416c-b789-d9186977dbc8",
      "adv_source_id": "0085d2e8-a7fc-416c-b789-d9186977dbc8",
      "source_id": "53e6c118-bd9b-4e37-9670-7c4ee5621a66",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Contrastive Registration for Unsupervised Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Contrastive Registration for Unsupervised Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has propelled medical image segmentation towards a fully automated pipeline, however, most approaches rely on large amounts of manually annotated training data. This is especially challenging in the domain of medical imaging, where expert annotation is expensive and time-consuming. In this paper, we tackle the problem of training a segmentation model from unsupervised learning, i.e., without relying on annotated examples. Our approach leverages a robust registration algorithm that aligns clinically acquired scans to a baseline atlas, and a contrastive loss that encourages the segmentation model to output consistent segmentations across the aligned images. We show that our approach is capable of learning representations that are highly transferable to a variety of segmentation tasks, and moreover, that the use of contrastive learning significantly improves generalization performance. We evaluate our approach on multi-atlas based segmentation and on per-patient segmentation without any atlas, demonstrating state-of-the-art performance on the ISLES 2017 challenge and the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset."
    },
    {
      "id": "3561337e-59b4-4c8b-bb3a-1941962fad04",
      "adv_source_id": "3561337e-59b4-4c8b-bb3a-1941962fad04",
      "source_id": "53e6c118-bd9b-4e37-9670-7c4ee5621a66",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Contrastive Registration for Unsupervised Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Contrastive Registration for Unsupervised Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel framework for unsupervised segmentation of medical images. Our method leverages the inherent spatial regularity of anatomical structures and uses a registration framework to compare the intensity distribution of structures in the target image to a library of source images. We use a similarity measure based on a quarternion-based registration technique to compare the library source images to the target image. Segmentation is performed by integrating a geodesic shotgun algorithm along the direction of highest similarity. This framework does not require any segmentation a-priori on the library images and is therefore suitable for fully unsupervised segmentation. We show that geodesic shotgun with integration along the direction of highest similarity performs better than a geodesic shotgun algorithm without any integration and therefore benefits from the registration framework. We evaluate our method on synthetically generated data and abdominal CTs, and show that it performs competitively with state-of-the-art unsupervised segmentation methods while being much faster."
    },
    {
      "id": "108ed3c5-1fdd-4d80-b480-22ddc5735e10",
      "adv_source_id": "108ed3c5-1fdd-4d80-b480-22ddc5735e10",
      "source_id": "07dc10ce-198b-4398-901a-872cd727f5e9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has led to numerous successful applications, but most of these applications focus on confined datasets and trained models lacking generalization capability. In this paper, we tackle the problem of learning a deep neural network for food portion estimation with limited training data. We propose a method to leverage knowledge from a source domain with more training data (e.g., image segmentation) to adapt a pre-trained model to a target domain (i.e., food portion estimation) using few training samples. We demonstrate the effectiveness of our approach on a public food portion estimation dataset, showing consistent performance improvement over the state-of-the-art. Our code and models are available at https://github.com/AI4FOOD-Foundation/CrossDomainFeatureAdaptation."
    },
    {
      "id": "e66cbef1-90ef-44ff-8b56-b2174778ddc1",
      "adv_source_id": "e66cbef1-90ef-44ff-8b56-b2174778ddc1",
      "source_id": "07dc10ce-198b-4398-901a-872cd727f5e9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation\" from arxiv.org:",
      "generation": " Distinctive appearance, arbitrary shape, and intensive intra-class variation of food in different dishes complicate the recognition of food portion in real-world scenarios. To tackle these challenges, in this paper, we exploit the cross-domain adaptation idea that facilitates the learning of a general feature representation from images in one domain (e.g., natural images) and adapts it to another domain (i.e., food images) to achieve effective food portion recognition. Specifically, we propose a Cross-Domain Feature Adaptation (CDFA) framework that comprises a feature extractor and an adapter. The feature extractor is pre-trained on ImageNet, which is then fine-tuned with a small number of food images. An adapter is designed to harness the knowledge in the pre-trained feature extractor and adapt it to the new domain. In this way, the adapted feature extractor can capture the distinctive appearance of food in various dishes. We evaluate our approach on three public food recognition datasets, i.e., Food101, Food50, and Fungible Goods. Comparative experiments with the state-of-the-art approaches validate the effectiveness of the proposed approach."
    },
    {
      "id": "512db4c0-795b-4e22-8907-37227063d199",
      "adv_source_id": "512db4c0-795b-4e22-8907-37227063d199",
      "source_id": "427b7a84-0ebe-4169-b118-abdaed57d65f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space\" from arxiv.org:",
      "generation": " Recent progress in medical image segmentation has largely been achieved by leveraging large institutional datasets. However, such efforts suffer from dataset heterogeneity, inhibiting the generalization of learned models across institutions and resulting in poor performance on datasets outside the training distribution. In this paper, we propose FedDG, a flexible framework for learning robust segmentors via federated domain generalization (FedDG). FedDG leverages episodic learning, a technique inspired by human cognitive learning, to model distributions in continuous frequency space. We demonstrate that our model can learn robust segmentors from only a few institutional contributions, resulting in improved performance on both in-distribution and out-of-distribution datasets. We showcase the efficacy of our approach on three public medical image segmentation benchmarks, yielding state-of-the-art performance with trained models that are over 200 times smaller. Code is available at https://github.com/AI4Finance-Foundation/FedDG."
    },
    {
      "id": "4ae5bdf3-c6a0-4d55-80d4-6d9b9c4c92cc",
      "adv_source_id": "4ae5bdf3-c6a0-4d55-80d4-6d9b9c4c92cc",
      "source_id": "427b7a84-0ebe-4169-b118-abdaed57d65f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space\" from arxiv.org:",
      "generation": " Recent advances in medical imaging, particularly in imaging modalities with limited training data such as MRI, have motivated research in federated learning (FL) and transfer learning (TL). In this work, we tackle the problem of medical image segmentation under domain variance by drawing connections between FL and TL and proposing a new method called FedDG (Federated Domain Generalization). Given a dataset of medical images from different domains (e.g., different hospitals or institutions), FedDG aims to train a universal segmentation model that is capable of segmenting images from any domain while training all models at their own institutions, avoiding the need for data sharing while preserving privacy. To achieve this, we propose to use episodic learning (EL) to extract domain-invariant information from data in the continuous frequency space, which is adaptive to data distribution variations across different institutions. We demonstrate the effectiveness of our approach on segmenting prostate MRI images and achieve state-of-the-art performance in the presence of significant domain variance."
    },
    {
      "id": "e2bb0a1c-b034-42a6-9c7f-e341b8b79e0f",
      "adv_source_id": "e2bb0a1c-b034-42a6-9c7f-e341b8b79e0f",
      "source_id": "823ab1a3-8a1d-4d4b-bab4-92dcb32285a2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Medical Image Segmentation through Dual-task Consistency",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised Medical Image Segmentation through Dual-task Consistency\" from arxiv.org:",
      "generation": " We present a semi-supervised segmentation framework that utilizes the dual-task consistency (DTC) loss, which aligns the hidden representations of a shared encoder across two tasks: image classification and segmentation. Benefitting from large amounts of unlabeled data and the powerful representations learned by the encoder, our approach shows robust performance on three challenging medical image segmentation tasks: abdominal CT, cardiac MRI, and brain tumor MRI. In particular, given a single fully-labeled image, our method achieves 42.0%, 56.4%, and 66.4% mean accuracy on the abdominal CT, cardiac MRI, and brain tumor MRI segmentation benchmarks, respectively, significantly outperforming the state-of-the-art."
    },
    {
      "id": "9ae9535d-b019-4370-bfd6-8c693b0efe58",
      "adv_source_id": "9ae9535d-b019-4370-bfd6-8c693b0efe58",
      "source_id": "823ab1a3-8a1d-4d4b-bab4-92dcb32285a2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Medical Image Segmentation through Dual-task Consistency",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised Medical Image Segmentation through Dual-task Consistency\" from arxiv.org:",
      "generation": " We present a semi-supervised segmentation framework that utilizes the inherent spatial consistency of medical images. Given a limited amount of labeled training data, our approach leverages the available unlabeled examples via a semi-supervised loss that enforces labeled and unlabeled example consistency within a unified framework. We pose medical image segmentation as a dual-task learning problem, where the consistency among unlabeled examples is framed as a second task. This allows us to utilize the powerful semi-supervised loss in a manner that is directly applicable to anatomical image labeling, while avoiding costly data augmentation techniques. We demonstrate the efficacy of our approach on three different medical image segmentation datasets: ISLES, MIASICS, andaisawhl2018semi, where we achieve state-of-the-art performance on two of them."
    },
    {
      "id": "9d4f5a66-7fb8-4de7-b891-c19f40cc90d4",
      "adv_source_id": "9d4f5a66-7fb8-4de7-b891-c19f40cc90d4",
      "source_id": "285debed-9666-4c9c-b086-1d1ecbbcfff6",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning greatly advanced biomedical image segmentation, but the trained models are hardly transferable to clinical settings, because the amount of manually annotated clinical data is scarce. We propose a category-centric prototype aligner to effectively align the source and target distributions in unsupervised domain adaptation (UDA), which is a key challenge in the application of deep learning to biomedical image segmentation. In the category-centric prototype aligner, we introduce a category-centric loss to align the data distributions of different categories in the source and target domains, and a prototype-centric loss to align the data distribution of prototypes in the source and target domains, and we optimize both of them jointly via a stacked progressive four-layer encoder-decoder network. In the encoder layer, we utilize a categorize-then-distribute mechanism to obtain category-centric and prototype-centric features. In the decoder layer, we utilize a category-centric prototype aligner to align the features of different categories and prototypes of the source domain to their counterparts in the target domain. We evaluate our method on the ISLES challenge 2019 dataset for liver and kidney segmentation, and achieve the best average precision and recall scores among the eight competing methods on both the source and target domains."
    },
    {
      "id": "752dd879-c317-40df-a9ec-e36e07af9070",
      "adv_source_id": "752dd879-c317-40df-a9ec-e36e07af9070",
      "source_id": "285debed-9666-4c9c-b086-1d1ecbbcfff6",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation\" from arxiv.org:",
      "generation": " Deep learning models have achieved remarkable performance in semantic segmentation of images from different domains, such as natural and synthetic images. However, training such models on multiple domains becomes expensive when the amount of available labeled data is limited. In this work, we tackle the problem of unsupervised domain adaptation for semantic segmentation, where the goal is to utilize data from an unseen domain without using any labeled data. We propose a Category-Centric Prototype Aligner (CCPA) module that aligns the data of different domains in a category-specific manner, by learning a common feature representation via few-shot learning. Our proposed approach exhibits excellent performance in adapting segmentation models between different biomedical image domains, achieving 85.3% and 91.4% of the performance of a model trained on the target domain, on MR and CT lungs segmentation respectively."
    },
    {
      "id": "14f2ccab-b66b-412d-9eaf-b4eac08742a5",
      "adv_source_id": "14f2ccab-b66b-412d-9eaf-b4eac08742a5",
      "source_id": "27db1b2d-f18b-492a-afb6-0ef17287a61e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models\" from arxiv.org:",
      "generation": " Recent advances in deep learning have achieved remarkable performance in medical image segmentation, aiming to provide accurate segmentation results with minimal human effort. However, fully supervised learning still demands a significant amount of manually labeled training samples, which is difficult to obtain in the medical field due to the expertise and time required. In this paper, we conduct a comprehensive review of recent deep learning models that address the medical image segmentation task with limited supervision. We first categorize existing methods into three categories based on the level of supervision they require. Then, we discuss the deep learning models that have been applied to each category, including fully supervised learning, weakly supervised learning, and unsupervised learning. We also review the recently emerged few-shot learning approaches that are designed to segment new classes with only a few labeled examples. Next, we survey the state-of-the-art methods designed for specific medical image segmentation tasks, including neuronal tissue segmentation, tumor segmentation, and cardiac image segmentation. We also discuss the evaluation metrics commonly used in the field. Finally, we highlight the advantages and limitations of each approach and point out potential research directions."
    },
    {
      "id": "f261d497-e7cc-4206-9e81-27fce27fef16",
      "adv_source_id": "f261d497-e7cc-4206-9e81-27fce27fef16",
      "source_id": "27db1b2d-f18b-492a-afb6-0ef17287a61e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models\" from arxiv.org:",
      "generation": " Recent advances in deep learning have achieved tremendous success in medical image segmentation. However, these methods require large amounts of annotated training data, which are expensive to obtain. In this paper, we review recent progress in limited supervision segmentation, where the amount of training data is significantly reduced. We first introduce the problem setting and discuss the challenges of working with limited supervision. We then review the state-of-the-art deep learning models that have been applied to medical image segmentation with limited supervision. These models include segmentation via registration, transfer learning, knowledge transfer, and few-shot learning. We also discuss the evaluation metrics used to assess the performance of these models. Finally, we highlight the challenges and opportunities for future research in this field."
    },
    {
      "id": "d4d2be43-c1c1-40b8-aa4b-475837384921",
      "adv_source_id": "d4d2be43-c1c1-40b8-aa4b-475837384921",
      "source_id": "3f74bd45-21ac-4446-8591-88f32baafbaf",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks\" from arxiv.org:",
      "generation": " EMDS-5 is an ongoing project to create a large-scale dataset of microbial samples, which can be used for training and testing of machine learning algorithms for classification, segmentation, and detection tasks. The dataset is composed of microscopic images of microorganisms, including bacteria, fungi, and protozoa, as well as samples of non-living objects, such as dust and dirt. The images are manually annotated with labels at the pixel level, and the dataset is designed to be challenging for machine learning algorithms, with a high level of variability in the appearance of the microorganisms, and a large number of classes and instances. This paper describes the creation of the EMDS-5 dataset and the challenges associated with its creation, and it presents a set of baseline results for several image analysis tasks. We hope that the EMDS-5 dataset will be a useful resource for the machine learning and microbiology communities, and will help to advance the state of the art in the analysis of microscopic images of microorganisms."
    },
    {
      "id": "fd5acc7c-a3ee-48e1-99f5-801a9effaceb",
      "adv_source_id": "fd5acc7c-a3ee-48e1-99f5-801a9effaceb",
      "source_id": "3f74bd45-21ac-4446-8591-88f32baafbaf",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks\" from arxiv.org:",
      "generation": " EMDS-5 is an ongoing project to create a large-scale dataset of microscopic images of microorganisms, usable for training and evaluation of machine learning models for automated identification and classification. This fifth version contains 13,163 images of 11 different types of microorganisms, each image being labeled with its corresponding type. The images were collected from multiple sources, including online databases, academic papers, and direct contributions from researchers. The dataset is divided into a training set (80% of the images) and a test set (20% of the images), and can be used for multiple image analysis tasks, such as image classification, segmentation, and feature extraction. The dataset is available in the TensorFlow format, and can be downloaded from the project's GitHub page."
    },
    {
      "id": "1b3a55f4-d5c6-43b1-a55d-8378b5b67c89",
      "adv_source_id": "1b3a55f4-d5c6-43b1-a55d-8378b5b67c89",
      "source_id": "256bd567-328d-4cf4-a87c-7d3226f89ef6",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Saliency-Aware Class-Agnostic Food Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Saliency-Aware Class-Agnostic Food Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in convolutional neural network (CNN) architecture design has propelled the advancement of semantic segmentation, especially in the context of natural image datasets. However, the assumption that the training data is abundantly labeled, which is often satisfied in natural image segmentation, does not hold for specialized domains such as food image segmentation, which demands much effort and expense to collect and label. To overcome the lack of labeled training data, we can utilize the prior knowledge in the form of rules or culinary knowledge to learn a segmentation model. In this work, we borrow ideas from the recent advancement in the field of salient object detection and propose a saliency-aware class-agnostic segmentation approach for food images. Specifically, we first utilize a pre-trained CNN as a feature encoder to extract mid-level features and then build a segmentation model that is trained by explicitly incorporating the prior knowledge of food images, which is in the form of a global saliency map. The global saliency map is learned in a class-agnostic manner, but is tailored to the food domain. We demonstrate the efficacy of our approach on the public FOOD-101 dataset."
    },
    {
      "id": "60d45367-aa80-45f3-b83c-a32a75244db8",
      "adv_source_id": "60d45367-aa80-45f3-b83c-a32a75244db8",
      "source_id": "256bd567-328d-4cf4-a87c-7d3226f89ef6",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Saliency-Aware Class-Agnostic Food Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Saliency-Aware Class-Agnostic Food Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in convolutional neural network (CNN) image segmentation has largely focused on person-centric or object-centric datasets, and in these cases, the rest of the scene is often treated as a background that is ignored by the network. In this work, we seek to address this limitation and develop a method for segmenting foods in a scene, without needing training examples of specific foods. We introduce a method for adding saliency information to an otherwise person- or object-centric CNN, in order to improve its ability to segment foods in a scene. We demonstrate the effectiveness of our approach on the validation set of the 2018 Food-101 Segmentation Benchmark, where our method achieves a mean pixel accuracy of 67.2%, compared to 63.2% for the person-centric baseline."
    },
    {
      "id": "d5b7743c-b5f6-4fe6-85fb-e82bf608efa0",
      "adv_source_id": "d5b7743c-b5f6-4fe6-85fb-e82bf608efa0",
      "source_id": "a511beab-f399-4434-aa76-f484c25fb23e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Co-Seg: An Image Segmentation Framework Against Label Corruption",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Co-Seg: An Image Segmentation Framework Against Label Corruption\" from arxiv.org:",
      "generation": " Recent progress in image segmentation has primarily been evaluated on tightly-controlled benchmark datasets, which inevitably skew the distribution of real-world images. In practice, the performance of these models can degrade significantly given small amounts of label corruption, motivating research on robust methods. We propose Co-Seg, a simple yet effective framework for image segmentation that is capable of handling label noise. Co-Seg leverages a pre-trained segmentation model as a soft prior and learns a small classifier to refine the segmentation by aligning with the prior. We show that this approach is capable of correcting label noise, and also naturally integrates edge information to improve performance on boundary regions. Co-Seg achieves state-of-the-art performance on the Cityscapes and Mapillary Vistas datasets even without any form of regularization, and is on-par with state-of-the-art methods on the CamVid dataset with a simple regularization technique."
    },
    {
      "id": "53b455e0-edb8-4c2d-a7c0-0fc4b1f7b779",
      "adv_source_id": "53b455e0-edb8-4c2d-a7c0-0fc4b1f7b779",
      "source_id": "a511beab-f399-4434-aa76-f484c25fb23e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Co-Seg: An Image Segmentation Framework Against Label Corruption",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Co-Seg: An Image Segmentation Framework Against Label Corruption\" from arxiv.org:",
      "generation": " Label corruption in image segmentation is prevalent in real-world applications. In this paper, we tackle the problem of image segmentation under label corruption. Our key insight is that, in addition to the labeled data, we can utilize a large number of unlabeled images that are available on the web through image-retrieval. We propose Co-Seg, a framework that leverages the retrieved unlabeled images to correct the label corruption in the labeled images. Specifically, we estimate a segmentation mask for each image through a convolutional neural network (CNN), and then aggregate the estimated masks from the retrieved images to refine the segmentation masks of the labeled images. We further present an efficient algorithm to compute the proposed aggregation method, avoiding expensive pairwise comparisons among all the images. Our experiments on the Cityscapes dataset, which contains images with significant label corruption, show the efficacy of the proposed method against several strong baselines."
    },
    {
      "id": "e3130846-730f-4ab7-a3cb-9e8f3d129152",
      "adv_source_id": "e3130846-730f-4ab7-a3cb-9e8f3d129152",
      "source_id": "b4300583-85ba-4980-8b29-8b312ef44492",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Network-Agnostic Knowledge Transfer for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Network-Agnostic Knowledge Transfer for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progresses in medical image analysis have been largely driven by large-scale annotation efforts, which however are difficult to replicate for specific domains and hospital settings, especially in surgical imaging. In this work, we tackle the problem of transferring segmentation knowledge from large-scale annotated images to novel domains, without requiring any explicit pairing or domain-specific training data. We introduce a new approach to domain adaptation based on a network-agnostic knowledge transfer framework. Our method operates in the image domain and does not require any pairing between source and target domains, nor any pre-trained models. We demonstrate the efficacy of our approach on three different intraoperative imaging datasets (cranial ultrasound, laparoscopic and endovascular ultrasound), showing improvements over strong baselines in terms of both segmental and boundary accuracy, as well as a significant reduction of segmentation errors."
    },
    {
      "id": "f28117ec-7490-4a0a-8673-621590438c56",
      "adv_source_id": "f28117ec-7490-4a0a-8673-621590438c56",
      "source_id": "b4300583-85ba-4980-8b29-8b312ef44492",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Network-Agnostic Knowledge Transfer for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Network-Agnostic Knowledge Transfer for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in medical image analysis has resulted in highly specialized deep learning networks for various segmentation tasks. Directly applying these networks to new datasets requires significant effort in preprocessing and fine-tuning, which poses challenges in model deployment and adaptation. To address these issues, we propose a network-agnostic knowledge transfer approach for medical image segmentation, which enables the direct usage of pre-trained models on new datasets without fine-tuning. We introduce a meta-learning framework that learns a knowledge transfer process from a source network to a target network. The target network is able to directly utilize the knowledge from the source network without the need for fine-tuning, while maintaining competitive performance with the source network. The proposed approach is general and can be applied to any pre-trained network for segmentation tasks. We demonstrate the effectiveness of our method on three different public segmentation benchmark datasets in the medical image domain."
    },
    {
      "id": "920cbea1-8753-4f13-9328-4dbc1a446a47",
      "adv_source_id": "920cbea1-8753-4f13-9328-4dbc1a446a47",
      "source_id": "b7c181d9-7c1a-4cbc-80ec-ff4b60ec7f3e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Nonparametric clustering for image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Nonparametric clustering for image segmentation\" from arxiv.org:",
      "generation": " Image segmentation is typically approached as a clustering problem, whereby pixels in an image are assigned labels belonging to different clusters, each characterized by a mean color. In this paper we study the performance of several nonparametric Bayesian models, including Gaussian mixtures and Ising models, for segmenting images using a variational approach. We derive a new energy function which assigns costs to labeled regions based on how consistent they are with observed data and with a specified labeling structure. We introduce a new prior on the means of the Gaussian mixtures to encourage natural, visually meaningful segmentations. Finally, we introduce a new algorithm for tuning the hyperparameters of the segmentation model. This new approach is fully automatic, and is shown to produce state-of-the-art image segmentations."
    },
    {
      "id": "1b6e67dd-0fda-4f4b-9703-fe347713f688",
      "adv_source_id": "1b6e67dd-0fda-4f4b-9703-fe347713f688",
      "source_id": "b7c181d9-7c1a-4cbc-80ec-ff4b60ec7f3e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Nonparametric clustering for image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Nonparametric clustering for image segmentation\" from arxiv.org:",
      "generation": " Abstract: We present a nonparametric clustering method for image segmentation based on a probabilistic approach. The method is inspired by the idea of probabilistic clustering, which has been successfully used in various machine learning applications. The proposed method utilizes a probabilistic model called the Gaussian mixture model (GMM) to capture the statistical properties of the image pixels. The GMM is a nonparametric model that can adapt to any given dataset without any assumption of its statistical distribution. We propose a novel algorithm to train the GMM on the image pixels and to segment the image based on the GMM. The proposed method is evaluated on several benchmark image segmentation datasets, and the results show that the method is competitive with other state-of-the-art image segmentation methods."
    },
    {
      "id": "c61c94b5-98a0-43fc-a9e2-c6b699d19ef7",
      "adv_source_id": "c61c94b5-98a0-43fc-a9e2-c6b699d19ef7",
      "source_id": "15ed2c14-ddcf-45ef-93a7-3d2a9485dff4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds\" from arxiv.org:",
      "generation": " We present a novel boundary-aware geometric encoding (BAGE) for efficient and effective semantic segmentation of point clouds. BAGE utilizes a boundary-aware interpolation strategy to encode geometric information of point clouds into a continuous and dense feature space, where semantic segmentation can be performed. Specifically, we design a boundary-aware weight function to emphasize the boundary points and grant more reliable predictions on object boundaries. Furthermore, we propose a distance-weighted voting mechanism to refine the segmentation results and highlight the regions with complex shapes. To evaluate the performance of BAGE, we conduct extensive experiments on the Semantic3D and S3DIS datasets, and the results demonstrate the effectiveness of our approach in comparison to the state-of-the-art methods. Our source code is available at https://github.com/AI4SEG/BAGE."
    },
    {
      "id": "d194f215-a53a-4b79-ab4b-361715db7761",
      "adv_source_id": "d194f215-a53a-4b79-ab4b-361715db7761",
      "source_id": "15ed2c14-ddcf-45ef-93a7-3d2a9485dff4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds\" from arxiv.org:",
      "generation": " We propose a novel boundary-aware geometric encoding (BAGE) to effectively capture both the geometry and the boundaries of objects in point clouds for semantic segmentation. BAGE extends conventional geometric encoding to incorporate boundary information in a self-adaptive manner, by estimating an encoding range for each point based on its local geometry. With the estimated encoding range, we devise an effective boundary-aware encoding scheme that adapts the quantization steps to the point's proximity to object boundaries. This boundary-awareness enables BAGE to better capture object geometries and their semantic meanings. We integrate BAGE into a cascaded segmentation framework as a baseline method to demonstrate its effectiveness in capturing boundaries for point-based segmentation. In extensive experiments on publicly available ScanNet and S3DIS datasets, our approach outperforms the state-of-the-art methods in terms of segmentation accuracy."
    },
    {
      "id": "dea938ca-04f3-4cde-ba77-c122b82bae38",
      "adv_source_id": "dea938ca-04f3-4cde-ba77-c122b82bae38",
      "source_id": "bf86b311-04cb-4e22-9749-ff8c962eafb9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation\" from arxiv.org:",
      "generation": " Recent progress in computer vision has resulted in breakthroughs in well-known datasets such as ImageNet, which however often represent constrained and controlled conditions. In contrast, medical imaging datasets usually come from different domains (different hospitals, different acquisition modalities, etc.), leading to a data distribution shift and a degradation of the performance of the models. In this work, we address the problem of domain adaptation on hip joint bone segmentation, with a particular focus on intra-modality and cross-modality inconsistencies. We propose ICMSC, a simple and effective method that combines an inconsistency-aware loss and a modality-consistent regularization. We show that our approach is able to adapt effectively both within each individual modality and across different modalities, leading to state-of-the-art performance on a public dataset of hip x-ray images."
    },
    {
      "id": "8bd01a6b-7ed7-42ac-8514-070d2efcd45f",
      "adv_source_id": "8bd01a6b-7ed7-42ac-8514-070d2efcd45f",
      "source_id": "bf86b311-04cb-4e22-9749-ff8c962eafb9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation\" from arxiv.org:",
      "generation": " Recent progress in computer vision has resulted in breakthroughs in image recognition, retrieval and classification. However, many of these methods rely on large amounts of labelled data, which are expensive to collect. Unsupervised domain adaptation is a technique that addresses this problem by adapting a model trained on one domain to perform well on a target domain without using any labelled data from the target domain. In this paper we propose a new approach to unsupervised domain adaptation for semantic segmentation of the hip joint bone in computed tomography (CT) scans. We exploit the intra-modality and cross-modality consistency in the target domain by maximizing a joint likelihood function of the segmentation masks over all the modalities. We show that our approach is able to improve upon the state-of-the-art in hip joint bone segmentation, and that it is applicable to other anatomical structures."
    },
    {
      "id": "192543a7-91cb-466a-9833-3a5ff8876034",
      "adv_source_id": "192543a7-91cb-466a-9833-3a5ff8876034",
      "source_id": "24ad0185-90a9-47b6-a029-30b1d4f3c650",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks\" from arxiv.org:",
      "generation": " Recent works have shown that 3D integration is a promising direction for technology scaling of AI accelerators, but the rapidly increasing computational complexity and large volume of 3D networks pose significant challenges for designing high-performance and area-efficient 3D networks. In this paper, we propose Cakes, a channel-wise automatic kernel shrinking (CAKES) framework that can effectively reduce the volume of 3D networks while preserving their computational integrity and without requiring any kernel-level modification. Cakes can be readily applied to any 3D network with minimal effort. We present an extensive evaluation of Cakes on a wide range of 3D networks, where up to 50% volume reduction is achieved with an average of 29.4% volume reduction, and the overall computational latency is improved by up to 4.35% with an average of 1.77% for the Cake-treated networks compared to their original counterparts."
    },
    {
      "id": "3b55b817-910d-4104-94f1-2d81d7b42430",
      "adv_source_id": "3b55b817-910d-4104-94f1-2d81d7b42430",
      "source_id": "24ad0185-90a9-47b6-a029-30b1d4f3c650",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks\" from arxiv.org:",
      "generation": " Recent works have shown that 3D integration can help alleviate the inter-die communication bottleneck, however, 3D network design is largely an uncharted territory, with nearly all existing network design principles being derived for 2D networks. In this paper, we aim to fill this gap and provide the first comprehensive design framework for 3D-IC networks. We begin by identifying two major bottlenecks: (1) the bottleneck between a network interface and a 3D-IC layer, and (2) the bottleneck within a 3D-IC layer. To address these bottlenecks, we propose CAKES, a general and automatic framework that can effectively shrink the network topology between layers, as well as within a layer. Specifically, we propose a channel-wise kernel shrinking method that reduces the computational load by utilizing fewer kernels, while still maintaining important properties from the original network design. Our extensive experiments on image classification and object detection tasks show that CAKES can achieve excellent performance even without increasing any operation complexity (e.g., MAC operations), making it a practical and effective solution for 3D network design."
    },
    {
      "id": "70bb70fb-3244-409f-b1b6-517104d33d7f",
      "adv_source_id": "70bb70fb-3244-409f-b1b6-517104d33d7f",
      "source_id": "c9883f32-e69f-4725-b09b-327291b0391b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding\" from arxiv.org:",
      "generation": " Few-shot learning aims to reduce the amount of labeled training data required to build a robust model by transferring knowledge from a source task with abundant labeled data to a target task with only a few labeled samples. In this work, we propose a few-shot learning approach for medical image segmentation problems, which are typically characterized by the scarcity of labeled images. We exploit a global correlation network as a universal feature encoder for learning robust representations that facilitate the few-shot segmentation. Specifically, we pre-train the global correlation network on a large dataset of images in an unsupervised manner, and then fine-tune it with a discriminative embedding on a few labeled images. The proposed method is evaluated on four public abdominal organ segmentation benchmarks, and compared with other few-shot learning methods as well as the state-of-the-art fully-supervised methods. Our results show that the proposed method achieves promising performance in terms of segmentation accuracy, and is particularly effective for few-shot learning scenarios."
    },
    {
      "id": "39faaf81-de92-4ab8-b0cc-8406442606a7",
      "adv_source_id": "39faaf81-de92-4ab8-b0cc-8406442606a7",
      "source_id": "c9883f32-e69f-4725-b09b-327291b0391b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding\" from arxiv.org:",
      "generation": " Few-shot medical image segmentation is a challenging problem in clinical translation. In this paper, we exploit the global correlation and local discriminatory information in few-shot segmentation. We propose a Global Correlation Network (GCN) to effectively leverage the global structural information of images, and a Discriminative Embedding (DE) module to capture local appearance information. The DE module learns a low-dimensional embedding of image patches via a latent variable model, which captures the between-class variations and effectively utilizes the intra-class similarity for few-shot segmentation. We evaluate our method on the OsiriX-Challenge 2020 dataset for lymph node segmentation, and achieve competitive results compared with the state-of-the-art methods."
    },
    {
      "id": "07bbf4a0-576a-47f9-b607-22455097b37d",
      "adv_source_id": "07bbf4a0-576a-47f9-b607-22455097b37d",
      "source_id": "8169f839-943c-4c76-831f-1cba9e363b8d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity\" from arxiv.org:",
      "generation": " In this paper, we propose a novel active contour model for infrared image segmentation with intensity inhomogeneity. The proposed method is driven by a set of multi-features, including gradient, gradient magnitude, and curvature. By introducing a new energy term, the proposed method is capable of adapting to the intensity inhomogeneity of infrared images. In addition, a novel algorithm is presented to optimize the proposed model. The proposed method was evaluated on a public infrared image segmentation benchmark, and the experimental results demonstrate the effectiveness of the proposed approach."
    },
    {
      "id": "6573b691-ad18-492c-8373-955e2a2586fe",
      "adv_source_id": "6573b691-ad18-492c-8373-955e2a2586fe",
      "source_id": "8169f839-943c-4c76-831f-1cba9e363b8d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity\" from arxiv.org:",
      "generation": " In this paper, we propose a new active contour model for segmenting infrared images. We exploit the regional intensity homogeneity of infrared images to develop an energy driven active contour model. Difference of Gaussian filter is used to estimate the intensity inhomogeneity of the infrared images. A multi-feature energy driven active contour model is then developed. The proposed method is evaluated on synthetic and real infrared images and compared with three state-of-the-art active contour models. The experimental results demonstrate the efficacy of the proposed method."
    },
    {
      "id": "6c9a0aed-009e-41ea-b715-ea38d6bb4da0",
      "adv_source_id": "6c9a0aed-009e-41ea-b715-ea38d6bb4da0",
      "source_id": "9eda5549-966a-4eb7-bdbb-572dc585a4d3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel approach for few-shot learning of 3D medical image segmentation based on bidirectional recurrent neural networks (BiRNNs). Our method utilizes a pre-trained convolutional neural network (CNN) for feature extraction and a BiRNN for learning temporal dependencies between frames in a video. We introduce a novel few-shot learning framework that combines both global and local information for segmenting 3D medical images. Our global information is derived from the pre-trained CNN, while our local information is generated by the BiRNN. We demonstrate the effectiveness of our approach on the task of few-shot learning for 3D medical image segmentation.\n\nOur experimental results show that our method achieves state-of-the-art performance on the MRI skull stripping dataset and is competitive with other methods on the X-ray chest dataset. Furthermore, we present an analysis of the impact of different design choices on the performance of our method."
    },
    {
      "id": "437ff941-b262-421a-8229-464874bd231c",
      "adv_source_id": "437ff941-b262-421a-8229-464874bd231c",
      "source_id": "9eda5549-966a-4eb7-bdbb-572dc585a4d3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel approach for few-shot learning of 3D medical image segmentation based on bidirectional recurrent neural networks (RNNs). Our method leverages the inherent temporal coherence of 3D medical images to learn a mapping from input features to segmentation labels. We introduce a variant of the recently proposed few-shot learning via meta-learning (FLAME) algorithm that uses a bidirectional RNN as the meta-learner. Our experiments show that the bidirectional RNN model significantly improves the performance of the FLAME method on the 3D medical image segmentation task, and yields state-of-the-art results on the MRI segmentation dataset ISLES. Code is available at https://github.com/barrachina/few-shot-medical-image-segmentation."
    },
    {
      "id": "4071381f-989a-4393-9dc8-6ee822afa598",
      "adv_source_id": "4071381f-989a-4393-9dc8-6ee822afa598",
      "source_id": "8054ca70-ccbe-43ec-9597-802980689d39",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution",
      "prompt": "The following is the full text of the abstract for a research paper titled \"w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution\" from arxiv.org:",
      "generation": " w-Net: A Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolutional Neural Network. \n\nIn this paper, we propose a novel approach for medical image segmentation based on a deep learning model. Our method is based on a multi-dimensional attention mechanism and a cascade of multi-scale convolutional layers. This allows us to capture complex spatial and contextual information in medical images, resulting in accurate and robust segmentation. We evaluate our method on three different medical image segmentation datasets, and show that it outperforms the state-of-the-art in all cases. Our method can be generalized to other medical image segmentation tasks, and we believe it will be a valuable tool for medical image analysis. \n\nThe proposed method consists of a convolutional neural network (CNN) with a multi-dimensional attention mechanism and a cascade of multi-scale convolutional layers. The CNN is trained on a large dataset of medical images and their corresponding segmentations. The multi-dimensional attention mechanism allows the model to focus on different aspects of the input image, while the cascade of multi-scale convolutional layers enables the model to capture complex spatial and contextual information.\n\nThe proposed method was evaluated on three different medical image segmentation datasets: the ISBI 2017 Challenge on Mitosis Detection, the Camelyon 2017 Challenge on Breast Cancer Metastasis Detection, and the MedIA 2017 Challenge on Pancreas Tumor Segmentation. The results show that the proposed method outperforms the state-of-the-art in all cases.\n\nThe proposed method has the advantage of being able to handle large and complex datasets, and can be generalized to other medical image segmentation tasks. It can be used as a stand-alone tool for medical image analysis, or as a pre-processing step for other medical image analysis tasks such as classification or detection."
    },
    {
      "id": "b805c05d-3066-4783-af16-eb341296bedb",
      "adv_source_id": "b805c05d-3066-4783-af16-eb341296bedb",
      "source_id": "8054ca70-ccbe-43ec-9597-802980689d39",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution",
      "prompt": "The following is the full text of the abstract for a research paper titled \"w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution\" from arxiv.org:",
      "generation": " w-Net: A Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution Kernel Selection for Peripheral Lung Nodules Avalanche inhibition and the percolation threshold permeate many disciplines, ranging from physics and geophysics to the analysis of complex networks. In this paper, we use the concept of avalanche inhibition for nodule segmentation in lung CT images. In particular, we introduce a new deep learning architecture, called w-Net, that is based on a multi-dimensional attention mechanism and cascade multi-scale convolution. w-Net does not require any parameter tuning as it automatically learns the optimal number of segments (or classes) for each input image directly in the network. We evaluate our approach on 420 CT slices from the NLST dataset, as well as on 210 slices from the LUNA16 dataset. Our model achieves a dice similarity coefficient of 0.81\\lparen\\mathrm{ NLST}\\rparen and 0.84\\lparen\\mathrm{ LUNA16}\\rparen on the test set, while keeping a low false positive rate."
    },
    {
      "id": "6578709b-0864-4f1e-830d-963aab958924",
      "adv_source_id": "6578709b-0864-4f1e-830d-963aab958924",
      "source_id": "b879a41c-044d-4adf-8404-796a1f66bc0f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Disentangle, align and fuse for multimodal and semi-supervised image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Disentangle, align and fuse for multimodal and semi-supervised image segmentation\" from arxiv.org:",
      "generation": " Recent progress in image segmentation is largely driven by large-scale supervised learning with abundant labeled data. However, in many real-world applications, obtaining abundant labeled data is difficult, and a supervised learning paradigm is therefore insufficient. In this paper, we present a new framework that leverages not only labeled data but also other modalities (e.g., unlabeled images and point clouds) for image segmentation. We disentangle the shared and individual information among different modalities to achieve segmentation by aligning and fusing the disentangled information. The key challenge lies in the unlabeled images and point clouds, which may contain severe misalignment and large domain gaps. To handle it, we propose a novel geometric alignment method to establish a coherent feature space across different modalities. Then, we exploit the geometric structures of the point clouds and the spatial-temporal coherence of videos to propagate the labeled information to multiple modalities for segmentation. Extensive experiments on public datasets (e.g., SceneNN and NYUR) show the superiority of our approach over the state-of-the-art."
    },
    {
      "id": "0066f1f2-fb0a-42de-b92d-0728c9c2e7fd",
      "adv_source_id": "0066f1f2-fb0a-42de-b92d-0728c9c2e7fd",
      "source_id": "b879a41c-044d-4adf-8404-796a1f66bc0f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Disentangle, align and fuse for multimodal and semi-supervised image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Disentangle, align and fuse for multimodal and semi-supervised image segmentation\" from arxiv.org:",
      "generation": " Recent progress in image segmentation is largely driven by large convolutional neural networks (CNNs) trained on massively annotated images. However, manual segmentation is tedious and costly, preventing its application to multimodal and semi-supervised segmentation, which would benefit from multiple sources of information. We present a modular framework for advancing semi-supervised and multimodal segmentation. Given a supervised learning algorithm, our framework can enhance its performance with additional information from multiple sources like images, edges, and regions. It can also adapt a supervised algorithm to a semi-supervised setting, and vice versa. We implement our framework as an extension of a state-of-the-art image segmentation network, and evaluate it on multimodal and semi-supervised segmentation benchmarks. Our method achieves semi-supervised segmentation performance on the PASCAL VOC 2012 dataset that is within 1.5% of fully supervised performance, and beats prior work by a large margin. For multimodal segmentation, our method outperforms state-of-the-art by a large margin, achieving 84.3% on the PASCAL VOC 2012 dataset."
    },
    {
      "id": "5ff3313f-0baf-4c58-9c5f-1f7b0e04ba6d",
      "adv_source_id": "5ff3313f-0baf-4c58-9c5f-1f7b0e04ba6d",
      "source_id": "c840fcd3-26ff-40d9-8054-692fa48b1d37",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision\" from arxiv.org:",
      "generation": " In this paper, we present a teacher-student framework with an attention module for semi-supervised medical image segmentation from mixed supervision. We train a teacher model using only labeled images, and then use the pre-trained teacher model to generate pseudo labels for unlabeled images. We propose an attention module to weigh the importance of different information for segmentation. We optimize the student model with both labeled and pseudo labeled data, and the proposed attention module further improves the segmentation performance. In addition, we investigate the use of multiple teachers for the purpose of knowledge transfer. The experimental results on three different public lung segmentation benchmarks, using mixed supervision of CT and MRI scans, show that our method outperforms the state-of-the-art approaches."
    },
    {
      "id": "3a4cfcb3-52f6-4de0-969f-f601f9aacb8d",
      "adv_source_id": "3a4cfcb3-52f6-4de0-969f-f601f9aacb8d",
      "source_id": "c840fcd3-26ff-40d9-8054-692fa48b1d37",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision\" from arxiv.org:",
      "generation": " In this paper, we present a teacher-student framework that leverages mixed supervision for medical image segmentation. Our method combines labelled and unlabelled data for training, which is beneficial due to the significant cost and labour involved in manually labelling medical images. We propose to use a teacher model that is trained on labelled data to provide pseudo-labels for unlabelled data, and then use these pseudo-labels to train a student model. Our experimental results on three different medical image segmentation datasets show that our method can achieve state-of-the-art performance. We further analyze the importance of the teacher and student models' architectures, as well as the impact of the amount of labelled data on performance. \n\n## 1. Introduction \n\nMedical image segmentation is a crucial step in the analysis of anatomical structures and the diagnosis of diseases. The goal of segmentation is to classify each voxel in an image into one of several predefined classes. This leads to a binary or multi-class segmentation mask that can be used for further analysis.\n\nTraditional medical image segmentation methods rely on manually labelled training data, which is expensive and time-consuming to collect. This is particularly challenging for rare diseases or cases where there is limited access to clinical experts.\n\nRecently, semi-supervised learning has been proposed as a method for training medical image segmentation models using a combination of labelled and unlabelled data. The idea is to use the labelled data to learn a mapping function that projects the unlabelled data into a higher-dimensional feature space, where it can be used to train a segmentation model.\n\nIn this paper, we present a teacher-student framework that leverages mixed supervision for medical image segmentation. Our method combines labelled and unlabelled data for training, which is beneficial due to the significant cost and labour involved in manually labelling medical images. We propose to first use a teacher model (trained with labelled data) to provide pseudo-labels for unlabelled data, and then use these pseudo-labels to train a student model.\n\n## 2. Method \n\n### 2.1. Teacher Model\n\nThe teacher model is a convolutional neural network (CNN) trained on labelled data. During training, the teacher model learns to classify medical images into one of several predefined classes. \n\n### 2.2. Student Model\n\nThe student model is also a CNN trained on a combination of labelled and unlabelled data. The labelled data is used to learn the mapping function that projects the unlabelled data into a higher-dimensional feature space, where it can be used to train"
    },
    {
      "id": "10a24be8-1ea4-4c97-83ed-6360d26d1d68",
      "adv_source_id": "10a24be8-1ea4-4c97-83ed-6360d26d1d68",
      "source_id": "ca413b45-4e39-4c31-a986-9d8f38c02a9f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network",
      "prompt": "The following is the full text of the abstract for a research paper titled \"HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\" from arxiv.org:",
      "generation": " HS-ResNet employs a novel hierarchical-split (HS) block that performs well in the computer vision tasks. In addition, we systematically study the effectiveness of the HS block compared with the state-of-the-art models, in which the HS block is inserted into the pre-trained ResNet models, including ResNet-18, ResNet-34, and ResNet-50. In the experiment, we evaluate the performance of our proposed models on ImageNet-1k dataset, and the results show that our proposed models achieve state-of-the-art performance."
    },
    {
      "id": "a7523227-eb8d-467e-b3f8-b43147b8b62c",
      "adv_source_id": "a7523227-eb8d-467e-b3f8-b43147b8b62c",
      "source_id": "ca413b45-4e39-4c31-a986-9d8f38c02a9f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network",
      "prompt": "The following is the full text of the abstract for a research paper titled \"HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\" from arxiv.org:",
      "generation": " ArXiv abstract:\n\nHS-ResNet is a novel approach of modifying ResNet to make it more resilient to indistinguishable junk (ADJ) attacks. By inserting a hierarchical-split block into ResNet, we make the network more robust by preventing the adjacent parameters from being correlated, then the network becomes more resilient to ADJ attacks. We evaluate the proposed method on CIFAR-10 and ImageNet datasets, and the result shows that our approach achieves a better trade-off between accuracy and resilience compared with other state-of-the-art methods specialized in resisting ADJ attacks. The source code and models will be released."
    },
    {
      "id": "d120a397-366f-48aa-94e1-8df570adf371",
      "adv_source_id": "d120a397-366f-48aa-94e1-8df570adf371",
      "source_id": "9459e172-bc0f-48bf-b238-eab238e95ac2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps",
      "prompt": "The following is the full text of the abstract for a research paper titled \"MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps\" from arxiv.org:",
      "generation": " Recent results on semantic segmentation largely benefited from the use of large datasets and powerful models, such as Encoder-Decoder based models like U-Net or more recent object detection architecture adaptations. However, acquiring a large amount of labeled data is expensive and time-consuming, especially when only a small amount of initial training data is available. In this context, active learning appears to be a promising solution. Nevertheless, most of the state-of-the-art active learning methods for semantic segmentation are based on point-based approaches, which require a dense annotation for each pixel, and often rely on heuristics that may not perform well in many cases. In this paper, we present MetaBox+, a new region-based active learning approach for semantic segmentation. MetaBox+ leverages an innovative priority map that is built from both a global and local perspective, and a novel sampling strategy that aims to minimize the risk of missing hard positive and negative examples. We conduct experiments on a recently introduced dataset, called SceneCrafter, which contains pixel-wise annotations for about 800 images. We show that our method outperforms the state-of-the-art point-based active learning methods for semantic segmentation."
    },
    {
      "id": "0641314e-1696-48ee-b4ad-ea1a31548892",
      "adv_source_id": "0641314e-1696-48ee-b4ad-ea1a31548892",
      "source_id": "9459e172-bc0f-48bf-b238-eab238e95ac2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps",
      "prompt": "The following is the full text of the abstract for a research paper titled \"MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps\" from arxiv.org:",
      "generation": " Recent results on semantic segmentation, achieved with deep learning models, largely depend on very large labeled datasets. Nevertheless, labeling images is expensive, time consuming, and requires skilled workers. This is particularly true when dealing with fine-grained segmentation maps, as in instance-based segmentation or pixel-level labeling of semantic classes. One way to reduce the label effort is to use active learning, where the model itself can choose the most informative samples to be annotated. However, most of the state-of-the-art active learning methods for semantic segmentation are instance based, deciding independently for each instance which ones to annotate, and they do not explicitly take into account the spatial information. In this work, we present a new region based active learning framework for semantic segmentation that uses priority maps. Our method automatically discovers parcels of homogeneous regions of the priority maps, and uses them to choose the most informative regions to be annotated. We instantiate our method with a specific priority map generation algorithm based on a deep clustering model. We show that our method MetaBox+ is able to reduce the amount of required annotations significantly, while improving performance w.r.t. the baseline model that uses all the available annotations."
    },
    {
      "id": "d25d3d35-fd93-4159-a732-bf59325e5e8e",
      "adv_source_id": "d25d3d35-fd93-4159-a732-bf59325e5e8e",
      "source_id": "256e9cc8-7739-48cc-aba4-30a8840090f0",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images\" from arxiv.org:",
      "generation": " Interactive image segmentation differs from automatic segmentation in that it allows the user to impose his or her prior knowledge on the segmentation process. In many real-world applications, the user has specific prior knowledge about the regions of interest in the image and can provide initial segmentations. Once an initial segmentation is provided, an interactive segmentation algorithm propagates this prior information in order to produce a final segmentation. The success of the interactive segmentation algorithm depends on the accuracy of the initial segmentation and the robustness of the propagation process. In this paper, we propose a new interactive image segmentation algorithm that combines advantages of gradient-based and texture-based approaches to segmentation. In particular, we propagate user markings based on image gradients when the image regions have distinct gradients, and based on image texture when gradients are absent. Furthermore, to the best of our knowledge, we are the first to address the problem of interactive segmentation of patterned images, i.e., images in which texture is the dominant factor in the scene and for which classical texture-based approaches do not work well. We demonstrate the robustness of our approach on a database of patterned images."
    },
    {
      "id": "74b02c3c-2a1c-4efe-bb38-03092fe3a5ca",
      "adv_source_id": "74b02c3c-2a1c-4efe-bb38-03092fe3a5ca",
      "source_id": "256e9cc8-7739-48cc-aba4-30a8840090f0",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images\" from arxiv.org:",
      "generation": " Interactive image segmentation nowadays faces the challenge of producing meaningful segmentations of images in a timely manner. To address this issue, we propose a method to propagate user markings in a content-based manner. Our approach incorporates a statistical model for each pixel based on the image histogram that enables the system to make reasonable decisions when propagating labels. We demonstrate that our approach achieves state-of-the-art performance on multiple benchmarks while being several times faster than the fastest alternative. In addition, we introduce a new benchmark for interactive segmentation with significant segmentation complexity."
    },
    {
      "id": "7b46fa7d-8c32-481a-9b3f-08ac270de2c8",
      "adv_source_id": "7b46fa7d-8c32-481a-9b3f-08ac270de2c8",
      "source_id": "e84fe950-80c8-4cab-ac0a-f2d2232477b3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to segment microscopy images with lazy labels",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning to segment microscopy images with lazy labels\" from arxiv.org:",
      "generation": " Recent works have shown that training powerful semantic segmentation models with limited amounts of labeled data is possible by leveraging large amounts of unlabeled data. However, manually labeling pixels in microscopy images is expensive, and relying on expensive labels for training has so far limited the amount of training data that can be used. We propose a simple yet effective method for training on limited amounts of manually labeled data by learning to segment images using a small number of \"lazy\" labels (i.e. labels where only a small subset of pixels within an image are manually annotated). We show that lazy labels can be used to learn a model that generalizes well to the entire image and evaluate our method on two different microscopy image segmentation benchmarks, achieving state-of-the-art performance."
    },
    {
      "id": "5fd7a3d1-a9cf-4055-b95f-ab8a3ce6bac9",
      "adv_source_id": "5fd7a3d1-a9cf-4055-b95f-ab8a3ce6bac9",
      "source_id": "e84fe950-80c8-4cab-ac0a-f2d2232477b3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to segment microscopy images with lazy labels",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning to segment microscopy images with lazy labels\" from arxiv.org:",
      "generation": " Recent works have shown that training powerful segmentation models for microscopy images with supervised learning requires a large amount of manually annotated pixel-level ground truth, which is expensive to obtain. We propose an approach for training segmentation models without requiring any ground truth during training. Our approach consists of a teacher model that is trained with ground truth, and a student model that is trained with the teacher's predictions as ground truth. At test time, the student model produces the final results. Our main contribution is a new kind of regularizer that encourages the teacher model to produce teachable predictions: in addition to predicting segmentation labels, the teacher is asked to predict \u201clazy labels\u201d, which are random vectors that can be used as ground truth for the student model. We show that this lazy labeling approach significantly improves the segmentation quality of the student model, and that this improvement increases with the number of lazy labels. We demonstrate the effectiveness of our approach on a publicly available digital pathology dataset, as well as a challenging new dataset of microscopy images from kidney biopsies."
    },
    {
      "id": "b5146dff-3b1c-47cb-ae60-34384d59edbc",
      "adv_source_id": "b5146dff-3b1c-47cb-ae60-34384d59edbc",
      "source_id": "bf7d13ac-ed40-46b5-a0a4-fae8f7e8e6b0",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mask-guided sample selection for Semi-Supervised Instance Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mask-guided sample selection for Semi-Supervised Instance Segmentation\" from arxiv.org:",
      "generation": " Recent semi-supervised instance segmentation methods mostly focus on modeling spectral or spatial relationships between pixels in different images through consistency and/or similarity learning. These methods either treat all pixels within an object as a unit or assign labels independently at each pixel without considering their spatial coherence, which limits their ability to capture complex object structures. In this paper, we propose a Mask-guided Sample Selection (MSS) approach for training a semi-supervised pixel-level classifier. Given a pixel of interest, our method selects its class-consistent neighboring pixels as semantic contextual samples through a learned mask. We pose the learning of the mask as a binary classification problem, where the mask is the decision boundary between the foreground (object) and background (rest of the image). Specifically, given an initial guess of the mask, we minimize the cross-entropy loss by iteratively refining the mask through gradient descent. We optimize the mask in a bottom-up fashion, starting from the pixels with the highest response and traversing the areas with the most uncertain labels. We evaluate MSS on four semi-supervised segmentation benchmarks, achieving state-of-the-art performance."
    },
    {
      "id": "4dabb958-46e1-41d5-bcb7-07a11565c55f",
      "adv_source_id": "4dabb958-46e1-41d5-bcb7-07a11565c55f",
      "source_id": "bf7d13ac-ed40-46b5-a0a4-fae8f7e8e6b0",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mask-guided sample selection for Semi-Supervised Instance Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mask-guided sample selection for Semi-Supervised Instance Segmentation\" from arxiv.org:",
      "generation": " Recent semi-supervised instance segmentation (IS) methods mostly focus on mining object-level annotations from label maps, which may fail when the object instances are rare or when the labels are noisy. In this paper, we argue that a significant amount of implicit instance-level information lies within the instance masks, which is underutilized thus far. We find that for each instance, not all pixels are created equal -- instance masks often contain salient structure and context information around instances that can be harnessed for recognizing instances in new images. In this paper, we propose a Mask-guided Sample Selection (MSS) approach for enhancing the quality of instance-level pseudo labels, and hence improving semi-supervised IS performance. Our key insight is that by sampling training images that contain instances similar in appearance to the instances in a given query mask (e.g., the union of the positive masks in COCO), we can obtain more reliable pseudo labels for these instances. We demonstrate the effectiveness of MSS on various state-of-the-art semi-supervised IS methods. Our approach significantly improves the mAP by +7.3% and +5.4% on VOC2012 and COCO datasets, respectively."
    },
    {
      "id": "26bd6721-5360-499d-80e8-59eca5f28932",
      "adv_source_id": "26bd6721-5360-499d-80e8-59eca5f28932",
      "source_id": "f9bd0e9d-1fba-4d46-a478-4c128430e1a3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data",
      "prompt": "The following is the full text of the abstract for a research paper titled \"PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data\" from arxiv.org:",
      "generation": " Current clinical practice for cardiac assessment in patients with congenital heart disease (CHD) is mostly based on 2D evaluations of the cardiac chambers and great vessels. However, a more comprehensive assessment would additionally analyze the cardiac walls to measure cardiac muscle thickness and evaluate the presence of wall-related CHD. A standard clinical practice to assess the cardiac walls is to manually segment the cardiac chambers in MIP views and then measure wall thicknesses in a few selected views. This approach is time-consuming, subjective, and does not provide comprehensive information on the cardiac wall thicknesses. In this paper, we present a deep learning-based framework for automatic reconstruction and segmentation of the full cardiac walls in 3D from a single CT scan. Our framework learns to jointly reconstruct the whole cardiac volume and segment the cardiac walls in a fully-supervised learning framework. We introduce a novel deep learning architecture, namely PC-U net, to address the aforementioned tasks. The proposed method is evaluated on 20 clinical CT scans (including healthy and pathological cases), and the results show that the PC-U net is capable of reconstructing and segmenting the full cardiac walls with an accuracy that is sufficient for clinical evaluation. The framework is implemented in clinical routine at our institute since 2019."
    },
    {
      "id": "c7fc8e9c-cebd-4fca-9fe0-110025583e62",
      "adv_source_id": "c7fc8e9c-cebd-4fca-9fe0-110025583e62",
      "source_id": "f9bd0e9d-1fba-4d46-a478-4c128430e1a3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data",
      "prompt": "The following is the full text of the abstract for a research paper titled \"PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data\" from arxiv.org:",
      "generation": " Abstract: Reconstruction and segmentation of the cardiac walls in 3D from CT data is a fundamental step in cardiac analysis. Existing methods rely on a priori information about either the shape of the heart or the arrival time of the contrast agent. However, such information is rarely available in clinical practice. In this paper, we present a fully automatic method to reconstruct and segment the cardiac walls in 3D, without any a priori information. Our method learns a deep neural network that directly regresses the 3D volume of the cardiac walls from the raw CT data. To deal with the limited amount of training data, we propose to use a shape prior, which is learned from a set of synthetic shapes. We evaluate our method on a public benchmark dataset, and show that it achieves state-of-the-art performance."
    },
    {
      "id": "2390f12d-f4d2-4ed4-b452-8eac440623ed",
      "adv_source_id": "2390f12d-f4d2-4ed4-b452-8eac440623ed",
      "source_id": "ce9b46ec-d192-4df9-9131-0a0851f72029",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image segmentation via Cellular Automata",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Image segmentation via Cellular Automata\" from arxiv.org:",
      "generation": " A new method for image segmentation via Cellular Automata is presented in this paper. It is based on a simple yet effective technique for the detection of edges and regions in binary images. The method is motivated by the observation that edges in binary images often correspond to transitions between regions of different colors or intensities. The proposed method utilizes this property to partition an image into a set of regions, each of which is characterized by a distinct set of colors or intensities. The method is simple, efficient, and easy to implement, and it can be used to segment a wide variety of binary images, including those with complex shapes and structures. The experimental results presented in this paper demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "dfe6cf3d-39af-4dd5-a7f7-9b25ccf8b966",
      "adv_source_id": "dfe6cf3d-39af-4dd5-a7f7-9b25ccf8b966",
      "source_id": "ce9b46ec-d192-4df9-9131-0a0851f72029",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image segmentation via Cellular Automata",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Image segmentation via Cellular Automata\" from arxiv.org:",
      "generation": " Images are commonly represented as arrays of pixel values, however, such a representation details an immense amount of spatial information, much of which is redundant. This paper details a method of segmenting images using Cellular Automata, enabling images to be efficiently represented as a collection of regions, bound by a finite number of pixels, allowing for compression of the image via the elimination of redundant information. The performance of the algorithm was evaluated using a number of quantitative metrics detailed in the paper."
    },
    {
      "id": "b8e4ed3b-e2fa-4c50-b2af-23c5b3cb4b6c",
      "adv_source_id": "b8e4ed3b-e2fa-4c50-b2af-23c5b3cb4b6c",
      "source_id": "c0292110-f6e8-4a0f-a23c-1baafe82481c",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field\" from arxiv.org:",
      "generation": " In this paper, we propose a new polarimetric SAR image segmentation method based on 3D discrete wavelet transform (DWT) and Markov random field (MRF). The proposed method extracts texture information from polarimetric SAR images in various scales using 3D DWT. The MRF is then applied to model the spatial dependence of the segmented images. The experimental results show that the proposed method achieves better segmentation performance in terms of accuracy and speed, compared to the state-of-the-art methods. The proposed method is also validated on a polarimetric SAR image dataset from the literature.\n\nExtraction of texture information from polarimetric SAR images is a crucial step for various applications such as object detection, classification, and change detection. The polarimetric SAR images often contain complex texture information that is challenging to extract using traditional image processing techniques. In this paper, we propose a new method based on 3D DWT to extract texture information from polarimetric SAR images in various scales. The 3D DWT is a multi-scale, multi-dimensional image processing technique that has been widely used for image analysis and classification.\n\nThe proposed method extracts texture information from polarimetric SAR images in various scales using 3D DWT. The extracted texture features are then used to train a MRF model that models the spatial dependence of the segmented images. The MRF model is a probabilistic model that captures the spatial dependence of the segmented images using a set of random variables. The experimental results show that the proposed method achieves better segmentation performance in terms of accuracy and speed, compared to the state-of-the-art methods. The proposed method is also validated on a polarimetric SAR image dataset from the literature.\n\nThe proposed method offers several advantages over the state-of-the-art methods. First, the proposed method is able to extract texture information from polarimetric SAR images in various scales using 3D DWT, which allows for more accurate segmentation. Second, the proposed method is able to model the spatial dependence of the segmented images using MRF, which allows for more robust segmentation. Third, the proposed method is able to achieve better segmentation performance in terms of accuracy and speed, compared to the state-of-the-art methods."
    },
    {
      "id": "426f3b9c-5cb0-4f3c-a53a-261e30b2766b",
      "adv_source_id": "426f3b9c-5cb0-4f3c-a53a-261e30b2766b",
      "source_id": "c0292110-f6e8-4a0f-a23c-1baafe82481c",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field\" from arxiv.org:",
      "generation": " In this paper, we propose a new polarimetric SAR image segmentation method based on 3D discrete wavelet transform (DWT) and Markov random field (MRF). The proposed method can accurately extract various objects in polarimetric SAR image and overcome the difficulties of traditional methods in segmenting complex images with strong texture information. Firstly, the polarimetric SAR image is decomposed into different frequency bands by 3D DWT. Then, the segmentation is performed by MRF on the wavelet coefficients. The proposed method is evaluated on three public polarimetric SAR image datasets. The experimental results show that the proposed method achieves better performance in terms of precision and recall, and the segmentation results are more visually satisfactory."
    },
    {
      "id": "66ab6b88-e521-4d2e-ad42-f7e9e3cc23fb",
      "adv_source_id": "66ab6b88-e521-4d2e-ad42-f7e9e3cc23fb",
      "source_id": "b34196e2-ce17-4cc3-bddf-9dbec580bda5",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep learning for image segmentation: veritable or overhyped?",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep learning for image segmentation: veritable or overhyped?\" from arxiv.org:",
      "generation": " Deep learning has been demonstrating state-of-the-art performance in various computer vision tasks, including image segmentation. However, whether deep learning is the best choice for image segmentation remains questionable. This paper compares deep learning-based image segmentation with other classical image segmentation methods, including edge-based segmentation, region-based segmentation, and hybrid methods, on various metrics. To perform a fair comparison, the same evaluation metrics and data sets are used for all methods. The comparison results reveal that deep learning-based methods perform better in terms of accuracy but not in terms of runtime. Furthermore, the state-of-the-art deep learning-based methods do not outperform classical methods by a large margin, which questions whether deep learning is necessary for image segmentation."
    },
    {
      "id": "5af24ca0-1977-4037-9b21-ffd30f9aa2c1",
      "adv_source_id": "5af24ca0-1977-4037-9b21-ffd30f9aa2c1",
      "source_id": "b34196e2-ce17-4cc3-bddf-9dbec580bda5",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep learning for image segmentation: veritable or overhyped?",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep learning for image segmentation: veritable or overhyped?\" from arxiv.org:",
      "generation": " Deep learning has been proving itself as a powerful tool in computer vision for the past few years, and it seems that there is no task it cannot tackle. However, the perceived limit of its abilities is often extrapolated from the advancement of specific sub-fields, like image classification and object detection. When assessing the applicability of deep learning to other tasks, one must ask themselves: are these technologies transferable to different scenarios, or are they bound to be specialized tools? In this paper, we investigate deep learning applied to medical image segmentation, a classic problem in computer vision with a vast literature. We compare state-of-the-art methods against classical baselines, as well as each other, in order to assess the benefit of deep learning. We show that deep learning methods do not always perform optimally, and that classical methods still have merit. We also provide empirical evidence that a simpler model can outperform state-of-the-art when given more labeled data."
    },
    {
      "id": "c12f86a6-4eeb-4ec3-bf85-5ed2765428fb",
      "adv_source_id": "c12f86a6-4eeb-4ec3-bf85-5ed2765428fb",
      "source_id": "656f2cf4-7ba8-466e-bd2b-85c1a64fc56b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images\" from arxiv.org:",
      "generation": " Recent works on 3D semantic segmentation largely focus on natural images, where large-scale training data is readily available. In contrast, medical images provide critical information for clinical diagnosis, but they pose significant challenges for 3D semantic segmentation due to the scarcity of labeled training data and the uniqueness of anatomical structures' shapes. In this paper, we present a shape-aware semi-supervised 3D semantic segmentation framework that utilizes the shape information of structures and leverages available unlabeled data from multiple sources for training. We propose a novel shape-aware loss function that encourages the network to produce segmentations with coherent and meaningful shapes. Furthermore, we introduce a data augmentation module that synthetically generates additional training samples with anatomical structure shapes from unlabeled data. We evaluate our method on two public available abdominal CT datasets with limited labeled data and achieve promising results compared to the state-of-the-art."
    },
    {
      "id": "34bf0724-46d9-42ed-8bee-3fdd61d414bd",
      "adv_source_id": "34bf0724-46d9-42ed-8bee-3fdd61d414bd",
      "source_id": "656f2cf4-7ba8-466e-bd2b-85c1a64fc56b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images\" from arxiv.org:",
      "generation": " Recent progress in 3D segmentation has led to methods that utilize geometric and structural information for accurate localization of objects in 3D medical images. However, most of these methods rely on fully supervised training, which requires a significant amount of manually labeled volume data. This is a laborious and expensive process, especially in the medical domain where the acquisition of labeled images is costly and time-consuming. In this paper, we present a semi-supervised approach for 3D semantic segmentation of medical images. Our method utilizes geometric and structural information in a shape-aware network that leverages both labeled and unlabeled data for training. We introduce a new loss function that combines supervised and self-supervised learning in a unified framework. We show that our method is able to effectively capture complex shape information, resulting in improved segmentation performance compared to state-of-the-art semi-supervised methods. We evaluate our method on a challenging dataset of abdominal CT volumes and show that it achieves competitive results compared to fully supervised methods while requiring less than 25% of the labeled data."
    },
    {
      "id": "91e7ecf7-35af-447d-96bd-bb7cdfd47abd",
      "adv_source_id": "91e7ecf7-35af-447d-96bd-bb7cdfd47abd",
      "source_id": "57782d19-642c-44e0-8ebc-1408904b770e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Image Labelling at Pixel Level",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Automatic Image Labelling at Pixel Level\" from arxiv.org:",
      "generation": " Recent advances in deep learning have enabled automatic image labelling at superpixel level. However, most of these methods rely on some prior knowledge about the images such as large amounts of paired superpixel and label annotations or a fixed set of object classes. In this paper, we present a method for automatic image labelling at pixel level that is completely data-driven, needing only a small amount of image-level labels to train. Our method is based on a simple yet effective approach: we generate pseudo labels at the pixel level by aggregating predictions from multiple deep learning models, and then use these pseudo labels to train a convolutional neural network for pixel-level labelling. We demonstrate the effectiveness of our method on the VOC 2012 semantic segmentation benchmark, where we achieve a mIOU of 0.542 on the test set, compared to 0.441 with the baseline method."
    },
    {
      "id": "bbf4c815-3f48-4cd0-b96e-ffcbcab2e09f",
      "adv_source_id": "bbf4c815-3f48-4cd0-b96e-ffcbcab2e09f",
      "source_id": "57782d19-642c-44e0-8ebc-1408904b770e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Image Labelling at Pixel Level",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Automatic Image Labelling at Pixel Level\" from arxiv.org:",
      "generation": " Abstract: We present a method for automatic image labelling at pixel level. It is based on a feed-forward neural network that is trained using a supervised learning approach. The network is composed of eight layers, where the first five are convolutional layers and the remaining are fully connected layers. The objective function is composed of three different losses: a localization loss, a classification loss, and an edge-preserving loss. The localization loss is based on the distance between the predicted and ground truth labels, while the classification loss is based on the cross-entropy error. The edge-preserving loss is used to keep the boundaries of the predicted labels sharp. The network is trained on a dataset of 50,000 images and the performance is evaluated on a separate test set of 1,000 images. The results show that the method is able to achieve a precision of 92% and a recall of 87% on the test set."
    },
    {
      "id": "f38d4320-2075-43c2-92a6-4f97dd7b0a1d",
      "adv_source_id": "f38d4320-2075-43c2-92a6-4f97dd7b0a1d",
      "source_id": "400c2edc-21b5-42aa-a700-335c3fce8264",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis\" from arxiv.org:",
      "generation": " Recent progress in wearable sensors and IoT technology enables the measurement of human-tool interactions in the workplace. In this paper, we present an uncertainty-based human-in-the-loop system for industrial tool wear analysis. We first design a wearable sensing module, which contains nine sensors, to measure the human-tool interaction simultaneously. Then, we propose a tool wear analysis method based on the proposed sensing module. The proposed method consists of four steps: measurement, preprocessing, feature extraction, and wear level estimation. In the measurement step, we simultaneously collect tactile, acoustic, and motion data during the human-tool interaction. In the preprocessing step, the collected data are preprocessed and fused to obtain a feature vector. In the feature extraction step, the uncertainty of each feature is estimated using a novel deep learning architecture, namely the unified adaptive mechanism (UAM). In the wear level estimation step, a support vector machine is utilized to estimate the wear level of the tool. We evaluate the proposed method using two public datasets and a real-world experiment. The experimental results demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "0a0545ed-07c6-454b-9f9f-94921f1ad209",
      "adv_source_id": "0a0545ed-07c6-454b-9f9f-94921f1ad209",
      "source_id": "400c2edc-21b5-42aa-a700-335c3fce8264",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis\" from arxiv.org:",
      "generation": " An active involvement of domain experts is important for effective analysis of tool wear data in the manufacturing industry. In this paper, we present an uncertainty-based human-in-the-loop system that provides effective visualization and interaction methods for tool wear data analysis. The system incorporates uncertainty in the form of error bars in the visualization and utilizes interactive methods for error bar editing, allowing domain experts to express their uncertainty knowledge. A pilot study was conducted with two domain experts who analyzed tool wear data using the system. The study revealed the value of our system and provided insights for further development."
    },
    {
      "id": "3bd11622-5827-48ab-998e-a0a23bebc088",
      "adv_source_id": "3bd11622-5827-48ab-998e-a0a23bebc088",
      "source_id": "cffac8c8-68a7-4d71-8489-a40318541075",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation\" from arxiv.org:",
      "generation": " Recent progress in semantic segmentation largely benefited from large-scale labeled datasets and powerful models, which however are in short supply for medical image analysis, especially for domain-specific tasks like cardiac segmentation. In this work, we exploit the power of teacher networks to alleviate the lack of labeled examples for training. We propose a dual-teacher learning framework that integrates both intra-domain and inter-domain teachers. The former refines the segmentation results within the same domain while the latter leverages labeled data from different domains. To effectively combine the complementary information from both teachers, we introduce a confidence-weighted fusion strategy that flexibly weighs the teacher predictions based on their confidence levels. We evaluate our method on the CAMELYON 2017 challenge dataset for cardiac segmentation, and achieve promising results compared to the state-of-the-art methods. Our code and models are available at https://github.com/VITA-Group/Dual-Teacher."
    },
    {
      "id": "02a0d565-c724-44c1-af7d-dbd94a4da0f1",
      "adv_source_id": "02a0d565-c724-44c1-af7d-dbd94a4da0f1",
      "source_id": "cffac8c8-68a7-4d71-8489-a40318541075",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation\" from arxiv.org:",
      "generation": " Recent progress in semantic segmentation largely benefitted from large-scale annotated datasets. However, annotating medical images remains a challenging task, due to the costly and tedious nature of the labeling process. In this work, we aim to reduce the amount of required annotations for training a cardiac segmentation model. We introduce a teacher-student paradigm with two teachers: an intra-domain teacher and an inter-domain teacher. The intra-domain teacher is a segmentation model trained on a small number of annotated images, while the inter-domain teacher is a pre-trained model on a different dataset (e.g.,segmentation of natural objects). We propose a new loss function that leverages the outputs from both teachers to guide the training of our model. With a significantly reduced amount of annotation, our method achieves state-of-the-art performance on the Cardiac swag segmentation challenge."
    },
    {
      "id": "7350c855-a01c-44c3-94e5-971f5ac61185",
      "adv_source_id": "7350c855-a01c-44c3-94e5-971f5ac61185",
      "source_id": "a3ac3bc2-73ce-4a07-90a0-8eb60b5215f1",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Hyperbolic Convolutional Neural Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fully Hyperbolic Convolutional Neural Networks\" from arxiv.org:",
      "generation": " ArXiv displays the abstract of \"Fully Hyperbolic Convolutional Neural Networks\" as follows:\n\nWe introduce Hyperbolic Neural Networks (HyNNs), a family of continuous neural networks based on hyperbolic geometry, and we prove that HyNNs with analytic activations are infinitely wide and hence fully convergent. This answers in the affirmative an open question posed by Bengio et al. Additionally, we derive an expression for the gradient of the loss function with respect to the weights, which lets us prove that gradients are bounded in norm by a constant independent of the network width; this result is asymptotically optimal. We present an algorithm for training HyNNs, and show that the number of iterations required to converge is proportional to the network width to the power of \u22121. Our experiments show that HyNNs perform competitively with state-of-the-art deep learning algorithms on a variety of benchmark tasks, such as image classification and sentiment analysis.\n\nWould you like to know more about this paper? "
    },
    {
      "id": "759509ca-a272-425c-836b-c776b5bad7ff",
      "adv_source_id": "759509ca-a272-425c-836b-c776b5bad7ff",
      "source_id": "a3ac3bc2-73ce-4a07-90a0-8eb60b5215f1",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Hyperbolic Convolutional Neural Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fully Hyperbolic Convolutional Neural Networks\" from arxiv.org:",
      "generation": " ArXiv abstract:\n\nLearning in the realm of geometric deep learning has enabled numerous breakthrough developments, but only a few methods exploit the intrinsic geometry of the data, which remains an underutilized resource. In this work, we focus on a prominent geometric feature, namely curvature, and present a novel Fully Hyperbolic Convolutional Neural Network (FH-CNN) architecture that leverages the full hyperbolicity of data, for the purpose of learning deep representations. We show that fully hyperbolic neural networks provide a framework for learning from data endowed with a Riemannian structure, and illustrate this in two applications: geodesic regression on the Poincare model of the hyperbolic plane, and on the unit sphere of arbitrary dimension. Our experiments demonstrate that the FH-CNN algorithmically exploits the curvature information of the data, resulting in improved performance relative to state-of-the-art methods. \n\nThis report describes the implementation details and presents results on standard benchmark datasets, as well as its theoretical foundations."
    },
    {
      "id": "f08bd56f-b4c4-4ec5-b256-4d7fd3d3c6e3",
      "adv_source_id": "f08bd56f-b4c4-4ec5-b256-4d7fd3d3c6e3",
      "source_id": "e346ff27-0566-4875-93c4-2099b7a279d6",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation\" from arxiv.org:",
      "generation": " Semantic segmentation is a fundamental task in computer vision, with many applications in robotics and autonomous driving. However, this task becomes considerably more challenging in adverse weather conditions, such as heavy rain, fog, or snow. In this paper, we present a novel approach for robust semantic segmentation in such challenging scenarios. We exploit the temporal dimension by using a video-sequence segmentation framework, which allows us to capture the dynamics of the scene and mitigate the adverse effects of weather conditions. Our approach is based on a fast, yet effective segmentation algorithm that operates on video frames and produces a sequence of segment maps. These maps are then processed to obtain a final segmentation output. We demonstrate the effectiveness of our approach on the Cityscapes dataset, as well as on a new dataset we created to simulate adverse weather conditions. Our experiments show that our method achieves state-of-the-art performance in terms of segmentation accuracy, even in the presence of heavy rain or fog."
    },
    {
      "id": "613db671-64f5-4e0d-9b6c-36c22d525d74",
      "adv_source_id": "613db671-64f5-4e0d-9b6c-36c22d525d74",
      "source_id": "e346ff27-0566-4875-93c4-2099b7a279d6",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation\" from arxiv.org:",
      "generation": " Semantic segmentation is a fundamental task in computer vision, with many applications in robotics and autonomous driving. However, this task becomes considerably more challenging in adverse weather conditions, such as during heavy rain, fog, or snow. In this paper, we propose a new approach for robust semantic segmentation in such challenging scenarios. We exploit the temporal dimension by using a video-sequence segmentation network. This network takes a sequence of frames as input and predicts a mask for each frame. We formulate the segmentation problem as a temporal consistent labeling task, such that the predicted masks of adjacent frames agree with each other. Furthermore, we introduce an adaptive weighting strategy to weigh the contributions of the frames in the sequence according to their robustness to adverse weather conditions. We evaluate our method on the Cityscapes and KITTI datasets, and show that it significantly improves the performance of semantic segmentation in adverse weather conditions, over the state-of-the-art."
    },
    {
      "id": "6a8ba4a2-1ad2-4e9f-9a75-cbdc8e9ce6c6",
      "adv_source_id": "6a8ba4a2-1ad2-4e9f-9a75-cbdc8e9ce6c6",
      "source_id": "7f6e9b63-40ad-4587-b03e-3492501ed835",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Region-of-interest guided Supervoxel Inpainting for Self-supervision",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Region-of-interest guided Supervoxel Inpainting for Self-supervision\" from arxiv.org:",
      "generation": " Recent works have shown the benefit of using supervoxels for inference in segmentation models. However, supervoxel inpainting, i.e., exactly inferring all supervoxels in an image, is still a costly task, prohibiting their use in self-supervised learning. In this work, we instead propose a region-of-interest (ROI) guided approach for supervoxel inpainting, where we inpaint only the regions of interest within supervoxels. We demonstrate that our proposed method can significantly reduce memory usage and speed up inference, enabling the use of supervoxels for self-supervised learning while retaining performance. We further present a simple, yet effective approach for selecting regions of interest and discuss the trade-offs of our method. Finally, we show that our proposed method, when used for self-supervised learning, either matches or significantly improves upon the state-of-the-art on four benchmarks. Code is available at https://github.com/NVlabs/superviself."
    },
    {
      "id": "7502d937-9ec6-411e-9a97-2e7ab8d90294",
      "adv_source_id": "7502d937-9ec6-411e-9a97-2e7ab8d90294",
      "source_id": "7f6e9b63-40ad-4587-b03e-3492501ed835",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Region-of-interest guided Supervoxel Inpainting for Self-supervision",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Region-of-interest guided Supervoxel Inpainting for Self-supervision\" from arxiv.org:",
      "generation": " Recent works have shown the benefit of using larger patches for self-supervised learning, but existing data loaders limit the use of such large patches since they do not sample enough points to cover the entire patch. We overcome this limitation by inpainting the points that are not covered by the data loader, while maintaining the geometry of the original points. Specifically, we estimate a fuzzy surface around the points sampled by the data loader and inpaint only within a tube around this surface avoiding the boundary. This way, we can perform inpainting in higher resolution and generate a denser point cloud than the input while keeping the sampling locations from the input. In order to inpaint only within a tube around the fuzzy surface, we employ a supervoxel partitioning method and inpaint only the supervoxels that intersect with the tube. We evaluate our method on the ShapeNet part segmentation benchmark and show that our method achieves similar performance to the state-of-the-art while using 4 times fewer points."
    },
    {
      "id": "4d5bb187-a14e-4348-8dee-efde37fa37f4",
      "adv_source_id": "4d5bb187-a14e-4348-8dee-efde37fa37f4",
      "source_id": "17f6f9c7-dcc8-44e1-82b4-8233059f569b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift\" from arxiv.org:",
      "generation": " Medical image segmentation is prone to label corruption, which poses a severe impediment to clinical deployment. In this paper, we tackle the challenge of label corruption by learning a cross-denoising network (CDN) with a denoising branch and a correcting branch, aiming to restore clean labels for corrupted ones. Specifically, the denoising branch leverages the inherent smoothness of segmentations to remove noise, while the correcting branch focuses on the dissimilarity between the corrupted labels and their denoised versions to correct them. To effectively train the correcting branch, we introduce a domain shift adaptation module to align the distributions of uncorrupted and corrupted data, enabling the network to learn from both sources of data without leaking the clean labels to the corrupted side. We evaluate our method on three public medical image segmentation benchmarks with different degrees of label noise (20%, 40%, and 60%), and demonstrate its effectiveness against existing state-of-the-art methods. Code is available at https://github.com/VITA-Group/CDN."
    },
    {
      "id": "543259da-2396-4fd6-bb80-ae9c3eee3c69",
      "adv_source_id": "543259da-2396-4fd6-bb80-ae9c3eee3c69",
      "source_id": "17f6f9c7-dcc8-44e1-82b4-8233059f569b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift\" from arxiv.org:",
      "generation": " Medical image segmentation is significantly challenged by the presence of label noise, which can lead to inaccurate disease diagnosis and treatment. We propose a novel approach to address the issue of label noise based on a cross-denoising network (CDN) that leverages the domain shift technique. The CDN is designed to learn a robust segmentation model that can generalize well to new images and effectively resist the impact of label noise. Our method is evaluated on three different medical image segmentation datasets (liver, prostate, and abdominal), where we compare the performance of the CDN against state-of-the-art methods. The results show that the CDN significantly improves segmentation accuracy, even when trained on datasets with moderate levels of label noise. Our approach can be widely used in medical image analysis and has the potential to improve the accuracy of various clinical applications."
    },
    {
      "id": "c324c98f-0862-49ab-8b4a-39d53a1fec9e",
      "adv_source_id": "c324c98f-0862-49ab-8b4a-39d53a1fec9e",
      "source_id": "d70883cd-cb2b-4b55-be3c-861b1e796fbd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Elastic Boundary Projection for 3D Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Elastic Boundary Projection for 3D Medical Image Segmentation\" from arxiv.org:",
      "generation": " In this paper, we present an effective 3D medical image segmentation method using elastic boundary projection. Given a label image as input, our method estimates the gray level of the foreground object and then automatically detects the object boundary to separate it from the background. Unlike the conventional techniques relying on accurate initialization, our method is capable of segmenting the object even when the initialization is far from the object boundary. In particular, we estimate the gray level of the object using a robust weighted least square fitting between the observed image and the model image, where the weights are determined by the intensity similarity between the neighboring pixels. Furthermore, the boundary of the object is detected by projecting the estimated gray level image onto the model boundary. The proposed method has been evaluated on a number of 3D medical images obtained from real patients and it shows very promising results."
    },
    {
      "id": "3738a67e-a6ab-4a24-be95-90c7cb9ac3ec",
      "adv_source_id": "3738a67e-a6ab-4a24-be95-90c7cb9ac3ec",
      "source_id": "d70883cd-cb2b-4b55-be3c-861b1e796fbd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Elastic Boundary Projection for 3D Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Elastic Boundary Projection for 3D Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel method for 3D medical image segmentation based on elastic boundary projection (EBP). Our method combines the advantages of both atlas-based and image-based approaches, while avoiding their limitations. An atlas is first initialized with the image using an image-based approach. Then, an atlas-based segmentation is performed, where the atlas is deformed towards the image until they converge. The deformation is computed by minimizing an energy function that enforces both image similarity and anatomical structure preservation. Our approach is fully automatic, and results in accurate and robust segmentations. We demonstrate the effectiveness of our method on a variety of 3D medical images, including MR brains, CT chests, and MRI multispectral breasts."
    },
    {
      "id": "939bffba-2e6d-4d56-a79c-1d95f654588d",
      "adv_source_id": "939bffba-2e6d-4d56-a79c-1d95f654588d",
      "source_id": "48e0cc4d-96c8-4986-85d7-48cd599984ec",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation\" from arxiv.org:",
      "generation": " We introduce a simple and effective image segmentation approach called Super-BPD (Super Boundary-to-Pixel Direction), which achieves fast and almost linear runtime. Given an image, our method first estimates pixel-level labels by a super-resolution network, and then predicts boundary-to-pixel direction (BPD) fields from the estimated labels. The BPD fields are formulated as a sparse coding problem, which can be efficiently solved by modern optimization methods. During the segmentation, we only need to perform a few iterations (less than 10) to achieve the results with promising quality. Our approach is trained in a self-supervised manner, and able to run at almost linear speed, thus it is suitable for large-scale images. In the experiment, we test our method on two public available datasets, and the results demonstrate the effectiveness of our approach against other state-of-the-art segmentation approaches."
    },
    {
      "id": "97b28682-efa9-45d8-81de-72a980db8080",
      "adv_source_id": "97b28682-efa9-45d8-81de-72a980db8080",
      "source_id": "48e0cc4d-96c8-4986-85d7-48cd599984ec",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation\" from arxiv.org:",
      "generation": " We introduce a new image segmentation approach called Super Boundary-to-Pixel Direction (Super-BPD). It improves state-of-the-art methods on the benchmark datasets of BSDS500, PASCAL VOC 2012, and ACDC. The approach is based on the intuition that, for a given segmentation mask, the direction from boundaries to pixels within the object can reflect the organization of objects in the image. Our method discovers such directions in a unified framework through a process of locality-constrained interpolation. We introduce a novel boundary-to-pixel energy function that synergistically combines a weighted gradient term, a rectangularity term, and a discontinuity-sensitivity term. An iterative process is employed to enable each term to be optimized in a cascade fashion. In the process, we introduce techniques to speed up the optimization, including (1) a boundary-only iteration in the early stages to quickly converge on a good approximation of the final solution, and (2) an effective iterative scheme based on the alternating direction method of multipliers (ADMM) to optimize our energy function with provable convergence. We demonstrate the effectiveness of our approach through comprehensive experiments. In particular, on the BSDS500 dataset, our approach achieves a score of 95.4 on the measure of foreground pixels. It also outperforms the state-of-the-art methods on PASCAL VOC 2012 and ACDC datasets with noticeable margins."
    },
    {
      "id": "d4968d49-17fa-4432-a210-551034c72199",
      "adv_source_id": "d4968d49-17fa-4432-a210-551034c72199",
      "source_id": "6bdc1744-0c1f-4e11-82d5-0ecd0c7578c4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Traditional Method Inspired Deep Neural Network for Edge Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Traditional Method Inspired Deep Neural Network for Edge Detection\" from arxiv.org:",
      "generation": " Traditional Methods Inspired Deep Neural Network for Edge Detection: A Survey 1\n\nAbstract\n\nIn this paper, we survey works that incorporate the ideas of traditional edge detection methods into deep neural networks for edge detection. We categorize these methods into three categories: (1) methods that use pre-trained deep neural networks as feature extractors for traditional edge detection methods, (2) methods that combine deep neural networks and traditional edge detection methods to achieve better performance, and (3) methods that modify the architecture of deep neural networks to make them more suitable for edge detection. We discuss the advantages and limitations of each approach and highlight open problems and future research directions.\n\n1 https://arxiv.org/abs/2103.08725"
    },
    {
      "id": "35ddb940-c570-49c8-b374-95b6142aa6c5",
      "adv_source_id": "35ddb940-c570-49c8-b374-95b6142aa6c5",
      "source_id": "6bdc1744-0c1f-4e11-82d5-0ecd0c7578c4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Traditional Method Inspired Deep Neural Network for Edge Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Traditional Method Inspired Deep Neural Network for Edge Detection\" from arxiv.org:",
      "generation": " Traditional methods inspire deep neural networks for edge detection, considering the drawbacks of the traditional methods and the advantages of deep learning methods, a U-Net based on deep learning is proposed to perform medical image edge detection. Experiments showed that our method is not only fast in training and testing stages but also outperforms existing state-of-the-art methods for liver lesion segmentation in terms of quantitative measures. Our method also achieved promising results for cartilage and lung lesion segmentation."
    },
    {
      "id": "9445c174-046e-4354-9e54-db670af648e0",
      "adv_source_id": "9445c174-046e-4354-9e54-db670af648e0",
      "source_id": "c7117aa4-b7c6-4d3f-93fb-d61228c887ca",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation\" from arxiv.org:",
      "generation": " We propose a convex shape prior (CSP) for training deep neural networks (DNNs) to segment eye fundus images (EFIs). CSP is implemented by constraining the output probability distribution of convolutional layers within a convex set, which is defined by the prior knowledge of the target objects' shapes. Without introducing any extra cost or loss in the optimization objective, CSP encourages the learned representations to be aligned with our prior knowledge, which effectively suppresses the unwanted regions and helps to focus on the foreground, especially in the early stages of training. In addition, the spatial extent of the foreground objects is constrained by CSP, which helps the model to learn more accurate localization. The proposed method is evaluated on two public EFI segmentation benchmarks, and the experimental results show that the proposed method outperforms the state-of-the-art methods."
    },
    {
      "id": "5fcc81c3-82c6-4f90-97ec-9a8376db2eb1",
      "adv_source_id": "5fcc81c3-82c6-4f90-97ec-9a8376db2eb1",
      "source_id": "c7117aa4-b7c6-4d3f-93fb-d61228c887ca",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation\" from arxiv.org:",
      "generation": " We propose a convex shape prior (CSP) for training deep neural networks (DNNs) to perform eye fundus images segmentation. Given a dataset of eye fundus images, we first estimate the contour of the retina in each image, and then represent the image as a linear combination of the two shapes forming the contour. The parameters of the two shapes are the input to the CSP. We propose to optimize the CSP with the back propagation algorithm, in which the DNN is trained to search for a convex combination of the two shapes that can best separate the foreground from the background in the image. The proposed method has been evaluated on a public dataset, and the experimental results demonstrate the effectiveness of the proposed approach."
    },
    {
      "id": "e56268e9-9126-48ec-9c82-8146fac30e75",
      "adv_source_id": "e56268e9-9126-48ec-9c82-8146fac30e75",
      "source_id": "996a50cb-2911-4900-ad48-7ec8df8c4eaa",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Meta-Learning Initializations for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Meta-Learning Initializations for Image Segmentation\" from arxiv.org:",
      "generation": " Recent works have shown that training deep neural networks (DNNs) for image segmentation from scratch can be very time-consuming due to the large amount of labels required for supervised learning. In this paper, we show that pre-trained models, especially those trained on large-scale datasets like ImageNet, can be used as strong initializations for training deep segmentation models. We propose a simple yet effective approach to fine-tuning pre-trained models for image segmentation. Our method first applies an initial segmentation model to segment the images into super-pixels. Then, a pre-trained model is fine-tuned using the segmented images. Experiments on two public segmentation benchmarks, PASCAL VOC 2012 and Cityscapes, show that our approach can significantly speed up the training process while maintaining high quality segmentation results. Our meta-learning initialization achieves the best single model performance among methods that use limited training data on the Cityscapes dataset."
    },
    {
      "id": "b523d290-6d39-4bdb-9dbd-7972ee84fbf7",
      "adv_source_id": "b523d290-6d39-4bdb-9dbd-7972ee84fbf7",
      "source_id": "996a50cb-2911-4900-ad48-7ec8df8c4eaa",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Meta-Learning Initializations for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Meta-Learning Initializations for Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel initialization for any off-the-shelf image segmentation model that boosts performance across a wide range of datasets. Our initialization is derived from a meta-learning approach where a single model learns to initialize other models in a way that boosts their performance. In our method, the initialization network is trained on a collection of image segmentation models and their respective initialization points. We show that this produces an initial model state that is particularly effective across a wide variety of models and datasets. We present extensive experiments on popular segmentation models as well as standard benchmark datasets, showing that our initialization consistently improves performance, especially in the presence of limited data. Furthermore, we show that initialization performance is predictive of segmentation model performance, indicating that our initialization captures essential information about the dataset and the model's capacity."
    },
    {
      "id": "7d8fa0ab-5e7f-41ef-b980-23df9d9bcfe1",
      "adv_source_id": "7d8fa0ab-5e7f-41ef-b980-23df9d9bcfe1",
      "source_id": "20d41ea9-f3cc-4c0c-b3bf-4d421e3c1b38",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Importance Driven Continual Learning for Segmentation Across Domains",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Importance Driven Continual Learning for Segmentation Across Domains\" from arxiv.org:",
      "generation": " Importance driven continual learning (IDCL) is a simple and effective method to train a model across domains. Given a set of domains, IDCL trains a single model to perform a task such as segmentation, by alternating between source and target domains. In each iteration, the model is first trained on the source domain with a sparse constraint on the target domain. Then, the gradients are projected onto the target domain using an importance ratio, which is defined as the improvement in performance on the target domain divided by the decrease in performance on the source domain. This ensures that the model is updated to perform well on both domains, while maintaining performance on the source domain. Next, the model is fine-tuned on the target domain. We apply IDCL to segment neuronal structures in electron microscopic images from different domains including different organisms, stain types, and imaging conditions. Our results show that IDCL can effectively segment structures across domains, and that the importance ratio is a useful measure to compare the performance of a model on different domains."
    },
    {
      "id": "c48820c6-5126-4bc5-96e6-f1f42f74c0a7",
      "adv_source_id": "c48820c6-5126-4bc5-96e6-f1f42f74c0a7",
      "source_id": "20d41ea9-f3cc-4c0c-b3bf-4d421e3c1b38",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Importance Driven Continual Learning for Segmentation Across Domains",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Importance Driven Continual Learning for Segmentation Across Domains\" from arxiv.org:",
      "generation": " Importance driven continual learning (IDCL) aims to focus on the most important examples in order to optimize for efficient learning. In this paper, we extend IDCL to semantic segmentation, and demonstrate its effectiveness on the task of segmenting images from different domains, such as indoor and outdoor scenes. We define importance for segmentation as the informativeness of an example with respect to the learned representations of previous examples. In particular, we employ a metric-based approach to obtain the importance scores for each example. Unlike previous work on continual learning for segmentation that treats all data as equally important, our method provides a mechanism for balancing the computational cost of learning from all data versus selecting the most informative data for learning. We show that, given a fixed budget of training examples, our method can achieve better performance compared to learning from all data by a large margin. We conduct experiments on both synthetic and real datasets, and show the advantages of our method over the state-of-the-art."
    },
    {
      "id": "d000eaaf-f8f1-44c9-bf56-0e1792fbe667",
      "adv_source_id": "d000eaaf-f8f1-44c9-bf56-0e1792fbe667",
      "source_id": "f71ade37-4be2-40be-ab66-ede2b049ba02",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means\" from arxiv.org:",
      "generation": " Color image segmentation has always been an active research domain. In this paper, a new approach for color image segmentation is proposed, which combines adaptive particle swarm optimization (APSO) with fuzzy C-means (FCM). In the proposed approach, APSO is first used to optimize the initial centroid positions, and then FCM is applied to obtain the final segmentation. The objective of APSO is to improve the initialization of FCM, so that FCM can converge faster and produce more accurate segmentation results. In the experiment, the proposed approach is compared with other state-of-the-art color image segmentation methods. The experimental results show the superiority of the proposed approach in terms of segmentation quality."
    },
    {
      "id": "feeecc3d-2ec7-4508-8464-ba6a91ea2a61",
      "adv_source_id": "feeecc3d-2ec7-4508-8464-ba6a91ea2a61",
      "source_id": "f71ade37-4be2-40be-ab66-ede2b049ba02",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means\" from arxiv.org:",
      "generation": " Color image segmentation has always been an active research domain. In this paper, a new approach to color image segmentation is proposed which combines the advantages of adaptive particle swarm optimization (APSO) and fuzzy C-means (FCM). In the proposed approach, the APSO algorithm is first used to optimize the initial centroids of FCM, which are determined using intensity and color information. The optimized centroids are then used by the FCM algorithm to obtain the final segmentation. Experimental results show the effectiveness of the proposed approach for color image segmentation."
    },
    {
      "id": "52b89b7d-bf39-4ad0-9f52-8fcc7e1ebdea",
      "adv_source_id": "52b89b7d-bf39-4ad0-9f52-8fcc7e1ebdea",
      "source_id": "a5251cdf-e973-4a19-8c07-3bd0ebdbc0d6",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised few-shot learning for medical image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised few-shot learning for medical image segmentation\" from arxiv.org:",
      "generation": " Recent advances in few-shot learning have achieved remarkable performance in image classification tasks, but have not been extensively explored for medical image segmentation, which is a fundamental but challenging task in medical imaging analysis. In this paper, we present a semi-supervised few-shot learning approach for medical image segmentation. Our approach is based on a novel few-shot learning framework that combines semi-supervised learning and few-shot learning, which we call semi-supervised few-shot learning. We demonstrate the effectiveness of our approach on three public medical image segmentation benchmarks, and show that our approach outperforms the state-of-the-art in few-shot learning and semi-supervised learning.\n\nOur approach has two main components. The first component is a novel few-shot learning framework that combines semi-supervised learning and few-shot learning. This framework utilizes a pre-trained model to extract features from the input images, and then uses a few-shot learning approach to train a new model based on these features. The few-shot learning approach is based on the idea of transfer learning, which is a technique used to train a new model using a pre-trained model as a starting point. The second component is a semi-supervised learning approach that is used to train the few-shot learning framework. This approach utilizes a set of labeled and unlabeled images to train the model, and uses a semi-supervised learning technique to improve the model's performance on the labeled images. The semi-supervised learning technique is based on the idea of self-training, which is a technique used to train a model using a set of labeled and unlabeled images."
    },
    {
      "id": "7237217e-f504-4b1a-9fe3-7041a5fb2bef",
      "adv_source_id": "7237217e-f504-4b1a-9fe3-7041a5fb2bef",
      "source_id": "a5251cdf-e973-4a19-8c07-3bd0ebdbc0d6",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised few-shot learning for medical image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semi-supervised few-shot learning for medical image segmentation\" from arxiv.org:",
      "generation": " Recent advances in few-shot learning have enabled the transfer of knowledge from a large dataset to a limited number of labeled examples, making it possible to train models on datasets where the cost of labeling samples is prohibitive. In this work, we apply few-shot learning to medical image segmentation, transferring knowledge from a large dataset of abdominal CT scans to a limited number of labeled liver and lung CT scans. We propose a novel few-shot learning framework that utilizes a semi-supervised learning approach, where an unsupervised clustering algorithm is used to infer labels for a subset of the data. These inferred labels are then used to train a model for few-shot learning. Our experimental results show that the proposed method achieves state-of-the-art performance on the CLEAR Chest CT and Lung CT datasets."
    },
    {
      "id": "13f63797-3591-4880-a288-f7e23c9d262d",
      "adv_source_id": "13f63797-3591-4880-a288-f7e23c9d262d",
      "source_id": "32640571-5d21-4e21-8ec3-365726cc1d77",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation\" from arxiv.org:",
      "generation": " We propose a fast fully octave convolutional neural network (FCN) for document image segmentation. Our network dynamically scales the convolutional kernels by a factor of two between every two layers, which we refer to as octave convolution. This approach reduces the parameters by a factor of two per layer and shrinks the computational cost, yet it does not reduce the receptive field of the network. We evaluate our method on the publicly available ICDAR 2015 document image segmentation benchmark. Our network achieves a higher accuracy with a significantly reduced computational cost compared to the state-of-the-art methods. Our network also reduces the running time for training and testing by a factor of 2.1 and 3.4, respectively, compared to the second fastest method in the benchmark."
    },
    {
      "id": "4407a6f0-9e40-46ef-b68e-0f1c8b324d0b",
      "adv_source_id": "4407a6f0-9e40-46ef-b68e-0f1c8b324d0b",
      "source_id": "32640571-5d21-4e21-8ec3-365726cc1d77",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation\" from arxiv.org:",
      "generation": " We propose a fully octave convolutional neural network (FCN) for document image segmentation that significantly improves the computational efficiency without sacrificing the segmentation quality. Our network dynamically switches the convolutional filters between integer-pixel and fractional-pixel scales implemented by warped convolutions, allowing the network to efficiently utilize the redundant information across different scales. Furthermore, our FCN is fully convolutional such that the output feature maps can be produced in one forward pass, making it more efficient than the recently proposed cost aggregation methods that require additional cost aggregation layers. We evaluate our method on the publicly available ICDAR 2015 document image segmentation benchmark. Our method achieves a document boundary recall of 0.9458 and runs 55.4 times faster than the state-of-the-art method on average."
    },
    {
      "id": "24cbd2db-66ff-4fce-804c-b6d93ff14f33",
      "adv_source_id": "24cbd2db-66ff-4fce-804c-b6d93ff14f33",
      "source_id": "e68fce69-dbdc-43a7-a9b6-a799d85dda13",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Grouping Model for Unified Perceptual Parsing",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Grouping Model for Unified Perceptual Parsing\" from arxiv.org:",
      "generation": " Recent works on perceptual parsing mainly focus on specific tasks or specific domains, such as scene parsing, object parsing, or person parsing. These methods often utilize separate deep models to handle different tasks, which may result in low efficiency and redundant computations. In this paper, we present a deep grouping model for unified perceptual parsing (DGMP). DGMP introduces a novel pyramid grouping module (PGM) to effectively group and convey multi-scale contextual information, and a joint parsing and refining network (JPRN) to jointly parse and refine the predicted results. Specifically, PGM can automatically group the interested regions and refine the grouped features within a single network architecture, which achieves multiple grouping behaviors with only one forward pass. Furthermore, our JPRN can not only parse the whole image but also refine the predicted results, which further improves the performance. Compared with state-of-the-art methods, DGMP can achieve better performance with a faster inference speed."
    },
    {
      "id": "344c25cc-4887-4302-8c70-59bd023ee4d7",
      "adv_source_id": "344c25cc-4887-4302-8c70-59bd023ee4d7",
      "source_id": "e68fce69-dbdc-43a7-a9b6-a799d85dda13",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Grouping Model for Unified Perceptual Parsing",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Grouping Model for Unified Perceptual Parsing\" from arxiv.org:",
      "generation": " Deep grouping model (DGM) is a simple yet effective method for unified perceptual parsing. Given an image and a list of parse trees, the DGM learns a deep network to predict the grouped parse trees, with each group representing a coherent object or relationship. Unlike the recent breakthrough in object detection with convolutional neural networks, the DGM can handle highly variable object instances and capture detailed object and relationship descriptions, making it suitable for applications such as scene interpretation and image question answering. Furthermore, the DGM is able to unify perceptual parsing and recognition, as the grouped parse trees can be viewed as recognition outputs. Our approach achieves the state-of-the-art performance on the benchmark of COCO entity detection and instance-level relation detection."
    },
    {
      "id": "1d663a5f-2ad7-4387-a449-762d317d409b",
      "adv_source_id": "1d663a5f-2ad7-4387-a449-762d317d409b",
      "source_id": "4be64437-d36f-4dd1-a995-92ea99cc97e4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours\" from arxiv.org:",
      "generation": " Deep learning is a promising method for tongue surface tracking from real-time ultrasound videos. We present a simple yet effective method to train a deep neural network for tongue surface detection. Unlike most previous methods that trace the contour of the tongue, our method detects the landmarks on the tongue surface. It is more robust to the ambiguous contour around the tongue border in ultrasound videos, and it is more efficient in terms of computation and training data. We evaluated our method on real-time ultrasound videos, and the experimental results demonstrate the effectiveness of our approach.\n\nOur tongue surface detection network is based on a modified version of the popular Faster R-CNN [1] with additional novel improvements. In the training phase, the network is optimized to classify each pixel on the ultrasound image as a tongue surface or background. In the inference phase, the network outputs the full tongue surface as an assembly of point clouds. The output points are then connected to form the tongue contour. The whole process is performed in real-time with an average speed of 7.8 FPS on our test videos."
    },
    {
      "id": "c09c09f1-e5b4-409e-800b-925d3d2e3a19",
      "adv_source_id": "c09c09f1-e5b4-409e-800b-925d3d2e3a19",
      "source_id": "4be64437-d36f-4dd1-a995-92ea99cc97e4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours\" from arxiv.org:",
      "generation": " Deep learning approaches have been shown to be effective in extracting tongue surface information from ultrasound video frames. In this work, we present a deep learning-based method for the automatic tracking of tongue surface in real-time ultrasound videos, using landmarks instead of contours. We employ a two-stream network, with one stream for feature extraction and another for landmark localization. The feature extraction stream utilizes a 3D convolutional neural network (3D-CNN) to extract tongue surface features from ultrasound video frames. The landmark localization stream leverages a multi-scale convolutional attention model (MSCAM) to localize landmarks on the tongue surface. The two streams are trained jointly in an end-to-end manner. We demonstrate the effectiveness of our method on real-time ultrasound videos of the tongue, and show that our method can accurately track the tongue surface in real-time, without any prior training or user interaction.\n\nAbstract only, the full paper is available at https://arxiv.org/abs/2201.09383"
    },
    {
      "id": "1671d7b8-496b-4513-be57-6ee49a85c6c5",
      "adv_source_id": "1671d7b8-496b-4513-be57-6ee49a85c6c5",
      "source_id": "6f053b54-071e-40b2-855d-37fd4885addf",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Improving Training on Noisy Stuctured Labels",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Improving Training on Noisy Stuctured Labels\" from arxiv.org:",
      "generation": " We consider the setting in which a classifier must be trained to classify instances of never-before-seen entities, using only a small number of noisy and incomplete labels for each entity. We find that training on structured labels, where information is encoded in both the label text and in the label structure (through pointers to other related labels), significantly improves accuracy compared to training on plain text labels, and performs competitively with state-of-the-art performance on cleanly annotated data."
    },
    {
      "id": "bf69d0eb-3bff-429d-a85e-2c85f2b0a8ea",
      "adv_source_id": "bf69d0eb-3bff-429d-a85e-2c85f2b0a8ea",
      "source_id": "6f053b54-071e-40b2-855d-37fd4885addf",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Improving Training on Noisy Stuctured Labels",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Improving Training on Noisy Stuctured Labels\" from arxiv.org:",
      "generation": " Recent results in distributional semantics have achieved state-of-the-art performance on a variety of natural language inference tasks using noisy inputs, in which the gold-standard label for a given sample is missing or incorrect. While these results demonstrate the viability of training robust models in the presence of noise, they rely on expensive supervised or self-supervised artifacts (e.g., pseudo-labels or backtranslation) for training. In this work, we show that a simple procedural augmentation can generate synthetic noisy data, and that this data can be used to improve the robustness of models to in-distribution noise. We demonstrate our approach on a state-of-the-art neural model for semantic role labeling, and show that our method performs to the level of the current state of the art on the standard CoNLL 2012 evaluation benchmark, but more importantly, also matches the performance of the current state of the art on noisy data."
    },
    {
      "id": "c99d1ffa-d8db-442b-b2dd-b10cba1cb0b7",
      "adv_source_id": "c99d1ffa-d8db-442b-b2dd-b10cba1cb0b7",
      "source_id": "9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization\" from arxiv.org:",
      "generation": " We present a variational model for image segmentation based on normalized cut with adaptive similarity and spatial regularization. In contrast to previous works on normalized cut, we model the similarity between pixels by a continuous non-linear function, which is learnt from the data. This allows us to capture more complex relationships between pixels. To overcome the limitation of current segmentation techniques that only capture spatial regularity, we propose to use a spatial regularization term that enforces a smooth transition between segments. The proposed method is compared with state-of-the-art image segmentation techniques on the Berkeley Segmentation Dataset and the MIT-CBCL face segmentation dataset, and experimental results show the effectiveness of the proposed method."
    },
    {
      "id": "9a9c2196-5ca3-4120-a696-ea25730fa271",
      "adv_source_id": "9a9c2196-5ca3-4120-a696-ea25730fa271",
      "source_id": "9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization\" from arxiv.org:",
      "generation": " We present a variational model for image segmentation based on normalized cuts. The model encompasses adaptive similarity and spatial regularization terms to enforce smoothness and produce visually plausible segmentations. In particular, we introduce a new adaptive similarity term which computes a weighted average of pixel similarity functions and adapts to the local structure of the image. We demonstrate the effectiveness of our approach on a benchmark dataset and show that the proposed method outperforms the state of the art in terms of segmentation quality. Our source code is available at https://github.com/ShaofanXie/Adaptive-Normalized-Cut."
    },
    {
      "id": "c16e0157-7db4-4f48-b9a2-393412ee2938",
      "adv_source_id": "c16e0157-7db4-4f48-b9a2-393412ee2938",
      "source_id": "9f504e80-00f5-491e-a43d-003687f64ea4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Neural arbitrary style transfer for portrait images using the attention mechanism",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Neural arbitrary style transfer for portrait images using the attention mechanism\" from arxiv.org:",
      "generation": " We address the problem of style transfer for portrait images, which transfers the style of an image (watermark) to a target image (portrait) while preserving the content of the portrait. We propose to use a neural network to achieve this goal. We employ an attention mechanism to align the content of the watermark and the portrait, and we use a style transfer algorithm to transfer the style of the watermark to the portrait. Our approach achieves a good balance between the preservation of the content and the transfer of the style. We evaluate our method using the portrait image database from PolyU, and the results demonstrate the effectiveness of our approach."
    },
    {
      "id": "93a49af3-482d-40bd-9c9a-0bf5b237b5d0",
      "adv_source_id": "93a49af3-482d-40bd-9c9a-0bf5b237b5d0",
      "source_id": "9f504e80-00f5-491e-a43d-003687f64ea4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Neural arbitrary style transfer for portrait images using the attention mechanism",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Neural arbitrary style transfer for portrait images using the attention mechanism\" from arxiv.org:",
      "generation": " We address the problem of arbitrary style transfer for portrait images. We propose a neural network with the attention mechanism to combine the content of a source image with the style of a reference image. Our network performs style transfer in portrait images effectively by jointly modeling the attributes of both the source and reference images. In addition, we introduce a component to preserve the attributes of the source image to produce photo-realistic results. By combining the attention mechanism, content preservation component, and style mapping component, our network can effectively transfer the style of a reference image to a target image while preserving the content of the target image. Experiments show the effectiveness of our approach compared to state-of-the-art methods."
    },
    {
      "id": "86a23531-28fc-4e5d-8201-c0f34acb0c81",
      "adv_source_id": "86a23531-28fc-4e5d-8201-c0f34acb0c81",
      "source_id": "34071d29-fbbf-4230-9ee4-55f1c54b2b5e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-scale self-guided attention for medical image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Multi-scale self-guided attention for medical image segmentation\" from arxiv.org:",
      "generation": " Recent advances in deep learning have achieved remarkable performance in medical image segmentation. However, these models usually demand a large amount of labeled data for superior performance, which is difficult to obtain in the medical field. To overcome this challenge, we propose a novel multi-scale self-guided attention (MSA) module to enhance the performance of medical image segmentation models. Our approach combines a multi-scale attention module and a self-guided attention module to capture the global and local features respectively. The multi-scale attention module can capture the global context by aggregating the features at multiple scales, while the self-guided attention module can focus on the important regions by learning a weighting function. We evaluate our approach on three public medical image segmentation benchmarks, and our method outperforms the state-of-the-art approaches. The code is available at https://github.com/AI4MEDICAL/MSA-Net."
    },
    {
      "id": "8163ebd4-81f7-46d6-8712-7e6470a6910c",
      "adv_source_id": "8163ebd4-81f7-46d6-8712-7e6470a6910c",
      "source_id": "34071d29-fbbf-4230-9ee4-55f1c54b2b5e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-scale self-guided attention for medical image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Multi-scale self-guided attention for medical image segmentation\" from arxiv.org:",
      "generation": " Recent advances in Deep Learning (DL) have achieved remarkable performance in diagnostic image analysis. However, the lack of large-scale annotated datasets hinders the development of effective DL models for medical image segmentation (MIS). To overcome this limitation, we propose a multi-scale self-guided attention (MSA) framework that utilizes a single model to segment instances at multiple scales. Our method incorporates attention modules at each scale to focus on the regions of interest for accurate segmentation. A key contribution of our work is a self-guided attention mechanism that utilizes the contextual information within the image to automatically highlight the relevant features for segmentation. We evaluate our method on three public abdominal CT datasets with variable instance sizes and achieve state-of-the-art performance. Our method segments small vessels as well as large organs with high accuracy. Furthermore, the proposed framework is efficient, requiring less than 1.5 seconds for inference on a modern GPU. Code is available at https://github.com/RadiologyAI/MSA-Seg."
    },
    {
      "id": "8abbca26-4c62-42c9-8d1f-901528eb0df9",
      "adv_source_id": "8abbca26-4c62-42c9-8d1f-901528eb0df9",
      "source_id": "01b52818-c3d8-49e2-b33c-100d6262b860",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation\" from arxiv.org:",
      "generation": " We propose an image segmentation approach based on deep convolutional neural networks (CNNs) with spatial regularization, volume and star-shape priori for microscopic images of cells. In our approach, a CNN architecture, which is composed of several convolutional layers with leaky ReLU activation functions and batch normalization layers between them, is first trained without any spatial regularization on a large number of labeled cells. Then, the same network is fine-tuned with three different spatial regularization terms: (1) a volume prior that favors smooth predictions in the vicinity of cell boundaries, (2) a star-shape prior that encourages predictions to have star-shape boundaries and (3) a spatial regularization term that imposes a sparse and smooth constraint on the predictions. The three regularization terms are combined via a weighted sum, where the weights are optimized during the fine-tuning phase. We evaluate our method on the 2016 Miccai Cell Segmentation Challenge dataset and achieve promising results compared to the state-of-the-art methods."
    },
    {
      "id": "1f3315fc-b482-4367-8b62-39bd10b0021b",
      "adv_source_id": "1f3315fc-b482-4367-8b62-39bd10b0021b",
      "source_id": "01b52818-c3d8-49e2-b33c-100d6262b860",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation\" from arxiv.org:",
      "generation": " We propose an image segmentation method based on deep convolutional neural networks (DCNNs) with spatial regularization, volume and star-shape priori for MRI tumor images. In our method, we adopt a DCNN called U-Net, which has been successfully applied to image segmentation. On the basis of the U-Net, we add spatial regularization to improve the segmentation performance. The spatial regularization is implemented by a Gaussian kernel, which can effectively capture the spatial information of the images. Besides, we propose to introduce both volume and star-shape priori to constraint the segmentation results. The volume priori can prevent the tumor from being segmented as multiple parts, while the star-shape priori can make the tumor have a star-shape form in the segmentation results. We evaluate our method on the MRI tumor images of the ISBI 2017 Challenge. The experimental results show that our method outperforms the other state-of-the-art methods."
    },
    {
      "id": "f3072851-bbb7-41a4-a92b-5c113a15ef52",
      "adv_source_id": "f3072851-bbb7-41a4-a92b-5c113a15ef52",
      "source_id": "26407846-eb66-4f03-a951-ac6ab785adf2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual Convolutional LSTM Network for Referring Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Dual Convolutional LSTM Network for Referring Image Segmentation\" from arxiv.org:",
      "generation": " We present a new framework for referring image segmentation, a task that aims to assign semantic labels to every pixel in an image while also providing a referring expression (e.g., \u201cman with a guitar\u201d) that describes the image region labeled. Unlike prior work on referring image segmentation, which treats the referring expression generation as a natural language conditioning task, our framework directly models referring expressions using a neural graphics engine. This engine reasons about what is being referred to in the image, and generates a referring expression as a by-product. To effectively train our neural graphics engine, we propose a novel referring training target that encourages the model to produce referring expressions that are both accurate and concise. We show that training on this target enforces the referring expression generation as a by-product of learning a good segmentation, even without any natural language conditioning. Furthermore, to handle the referring expression generation, an architecture with a spatial pyramid structure and an LSTM module is proposed to capture the spatial and temporal contexts, respectively. The proposed framework significantly improves over the state of the art on the ReferItSeg dataset, and produces referring expressions that are more human-like and concise."
    },
    {
      "id": "2179e9cd-1fc5-4a53-b0a8-9c1d1abdc6de",
      "adv_source_id": "2179e9cd-1fc5-4a53-b0a8-9c1d1abdc6de",
      "source_id": "26407846-eb66-4f03-a951-ac6ab785adf2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual Convolutional LSTM Network for Referring Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Dual Convolutional LSTM Network for Referring Image Segmentation\" from arxiv.org:",
      "generation": " We present a new architecture for the task of referring image segmentation, built upon recent advances in CNN architecture design for this task. Our network architecture consists of two separate streams, one based on 2D CNNs for appearance and another based on 3D CNNs for shape. These two streams independently compute mid-level representations for the input image. A third stream, based on LSTMs, learns to combine the mid-level representations from the 2D and 3D streams to produce referring segment predictions. We show that our network architecture improves over previous state-of-the-art methods on the 2014 and 2015 ReferIt datasets, and we provide a comprehensive analysis as to why this is the case."
    },
    {
      "id": "57331e38-af25-4cc1-aa8a-a2780223f8de",
      "adv_source_id": "57331e38-af25-4cc1-aa8a-a2780223f8de",
      "source_id": "dc3ec32e-2230-4be2-9c40-8268932279f5",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning\" from arxiv.org:",
      "generation": " We propose a volume-preserving image segmentation method that utilizes the entropic regularization optimal transport (EROT) to combine the region similarity and the volume preservation into a unified energy function. The region similarity is measured by the Kullback-Leiber (KL) divergence between the region proposals and the ground truth, while the volume preservation term is formulated as a regularization term on the optimal transport plan. The EROT method simultaneously optimizes the region similarity and volume preservation in a unified framework, which is different from the existing methods that tackle these two criteria separately. The proposed method is applicable to both traditional watershed-based methods and deep learning-based methods. We validate the proposed method on both synthetic and real-world images, and the experimental results demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "78a29fef-6d92-46a3-9cf3-6581a690e328",
      "adv_source_id": "78a29fef-6d92-46a3-9cf3-6581a690e328",
      "source_id": "dc3ec32e-2230-4be2-9c40-8268932279f5",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning\" from arxiv.org:",
      "generation": " We present a new method for the volume preserving image segmentation problem based on the entropic regularization of optimal transport. This approach combines the benefits of continuum mechanics, namely its ability to handle difficult geometries, with the strong implicit regularization of entropic regularization. We show how to solve the resulting inverse problem by introducing a new transportation loss, which encourages the minimization of an entropic regularization functional. We show the efficacy of our approach on a number of 2D and 3D image segmentation problems. In particular, we show that our method competes with state-of-the-art learning based methods on the challenging Matterport3D dataset while advantageously providing a surface that undergoes volume preservation."
    },
    {
      "id": "950cb240-2033-4ab8-89ec-ef2f64de52c9",
      "adv_source_id": "950cb240-2033-4ab8-89ec-ef2f64de52c9",
      "source_id": "32d76482-0724-4631-9784-6be3ab41cd44",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification\" from arxiv.org:",
      "generation": " We present a self-learning AI framework utilizing a generative adversarial network (GAN) for skin lesion image segmentation and classification. Based on a large number of labeled images, a generative model is learned in an unsupervised manner to synthesize realistic skin lesion images. Utilizing the generated images, we train a segmentation and classification model in a self-supervised manner. We evaluate our framework on a public skin lesion image dataset. The experimental results demonstrate the effectiveness of our framework in terms of segmentation and classification performance. Our code is available at https://github.com/AI4Skin/skin_lesion_segmentation."
    },
    {
      "id": "caea249e-89e0-4b9c-9472-250426f4c96f",
      "adv_source_id": "caea249e-89e0-4b9c-9472-250426f4c96f",
      "source_id": "32d76482-0724-4631-9784-6be3ab41cd44",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification\" from arxiv.org:",
      "generation": " We present a self-learning framework for the diagnosis of skin lesions using image segmentation and classification. A skin lesion is typically characterized by multiple heterogeneous features, including color, texture, and shape. Therefore, a deep learning architecture that effectively captures spatial and channel correlations from images is crucial for the accurate segmentation and classification of skin lesions. Our framework utilizes a U-Net-based architecture, which has been successfully applied to image segmentation tasks. We extend the U-Net architecture by incorporating a multi-scale discriminative correlation module (MDCM) to improve the network's ability to capture spatial and channel correlations. MDCM employs multiple scaling factors to capture features at different scales, coupled with a discriminative correlation module to enhance the representation learning capability of the network. To the best of our knowledge, this is the first attempt to incorporate a correlation module within the U-Net architecture for image segmentation. We evaluate the proposed method on two public datasets, namely, the ISIC 2018 Skin Lesion Analysis Towards Diagnosis (SLAD) and the Dermatologist Quality Assurance Program (DAQAP) datasets. The experimental results show that the proposed framework achieves state-of-the-art performance in skin lesion image segmentation and classification, with an F1-score of 0.9451 and 0.9282 on the ISIC 2018 SLAD validation and test sets, respectively, and an F1-score of 0.9442 and 0.9208 on the DAQAP dataset."
    },
    {
      "id": "2f8a6b22-b280-4299-abe9-3b7795dabaad",
      "adv_source_id": "2f8a6b22-b280-4299-abe9-3b7795dabaad",
      "source_id": "ef6fa6f5-e136-4ba4-b009-053b5dc61476",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\" from arxiv.org:",
      "generation": " Deepfake refers to the phenomenon of maliciously manipulating multimedia contents, especially image and video data, using artificial intelligence techniques to impersonate a person doing or saying something they never did or said. Detecting deepfake contents is of paramount importance to uphold the integrity of media communication. In this paper, we focus on detecting forged facial expressions in still images, which is a fundamental yet challenging task. We propose a novel approach to detect forged facial expressions using a separable convolutional neural network (SCNN) and image segmentation. Our approach first extracts a compact feature vector from the input image using a deep convolutional neural network (DCNN). Then, we apply image segmentation to extract the regions of interest (ROIs) from the input image. The extracted ROIs are then classified using a shallow convolutional neural network (SHCN). The proposed approach has been evaluated on a large dataset of images with forged facial expressions. The experimental results show that our approach achieves a high detection accuracy with a low computational cost."
    },
    {
      "id": "1417f141-9565-4506-8dd0-d87dcd65afa2",
      "adv_source_id": "1417f141-9565-4506-8dd0-d87dcd65afa2",
      "source_id": "ef6fa6f5-e136-4ba4-b009-053b5dc61476",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\" from arxiv.org:",
      "generation": " Arxiv is a popular e-print archive for machine learning and computer science papers, amongst other domains. Hence, Arxiv covers a wide range of paper topics. As an AI language model, I am trained to be thorough, accurate, and useful pertaining to any topic. Please provide further information about the paper you would like to extract the abstract for in the prompt. "
    },
    {
      "id": "cf12a464-4695-4105-954e-0b784ef365ca",
      "adv_source_id": "cf12a464-4695-4105-954e-0b784ef365ca",
      "source_id": "38d4f731-4259-4770-9669-255b61bf61b2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computing Valid p-values for Image Segmentation by Selective Inference",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Computing Valid p-values for Image Segmentation by Selective Inference\" from arxiv.org:",
      "generation": " Recent progress in deep learning enabled the development of effective neural networks for image segmentation. However, validating the performance of these models is challenging. The most common practice is to use pixel-wise measures, such as the Intersection over Union (IoU). Yet, these metrics are not capable of capturing the uncertainty of the predictions and, in particular, they do not provide confidence scores. In this paper, we propose a novel way to produce valid p-values for image segmentation, which overcome these issues. We employ a selective inference framework that is based on a robust probabilistic model. Specifically, we model the uncertainty of the predictions obtained from a deep segmentation model via a Bayesian approach. Then, we use selective inference to obtain valid p-values for the segmentation performance measure that is most commonly used, namely, the IoU. We demonstrate the efficacy of our approach on two public segmentation benchmarks, while achieving significant improvements over the state-of-the-art. Our code and models are available at this https URL."
    },
    {
      "id": "0e0181d4-85c5-4573-af39-7a1c8b8329c3",
      "adv_source_id": "0e0181d4-85c5-4573-af39-7a1c8b8329c3",
      "source_id": "38d4f731-4259-4770-9669-255b61bf61b2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computing Valid p-values for Image Segmentation by Selective Inference",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Computing Valid p-values for Image Segmentation by Selective Inference\" from arxiv.org:",
      "generation": " Abstract: Statistical significance is a useful concept in imaging science. However, direct application of statistical significance tests to image segmentation is problematic: a typical segmentation algorithm segments images based on all pixels, yet the null distributions of interest are those derived from datasets where a particular pixel is omitted. We propose an estimation procedure for valid p-values in this context. The procedure is intuitive: regenerate the test dataset by estimating pixel values from the original dataset, then reassign the test pixel a new value and run the segmentation algorithm again. We demonstrate that this method yields a valid p-value in the presence of localization and smoothing. We apply the method to the binary segmentation problem and the multi-class segmentation problem, and illustrate its use via an application in neuroimaging."
    },
    {
      "id": "5c275fce-a4b4-4657-90f1-fcfbb018ba22",
      "adv_source_id": "5c275fce-a4b4-4657-90f1-fcfbb018ba22",
      "source_id": "36f82111-ad27-4407-9c52-b884f3302815",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages\" from arxiv.org:",
      "generation": " Recent progress in image collage generation has largely focused on unconditional methods that apply random transformations to images, followed by optimization of a collage objective. In this work, we revisit the paradigm of collage optimization and propose an approach that transforms image sets by attending to memory of previously seen images, while iteratively generating a collage. We introduce a new memory-based loss that encourages the preservation of memorized image features in the generated collage, and a memory attentive loss that further enforces memorization of recent images. Furthermore, we introduce a new dataset of image sets, where each set is composed of a guiding image and context images related to the same scene. We show that our method, when applied to the generation of image collages from sets of images, either with or without the guiding image, consistently improves upon the state of the art in collage generation. Our code is released at https://github.com/AI4Finance-Foundation/ImageCollage."
    },
    {
      "id": "d1a5f447-6c9e-40de-9f06-7818c4a9460f",
      "adv_source_id": "d1a5f447-6c9e-40de-9f06-7818c4a9460f",
      "source_id": "36f82111-ad27-4407-9c52-b884f3302815",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages\" from arxiv.org:",
      "generation": " Current image collage generation methods tend to produce collages that lack coherence, often failing to tell a clear story. In this paper, we propose a simple yet effective method to transform a set of input images into a memory-guided collage, where the generated collage shows an apparent relationship between its components, telling a clear story. We introduce a neural architecture that combines a memory-based generator with an attention mechanism. It transforms a given set of input images into a collage, while capturing the memories of the input images to guide the generation. In addition, we explore several input settings, including conditioned upon image prompts and unguided. We show that our method outperforms the state of the art in collage generation on both quantitative and qualitative evaluations. We open-source our code at https://github.com/YuZhouWang/Collage-Attention."
    },
    {
      "id": "32580291-8812-4cc9-833c-a04a412f4ee6",
      "adv_source_id": "32580291-8812-4cc9-833c-a04a412f4ee6",
      "source_id": "6524540e-d24e-47e1-8956-86b603c57120",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Identify the cells' nuclei based on the deep learning neural network",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Identify the cells' nuclei based on the deep learning neural network\" from arxiv.org:",
      "generation": " In this paper, we propose a method based on deep learning neural network to identify the cells' nuclei in the microscopic image. The microscopic image is pre-processed to enhance the contrast and then the proposed method is used to detect the nuclei. The proposed method is evaluated on a public dataset and the results show that it outperforms the state-of-the-art methods.\n\nThe pre-processing method is based on the local contrast enhancement algorithm which enhances the contrast of the microscopic image by comparing the intensity of each pixel with its neighboring pixels. The proposed method is a multi-layer convolutional neural network which is trained on a large dataset of microscopic images with ground truth information. The network takes a microscopic image as input and outputs a binary image where the pixels corresponding to the nuclei are labeled as 1 and the rest are labeled as 0. The network is trained using backpropagation algorithm and the weights are updated using stochastic gradient descent.\n\nThe proposed method is evaluated on a public dataset of microscopic images called the Cell nuclei segmentation challenge dataset. The results show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and speed. The proposed method is also able to handle noise and artifacts in the microscopic image better than the state-of-the-art methods."
    },
    {
      "id": "5c434f53-c2e6-402f-a2e7-3a9100e7a933",
      "adv_source_id": "5c434f53-c2e6-402f-a2e7-3a9100e7a933",
      "source_id": "6524540e-d24e-47e1-8956-86b603c57120",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Identify the cells' nuclei based on the deep learning neural network",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Identify the cells' nuclei based on the deep learning neural network\" from arxiv.org:",
      "generation": " In this paper, we propose a method based on deep learning neural network to identify cells' nuclei in histopathology images. We use a multi-channel convolutional neural network (MC-CNN) to extract features of nuclei. The MC-CNN is composed of several convolutional layers with different filters, which can extract different features from the input images. The proposed method is evaluated on a public available dataset, and the results show that our method outperforms the state-of-the-art methods.\n\nOur approach can be divided into three parts: data preprocessing, model training, and segmentation. In the first step, we normalize the size of images and convert them from RGB to YCbCr color space. Next, we train a MC-CNN using a set of images with labeled nuclei. The trained model is then used to segment the nuclei in the input images. In the testing phase, we evaluate the proposed method on a public available dataset and compare the performance with some state-of-the-art methods.\n\nOur results show that the proposed method outperforms the state-of-the-art methods in terms of accuracy and speed. The proposed method is able to identify cells' nuclei with high accuracy and speed, and can be used for medical image analysis and diagnosis."
    },
    {
      "id": "33275d4c-104f-45ad-b4ab-cbfc6413edd1",
      "adv_source_id": "33275d4c-104f-45ad-b4ab-cbfc6413edd1",
      "source_id": "be0133e4-a4b2-4611-a616-e941d7fa8fe2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning\" from arxiv.org:",
      "generation": " Abstract\nWe present a deep learning work flow for fast and robust segmentation of anatomical structures in CT images. The network was trained on publicly available datasets with ground truth annotations for lung and spine. Our network is based on a convolutional neural network architecture previously validated on the MIAS challenge. We exploit existing clinically available delineations, which are used for treatment planning, for the initialization of the network. The initializations are propagated through the network using a weighted vector flow scheme. The proposed method has been validated on our institutional dataset and compared to a state-of-the-art semi-automatic segmentation tool used in our clinic. The comparison showed good agreement between the automatic segmentations and the ground truth for all structures. Average Dice coefficients of 98.1% and 97.8% were achieved for tumor and organs-at-risk, respectively. Our approach led to a reduction of delineation time by 77% for tumors and 69% for organs-at-risk, while maintaining a similar quality of the delineations."
    },
    {
      "id": "0ee72f51-04ab-437b-bcc1-75b9c721665a",
      "adv_source_id": "0ee72f51-04ab-437b-bcc1-75b9c721665a",
      "source_id": "be0133e4-a4b2-4611-a616-e941d7fa8fe2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning\" from arxiv.org:",
      "generation": " Medical imaging from computed tomography (CT) has proven highly beneficial for radiation therapy treatment planning, where the accurate segmentation of anatomical structures is of utmost importance. We present a method for the fully-automatic segmentation of anatomical structures in CT scans, with a particular focus on thoracic organs critical for radiation therapy of lung and thoracic malignancies. Our method is based on a convolutional neural network (CNN) architecture, using a Dice loss function with augmentation and an attention module. We show that with careful data augmentation, including randomness in the propagation of the attention module, a significant increase in segmentation performance can be achieved. We evaluate our method on the challenging publicly available LUNA16 and ISA-CAT thoracic CT datasets, compared to the state-of-the-art, showing significant improvements. Our code and models will be made publicly available."
    },
    {
      "id": "f025f55d-f027-47c0-af8d-e364193937ea",
      "adv_source_id": "f025f55d-f027-47c0-af8d-e364193937ea",
      "source_id": "25bf5762-79a2-4034-ae85-e1035e44f71d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice\" from arxiv.org:",
      "generation": " Abstract: Medical image segmentation plays a pivotal role in advanced healthcare applications enabling precise and non-invasive diagnosis. Numerous approaches have been proposed for segmentation, with evaluation metrics such as Dice score (DS) and Jaccard index (JI) widely used to compare methods. Surprisingly, there exists no systematic study on the optimization of these metrics. In this study, we perform an in-depth analysis of DS and JI for medical image segmentation. We first present a theoretical analysis establishing a tight connection between DS and JI, and then propose an optimization framework based on this connection. We further provide an in-depth evaluation on a variety of real-world medical image segmentation tasks, where our optimization framework is applied. Results show that the proposed optimization delivers significant improvements, with up to 13.7% increase in DS and 23.0% increase in JI, providing stronger baseline performance for future research on medical image segmentation."
    },
    {
      "id": "da603764-472f-4cdb-b697-2f059e0b03c2",
      "adv_source_id": "da603764-472f-4cdb-b697-2f059e0b03c2",
      "source_id": "25bf5762-79a2-4034-ae85-e1035e44f71d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice\" from arxiv.org:",
      "generation": " Abstract: Medical image segmentation plays a pivotal role in aiding physicians with diagnosis and treatment. Compared to other modalities, the curability of lung CT images poses a unique challenge. In this paper, we aim to provide an in-depth analysis of popular evaluation metrics, with a focus on dice score and Jaccard index, for medical image segmentation. We analyze the sensitivity of these metrics to different types of errors and discuss the implications on algorithm design. We propose an optimized Dice score and Jaccard index evaluation framework, with a focus on CT images, which integrates a collection of improvements targeting lung CT images. This framework is publicly available as the DICEJaccard package. We demonstrate the effectiveness of our framework on the public LLIDE dataset. Our framework achieves \u224885% speedup with \u224888% memory reduction. We show through experiments the advantage of our framework over the commonly used Eval studiomatch framework and other alternative implementations. We further discuss potential ways to effectively use these metrics for automatic lung segmentation evaluation."
    },
    {
      "id": "ed49965d-ba33-4931-96f4-d224ffc03122",
      "adv_source_id": "ed49965d-ba33-4931-96f4-d224ffc03122",
      "source_id": "624d70db-276c-46c4-a8a1-b636b9308d04",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging",
      "prompt": "The following is the full text of the abstract for a research paper titled \"U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging\" from arxiv.org:",
      "generation": " Abstract: We introduce a novel approach for sleep staging based on deep learning, more specifically a fully convolutional network (FCN) for time series segmentation (TSM). This method is able to produce a segmentation of a sleep record into successive non-overlapping sleep stages. The novelty of our approach lies in the architecture of the FCN, which takes advantage of the temporal dependencies in the data by introducing a convolutional layer with stride 1, allowing the network to use a rolling window of inputs. We evaluate our method on a publicly available dataset, obtaining an accuracy of 94.4%, outperforming the current state-of-the-art methods."
    },
    {
      "id": "cfb98192-b48f-49fe-82e5-0538978391ce",
      "adv_source_id": "cfb98192-b48f-49fe-82e5-0538978391ce",
      "source_id": "624d70db-276c-46c4-a8a1-b636b9308d04",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging",
      "prompt": "The following is the full text of the abstract for a research paper titled \"U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging\" from arxiv.org:",
      "generation": " We present a novel, fully-convolutional network architecture for time series segmentation applied to sleep staging. Our method, termed U-Net, explicitly models the temporal dependencies in the data, thus making it suitable for time series analysis. In contrast to most previous work on time series classification, we do not chunk the data into short frames nor do we assume that the patterns of interest are localized in time. The proposed method has three main building blocks. The first one is a convolutional layer stack, where we apply a series of convolutional and max-pooling operations to the input time series. The second building block is a recurrent layer stack, where we apply a series of recurrent layers to the convolutional stack output. The third building block is designed to produce a segmentation mask and is composed of several convolutional layers. We evaluate our method on the publicly available Sleep-EDF dataset and achieve a performance on par with the state-of-the-art methods."
    },
    {
      "id": "c5070c61-14e3-44d6-81b5-9f25049a4618",
      "adv_source_id": "c5070c61-14e3-44d6-81b5-9f25049a4618",
      "source_id": "3a90d69b-7e0f-4553-86f6-ac29ffea9b61",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Multiphase Level Set for Scene Parsing",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Multiphase Level Set for Scene Parsing\" from arxiv.org:",
      "generation": " Recent works in scene parsing show that the multiphase level set method, which models image segmentation as a continuous evolution process, can achieve state-of-the-art performance. However, these methods rely on heuristic criteria to merge segments, and do not explicitly model the relationships between the segments, which can limit their performance in the presence of ambiguous or incomplete labels. In this paper, we propose a deep multiphase level set framework that combines the benefits of the level set method with deep learning. Our framework explicitly models the relationships between the segments using a graph neural network, which guides the evolution process of the level set method. We introduce a novel segmentation loss that combines the cross-entropy loss with an adversarial loss, which enables the model to learn from ambiguous or incomplete labels. Our method achieves state-of-the-art performance on the Cityscapes and CamVid datasets, and is able to generalize to new images in a way that other methods are not."
    },
    {
      "id": "317c71ee-580c-479b-9d10-f121a9008631",
      "adv_source_id": "317c71ee-580c-479b-9d10-f121a9008631",
      "source_id": "3a90d69b-7e0f-4553-86f6-ac29ffea9b61",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Multiphase Level Set for Scene Parsing",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Multiphase Level Set for Scene Parsing\" from arxiv.org:",
      "generation": " We present a deep learning approach to semantic scene parsing, a task that aims to assign a semantic label to each pixel in an input image. Our method is based on a multi-phase level set method, which is a variant of the popular watershed algorithm. We train a deep convolutional neural network to predict the evolution of a multiphase level set, where each phase corresponds to a different scene component. Our network takes an input image and outputs a sequence of latent sketches, which are then processed by the multi-phase level set to produce a final parsing. We demonstrate the effectiveness of our approach on the semantic scene parsing benchmark of the Cityscapes dataset, achieving state-of-the-art results."
    },
    {
      "id": "dfc0a474-4e82-4078-b5f5-7b5a0893d5d6",
      "adv_source_id": "dfc0a474-4e82-4078-b5f5-7b5a0893d5d6",
      "source_id": "4762b0a8-cbbc-4e95-bbbc-f6f7333946d8",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NeurReg: Neural Registration and Its Application to Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"NeurReg: Neural Registration and Its Application to Image Segmentation\" from arxiv.org:",
      "generation": " We propose a general framework for registering neural representations of objects. Given a set of reference objects, we learn a mapping function that aligns a test object with the reference objects in feature space. We apply our method to the image segmentation problem where the reference objects are segmentations of training images and the test object is a segmentation of a test image. Our neural registration method is able to improve the accuracy of state-of-the-art segmentation methods on three public datasets. We also demonstrate the general applicability of our framework by applying it to other computer vision problems, such as object recognition and scene understanding, and show improvements in accuracy."
    },
    {
      "id": "b9340d1f-ec4d-4c6b-bf4b-f6c90fdff414",
      "adv_source_id": "b9340d1f-ec4d-4c6b-bf4b-f6c90fdff414",
      "source_id": "4762b0a8-cbbc-4e95-bbbc-f6f7333946d8",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NeurReg: Neural Registration and Its Application to Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"NeurReg: Neural Registration and Its Application to Image Segmentation\" from arxiv.org:",
      "generation": " Emerging neuroimaging technologies have led to massive complex neural data, hindering effective visualization and analysis of brain structures and their functional specialization. Registration is a commonly used method to align neural structures, allowing effective visualization and comparison of heterogeneous neural data. However, current registration methods are mostly based on low-level image intensity similarities, which cannot capture the functional specialization of brain structures, and often result in suboptimal alignment. In this paper, we propose a new framework called NeurReg for neural registration based on a computational model of the neural system. Given a pair of brains with the corresponding functional specialties labeled, NeurReg learns a registration network that simultaneously aligns the structure and predicts the functional specialty, allowing it to discover an alignment that preserves functional specialization. We demonstrate the efficacy of NeurReg on both synthetic and real world image segmentation problems, showing that it can correctly recover the structural alignment and predict the functional mapping between brains with different functional specialties."
    },
    {
      "id": "2f3ba502-2b0e-4082-9c3d-996a265cc0d2",
      "adv_source_id": "2f3ba502-2b0e-4082-9c3d-996a265cc0d2",
      "source_id": "c29c60c6-dfc8-4a33-8ef9-d76b4ce39608",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth",
      "prompt": "The following is the full text of the abstract for a research paper titled \"SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth\" from arxiv.org:",
      "generation": " Recent progress in synthesizing realistic images has boosted the development of numerous computer vision tasks, but most works require a clean target modality ground truth for training. We introduce a new task, named synthetic segmentation, where the goal is to segment objects in a real-world image using only a synthetic image as reference. This setting is appealing since it enables using large amounts of easily obtainable synthetic data to train models for real-world applications. We build SynSeg-Net, a neural network architecture designed to tackle this new task. It leverages a conditional recurrent network that conditions on the input image and a synthetic reference image to produce a segmentation mask. We train and test our model on the challenging task of segmenting medical images, but our method is generic and can be applied to other domains. Our experiments show that our method is able to accurately segment objects in real-world images using only synthetic references, without the need of target modality ground truth."
    },
    {
      "id": "91239d5d-bb53-4364-a576-08ecbdab105a",
      "adv_source_id": "91239d5d-bb53-4364-a576-08ecbdab105a",
      "source_id": "c29c60c6-dfc8-4a33-8ef9-d76b4ce39608",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth",
      "prompt": "The following is the full text of the abstract for a research paper titled \"SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth\" from arxiv.org:",
      "generation": " Recent works have shown that the advent of deep learning has enabled the utilization of synthetic data for training machine learning models. However, all these methods rely on having ground truth (GT) data for the target modality, which is hard to obtain in many real-world applications. We overcome this limitation by casting the problem as a domain adaptation (DA) problem, where the source domain is a synthetic dataset generated using a 3D volumetric simulator and the target domain is the real test data. We do not use any GT data for the target domain and leverage only image-to-image (I2I) correspondences between the source and target domains as provided by the simulator. To this end, we propose SynSeg-Net, a powerful generative model that leverages an encoder-decoder architecture with skip connections, and trains the model with an adversarial loss and an I2I correspondence loss in a DA-setting. Our results show that our method is able to learn to segment 2D prostate MRI images with a Dice similarity coefficient (DSC) of 0.81, which is comparable to state-of-the-art methods that utilize GT data for the target modality."
    },
    {
      "id": "8455a2a3-3bb5-4bea-9990-056fe3a4811d",
      "adv_source_id": "8455a2a3-3bb5-4bea-9990-056fe3a4811d",
      "source_id": "ca486756-3abb-4c00-bdf8-a000bef913a4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent progresses in convolutional neural network (CNN) elevate image-to-image (I2I) translation to a new horizon, but most existing methods require a large amount of labeled source images for training, which is difficult to obtain in many real-world applications. In this paper, we attempt to address this issue by introducing a new I2I translation framework with extremely weak supervision, where only a few labeled source images and a large number of unlabeled source images are available for training. In particular, we first train a teacher network with a few labeled source images to obtain pseudo labels for all source images. Then, we learn a student network with the pseudo labels and the unlabeled source images for I2I translation. To further improve the translation quality, we present a cycle-consistent loss to enhance the learning of the student network. Finally, we evaluate our method on the Cityscapes dataset for urban scene understanding, and experimental results show the effectiveness of our method."
    },
    {
      "id": "66479987-50f8-4654-bd60-2628849087c5",
      "adv_source_id": "66479987-50f8-4654-bd60-2628849087c5",
      "source_id": "ca486756-3abb-4c00-bdf8-a000bef913a4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent progress in image-to-image translation has largely been driven by supervised learning approaches, which require a large number of aligned image pairs with segmentations. This is difficult and expensive to obtain for semantic segmentation, where the labeled images are often obtained by labeling the segments in the image manually, and the segmentation labels usually have a small number of classes. In this paper, we present a new approach to unsupervised image-to-image translation for semantic segmentation. It is based on the idea of learning a mapping from the image to a latent space, then mapping the latent space to the target domain. We train our model using only a small number of labeled images and a large number of unlabeled images. Our approach is capable of translating images from the source domain to the target domain, and it can also recover the segmentation labels from the source image. We demonstrate the effectiveness of our approach on the cityscapes dataset, where we achieve a translation quality of 68.3% in terms of the cityscapes evaluation metric. Our approach is trained using only 200 images with ground truth segmentation, which is an order of magnitude smaller than the number of images used in previous works."
    },
    {
      "id": "8e6b6f91-4cfd-4eaf-bb7b-6b509021988d",
      "adv_source_id": "8e6b6f91-4cfd-4eaf-bb7b-6b509021988d",
      "source_id": "6fd64468-6ce6-4c70-a32f-70d8f9095853",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mumford-Shah Loss Functional for Image Segmentation with Deep Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mumford-Shah Loss Functional for Image Segmentation with Deep Learning\" from arxiv.org:",
      "generation": " We revisit the Mumford-Shah functional for image segmentation, and introduce a continuous version of the energy functional that enables the use of deep learning architectures. We build upon recent progress in the field of geometric deep learning, and construct a supervised loss functional that is suitable for learning piecewise constant mappings in a setting where the ground truth segmentation is available. When training our model, we minimize a reformulation of the Mumford-Shah energy that is suitable for deep learning, and introduce a strategy to mitigate the lack of differentiability of the data term. We demonstrate the efficacy of our approach on a public available dataset, showing that our approach can recover the ground truth segmentations."
    },
    {
      "id": "688867a5-9053-4ca9-bff5-c10d332bd41d",
      "adv_source_id": "688867a5-9053-4ca9-bff5-c10d332bd41d",
      "source_id": "6fd64468-6ce6-4c70-a32f-70d8f9095853",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mumford-Shah Loss Functional for Image Segmentation with Deep Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mumford-Shah Loss Functional for Image Segmentation with Deep Learning\" from arxiv.org:",
      "generation": " We revisit the Mumford-Shah functional for image segmentation, and introduce a continuous relaxation of the famous infimal convolution equation, which appears in the computation of the functional's gradient. This relaxation yields a new loss functional for image segmentation, which we term Inception-Disception (ID) loss. In the context of deep learning, the ID loss functional can be viewed as an extension of the cross-entropy loss to image segmentation: just as the cross-entropy loss is an entropy-based generalization of the squared error loss, the ID loss is an entropy-based generalization of the Mumford-Shah functional. We show that the ID loss functional has a number of nice properties, and perform a series of experiments on the semantic segmentation of natural images, demonstrating that a deep learning model trained with the ID loss performs exceptionally well on the famous Cityscapes dataset, and compares favorably with a state-of-the-art model trained with the cross-entropy loss."
    },
    {
      "id": "34f8af1f-b596-4e93-ad1a-1950926db46d",
      "adv_source_id": "34f8af1f-b596-4e93-ad1a-1950926db46d",
      "source_id": "9dc9ce24-bb8d-4c22-a2e9-a9f6708a7aad",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Temporality for Semi-Supervised Video Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Exploiting Temporality for Semi-Supervised Video Segmentation\" from arxiv.org:",
      "generation": " Recent works on video segmentation commonly formulate the problem as a per-frame segmentation followed by a temporal aggregation, which discards important temporal information. In this work, we exploit the temporal coherence among video frames to improve the semi-supervised video segmentation. We propose a novel temporal coherence constraint, which regularizes the segmentation predictions across consecutive frames. Furthermore, in order to effectively utilize the abundant unlabeled data, we propose a novel edge-conditioned prototype learning framework to enhance the quality of the segmentation prototypes. Specifically, we train a classifier to predict edge presence based on the low-level features of the video frames, and then use the predicted edge maps to selectively update the segmentation prototypes. Extensive experiments on DAVIS 2016 and DAVIS 2017 validate the effectiveness of our approach compared to the state-of-the-art methods."
    },
    {
      "id": "053d8811-6892-4a83-9c53-1dcfabe6652c",
      "adv_source_id": "053d8811-6892-4a83-9c53-1dcfabe6652c",
      "source_id": "9dc9ce24-bb8d-4c22-a2e9-a9f6708a7aad",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Temporality for Semi-Supervised Video Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Exploiting Temporality for Semi-Supervised Video Segmentation\" from arxiv.org:",
      "generation": " Recent works on video segmentation commonly rely on a sparse temporal annotation, which requires extensive, often tedious, human labor. In this paper, we study the problem of exploiting temporal information from a single video for semi-supervised video segmentation, where the only provided annotation is a single image label of the video. Specifically, we propose to estimate the uncertainty of a pixel's segmentation label given its temporal history, which allows us to synthesize realistic pseudo-labels for all unlabeled frames. Furthermore, we show that this uncertainty can be effectively regularized by an adversarial network with minimal computational overhead. When our method is applied to the cityscapes dataset, we achieve 68.3% mean accuracy, outperforming the state-of-the-art fully supervised methods by 1.2% with only a single frame labeled."
    },
    {
      "id": "8553560a-4fc7-4b03-ad39-1a0a3ea329c5",
      "adv_source_id": "8553560a-4fc7-4b03-ad39-1a0a3ea329c5",
      "source_id": "3728d50f-31c2-42a0-ac3d-e3a4df58cdb3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths\" from arxiv.org:",
      "generation": " Recent works on pixel-level segmentation either rely on strong annotations such as pixel-wise labels or bounding box annotations, or assume that the segments of interest occupy a relatively large proportion of the image. We introduce a new framework for weakly-supervised segmentation of small segments without requiring pixel-wise annotations. Our key insight is to use a new variant of the popular Scan statistic that we name Scan-Dynamic. This new variant is based on a dynamic programming approach that iteratively contracts and expands a segment, and uses the statistical significance of the change in intensity value at the boundary of adjacent segments as a criterion for segmenting. We show that this criterion is able to correctly identify the segments in weakly-supervised fashion, and that our method generalizes well to different segmentation tasks. We implement this end-to-end system, evaluate it on the publicly available VASC- Segment dataset, and show that our method outperforms the state of the art in terms of F1-score on small segments."
    },
    {
      "id": "935c8983-415d-419b-9e00-5bcfad79d2b6",
      "adv_source_id": "935c8983-415d-419b-9e00-5bcfad79d2b6",
      "source_id": "3728d50f-31c2-42a0-ac3d-e3a4df58cdb3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths\" from arxiv.org:",
      "generation": " Biomedical image segmentation is a basic but difficult task in medical imaging analysis. Deep learning methods have shown promising results in this area but require large amounts of labeled training data, which are often scarce in biomedical imaging. In this paper, we present ACE-Net, a new deep learning framework for biomedical image segmentation that addresses the limited training data problem by leveraging abundant unlabeled data and only a small amount of labeled data. ACE-Net learns to segment images using a contracting and expansive path mechanism that alternately optimizes contracting and expansive path filters using only a small number of labeled and unlabeled training examples. Our framework is flexible and can be used to segment a variety of biomedical images, including histopathology images, fluorescence microscopy images, and MRI images. We evaluate ACE-Net on the ISBI 2018 microscopy segmentation challenge and achieve competitive results using only a small fraction of the training data. Our method also substantially improves the state-of-the-art in MRI knee cartilage segmentation against two strong baselines."
    },
    {
      "id": "3322841b-20e0-4a2f-95fe-b9ec13071a12",
      "adv_source_id": "3322841b-20e0-4a2f-95fe-b9ec13071a12",
      "source_id": "3045ff11-c626-427f-a9d9-7e9b98c5055e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization\" from arxiv.org:",
      "generation": " Recent works on visual localization have shown that the direct use of segmentation masks for localization, trained on large-scale datasets like Cityscapes and CamVid, improves localization performance. However, these segmentation masks are typically obtained by training networks on coarse object-level classes, which results in poor localization performance in regions where objects overlap or occlude each other. In this paper, we show that using fine-grained segmentation networks for localization yields significant improvements in accuracy. We introduce a new dataset, called SceneCrafter, which includes fine-grained segmentation masks for training and evaluate our method on the KITTI dataset. Our network learns to predict fine-grained segmentation masks purely from the image, without any ground truth segmentation information. In addition, we introduce a new loss function for fine-grained segmentation, which we call the coverage loss, that encourages the network to predict masks that cover most of the object instances in a given image. Our method achieves state-of-the-art performance on both the segmentation and localization benchmarks."
    },
    {
      "id": "41d08efb-0970-4666-b722-7167ee62b50a",
      "adv_source_id": "41d08efb-0970-4666-b722-7167ee62b50a",
      "source_id": "3045ff11-c626-427f-a9d9-7e9b98c5055e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization\" from arxiv.org:",
      "generation": " Current state-of-the-art visual localization algorithms rely on accurate segmentation masks for object detection and depth estimation. However, segmentation networks are trained independently of the visual localization algorithm, resulting in sub-optimal performance. We introduce a self-supervised segmentation network that is explicitly trained to improve long-term visual localization. Our network generates fine-grained segmentation masks, compared to the coarser annotations usually used for training. We show that fine-grained segmentation brings three benefits to visual localization: a better initialization for the subsequent detection pipeline, more reliable object detection, and better depth estimation. We evaluate our method on the KITTI odometry dataset and show that our network improves the accuracy of an industrial visual localization system."
    },
    {
      "id": "5cd65e96-a32e-4503-83b8-35aa65005a0e",
      "adv_source_id": "5cd65e96-a32e-4503-83b8-35aa65005a0e",
      "source_id": "d9e15ea1-7544-4aee-bf98-cfdae94eacdc",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation. In this paper, we introduce a simple yet effective plug-and-play module for joint segmentation of anatomical structures in medical images, which we call Conv-MCD. Our module is built upon a basic convolutional neural network (CNN) architecture, where we replace the last few layers with our proposed Conv-MCD layer. This layer computes consensus outputs among the individual structures in an image, while still retaining the ability to predict the probabilities of the structures jointly. We show that our method, with a properly chosen margin loss, is able to produce competitive results for different medical image segmentation tasks, compared to other state-of-the-art multi-task learning methods, while requiring minimal tuning. We evaluate our method on four different medical image segmentation datasets of different modalities (CT, MRI, and Micro-CT) and show its effectiveness in both single-structure and multi-structure segmentation."
    },
    {
      "id": "4f56ebcf-2102-415b-8c8a-0b07b18d2dac",
      "adv_source_id": "4f56ebcf-2102-415b-8c8a-0b07b18d2dac",
      "source_id": "d9e15ea1-7544-4aee-bf98-cfdae94eacdc",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation. In this paper, we introduce a simple yet effective module termed as Convolutional MCD (Conv-MCD) for joint segmentation of medical images. Our proposed module is formulated by a series of convolutional layers with a specific design, which can be easily plugged into any existing segmentation frameworks to significantly improve their performance on joint segmentation tasks. In particular, we design a new dimensional reduction module that reduces the input channels to a low-dimensional feature representation while keeping the spatial information unchanged. Two segmentation branches are then built upon such low-dimensional feature representations to segment the images with joint constraints. We optimize the proposed Conv-MCD module to minimize the disagreement of segmentation results from both branches, which is measured by an ad-hoc loss function. Since the proposed module only includes convolutional layers with channel reduction and no additional parameters are required, our Conv-MCD is quite easy to be implemented and plugged into any existing segmentation frameworks. In the experiment part, we demonstrate the effectiveness of our proposed module on four public medical image segmentation benchmarks, and achieve the state-of-the-art performance."
    },
    {
      "id": "103c9fb8-14ad-4ba8-9717-5a3084a1874b",
      "adv_source_id": "103c9fb8-14ad-4ba8-9717-5a3084a1874b",
      "source_id": "6c1e5ba0-032d-417c-8ebc-c5f45992f6a8",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks\" from arxiv.org:",
      "generation": " Recent works have shown that the attention mechanism can be applied to convolutional neural networks (CNNs) to improve performance on image recognition tasks. We observe that these works mostly focus on applying attention mechanism within the last few layers of CNNs, where the spatial dimensions of the feature maps are already very small, and the attention mechanism can effectively \"attend\" to the most important features. However, in the early layers of CNNs where the feature maps are still very large, the attention mechanism is often not effective, as there are simply too many positions for the attention to focus on. In this work, we propose a simple and effective approach to overcome this limitation. We introduce a new type of layer that we call the \"AttentionBoost\" layer, which works by combining the feature maps from the previous layer with the attention maps generated by a single-layer CNN. This allows the attention mechanism to effectively attend to the most important features in the early layers of the network, even when the feature maps are very large. Our experimental results on the CIFAR-10 and CIFAR-100 datasets show that our approach can improve the performance of CNNs by a large margin. Our approach is simple to use and effective, and can be easily applied to existing CNN architectures."
    },
    {
      "id": "ed09d420-559e-484a-8a15-60bf44f47fff",
      "adv_source_id": "ed09d420-559e-484a-8a15-60bf44f47fff",
      "source_id": "6c1e5ba0-032d-417c-8ebc-c5f45992f6a8",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks\" from arxiv.org:",
      "generation": " Recent works have shown that convolutional neural networks (CNNs) can learn robust representations of visual input without needing explicit attention mechanisms. We propose AttentionBoost, a simple method to boost the ability of fully-convolutional CNNs to attend to relevant information. AttentionBoost uses a boosting approach, training a series of models with increasing complexity, represented as additional pooling operations. During training, each model attends to different sized patches of input, with the most complex model attending to objects at the scale of the entire image. We demonstrate through exhaustive experiments that this simple method advances the state-of-the-art performance on the YouTube Object Detection dataset using the public Facebook Research (FSR) model releases without any changes to the architecture or training procedure. We achieve a mAP of 41.3% on YOUTUBE-VID and 52.7% on YOUTUBE- Still, outperforming the previous best by 2.2 and 3.2 points, respectively, and the FSR model by 3.4 and 4.2 points without utilizing any attention or post-processing methods."
    },
    {
      "id": "b533406d-f762-4758-a55a-cedd0d6f3549",
      "adv_source_id": "b533406d-f762-4758-a55a-cedd0d6f3549",
      "source_id": "00e000e6-0bf4-4e8a-8484-f377088eb55a",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in semantic segmentation mainly focuses on accurately modeling the underlying object structures in images, while the quality of the training data is often overlooked. Noisy annotations present in practice, however, may deteriorate the performance of the segmenters and hinder their generalization. In this paper, we aim to develop a simple yet effective automatic quality evaluation (QE) framework for noisy-labeled image segmentation, which can benefit both model training and result filtering. We find that the recently proposed Pick-and-Pack network [1] for segmenting objects with complex shapes is particularly suitable for QE due to its voxel-wise confidence output. We build upon it to formulate a QE framework, named Pick-and-Learn, which consists of two stages. Given a pre-trained network, we first apply Pick-and-Pack on the training data to obtain initial predictions. Then, we treat these initial predictions as noisy labels and fine-tune the network in an unsupervised learning manner, which we refer to as self-supervised learning. Specifically, we minimize the disagreement between the segmentation results and the initial predictions on the training data. We find that the fine-tuned network can effectively evaluate the quality of the initial predictions, and moreover, the segmentation results on the test data. We evaluate Pick-and-Learn on two public noisy-label segmentation benchmarks, and achieve promising results compared to the state-of-the-art QE methods."
    },
    {
      "id": "0d4e54d5-fdb0-4427-ab2e-429f2dda6e28",
      "adv_source_id": "0d4e54d5-fdb0-4427-ab2e-429f2dda6e28",
      "source_id": "00e000e6-0bf4-4e8a-8484-f377088eb55a",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in image segmentation is predominantly driven by large-scale annotated datasets. However, manually labeling images is expensive and time-consuming, and the quality of the labels decreases with the size of the dataset. In this paper, we introduce Pick-and-Learn, a simple yet effective solution to training segmentation models with noisy labels. Our key observation is that, during training, most pixels of interest for a pixel-level segmentation model reside in the confident regions, where the model can easily learn from the clean labels in a small set of carefully selected training samples. Hence, we cast the training process as a picking problem: given a limited budget, our goal is to select a (noisy) labeled sample that best preserves the confident region for each pixel. Specifically, we propose a picking strategy that minimizes the information loss of the confident region and a learning process that compensates for the noise in the picked samples. Experiments on both synthetic and real-world datasets demonstrate the efficacy of our approach against several strong baseline methods."
    },
    {
      "id": "bd9381a7-2a0e-4627-9705-f6c09bc870d7",
      "adv_source_id": "bd9381a7-2a0e-4627-9705-f6c09bc870d7",
      "source_id": "91c38d06-f963-4f7c-b22a-6942b2fe2917",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent progresses in medical imaging acquisition technology have resulted in a dramatic increase of image size, posing severe challenges to existing model-based segmentation methods. Existing work usually downsamples the data to manageable size, resulting in important details being lost. In this paper, we propose ET-Net, a generic edge-attention guidance network for medical image segmentation that is capable of handling large images at full resolution. ET-Net separates the learning of easy and hard information, the former being captured by a series of edge-attention modules and the latter by a guidance network. Specifically, the edge-attention modules can capture the object edge information and texture information effectively, while the guidance network focuses on learning the complex and ambiguous information. We apply ET-Net to three different segmentation tasks, and experimental results on three public benchmarks, includingLiver Tumor, Pancreas Tumor and Prostate, demonstrate the effectiveness of our approach compared to the state-of-the-art."
    },
    {
      "id": "b5041194-066b-423c-a221-b88644bb93e0",
      "adv_source_id": "b5041194-066b-423c-a221-b88644bb93e0",
      "source_id": "91c38d06-f963-4f7c-b22a-6942b2fe2917",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Abstract: We present a generic edge-attention guidance network (ET-Net) for medical image segmentation. ET-Net leverages the power of edge detection and attention mechanisms to capture multi-scale structural information and contextual relationships for effective segmentation. It introduces a novel edge-attention module that computes edge features and attention weights simultaneously from input image features. The detected edge features convey structural information, while the attention weights highlight the regions that contribute most to the segmentation task. These multi-scale features are then exploited for predicting tissue labels and refining the predictions via an iterative network design. We evaluate ET-Net on the ISLES 2017 segmentation challenge and achieve promising results, demonstrating the efficacy of the proposed approach compared to other state-of-the-art methods."
    },
    {
      "id": "bde7e048-6577-4b1a-ad6b-7badda696c6e",
      "adv_source_id": "bde7e048-6577-4b1a-ad6b-7badda696c6e",
      "source_id": "68f52eba-10ab-4832-a715-1a83b7304752",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient 3D CNN for Action/Object Segmentation in Video",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Efficient 3D CNN for Action/Object Segmentation in Video\" from arxiv.org:",
      "generation": " We present a spatiotemporal convolutional neural network (3D CNN) for action/object segmentation in video, which is efficient in terms of computational complexity and able to achieve real-time performance. Our approach leverages 3D convolutional kernels that operate directly on video frames, without the need for object proposals or temporal pooling. We introduce a new 3D convolutional layer, which enables our network to effectively capture spatial and temporal information, and a multi-scale training strategy to improve the network's performance. Our method achieves state-of-the-art performance on the YouTube-Object Segmentation dataset and is efficient enough to achieve real-time performance on a modern CPU. Code is available at https://github.com/yxgeee/Realtime-Object-Segmentation."
    },
    {
      "id": "980bc9be-b0ad-4c28-bc7e-022d76dda3b3",
      "adv_source_id": "980bc9be-b0ad-4c28-bc7e-022d76dda3b3",
      "source_id": "68f52eba-10ab-4832-a715-1a83b7304752",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient 3D CNN for Action/Object Segmentation in Video",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Efficient 3D CNN for Action/Object Segmentation in Video\" from arxiv.org:",
      "generation": " We present a spatio-temporal 3D convolutional neural network (3D CNN) for action/object segmentation in video, which takes a volumetric representation of a video clip and performs action/object segmentation in continuous dynamic 3D space. Our 3D CNN architecture follows the recently proposed I3D network that leverages the power of 3D convolutions for effective feature representation in video data. We adopt the I3D network and modify it for action/object segmentation by adding a semantic segmentation branch and a spatial temporal module in order to produce pixel-wise predictions. In order to evaluate the effectiveness of the proposed method, we conduct extensive experiments on the recently proposed ASTRA dataset, which includes 10 different action classes and 18 object classes. Our method achieves state-of-the-art performance on both action and object segmentation. We also provide a thorough ablation study to highlight the impact of different design choices."
    },
    {
      "id": "b084b15d-e584-41e6-b589-f7bed4a884a3",
      "adv_source_id": "b084b15d-e584-41e6-b589-f7bed4a884a3",
      "source_id": "8a130ebe-e41d-4815-9616-a12868315538",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis\" from arxiv.org:",
      "generation": " In this paper, we present an efficient traversability analysis approach for autonomous terrain vehicles using stereo vision. In order to perform real-time analysis of the terrain, we employ a stereo-based approach, where the terrain surface is reconstructed from the stereo pair using semi-global matching (SGM). The reconstructed surface is then segmented into traversable and non-traversable regions using a normal-based segmentation algorithm. To further improve the accuracy of the segmentation, we then analyze the superpixel surface of the reconstructed 3D terrain. Finally, we employ a machine learning approach to train a classifier for the traversability analysis. The experimental results, conducted on different datasets, show that our proposed approach can achieve accurate and efficient traversability analysis for autonomous terrain vehicles."
    },
    {
      "id": "caa4a0a1-9ba5-4069-8662-af6e5b7d2c2a",
      "adv_source_id": "caa4a0a1-9ba5-4069-8662-af6e5b7d2c2a",
      "source_id": "8a130ebe-e41d-4815-9616-a12868315538",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis\" from arxiv.org:",
      "generation": " Towards robust terrain traversability analysis, we propose a novel approach to detect traversable and non-traversable regions in stereo images based on normal-based segmentation and superpixel surface analysis. We exploit the geometric structure of stereo disparity space to capture surface normals. By integrating the baseline stereo semiglobal matching with a local refinement strategy, we generate accurate surface normal maps from stereo images. Surface normals are then utilized to derive geometric stereo features for traversability analysis. Specifically, we exploit the relationship between surface normal and disparity variation to capture local surface variations. By integrating the proposed approach within a larger system, we demonstrate the effectiveness of the proposed approach on a comprehensive dataset. The evaluation results show that our approach achieves precision and recall of 0.856 and  0.918, respectively."
    },
    {
      "id": "da2214c3-c3ed-498d-afbe-07cdfba71a6b",
      "adv_source_id": "da2214c3-c3ed-498d-afbe-07cdfba71a6b",
      "source_id": "a3dcfb77-b080-42ec-93cb-f00d1ed75ef4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Understanding Deep Learning Techniques for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Understanding Deep Learning Techniques for Image Segmentation\" from arxiv.org:",
      "generation": " Deep learning techniques have demonstrated state-of-the-art performance in various image processing tasks, particularly in image segmentation. However, despite their success, these techniques often struggle with segmenting images in a way that aligns with human intuition and expectations. In this paper, we aim to understand why deep learning techniques sometimes fail to produce acceptable results in image segmentation. We investigate the reasons for this gap between machine and human performance by comparing the decisions made by state-of-the-art deep learning models with human judgments. We find that deep learning models often struggle with segmenting objects that are visually similar to their surroundings, and with handling object boundaries that are not well defined in the image. To address these issues, we propose a number of modifications to deep learning architectures and training procedures that improve performance on these challenging segments. Our proposed methods focus on enhancing the representation learning capabilities of deep learning models, improving their ability to reason about object context and relationships, and making them more robust to variations in object appearance and boundary ambiguity. We demonstrate the effectiveness of our approaches on a variety of image segmentation tasks, and show that our methods can significantly improve the performance of deep learning models in these cases."
    },
    {
      "id": "7cb134f2-7e87-49fa-adf3-c21811924ecb",
      "adv_source_id": "7cb134f2-7e87-49fa-adf3-c21811924ecb",
      "source_id": "a3dcfb77-b080-42ec-93cb-f00d1ed75ef4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Understanding Deep Learning Techniques for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Understanding Deep Learning Techniques for Image Segmentation\" from arxiv.org:",
      "generation": " Abstract:\n\nDeep learning techniques have demonstrated remarkable performance in image segmentation tasks. In this paper, we provide a comprehensive review of recent advances in deep learning for image segmentation. We first introduce the concept of image segmentation and discuss the challenges in this field. We then review the state-of-the-art deep learning architectures and methods that have been developed for image segmentation, including fully convolutional networks (FCNs), U-Net, and Mask R-CNN. We discuss the advantages and limitations of each approach and highlight the most effective techniques. We also review the common evaluation metrics used in the image segmentation literature and discuss the impact of dataset size and quality on performance. Finally, we provide a brief overview of the application of image segmentation in medical imaging and discuss the potential of deep learning for clinical practice."
    },
    {
      "id": "2403cd62-3f5d-4c9f-933d-16390abfcd38",
      "adv_source_id": "2403cd62-3f5d-4c9f-933d-16390abfcd38",
      "source_id": "9d1b4e7c-7d7c-40fb-b26b-ccaa1c8242a8",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A General Framework for Complex Network-Based Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A General Framework for Complex Network-Based Image Segmentation\" from arxiv.org:",
      "generation": " We present a general framework for image segmentation based on complex networks. Our approach leverages the inherent relationships between pixels in an image to segment it into objects. These relationships are captured by a graph, where each pixel is a node and the edges capture the local pixel similarities. The segmentation is performed by first learning a complex network from the pixel graph, and then partitioning the complex network into segments using a spectral clustering approach. We demonstrate the efficacy of our approach on three different image segmentation datasets, showing that our method achieves state-of-the-art performance."
    },
    {
      "id": "36223160-a220-4344-9e36-2ab62e57031e",
      "adv_source_id": "36223160-a220-4344-9e36-2ab62e57031e",
      "source_id": "9d1b4e7c-7d7c-40fb-b26b-ccaa1c8242a8",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A General Framework for Complex Network-Based Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A General Framework for Complex Network-Based Image Segmentation\" from arxiv.org:",
      "generation": " We present a general framework for image segmentation based on complex networks. Our approach leverages the inherent relationships between pixels in an image, captured as a graphical model, to partition the image into semantically meaningful regions. We construct a complex network from the graphical model, where each node represents a pixel and edges capture pairwise relations. We then apply community detection to the complex network to identify regions within the image. Our framework is flexible and can be applied to a wide range of image segmentation tasks. We demonstrate the effectiveness of our approach on three different segmentation benchmarks: pixel-level labeling, super-pixel extraction, and region-based segmentation. Our method outperforms the state-of-the-art in pixel-level labeling on the PASCAL VOC 2012 dataset. We also achieve competitive results on super-pixel extraction and region-based segmentation, outperforming several recently proposed methods."
    },
    {
      "id": "d31ef00c-c7e9-4b19-bf81-d4123ca205de",
      "adv_source_id": "d31ef00c-c7e9-4b19-bf81-d4123ca205de",
      "source_id": "f2d40d9e-c6ac-4f4a-bae6-a5cd0cbd355b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning of Image Dehazing Models for Segmentation Tasks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning of Image Dehazing Models for Segmentation Tasks\" from arxiv.org:",
      "generation": " Arxiv is a website where researchers in physics, mathematics, computer science, and related fields can publish articles on their research that are incomplete or otherwise not suitable for submission to traditional journals. Arxiv is an important resource for researchers to communicate their findings and get feedback from the community.\n\nThe learning of image dehazing models is important for segmentation tasks. A new method for learning dehazing models is proposed based on the observation that the haze-free image can be generated by two different processes: restoration and separation. The proposed method learns a dehazing model by minimizing the difference between the predicted haze-free image and the ground truth image, where the ground truth image is obtained by combining the restoration and separation processes. The proposed method has been tested on the haze-free image dataset of 100 images and the results show that the method can learn a dehazing model that can improve the performance of segmentation tasks.\n\nThe learned dehazing models can be used for segmentation tasks such as scene understanding and object detection. The proposed method can also be used to learn a dehazing model for a specific image, where the specific image is used as the ground truth image. This allows the method to learn a dehazing model that is tailored to the specific image and can improve the performance of segmentation tasks on that image.\n\nArxiv is a valuable resource for researchers to communicate their findings and get feedback from the community. The learning of image dehazing models is an important topic for segmentation tasks, and the proposed method can learn a dehazing model that can improve the performance of segmentation tasks."
    },
    {
      "id": "90b3907d-a4a8-4666-b3b8-f1d37ac09c21",
      "adv_source_id": "90b3907d-a4a8-4666-b3b8-f1d37ac09c21",
      "source_id": "f2d40d9e-c6ac-4f4a-bae6-a5cd0cbd355b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning of Image Dehazing Models for Segmentation Tasks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning of Image Dehazing Models for Segmentation Tasks\" from arxiv.org:",
      "generation": " Arguably, the most direct application of dehazing in the field of computer vision is to improve the visibility of the scene contents, thereby aiding segmentation algorithms in identifying the principal objects of interest in the image. In this paper we propose a simple but effective approach for joint learning of dehazing and segmentation models. We show that by training an encoder with a suitable architecture in a self-supervised manner to remove haze from a given image, the decoder can be used as a standalone segmentation model, with performance on par with state-of-the-art unsupervised segmentation methods. We present a thorough evaluation of our method on the Cityscapes dataset with comparison to state-of-the-art segmentation methods in terms of both objective metrics and visual inspection."
    },
    {
      "id": "fa64d9e5-073f-43b7-b5a8-8f8365b52f5e",
      "adv_source_id": "fa64d9e5-073f-43b7-b5a8-8f8365b52f5e",
      "source_id": "43f9c3dd-2f60-4f4f-bdc1-03a60d49c4c4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images\" from arxiv.org:",
      "generation": " Deep learning approaches have shown promising results in image segmentation tasks. However, directly using deep learning architectures such as convolutional neural networks (CNNs) on small patches of images leads to an excessive number of parameters and thus a significant amount of memory and computation. In this paper, we propose a method for patching and aggregating predictions on image regions using fully convolutional networks (FCNs) with skip connections. Our approach allows us to deal with different patch sizes and shapes, and to aggregate predictions in an effective way. We demonstrate the performance of our approach on retinal image segmentation, but it can be applied to any other image segmentation task. In particular, when tested on the DRIVE dataset, our approach achieves a mean accuracy of 95.43% on the pixel level and 94.10% on the region level."
    },
    {
      "id": "b6a4c972-7674-418c-a0fc-253158a522af",
      "adv_source_id": "b6a4c972-7674-418c-a0fc-253158a522af",
      "source_id": "43f9c3dd-2f60-4f4f-bdc1-03a60d49c4c4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images\" from arxiv.org:",
      "generation": " Deep learning approaches have recently achieved great performance in image segmentation. However, in order to achieve such performance, they rely on a large amount of manually annotated training data. This is a difficult and expensive task, especially in the medical imaging field, where the segmentation labels are usually obtained by expert radiologists. In this work, we present an approach for image segmentation that learns from patches. We propose to directly learn a mapping from patches to segmentations, without relying on an intermediate image representation. To this end, we use a fully convolutional network trained with an appropriate loss function. We demonstrate the performance of our approach on retinal image segmentation, but it can be applied to any other image segmentation task. Compared to other state-of-the-art methods, our approach reduces the amount of training data required and inherits all the benefits of the fully convolutional network framework, such as the ability to deal with hierarchical architectures and to handle images of different sizes."
    },
    {
      "id": "c929a156-e984-407d-a878-d7219f8aaa02",
      "adv_source_id": "c929a156-e984-407d-a878-d7219f8aaa02",
      "source_id": "bec92e93-d17a-4252-86da-9e53c55211b4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation\" from arxiv.org:",
      "generation": " Skin lesion segmentation is an important yet challenging task in medical image analysis. We present an approach to handle inter-annotator agreement (IAA) for automated skin lesion segmentation. Our approach is based on a fully-automated system that segments skin lesions in clinical images. We use a deep learning architecture, called U-Net, to achieve state-of-the-art performance on the task. To handle IAA, we propose to use a ensemble of segmentation models trained on different subsets of the available training data. The proposed approach is evaluated on a large dataset of clinical images and compared to a single model trained on the full dataset. Results show that the ensemble approach significantly improves performance in terms of agreement with ground truth segmentation. Furthermore, the proposed ensemble approach shows excellent agreement with expert annotations, with a Cohen's kappa coefficient of 0.85. This agreement is comparable to that between experts, and indicates that the proposed approach is suitable for use in clinical practice."
    },
    {
      "id": "70258f3f-06d1-4f12-973d-7e791ab9d1e7",
      "adv_source_id": "70258f3f-06d1-4f12-973d-7e791ab9d1e7",
      "source_id": "bec92e93-d17a-4252-86da-9e53c55211b4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation\" from arxiv.org:",
      "generation": " Skin lesion segmentation is an important yet challenging task in medical image analysis. We present an approach to handle inter-annotator agreement (IAA) for automated skin lesion segmentation. We first define a new measure for IAA called the F-score. This measure is inspired by the F-score used in machine learning, and it captures the precision and recall of the annotations. We then propose a method to estimate the F-score for a set of annotations. This method is based on the assumption that the annotations are independently generated. We demonstrate the use of our approach on a dataset of skin lesion images and show that our method can handle IAA for automated skin lesion segmentation."
    },
    {
      "id": "8499255a-5c49-494b-982e-a5d1cdf45ea1",
      "adv_source_id": "8499255a-5c49-494b-982e-a5d1cdf45ea1",
      "source_id": "b4048678-ad25-460f-b1e3-c104fbf529dc",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary Loss for Remote Sensing Imagery Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Boundary Loss for Remote Sensing Imagery Semantic Segmentation\" from arxiv.org:",
      "generation": " Recent progress in deep learning has enabled the development of effective models for remote sensing imagery semantic segmentation. However, these models often suffer from boundary loss, which can lead to inaccurate results and hinder their performance in real-world applications. In this paper, we investigate the boundary loss problem and propose a novel approach to address it. Our approach consists of a boundary-aware loss function that explicitly considers the boundary regions during training. We demonstrate the effectiveness of our approach on two public remote sensing imagery semantic segmentation benchmarks, and achieve state-of-the-art performance. Our code and model weights are publicly available."
    },
    {
      "id": "94c83b4f-ca66-48ae-ad30-ed0b2967f46a",
      "adv_source_id": "94c83b4f-ca66-48ae-ad30-ed0b2967f46a",
      "source_id": "b4048678-ad25-460f-b1e3-c104fbf529dc",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary Loss for Remote Sensing Imagery Semantic Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Boundary Loss for Remote Sensing Imagery Semantic Segmentation\" from arxiv.org:",
      "generation": " Semantic segmentation of remote sensing imagery poses a great challenge due to the existence of boundary areas, which often contain complex textures, object appearance changes, and lacking of context. Many state-of-the-art deep learning based approaches tackle this problem via an encoder-decoder architecture to generate contextual information from both the source and target domains. However, existing methods usually encode the information of the whole image domain rather than just the boundary regions, which may not be an efficient solution in terms of computational cost. In this paper, we aim to explore an efficient approach for remote sensing imagery semantic segmentation that focuses on boundary loss. We design a Boundary Guided Attention module to concentrate on the boundary areas and greatly reduce the computational complexity compared to existing methods. We adopt a U-shaped encoder-decoder architecture to extract features for the source and target domains separately. Then the Boundary Guided Attention module is utilized to concentrate on the boundary regions of the target domain and generate precise contextual information with the guidance of the source domain. Finally, the predicted boundary mask is utilized to effectively mitigate the boundary effect via a boundary loss. Extensive experiments on two public remote sensing datasets (i.e., KK200 and RSC2016) validate the effectiveness of the proposed approach. Without bells and whistles, our approach achieves the state-of-the-art performance or even outperforms the existing methods."
    },
    {
      "id": "6f8191c6-79ca-4cc6-b89e-2efc51449747",
      "adv_source_id": "6f8191c6-79ca-4cc6-b89e-2efc51449747",
      "source_id": "d3e5887c-59e2-4cf4-ad32-d15bf49e3608",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Diversity in Machine Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Diversity in Machine Learning\" from arxiv.org:",
      "generation": " Abstract:\nWe study diversity in machine learning, a topic that has been largely ignored by the research community. Diversity is a key concept in machine learning, as it refers to the ability of a machine learning model to generalize well to new and unseen data. We define diversity in machine learning and study its impact on the performance of machine learning models. We find that increasing diversity in machine learning can improve the performance of machine learning models, and we present techniques for increasing diversity in machine learning. We hope that our work will encourage further research into this important topic.\n\nWould you like more information on this topic? "
    },
    {
      "id": "25971576-2471-4e55-9438-00c1411eed7f",
      "adv_source_id": "25971576-2471-4e55-9438-00c1411eed7f",
      "source_id": "d3e5887c-59e2-4cf4-ad32-d15bf49e3608",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Diversity in Machine Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Diversity in Machine Learning\" from arxiv.org:",
      "generation": " Machine learning (ML) is increasingly relevant to many fields, but its reliance on large datasets poses challenges for domains where such data is difficult to collect. We survey the state of the art in ML for specific application domains, including medicine, biology, physics, chemistry, and economics. We identify key sources of diversity in these domains across data, tasks, and algorithms and discuss practices that could facilitate ML progress in atypical domains. Finally, we highlight the need for algorithmic contributions that either perform well under constrained resources or cater to expert domain adaptions."
    },
    {
      "id": "85a7635a-988d-4d6e-b5b8-6c1445426d92",
      "adv_source_id": "85a7635a-988d-4d6e-b5b8-6c1445426d92",
      "source_id": "39bcc341-6760-4abf-a953-64499f3f3b73",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation of Video Sequences with Convolutional LSTMs",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semantic Segmentation of Video Sequences with Convolutional LSTMs\" from arxiv.org:",
      "generation": " We present a method for the semantic segmentation of video sequences using convolutional LSTM networks. Our approach employs a hybrid convolutional and recurrent architecture that models both appearance and motion dynamics over time. We introduce a novel temporal max-pooling layer, which enables the network to selectively focus on the most relevant information while being invariant to input order. Our method naturally handles variable-length inputs and is applicable to arbitrary video durations. We evaluate our approach on the CamVid dataset and show that our model outperforms the state of the art in semantic video segmentation.\n\n Our approach to semantic segmentation of video sequences is based on a hybrid convolutional and recurrent architecture that models both appearance and motion dynamics over time. The model is composed of a stack of convolutional LSTM layers, followed by a spatial max-pooling layer to reduce the spatial dimensions of the feature maps, and a final fully connected layer to produce a segmentation map. The use of max-pooling, rather than average pooling, allows the model to be more robust to noise and outliers in the input data.\n\nTo handle the variability in video duration, we introduce a novel temporal max-pooling layer that enables the network to selectively focus on the most relevant information while being invariant to input order. This layer max-pools over the input sequence, producing a fixed-length representation that can be processed by the subsequent layers. Because the temporal max-pooling layer effectively downsamples the sequence, the network can be trained on shorter sequences and then applied to longer ones without loss of performance.\n\nWe evaluate our approach on the CamVid dataset, which consists of videos of urban scenes recorded by a moving camera. Our model outperforms the state of the art in semantic video segmentation, and achieves a mIOU of 92.1% on the CamVid test set. Further, our model is able to generalize to videos of different lengths, demonstrating the effectiveness of our temporal max-pooling layer. We also show that our model is able to learn meaningful representations of video sequences, which can be used for a variety of tasks such as video summarization and prediction of future frames."
    },
    {
      "id": "104defe4-fa01-4b7b-a9ed-4c6f349bfb0c",
      "adv_source_id": "104defe4-fa01-4b7b-a9ed-4c6f349bfb0c",
      "source_id": "39bcc341-6760-4abf-a953-64499f3f3b73",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation of Video Sequences with Convolutional LSTMs",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semantic Segmentation of Video Sequences with Convolutional LSTMs\" from arxiv.org:",
      "generation": " We present a method for the semantic segmentation of video sequences using convolutional long short-term memory (ConvLSTM) networks. Our approach utilizes a dynamic programming framework to optimize the segmentation of each frame in a video sequence. We introduce a novel convolutional layer that is designed to handle the spatial-temporal dependencies in video data. This layer, which we refer to as the ConvLSTM layer, is based on the long short-term memory (LSTM) architecture and is designed to capture long-range dependencies in a video sequence. We demonstrate the effectiveness of our approach on the challenging semantic segmentation task of the PASCAL VOC 2012 dataset. Our method achieves state-of-the-art performance on this dataset, with an accuracy of 79.7%.\n\nOur approach utilizes a dynamic programming framework to optimize the segmentation of each frame in a video sequence. The dynamic programming framework is based on an energy function that is defined by a ConvLSTM network. The network is trained using a cross-entropy loss function, which is designed to optimize the representation of the input video frames. The optimized representation is then used to compute the energy function for the subsequent frame in the video sequence. The process is repeated until all frames in the sequence have been processed. The final segmentation of each frame is obtained by optimizing the energy function using the Viterbi algorithm.\n\nWe introduce a novel convolutional layer that is designed to handle the spatial-temporal dependencies in video data. This layer, which we refer to as the ConvLSTM layer, is based on the long short-term memory (LSTM) architecture and is designed to capture long-range dependencies in a video sequence. The ConvLSTM layer utilizes a set of learnable parameters that are shared across all video frames. These parameters are updated during training using backpropagation through time (BPTT).\n\nWe demonstrate the effectiveness of our approach on the challenging semantic segmentation task of the PASCAL VOC 2012 dataset. Our method achieves state-of-the-art performance on this dataset, with an accuracy of 79.7%. Our approach achieves a 1.2% improvement in accuracy over the previous state-of-the-art method. Furthermore, our approach is able to segment objects that are difficult to segment using traditional methods, such as objects that are partially occluded or in the background of a scene."
    },
    {
      "id": "95d06402-966e-403c-a914-0c6d837dccb2",
      "adv_source_id": "95d06402-966e-403c-a914-0c6d837dccb2",
      "source_id": "ba7ac9f3-6578-4aa6-a574-b8a7d4442d53",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel approach for cardiac cine MR image segmentation based on a distance map regularized convolutional neural network (DRM-CNN). Our method combines the advantages of both semantic and instance segmentation techniques to achieve robust and accurate segmentation of the left ventricle (LV) cavity in cardiac cine MR images. The distance map regularization helps to enforce spatial consistency in the segmentation output, while the CNN learns the underlying anatomical structures and their variations from the training data. We evaluate our method on the publicly available MICCAI 2012 cardiac segmentation challenge dataset and show that it achieves state-of-the-art performance in terms of both accuracy and speed. Furthermore, our method is able to segment the LV cavity in a fully automatic manner, without any user interaction or prior knowledge of the image content."
    },
    {
      "id": "dd86e1b5-7da3-4b82-908a-a96a130358e5",
      "adv_source_id": "dd86e1b5-7da3-4b82-908a-a96a130358e5",
      "source_id": "ba7ac9f3-6578-4aa6-a574-b8a7d4442d53",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel approach to segmenting cardiac cine MR images (CMR) into different tissue types using a distance map regularized convolutional neural network (DMRCNN). This approach incorporates a distance map regularization term within the cost function of a CNN, which encourages the CNN to produce segmentations that are consistent with the prior knowledge that voxels in different tissue types are generally not contiguous. In order to evaluate the effectiveness of the proposed DMRCNN, we conducted a comprehensive comparison on 400 randomly selected subjects from the Cardiac MRI Segmentation Benchmark (CMSB) dataset, comparing the DMRCNN to a state-of-the-art deep learning model, Wildfire, and also to a conventional watershed algorithm. The results show that the DMRCNN improves the baseline CNN model in terms of segmentation accuracy for all tissue types, and is also on par with Wildfire in terms of accuracy, while being an order of magnitude faster. When compared to the watershed algorithm, the DMRCNN produces segmentations that are more accurate and contiguous."
    },
    {
      "id": "827e5def-06b4-40ee-97d1-a24fc9e281d2",
      "adv_source_id": "827e5def-06b4-40ee-97d1-a24fc9e281d2",
      "source_id": "14eace65-a9c8-4af7-bd8f-058a26858920",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology\" from arxiv.org:",
      "generation": " Recent results on semantic segmentation could be leveraged to develop automated tissue analysis for pathology. However, the limited field-of-view of convolutional neural networks (CNNs) trained on natural images restricts the use of these methods in pathology, where whole slide images are large and have an extremely wide field-of-view. We overcome this limitation by (1) employing an adaptive weighting approach to effectively utilize multi-field-of-view CNNs and (2) proposing a multi-field-of-view encoder-decoder architecture that is suitable for semantic segmentation on whole slide images. We demonstrate that our approach yields state-of-the-art performance on two public pathology datasets with significant field-of-view coverage."
    },
    {
      "id": "275be9f1-37c4-4319-8f98-eac72820bd86",
      "adv_source_id": "275be9f1-37c4-4319-8f98-eac72820bd86",
      "source_id": "14eace65-a9c8-4af7-bd8f-058a26858920",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology\" from arxiv.org:",
      "generation": " Recent progresses in deep learning have achieved great success in semantic segmentation of images in natural domains. However, there are still unsolved problems when applying deep learning to medical images, especially for pathological images. Unlike natural images, pathological images are typically obtained from stained tissue slides, and the appearance of the same tissue region may show substantial variations from slide to slide, even within the same patient. In this paper, we propose a framework to effectively address the challenge of tissue appearance variation by adaptive weighting the features learned from multi-field-of-view (MFOV) convolutional neural networks (CNNs). We theoretically prove that the proposed adaptive weighting MFOV CNN can achieve optimal performance in the tissue appearance variation scenario when the weight matrix is known in advance. However, the weight matrix is not available in practice. Therefore, we propose an efficient algorithm to estimate the weight matrix from a small number of training examples. The proposed method is evaluated on two public pathological image datasets (Camelyon17 and SegmentationUltra) with promising results, which demonstrate the effectiveness of the proposed method for semantic segmentation in pathology."
    },
    {
      "id": "1c00973b-a0ae-49d4-bc13-1e0881222a8f",
      "adv_source_id": "1c00973b-a0ae-49d4-bc13-1e0881222a8f",
      "source_id": "cb534d5c-f0fd-48d3-9601-a353d1c2fe81",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds\" from arxiv.org:",
      "generation": " Recent developments in imaging medicine have led to a significant increase in the size of 3D medical images, posing a great challenge for efficient transmission, storage and segmentation in the clouds. In this paper, we propose a machine vision guided 3D medical image compression approach to address this issue. Our method leverages the power of deep learning to learn a compression model directly from the images, which is trained using a large number of images collected from clinical studies. The compression model is designed to take into account the specific characteristics of medical images, such as their typically low entropy and the need for precise segmentation. We evaluate our method on a large dataset of real clinical images and demonstrate that it achieves a significant reduction in the file size while maintaining high quality and enabling accurate segmentation in the clouds. Our method can be seamlessly integrated into the existing workflow of medical imaging and has the potential to significantly reduce the storage and transmission costs of 3D medical images."
    },
    {
      "id": "7ceab5ca-c9f1-4e77-a31c-f8e9af075c22",
      "adv_source_id": "7ceab5ca-c9f1-4e77-a31c-f8e9af075c22",
      "source_id": "cb534d5c-f0fd-48d3-9601-a353d1c2fe81",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds\" from arxiv.org:",
      "generation": " Towards teleconsultation and clinical support, efficiently transmitting volumetric imaging data between different sites is essential. Towards this end, we devise a framework that can compress 3D medical images to reduce their transmission bandwidth, while maintaining high levels of segmentation accuracy. This framework is guided by machine vision and leverages both local and global image characteristics. Our method consists of two main steps: a perception-inspired compression and a learned loss function for segmentability. In the compression step, we explore the spatial and channel attention mechanisms to capture local and global structures, respectively. We devise a new 3D convolutional neural network (CNN) architecture, namely nested-skipped self-supervised dense parallel (NS-SSDP), to efficiently exploit the powerful contextual information in medical images for compression. In the learned loss step, we aim to preserve the important features for segmentation by minimizing a joint segmentation-inspired and compression-induced loss function. We evaluate our method on a public lung CT dataset with ground truth segmentations and compare it with state-of-the-art methods. Our results show that our method achieves a higher segmentation accuracy compared to the state-of-the-art methods, while maintaining a high compression ratio."
    },
    {
      "id": "3647fd0b-cb4a-4ef7-9d0d-ab04116f75ca",
      "adv_source_id": "3647fd0b-cb4a-4ef7-9d0d-ab04116f75ca",
      "source_id": "1204734a-a062-4ab7-95ad-52664d55d8a9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\" from arxiv.org:",
      "generation": " ```\nAuto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n\nAbstract:\n\nSemantic image segmentation, which aims to assign semantic labels to every pixel in an image, is a fundamental task in computer vision. Recently, hierarchical architectures have demonstrated promising performance for the task. However, designing such architectures manually is highly challenging. In this work, we present a learnable hierarchical architecture search framework, named as Auto-DeepLab, which aims to automatically find a hierarchical architecture with promising performance for semantic image segmentation. Specifically, we first introduce a hierarchical architecture search space, which is learned from a set of pre-trained models. Then, a genetic algorithm is employed to search for the best architecture within the search space. Finally, the discovered architecture is fine-tuned for semantic image segmentation on the Cityscapes dataset, and the results demonstrate the effectiveness of our method.\n\n```"
    },
    {
      "id": "50fef561-0f7b-485d-a856-bfe1b1677039",
      "adv_source_id": "50fef561-0f7b-485d-a856-bfe1b1677039",
      "source_id": "1204734a-a062-4ab7-95ad-52664d55d8a9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\" from arxiv.org:",
      "generation": " ```\nAuto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n\nAbstract:\n\nSemantic image segmentation remains a fundamental yet challenging task in computer vision.\nMost recent state-of-the-art methods employ some variant of a fully convolutional network\n(FCN) architecture, in which multiple convolutional layers with pooling operations are\nstacked together, followed by upsampling and cropping operations for refinement. Even\nwith the success of these methods, it remains difficult to design an effective network\narchitecture that strikes a good balance between complexity and performance. In this\npaper, we propose a novel method to automate the process of designing an\neffective semantic image segmentation network by searching for an optimal\narchitecture within a predefined space using neural architecture search (NAS). We\npropose a hierarchical search space in which the searched architecture comprises\na series of blocks, each of which can either be a convolutional layer or an\nupsampling layer with a corresponding cropping layer. Within each block, we apply\nthe NAS algorithm to search for an optimal subset of operations that best\napproximates the desired output. Our method, termed Auto-DeepLab, can not only\nautomatically find an architecture that is superior to existing hand-designed\nnetworks but also provides a visualization of the searched space, allowing users\nto interpret the design principles of the final architecture. We demonstrate the\neffectiveness of our approach on semantic image segmentation, achieving\nstate-of-the-art performance on the PASCAL VOC 2012 and Cityscapes datasets with\nan order of magnitude fewer parameters compared to existing methods.\n```"
    },
    {
      "id": "31ad8976-08c3-404b-86c9-8004029544a1",
      "adv_source_id": "31ad8976-08c3-404b-86c9-8004029544a1",
      "source_id": "dc899aff-901a-47ea-a194-33ac1c5dc818",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Vehicle Image Generation Going Well with The Surroundings",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Vehicle Image Generation Going Well with The Surroundings\" from arxiv.org:",
      "generation": " We propose an image generation model that generates vehicle images based on rough sketches of the vehicle and its surroundings. In the training process, we use a unit of rectangular patches to simulate the rough sketches. We also propose an adaptive loss function that assigns different weights to the well-drawn and poorly-drawn patches according to the style strength of the input sketch. This model shows promising results in generating vehicle images that suit the surroundings."
    },
    {
      "id": "226c04a1-f412-4cce-8b68-4abe8ceec4d0",
      "adv_source_id": "226c04a1-f412-4cce-8b68-4abe8ceec4d0",
      "source_id": "dc899aff-901a-47ea-a194-33ac1c5dc818",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Vehicle Image Generation Going Well with The Surroundings",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Vehicle Image Generation Going Well with The Surroundings\" from arxiv.org:",
      "generation": " We propose an method for generating vehicle images combining a vehicle model and its surroundings. We simplify the generation process by imposing certain conditions. Namely, we model the surrounding environment and impose the condition that the vehicle model should appear in the image with a higher priority. Furthermore, the scale and rotation of the vehicle model and the lighting condition on the surrounding environment are also harmonized in the proposed method. We apply the proposed method to the dataset introduced in our previous paper. Note that this dataset consists of thousands of cluttered images in which multiple vehicles with different models, scales, and rotations appear in different environments. Then, we evaluate the quality of the generated images using three evaluation metrics. We also conduct a user study to compare the proposed method with other methods. The proposed method achieves better performance in terms of all the metrics we use."
    },
    {
      "id": "b65f1750-de67-4d32-9247-e9870a0f3450",
      "adv_source_id": "b65f1750-de67-4d32-9247-e9870a0f3450",
      "source_id": "52afd030-fc45-4f6e-8f11-d86c68aae4ac",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy\" from arxiv.org:",
      "generation": " Abnormal brain anatomy, such as congenital anomalies, may have an impact on the cerebral blood circulation, in particular, the DSC perfusion MR measurements. In this paper, we present a method to correct for the effect of abnormal anatomy. The method is based on the CUSUM filter, which has been successfully used for noise filtering in MR images. The CUSUM filter is applied to the DSC perfusion MR head scans to improve the segmentation of the brain. The method has been tested on a patient with congenital anomaly, and the results show that the CUSUM filter can improve the brain segmentation in DSC perfusion MR head scans with abnormal brain anatomy.\n\nThe procedure of the CUSUM filter for brain segmentation on DSC perfusion MR head scans is as follows. First, the DSC perfusion MR head scan is obtained from the patient. The CUSUM filter is then applied to the DSC perfusion MR head scan to improve the segmentation of the brain. The CUSUM filter is designed to remove the noise and artifacts in the DSC perfusion MR head scan, and to enhance the edges and contours of the brain. The resulting image is then segmented using a standard brain segmentation algorithm. The segmented image is then compared to the original DSC perfusion MR head scan, and the differences between the two images are used to improve the segmentation of the brain."
    },
    {
      "id": "5dcbc139-922e-4c35-98dd-1bcceaf24cd1",
      "adv_source_id": "5dcbc139-922e-4c35-98dd-1bcceaf24cd1",
      "source_id": "52afd030-fc45-4f6e-8f11-d86c68aae4ac",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy\" from arxiv.org:",
      "generation": " Abnormal brain anatomy, such as noticeable atrophy, mass effect, or previous surgery, often complicates the interpretation of DSC perfusion MR head scans. There is a need for automated segmentation methods that are capable of accurately identifying abnormal regions and adapting to their presence. In this work, we propose the use of the cumulative sum (CUSUM) filter, a data-driven, non-linear filter, for brain segmentation on DSC perfusion MR head scans with abnormal brain anatomy. The CUSUM filter is a convolution filter in which each pixel is compared to its neighbors. The filter is designed to emphasize regions with intensities that are significantly different from their neighbors. Thus, the CUSUM filter can identify regions of abnormally low or high signal intensity that are bounded by regions of normal signal intensity. This property makes the CUSUM filter well-suited for identifying abnormal regions in DSC perfusion MR head scans. To evaluate the performance of the CUSUM filter, we compared it to a conventional thresholding segmentation method on a dataset of DSC perfusion MR head scans with abnormal brain anatomy. The results show that the CUSUM filter is able to identify abnormal regions with high accuracy and is more robust to noise and artifacts than the thresholding method."
    },
    {
      "id": "76577d3c-04a7-4436-80ba-0ef5b5fdd51b",
      "adv_source_id": "76577d3c-04a7-4436-80ba-0ef5b5fdd51b",
      "source_id": "3d2553f0-ad50-4f5e-a675-a550982c3f7a",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive segmentation of medical images through fully convolutional neural networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Interactive segmentation of medical images through fully convolutional neural networks\" from arxiv.org:",
      "generation": " We present an efficient, fully-automated and interactive system for segmenting medical images through deep learning. Our system is based on fully convolutional neural networks (FCN), which enable efficient inference on entire slides with high throughput. Unlike existing FCN-based approaches, which rely on pre-defined network architectures and training protocols, we introduce an adaptive architecture that dynamically varies across tiles within each slide. This allows our system to achieve better segmentation performance by leveraging local information, and results in better utilization of computational resources during inference. We employ a tile-based processing paradigm that can effectively utilize modern GPU memory, and show that our method achieves real-time performance on slides with sizes up to 512^2 pixels. In a user study, we show that our interactive segmentation method achieves higher accuracy and faster performance compared to a state-of-the-art automated method. We open source our code."
    },
    {
      "id": "38f5acfe-4764-4c6b-8bbb-264c91fefa9f",
      "adv_source_id": "38f5acfe-4764-4c6b-8bbb-264c91fefa9f",
      "source_id": "3d2553f0-ad50-4f5e-a675-a550982c3f7a",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive segmentation of medical images through fully convolutional neural networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Interactive segmentation of medical images through fully convolutional neural networks\" from arxiv.org:",
      "generation": " We present an interactive segmentation method for medical images based on dense conditional gnocchi (DCG), a differentiable raster graphics library. DCG provides a way to propagate label uncertainty for segmented objects in a manner similar to the popular watershed algorithm, while also allowing for the use of external information via direct manipulation. We combine DCG with recent developments in fully convolutional neural network architectures to achieve efficient segmentation of 3D volumes with explicit handling of uncertainty. In the absence of ground truth data, we propose an alternative evaluation protocol geared towards clinical applications. We demonstrate the utility of our method on three different interactive segmentation scenarios from liver MRI and CT images.\n\n dstarmacov,Morphological antialiasing via closed dilates and optimizes, https://doi.org/10.1109/ICCV.1993.364019\n\n W. Hao and S. Osher, \"lipe: A level set method for implicit polygonal extraction,\" in Proceedings of the Conference on Neural Information Processing Systems, 2010.\n\n S. Drucker, \"Mathematical approaches to volumetric medical image segmentation,\" doctoral dissertation, Institute of Technology, University of Washington, 2004.\n\n F. Liang, M. Shechtman, J. Dong, C. Xu, K. Kankanhalli, and M. Ganapathi, \"Active learning and hierarchical segmentation of gastrointestinal images using maes,\" in Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, 2015, pp. 625\u2013634.\n\n Y. Chen, M. introduit, L. Caramico, S. Dooley, A. Aviv, D. Bowen, and C. Taylor, \"Interactive segmentation and classification of atherosclerotic plaques in coronary artery CT volumes using a graphical-model based approach,\" in Medical Image Computing and Computer-Assisted Intervention \u2013 MICCAI 2015, 2015, pp. 661\u2013670.\n\n S. Huang, J. Li, and A. Tannenbaum, \"Learning to segment in surgery: A semi-supervised approach to anastomosis detection in laparoscopic videos,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\n\nR. leaks, \"Conditional gnocchi: A differentiable graphics library for dense image segmentation,\" in Proceedings of the 32nd International Conference on Machine Learning (ICML), 2015."
    },
    {
      "id": "fd777102-cbd6-453f-ba90-899564defbad",
      "adv_source_id": "fd777102-cbd6-453f-ba90-899564defbad",
      "source_id": "b69792fc-988e-41fd-b78d-56f1d76676f1",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative\" from arxiv.org:",
      "generation": " We propose a novel, deep learning-based approach to segment multiple objects in 3D that are defined on multiple surfaces (i.e., cartilage, bone, and meniscus) from knee magnetic resonance imaging (MRI) scans taken over time (4D). This approach is designed to address the challenges associated with the complex and variable appearance of these objects across patients and time. We introduce a cost function that is trained using a supervised learning approach on a small set of manually segmented training images. The cost function is designed to segment all objects on all surfaces and is composed of a series of independently trained deep neural networks that operate on different image slices and different objects. We apply our method to segmentation of cartilage, bone, and meniscus across ten surfaces (i.e., the inner and outer cartilage and bone layers of the femur and tibia) from the Osteoarthritis Initiative, a public-access dataset of knee MRI scans. We compare our method to a state-of-the-art, manual segmentation approach and find that our method is both faster and more accurate."
    },
    {
      "id": "6a9743af-3f00-4509-bbe2-c9fbbfd9f477",
      "adv_source_id": "6a9743af-3f00-4509-bbe2-c9fbbfd9f477",
      "source_id": "b69792fc-988e-41fd-b78d-56f1d76676f1",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative\" from arxiv.org:",
      "generation": " We propose a novel, deep learning-based approach for segmenting multiple objects on multi-surface 3D renders of knee magnetic resonance images (MRI). We use manually-labeled training data from the Osteoarthritis Initiative to learn cost functions that find contours of different objects of interest on multiple surfaces of the 3D render. Our cost functions output a label image where each voxel is labeled with the name of the object it belongs to. Unlike most existing work, our approach performs well on scans from multiple vendors, including T1-weighted, T2-weighted, and proton-density weighted scans. In addition, we extend our approach to run in 4D, thereby handling variabilities in scan acquisition time, and localizing objects in the time domain. We evaluate our approach on 100 scans from the Osteoarthritis Initiative, and demonstrate good agreement with the manual segmentation ground truth."
    },
    {
      "id": "fd902eda-49ad-44aa-80d2-294570378599",
      "adv_source_id": "fd902eda-49ad-44aa-80d2-294570378599",
      "source_id": "1b31bd11-bd00-4350-a26e-1a9bb6b8ee50",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation\" from arxiv.org:",
      "generation": " We present a hybrid approach to left atrium segmentation in cardiac MR images. Our method combines global shape modeling, which we derive using a mixture model (MM) of left atrium shapes, and local intensity modeling, represented by a deep convolutional autoencoder (AA). Intensity features are extracted from the autoencoder's encoder layer and combined with the MM-based shape prior. We introduce an energy function that combines the global shape prior with the local intensity prior in a data-driven manner. We optimize the energy function using graph cuts. The proposed method was validated on 100 synthetic and 55 clinical MR images and compared to 16 state-of-the-art methods on the MICCAI 2018 Challenge on Mitral Valve and Left Atrium Segmentation and Labeling (MVLA). Our method achieved second place in the segmentation track and fourth place in the labeling track."
    },
    {
      "id": "725a1060-efe0-495a-8672-a5b01214c179",
      "adv_source_id": "725a1060-efe0-495a-8672-a5b01214c179",
      "source_id": "1b31bd11-bd00-4350-a26e-1a9bb6b8ee50",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation\" from arxiv.org:",
      "generation": " We present a fully automated method for left atrium segmentation in cardiac MRI. A global shape prior is learned from a database of segmentations and used to initialize an active contour. An autoencoder prior learns intensity features specific to the left atrium. The global shape prior encourages reasonable initial contours, while the autoencoder prior learns intensity features for semi-supervised edge labeling. This method does not require any manual initialization, and outperforms state-of-the-art automatic methods for left atrium segmentation. In addition, our framework is capable of segmenting the left atrium in about 15 seconds on a single CPU, making it potentially useful for clinical applications."
    },
    {
      "id": "655b6abd-70cf-4352-b238-fb29e1dd771a",
      "adv_source_id": "655b6abd-70cf-4352-b238-fb29e1dd771a",
      "source_id": "92e2b851-29c4-414c-a5c3-21d8702e74a2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation\" from arxiv.org:",
      "generation": " We present HyperDense-Net, a densely connected Convolutional Neural Network (CNN) for multi-modal image segmentation. It leverages recent advances in CNN architecture design to achieve state-of-the-art performance on the 2015 Multi-Modal Brain Tumor Segmentation (BraTS) Challenge. A novel feature of our approach is its use of a densely connected CNN architecture, whereby each layer in the network is connected to every other layer, rather than just the immediately preceding layer. This allows the network to learn complex, hierarchical representations of the input data, which proved critical for performance on the BraTS 2015 dataset. We evaluate our approach on the BraTS 2015 validation and test datasets and achieve a weighted intersection over union (IoU) of 0.8314 and 0.7981, respectively, ranking first in the challenge. Code and models are available at https://github.com/HyperDense-Net/HyperDense-Net."
    },
    {
      "id": "bee29ac9-fa18-41f2-bd74-cd14eb8e417c",
      "adv_source_id": "bee29ac9-fa18-41f2-bd74-cd14eb8e417c",
      "source_id": "92e2b851-29c4-414c-a5c3-21d8702e74a2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation\" from arxiv.org:",
      "generation": " We present HyperDense-Net, a novel convolutional neural network (CNN) for multi-modal image segmentation. It leverages a hyper-dense connection strategy to effectively capture contextual information across different modalities. Our approach is based on a simple yet effective observation: when labeling pixels in an image, local context from neighboring pixels in the same image is more important than context from distant pixels in other images. This motivated us to design a CNN with an increased number of skip connections from the same layer to the same layer within different modalities, allowing the network to capture more high-quality local context. We evaluate HyperDense-Net on three public multi-modal image segmentation benchmarks, and our approach achieves state-of-the-art performance on all datasets without using any dropout modules or data augmentation techniques. The source code and models are available at https://github.com/greatfet/HyperDense-Net."
    },
    {
      "id": "e77eeba1-1aee-4159-8712-b68f70b38ce7",
      "adv_source_id": "e77eeba1-1aee-4159-8712-b68f70b38ce7",
      "source_id": "7b04c7b1-2830-452a-9f54-77a2c8f612cd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dense 3D Visual Mapping via Semantic Simplification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Dense 3D Visual Mapping via Semantic Simplification\" from arxiv.org:",
      "generation": " We present an approach for dense 3D visual mapping of indoor scenes. We aim at building a compact 3D model of the environment that replaces the original photometric and geometric texture with a simplified yet semantically coherent one. In this way, our model is able to capture the intrinsic organization and structure of the scene, producing a coherent and recognizable 3D map. To achieve this, we start by learning an overcomplete dictionary of atoms via sparse coding, i.e., finding the best low-dimensional representation of the training data. Atoms are synthesized by combining a set of pre-defined 3D geometric primitives that are learned from the dataset. In order to enforce semantic coherence, we then optimize the dictionary atoms to conform to the scene structure, leveraging on a dense 3D reconstruction of the environment. The final 3D map is obtained by densely projecting the optimized atoms onto the scene surfaces. In order to evaluate the performance of our approach, we compare it to other state-of-the-art methods on two public benchmarks, and analyze the statistical properties of the learned dictionaries."
    },
    {
      "id": "c22b5346-9dbe-4bc6-9a03-c10ed6513cff",
      "adv_source_id": "c22b5346-9dbe-4bc6-9a03-c10ed6513cff",
      "source_id": "7b04c7b1-2830-452a-9f54-77a2c8f612cd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dense 3D Visual Mapping via Semantic Simplification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Dense 3D Visual Mapping via Semantic Simplification\" from arxiv.org:",
      "generation": " We present an approach for dense 3D visual mapping of indoor environments. We aim at building a coherent and exhaustive 3D model of a space, encompassing all the details captured by stationary cameras. In addition to modeling the geometry of the scene, we strive to reconstruct the object arrangements, i.e., the positions and poses of objects in the scene. To achieve this, we leverage recurrent neural networks trained with standard perspective-invariant object detection techniques. We rely on a moving camera paradigm where we virtually move a camera along the space. Unlike traditional structure-from-motion approaches, our method is able to gradually reconstruct the space without needing initial keyframes or an initial rough model of the scene. We avoid the initialization problem by learning a mapping from image coordinates to 3D ray coordinates. Specifically, we train a neural network to predict the depth and heading of a ray shooting through the center of the object bounding boxes. Our method, albeit learning from images alone, is able to reconstruct ambitious indoor scenes from ordinary webcams."
    },
    {
      "id": "7292cd65-d409-4d81-be9f-3076ba34b35e",
      "adv_source_id": "7292cd65-d409-4d81-be9f-3076ba34b35e",
      "source_id": "87aa367e-068d-49a2-92d3-a9f380cc6fff",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain MRI Segmentation using Rule-Based Hybrid Approach",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Brain MRI Segmentation using Rule-Based Hybrid Approach\" from arxiv.org:",
      "generation": " Brain MRI segmentation is a fundamental step in clinical MRI analysis. However, manual segmentation is tedious and time consuming, while automatic segmentation is challenging due to the large variation in brain morphology and image quality. In this paper, we present a rule-based hybrid approach for brain MRI segmentation that integrates both automatic and manual segmentation methods. Our approach combines the advantages of both methods to achieve accurate and efficient segmentation. In particular, we use a rule-based approach to automatically segment the brain, and we use a manual segmentation method to refine the automatic segmentation results. The automatic segmentation is based on a set of rules that use brain morphology and image quality to determine the segmentation. The manual segmentation is based on a set of editing tools that allow the user to refine the automatic segmentation results. We evaluated our approach on a dataset of 20 MRI scans, and our results show that our approach is accurate and efficient. Our approach achieved an average segmentation accuracy of 92%, and our average editing time was 6 minutes per scan."
    },
    {
      "id": "97165bda-4758-483a-866c-a6a36684bea5",
      "adv_source_id": "97165bda-4758-483a-866c-a6a36684bea5",
      "source_id": "87aa367e-068d-49a2-92d3-a9f380cc6fff",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain MRI Segmentation using Rule-Based Hybrid Approach",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Brain MRI Segmentation using Rule-Based Hybrid Approach\" from arxiv.org:",
      "generation": " Magnetic resonance imaging (MRI) is an important tool for clinical diagnosis and is used for brain imaging to obtain comprehensive anatomical information. In this work, we present a hybrid approach for brain MRI segmentation comprising of an anatomical network model (ANM) followed by a fuzzy rule-based system (FRBS). The proposed method has been evaluated on 100 different brain MRI scans for segmentation of brain MRI data into five different regions, namely, cerebrospinal fluid (CSF), gray matter (GM), white matter (WM), and left and right cortex. The proposed method achieves an accuracy of 97.12%, a specificity of 97.88%, a sensitivity of 96.96%, and an F1-score of 97.04%. The proposed method has been compared with three widely used state-of-the-art methods. A comprehensive comparison with the competitor methods in terms of accuracy, specificity, sensitivity, and F1-score has been carried out. The results demonstrate the superiority of the proposed method."
    },
    {
      "id": "e2ef6ae7-a62c-4eee-8d59-c1e27d46b314",
      "adv_source_id": "e2ef6ae7-a62c-4eee-8d59-c1e27d46b314",
      "source_id": "5ed7b754-5bb0-4aff-a803-d376e9e26edd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images\" from arxiv.org:",
      "generation": " We present a novel framework for learning to segment neurons in 3D electron microscopy images. Our approach is based on a metric graph learned by a convolutional neural network (CNN) from synthetic training images. During training, the CNN is taught to generate simulated images from a metric graph, which encapsulates the spatial relationships between neurons. Given a new image, our method estimates a metric graph that captures the local geometric structure of neuron branches. This graph is then searched to find the optimal segmentation. We show that the proposed approach is effective in segmenting neurons, even in challenging cases where other state-of-the-art methods fail. Our code and models are available at https://github.com/shenwanz/EM-Segmentation-Metric-Graph."
    },
    {
      "id": "79c4dcd3-fccb-4aef-b275-38bb4beb0a4d",
      "adv_source_id": "79c4dcd3-fccb-4aef-b275-38bb4beb0a4d",
      "source_id": "5ed7b754-5bb0-4aff-a803-d376e9e26edd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images\" from arxiv.org:",
      "generation": " We present a novel framework for learning to segment neurons in 3D electron microscopy images. Our method is based on metric graphs, a generalization of the graph cuts segmentation framework that defines an energy function via a metric on a graph. We propose a scheme to learn the graph metric from a small number of training examples, and show how to use stochastic gradient descent to minimize the energy function and find the segmentation. We demonstrate the effectiveness of our approach on a public dataset of electron microscopy images, achieving state-of-the-art performance. Our code and models are available at https://github.com/salccu/metric_graphs ."
    },
    {
      "id": "4f585f9f-7f24-474a-9722-c7acfd52bfb5",
      "adv_source_id": "4f585f9f-7f24-474a-9722-c7acfd52bfb5",
      "source_id": "3832df81-81ca-4296-8ce0-fbe676a4a967",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Joint shape learning and segmentation for medical images using a minimalistic deep network",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Joint shape learning and segmentation for medical images using a minimalistic deep network\" from arxiv.org:",
      "generation": " We present a deep learning framework for joint shape learning and segmentation of medical images. Our method leverages a minimalistic deep network architecture that is low-cost in memory and computational requirements, making it suitable for clinical deployment. We introduce a novel shape prior encoded into the network via a shape-conditioned loss function that promotes consistent object boundaries across image slices. Furthermore, our method combines expert-labeled and self-supervised shape priors into a single framework to overcome the limited availability of expert-labeled training data. We demonstrate the efficacy of our method on the problem of left-atrium segmentation from cardiac MR images. In comparison to related methods, our approach achieves a significantly higher segmentation quality while maintaining a low computational cost. Code is available at https://github.com/Radiology-ICU/ShapeSeg."
    },
    {
      "id": "b7661a08-72d1-4cad-bf11-2c91524ecd8c",
      "adv_source_id": "b7661a08-72d1-4cad-bf11-2c91524ecd8c",
      "source_id": "3832df81-81ca-4296-8ce0-fbe676a4a967",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Joint shape learning and segmentation for medical images using a minimalistic deep network",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Joint shape learning and segmentation for medical images using a minimalistic deep network\" from arxiv.org:",
      "generation": " We present a deep learning framework for joint shape learning and segmentation of anatomically relevant regions in medical images. Our framework is based on a minimalistic deep network architecture, referred to as Mikado, that is specifically designed for the purpose of learning representations of shapes in images. Mikado utilizes a series of convolutional layers to extract shape-related features from the input image, followed by a set of linear layers to perform segmentation. The framework is trained using a large dataset of manually annotated images, and achieves state-of-the-art performance on several benchmark datasets. Our framework can be easily extended to incorporate additional anatomically relevant shapes, and has the potential to enable a wide range of shape-based medical image analysis applications."
    },
    {
      "id": "23f7f3c1-4ae0-42e7-9d50-26032ef19b73",
      "adv_source_id": "23f7f3c1-4ae0-42e7-9d50-26032ef19b73",
      "source_id": "e58de623-8ccb-4452-bad0-85a2456277c2",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in deep neural networks have achieved significant performance in biomedical image segmentation. However, these models are data-hungry and require a large amount of labeled examples for training, which are difficult to obtain in the biomedical field due to the high cost of expert annotation. In this paper, we propose a universal decoding method, called Cascade Decoder, for training data-efficient segmentation models that can be applied to any image domain with only a few annotated examples. Cascade Decoder first utilizes a set of pre-trained decoders to extract features from the input image, and then learns a new decoder to refine the predictions based on the segmented images and their annotations. By cascading the decoders in a coarse-to-fine fashion, our method can effectively leverage both the global and local information in the image to achieve segmentation. We demonstrate the effectiveness of our method on three different image segmentation tasks: tumor cell segmentation from confocal images, neuron segmentation from electron microscopic images, and white matter fiber tract segmentation from T1-weighted MRI images, with only 10%, 4% and 0.5% of the images in the training set, respectively. Our method achieves comparable performance to the state-of-the-art while using only a fraction of the labeled data."
    },
    {
      "id": "f5d595e9-2979-4b8f-8dde-608f263df6aa",
      "adv_source_id": "f5d595e9-2979-4b8f-8dde-608f263df6aa",
      "source_id": "e58de623-8ccb-4452-bad0-85a2456277c2",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation\" from arxiv.org:",
      "generation": " biomedical image segmentation, while theoretically well-defined, is rarely formulated and solved in a fully deterministic fashion. In practice, segmentation protocols are often ad-hoc, involving heuristics and domain-specific knowledge. In this paper, we present a universal decoding method for biomedical image segmentation that is compatible with any encoding strategy that preserves the labels of pixels in an image. In principle, decoding should be straightforward: classify each pixel based on the encoded information. However, in practice, decoding is challenging because the encoder is typically a neural network that is used as a black box, and the raw prediction of the encoder is an implicit representation of the input image that is not directly amenable to pixel-wise classification. To overcome this challenge, we propose a cascade decoder that, given the raw prediction from the encoder, iteratively refines the prediction to obtain a segmentation. In the first stage, we introduce a distance function that measures the similarity between a pixel and its class, and refine the prediction by pulling pixels closer to their classes. In the second stage, we differentially update pixels based on the current prediction and a smoothed version of the previous prediction. This iterative process continues until convergence. We demonstrate the effectiveness of our method on three publicly available datasets, yielding state-of-the-art performance in two of them."
    },
    {
      "id": "b23e1cf3-fa55-4d7b-a15e-6fce59bd67a8",
      "adv_source_id": "b23e1cf3-fa55-4d7b-a15e-6fce59bd67a8",
      "source_id": "d7b9b9fb-356c-4126-9f09-e853fa89f5fd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks\" from arxiv.org:",
      "generation": " We explore the use of retrained off-the-shelf (OTS) deep neural networks for image segmentation in iris recognition. We train two segmentation networks, a U-Net and a Mask R-CNN, from scratch to segment iris images. Then we retrain both networks with ImageNet pre-trained weights. We use the segmented images to localize the iris and then recognize the iris using a pre-trained deep neural network. Our method achieves an accuracy of 97.53% on CASIA-Iris-VI database with SIFT as the feature extractor."
    },
    {
      "id": "b5961451-9a24-457b-8343-b2284eac8b92",
      "adv_source_id": "b5961451-9a24-457b-8343-b2284eac8b92",
      "source_id": "d7b9b9fb-356c-4126-9f09-e853fa89f5fd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks\" from arxiv.org:",
      "generation": " We explore the use of retrained off-the-shelf (OTS) deep neural networks for image segmentation of iris images. We propose a method to segment iris regions in color images of faces captured under natural illumination. The proposed method consists of three steps: (a) an unbalanced random forest (URF) is used to roughly segment the iris regions, (b) the URF segmentation result is used as an initial guess for a graph cut based segmentation, (c) finally, the iris region segmented by the graph cut is fine-tuned using a retrained deep neural network. The proposed method is evaluated on a recently introduced public dataset of color images of faces with irises. Our method achieved a mean overlap ratio of 0.92 with the ground truth. In comparison, the state-of-the-art method achieved a mean overlap ratio of 0.80."
    },
    {
      "id": "e8b98a64-d672-4523-9c22-7c637e8f2fee",
      "adv_source_id": "e8b98a64-d672-4523-9c22-7c637e8f2fee",
      "source_id": "45731520-dba4-4450-b64b-9dde0330cbdb",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images\" from arxiv.org:",
      "generation": " In this paper, we propose a novel holistic decomposition convolution (HDC) for effective semantic segmentation of 3D MR images. Our approach is designed to explicitly exploit the structural information of MR images via a novel convolution operation that we refer to as holistic decomposition convolution. Specifically, we decompose the 3D MR image into a set of 2D patches, and then we apply a convolution operation to the patches while maintaining the spatial structure of the original image. Our experiments on the CamVid and Cityscapes datasets show that our approach achieves state-of-the-art performance in 3D MR image semantic segmentation, outperforming other approaches in both accuracy and efficiency."
    },
    {
      "id": "b6827160-149c-4982-a744-2df818629c43",
      "adv_source_id": "b6827160-149c-4982-a744-2df818629c43",
      "source_id": "45731520-dba4-4450-b64b-9dde0330cbdb",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images\" from arxiv.org:",
      "generation": " In this paper, we propose a novel holistic decomposition convolution (HDC) for effective semantic segmentation of 3D MR images. Our method decomposes the 3D MR images into a set of 2D images and then performs convolution on the 2D images to obtain semantic segmentation results. We show that our HDC method can achieve better performance than the state-of-the-art 3D convolution methods in terms of both accuracy and efficiency. Furthermore, we present a new dataset for 3D MR image semantic segmentation, which contains 2,048 slices of T1-weighted MR images and their corresponding manual segmentation labels. The proposed method has been evaluated on our new dataset and achieved a mean accuracy of 97.53% on the testing set."
    },
    {
      "id": "9e663985-6244-4230-9d57-a130106f819c",
      "adv_source_id": "9e663985-6244-4230-9d57-a130106f819c",
      "source_id": "a3e91b6c-ef13-40cb-b0bd-7c551b74dc7c",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SwipeCut: Interactive Segmentation with Diversified Seed Proposals",
      "prompt": "The following is the full text of the abstract for a research paper titled \"SwipeCut: Interactive Segmentation with Diversified Seed Proposals\" from arxiv.org:",
      "generation": " Recent progress in interactive segmentation largely focuses on proposing sophisticated interfaces to enable users to generate precise seeds for the subsequent segmentation. However, it remains a challenging task for non-professional users to design effective seeds. In this paper, we present SwipeCut, a novel interactive segmentation method that actively utilizes diversified seed proposals to ease the burden on users. Given a sparse initialization, our method iteratively refines the segmentation results by densely sampling a set of candidate seeds from the proposal pool and propagating them using a learned diffusion model. Specifically, we generate a collection of seed proposals via a simple yet effective kerneling method that combines a segmentation mask with appearance and geometry features. At each iteration, we select a subset of proposals to guide the diffusion process and encourage exploration of diverse regions by explicitly modeling the proposal diversity. Furthermore, we present an adaptive refinement module to improve the quality of the candidate seeds, making them more applicable for the subsequent segmentation. Our method significantly outperforms the state-of-the-art on the interactive segmentation benchmark, even when the number of iterations is limited to only a few."
    },
    {
      "id": "6821735d-957c-4857-aec9-a2a1dd4b1c30",
      "adv_source_id": "6821735d-957c-4857-aec9-a2a1dd4b1c30",
      "source_id": "a3e91b6c-ef13-40cb-b0bd-7c551b74dc7c",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SwipeCut: Interactive Segmentation with Diversified Seed Proposals",
      "prompt": "The following is the full text of the abstract for a research paper titled \"SwipeCut: Interactive Segmentation with Diversified Seed Proposals\" from arxiv.org:",
      "generation": " Many image segmentation algorithms allow users to refine initial results by providing additional annotations. We propose to take this a step further and turn the initial segmentation into a new source of information for the user, turning an interactive segmentation tool into a smart segmentation tool. Our key observation is that, given a segmentation with errors, it is often easier to correct errors by swiping to form a cut, rather than selecting corrected regions directly. We exploit this observation by iteratively improving the initial segmentation where cuts are suggested to the user, allowing the user to swipe to adjust the cuts. We diversify the suggestions for cuts using an algorithm that proposes multiple cuts, and uses cuts suggested by the user to update the proposal of subsequent cuts. In a user study, we show that our algorithm requires half the corrections compared to a state-of-the-art interactive segmentation tool whilst providing comparable segmentation quality."
    },
    {
      "id": "b2a219e2-53cb-4996-a78a-08493a403eda",
      "adv_source_id": "b2a219e2-53cb-4996-a78a-08493a403eda",
      "source_id": "0cf92d6f-211c-4c83-8c42-c22cf86edde4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks\" from arxiv.org:",
      "generation": " We present a fast computation of the matrix-vector products involving the graph Laplacian of fully connected networks by combining two techniques: the Non-Uniform FFT (NFFT) for graph signals and Krylov methods for linear systems. The NFFT allows us to express the graph Laplacian in terms of a discrete convolution operator on the frequency domain, for which efficient matrix-vector products can be performed via FFT. The method relies on the choice of a suitable basis for the vector space of graph signals; we propose to use the basis of Krylov subspace methods for linear systems as this basis. We show that if the network is sparse (has a low density of edges), the proposed method is faster than the standard convolution technique, while being of the same order of magnitude when the network is dense. We finally discuss the potential of our method for solving sparse inverse problems on graphs."
    },
    {
      "id": "5cc3b388-00c9-4f35-be9c-bc0ac28efc87",
      "adv_source_id": "5cc3b388-00c9-4f35-be9c-bc0ac28efc87",
      "source_id": "0cf92d6f-211c-4c83-8c42-c22cf86edde4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks\" from arxiv.org:",
      "generation": " We present an efficient implementation of the Non-Uniform Fast Fourier Transform (NFFT) for the graph Laplacian of fully connected networks with $N$ nodes, running in $O(N \\log N)$ operations. This implementation yields fast matrix-vector products, which we employ to develop an algebraically stable and parameter-free Krylov method for very large fully connected networks, for which we require only matrix-vector products. The method is demonstrated on a real-world large network with $N=12,000$ nodes, where an approximation of the eigengaps is computed in several minutes on a single CPU core."
    },
    {
      "id": "2be412ab-39c8-471f-894a-f4bd29869e6c",
      "adv_source_id": "2be412ab-39c8-471f-894a-f4bd29869e6c",
      "source_id": "9a574030-0f87-43e1-9dec-5b367ec4bf89",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation\" from arxiv.org:",
      "generation": " We present an approach for exclusive independent probability (EIP) estimation from intensively sampled imagery data. Our specific application is brain region segmentation from IsoIntense infant brain magnetic resonance imaging (MRI), where the goal is to accurately estimate the joint probability of labeled regions given the image, while ensuring that the estimated probabilities are mutually exclusive and independent. Our key insight is that densely connected fully convolutional neural networks (FC-DNNs), with a suitable architecture and training scheme, can perform EIP estimation, as measured by the Bayesian Information Criterion (BIC). We achieve this by modifying a recent state-of-the-art FC-DNN architecture to estimate eight brain regions in infant brain MRI. We train our network using a recently proposed consistency loss, which enforces explicitly the EIP property during training. We show that our approach compares favorably with state-of-the-art methods, while providing explicit probabilities for each class, which can be used for assessing the uncertainty of the segmentation."
    },
    {
      "id": "381617db-35e2-4363-8c3c-460461328082",
      "adv_source_id": "381617db-35e2-4363-8c3c-460461328082",
      "source_id": "9a574030-0f87-43e1-9dec-5b367ec4bf89",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation\" from arxiv.org:",
      "generation": " Exclusive Independent Probability Estimation (EIPE) is a novel deep learning framework for image segmentation based on dense neural network layers. In EIPE, instead of estimating per-voxel probabilities, the neural network outputs the exclusive and independent probabilities, which can be interpreted as the probability that a voxel belongs to object A or B (exclusive probability) and the probability that the voxel belongs to class A, given it does not belong to class B (independent probability). EIPE naturally lends itself to an 3D fully convolutional network (FCN) architecture and moreover, when training such an FCN, the output activations of each layer can be reasoned about in a straightforward manner. We demonstrate the efficacy of this approach on the task of IsoIntense Infant Brain MRI segmentation, where the aim is to segment the brain into grey matter, white matter, and cerebrospinal fluid, while also identifying vascular tissue as a fourth class. Since this is a highly imbalanced problem, with the vascular class representing only a small fraction of all voxels, standard techniques that optimize the F1-score perform poorly. We show that EIPE is able to provide a large improvement in segmentation performance over more standard techniques."
    },
    {
      "id": "b386941f-3f70-4cec-b869-90cdeedd0752",
      "adv_source_id": "b386941f-3f70-4cec-b869-90cdeedd0752",
      "source_id": "dce41517-e86c-4d65-a674-43352e023a02",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features\" from arxiv.org:",
      "generation": " Brain tumor segmentation and classification is a crucial task in medical imaging analysis to diagnose and treat brain tumors. We propose an ensemble of 3D U-Nets (3D UNets) to segment and classify brain tumors on magnetic resonance (MR) images. Our method segments the tumor into three classes: tumor core, tumor mantle, and healthy tissue. The ensemble combines the outputs of multiple 3D U-Nets, each trained on a different slice of the MR image. The ensemble approach enables the network to utilize information from multiple slices, improving performance compared to a single 3D U-Net. In addition, we propose a new radiomic feature for overall survival (OS) prediction based on the tumor segmentation. This feature utilizes the information of the tumor core and tumor mantle to calculate the tumor volume and tumor doubling time. The proposed feature shows promising results on the OS prediction task."
    },
    {
      "id": "45a327d9-f8e8-406f-b1f3-d25ae0c900d2",
      "adv_source_id": "45a327d9-f8e8-406f-b1f3-d25ae0c900d2",
      "source_id": "dce41517-e86c-4d65-a674-43352e023a02",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features\" from arxiv.org:",
      "generation": " Brain tumor segmentation and classification is a prerequisite for effective radiation treatment and accurate overall survival prediction. In this work, we propose a deep learning based approach for brain tumor segmentation and classification. Our approach is based on 3D U-Net, which is a fully automated segmentation algorithm. We utilize an ensemble of 3D U-Nets to improve the performance of the algorithm. For the classification task, we employ a Radiomic Feature Based approach. The Radiomic Feature Based approach utilizes the radiological image characteristics of brain tumors, which are extracted using radiomic algorithms. The extracted features are then used to train a machine learning model to predict the overall survival of the patients. Our experimental results show that our approach achieves an F1-score of 0.9643 and an accuracy of 92.26% on the segmentation task, and an R2 score of 0.7289 on the overall survival prediction task."
    },
    {
      "id": "e028e73d-d774-45d2-9e59-d053cd7868e6",
      "adv_source_id": "e028e73d-d774-45d2-9e59-d053cd7868e6",
      "source_id": "a40cf626-40ec-439a-8944-5eb955709f88",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Foreground Clustering for Joint Segmentation and Localization in Videos and Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Foreground Clustering for Joint Segmentation and Localization in Videos and Images\" from arxiv.org:",
      "generation": " We present a simple and efficient foreground clustering approach for joint segmentation and localization of objects in videos and images. Our method leverages the prior knowledge of object geometry and motion to achieve robust and accurate results. Given a set of initial segmentations, our approach iteratively refines the segmentations and localization by clustering the foreground regions in a joint spatial-temporal space. We introduce a novel distance measure that considers both the spatial and temporal coherence of the pixels. This allows us to efficiently handle objects that exhibit complex shapes, appearances, and motions. We demonstrate the effectiveness of our approach on a variety of challenging video and image sequences, and show that our method compares favorably to the state-of-the-art in terms of accuracy and efficiency."
    },
    {
      "id": "767d2932-8b7c-4e72-a478-3630a5210918",
      "adv_source_id": "767d2932-8b7c-4e72-a478-3630a5210918",
      "source_id": "a40cf626-40ec-439a-8944-5eb955709f88",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Foreground Clustering for Joint Segmentation and Localization in Videos and Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Foreground Clustering for Joint Segmentation and Localization in Videos and Images\" from arxiv.org:",
      "generation": " We present a novel approach for joint segmentation and localization of objects in videos and images. Our method exploits the spatial coherence of objects in image and video frames for jointly optimizing object segments and their respective bounding boxes. Towards this end, we introduce an energy function that tightly integrates a segmentation term, based on a variation of the popular GrabCut algorithm, and a localization term, based on a new objectness measure that we introduce for this purpose. To efficiently optimize the energy function, we propose a novel foreground clustering technique, allowing us to deal with sparse, irregular object pixel assignments, while providing for simultaneous estimation of object segments and bounding boxes. In order to assess our method's performance, we introduce a new fully-automatic evaluation metric that gauges localization accuracy, as compared to ground truth object bounding boxes. Our method achieves state-of-the-art performance on the VOC 2012 segmentation benchmark, considerably improving over the previous best method on this dataset, while running at nearly real-time speed. Furthermore, we show that our foreground clustering technique, when applied to other recent joint segmentation and localization methods, improves their performance as well, thus making our method a strong baseline for future research."
    },
    {
      "id": "f25bb086-ea4d-42ed-8d60-51d87d839b9e",
      "adv_source_id": "f25bb086-ea4d-42ed-8d60-51d87d839b9e",
      "source_id": "dd063bd6-2998-43c7-9dbd-011fc7990abe",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans",
      "prompt": "The following is the full text of the abstract for a research paper titled \"RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans\" from arxiv.org:",
      "generation": " Abstract: Medical image segmentation plays a crucial role in the computer aided diagnosis and treatment. In this paper, we propose a hybrid deep learning network called RA-UNet to extract liver and tumor in CT scans. The proposed network consists of a U-Net based architecture integrated with attention mechanism and relevance aggregation (RA) module. The RA module is designed to capture the global and local information and it can be trained end-to-end jointly with the U-Net. We evaluate our method on two public available CT scan datasets: LiTS and Tumor Segmentation Challenge (TSC) 2018. The experimental results demonstrate the effectiveness of the proposed method in terms of precision, recall and F1-score. Furthermore, a comprehensive comparison between our method and other state-of-the-art methods on LiTS is presented. Our code and models are available at: https://github.com/RuiChen16/RA-UNet."
    },
    {
      "id": "a9eb1c1a-a034-4462-955e-8b8213629ca2",
      "adv_source_id": "a9eb1c1a-a034-4462-955e-8b8213629ca2",
      "source_id": "dd063bd6-2998-43c7-9dbd-011fc7990abe",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans",
      "prompt": "The following is the full text of the abstract for a research paper titled \"RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans\" from arxiv.org:",
      "generation": " Abstract: Medical image segmentation plays a crucial role in radiology and surgery for accurate diagnosis and treatment. In this paper, we propose a hybrid deep attention-aware network (RA-UNet) for hepatic tumor and liver segmentation in CT scans. RA-UNet consists of a fusion of a U-Net and a ResNet (Residual Network), which are respectively responsible for learning global and local features, where an attention module is introduced to weigh the importance of different features. To evaluate the performance of our method, we conducted extensive experiments on 20 patients who underwent contrast-enhanced CT scans for liver and tumor segmentation, and the segmentation results were evaluated using the metric proposed by Meyer et al. (2018). The experimental results show that our method achieved an accuracy of 97.83% for liver segmentation and 96.94% for tumor segmentation, outperforming existing state-of-the-art methods. Furthermore, the proposed method is able to segment liver and tumor in less than 5 minutes, which is several times faster than the state-of-the-art methods. Our code and dataset are available at https://github.com/ unquestionablehope/RA-UNet."
    },
    {
      "id": "ff4a660b-2a6e-4044-9e7d-5a64c198594b",
      "adv_source_id": "ff4a660b-2a6e-4044-9e7d-5a64c198594b",
      "source_id": "c5e404c8-c6dd-494b-994a-42e253ff8d8c",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images\" from arxiv.org:",
      "generation": " We present a fast graph-cut based optimization method for dense deformable registration of volume images. Our method is built upon a unified framework that optimally integrates motion estimation, interpolation, and gradient-based optimization. We utilize a B-spline kernel with a learned shape-dependent grid to achieve both flexibility and efficiency. Furthermore, we propose a new dense deformation vector field (DDF) initialization strategy that is suitable for 3D volumes. Our method can efficiently handle large datasets, such as cardiac images with approximately 10000 volumes, with a registration speed of about 1.4 seconds per volume. We validate our method on the recently released Cardiac Atlas, showing improved accuracy over the state-of-the-art. Code is available at: https://github.com/VitalCore/DDF-Registration."
    },
    {
      "id": "d141be88-8a78-4872-8ef3-3c40c54744a1",
      "adv_source_id": "d141be88-8a78-4872-8ef3-3c40c54744a1",
      "source_id": "c5e404c8-c6dd-494b-994a-42e253ff8d8c",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images\" from arxiv.org:",
      "generation": " We present a fast graph-cut based optimization algorithm for dense deformable registration of volume images. Our method incorporates a variational model with a regularized, symmetric diffeomorphism constraint into the graph-cut framework. We employ a quasi-Newton method to approximate the inverse of the deformation field's Jacobian, which typically has a full rank and is inverted to compute the step size for the optimizer. Since the quasi-Newton method has a computational cost of $O(n^2)$, where n is the number of grid points, we propose an approximation that brings the complexity down to $O(n)$, while maintaining the accuracy. Our algorithm is fast, robust, and accurate. We demonstrate the performance of our method on a number of difficult cases, showing that it is practically useful and outperforms other state-of-the-art algorithms."
    },
    {
      "id": "6cadae8b-e814-447a-8965-a638fc2e8222",
      "adv_source_id": "6cadae8b-e814-447a-8965-a638fc2e8222",
      "source_id": "38aab8a9-49b3-4d95-8775-7c898d09a3da",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Domain Adaptation Framework for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Novel Domain Adaptation Framework for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in medical imaging technology have resulted in a vast amount of labeled medical images from different domains, posing challenges for traditional medical image analysis approaches that are designed to specific domains. In this paper, we present a novel unsupervised domain adaptation framework for medical image segmentation. It selectively adapts the source domain (e.g., CT) to the target domain (e.g., MRI) by leveraging the structural similarity between the domains and the similarity in the annotations of the images. Specifically, we propose to optimize an objective function that minimizes the segmentation error on the target domain while keeping the structural similarity between the source and target domains. The proposed method is evaluated on four different public available abdominal organ segmentation datasets (CT and MRI) with different sizes. The experimental results demonstrate the effectiveness of the proposed method compared to the state-of-the-art approaches."
    },
    {
      "id": "396dca98-d92a-40f7-8f98-adcbbc965ae5",
      "adv_source_id": "396dca98-d92a-40f7-8f98-adcbbc965ae5",
      "source_id": "38aab8a9-49b3-4d95-8775-7c898d09a3da",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Domain Adaptation Framework for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Novel Domain Adaptation Framework for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in representation learning have made it possible to utilize pre-trained models for medical image segmentation. However, directly applying these models to medical images of different domains (e.g., different institutions or different scanning devices) still faces challenges due to the distribution shift between the source and target domains. In this paper, we present a novel unsupervised domain adaptation framework for medical image segmentation, which combines joint distribution matching and transfer learning. We match the distributions of the source and target domains in feature space and impose constraints between corresponding segments to alleviate the domain shift. Furthermore, we explore several strategies to effectively utilize pre-trained models for transfer learning. The experimental results on four public medical image segmentation datasets demonstrate the superiority of our method over the state-of-the-art unsupervised domain adaptation approaches. The code and models will be made publicly available."
    },
    {
      "id": "0d1e9cc0-3a28-4cac-ab69-38a0f29ae419",
      "adv_source_id": "0d1e9cc0-3a28-4cac-ab69-38a0f29ae419",
      "source_id": "0d25935a-141c-45ee-b6b0-7c84c0fb6365",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation\" from arxiv.org:",
      "generation": " We present a fully convolutional two-stream fusion network (TCN) for real-time interactive image segmentation. The TCN is designed to leverage the benefits of both feed-forward and recurrent neural networks while avoiding their limitations. Our network is composed of two streams: a feed-forward convolutional neural network (CNN) stream and a recurrent convolutional neural network (RCNN) stream. The CNN stream provides initial segmentation results, which are refined by the RCNN stream in a coarse-to-fine fashion. The two streams are combined at the feature level using a concatenation layer, which allows the network to leverage the strengths of both streams and improve the segmentation performance. We evaluate the TCN on the interactive image segmentation benchmark ISBI 2017 and achieve competitive results with state-of-the-art methods. Our network is implemented using the TensorFlow library and can run in real-time on a modern GPU. Code is available at: https://github.com/yzslzz/TCN."
    },
    {
      "id": "c45845ea-b288-4834-87bc-100bbdadece0",
      "adv_source_id": "c45845ea-b288-4834-87bc-100bbdadece0",
      "source_id": "0d25935a-141c-45ee-b6b0-7c84c0fb6365",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation\" from arxiv.org:",
      "generation": " We present a fully convolutional two-stream fusion network (TCFN) for real-time interactive image segmentation. The network is designed to leverage the benefits of both feed-forward and feedback networks while avoiding their limitations. Our network uses a two-stream architecture, with one stream processing the input image and the other processing the feedback from the user. The two streams are combined using a fusion layer that combines the strengths of both networks. We train the network using a novel loss function that encourages coherent object boundaries and pixel-wise labeling. The network is trained using a single GPU in less than an hour and can segment images in less than a second, making it suitable for real-time interactive segmentation. We evaluate our method on the Cityscapes and COCO- Stuff datasets and achieve state-of-the-art results."
    },
    {
      "id": "8509758c-1af3-4bd7-b7db-0328ea64e420",
      "adv_source_id": "8509758c-1af3-4bd7-b7db-0328ea64e420",
      "source_id": "6614f2e5-0751-45d0-9633-265eff0097aa",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS\" from arxiv.org:",
      "generation": " In this paper, we present an effective cloud detection and segmentation algorithm for satellite imagery based on a gradient-based approach. Our algorithm utilizes the gradient information of the input image to detect and segment clouds, and is capable of handling both bright and dark clouds. We apply our algorithm to the PERSIANN-CCS dataset, which consists of satellite images labeled with ground truth information, and evaluate its performance against the current state-of-the-art method. Our results show that our algorithm significantly improves the cloud detection and segmentation performance, resulting in an F1-score of 0.8519 and a precision of 0.8279.\n\nOur algorithm consists of three main steps: gradient computation, gradient-based filtering, and post-processing. The gradient computation step calculates the gradient information of the input image, which is then used in the gradient-based filtering step to detect and segment clouds. The post-processing step refines the cloud segments and removes small false positives.\n\nWe evaluate our algorithm using the PERSIANN-CCS dataset, which consists of satellite images labeled with ground truth information about the presence and location of clouds. Our results show that our algorithm achieves an F1-score of 0.8519 and a precision of 0.8279, outperforming the current state-of-the-art method.\n\nOur work demonstrates the effectiveness of using a gradient-based approach for cloud detection and segmentation in satellite imagery. By utilizing the gradient information of the input image, our algorithm is able to detect and segment clouds with high accuracy and efficiency. We believe that our algorithm has the potential to improve the performance of cloud detection and segmentation in satellite imagery, and can be applied to a wide range of applications such as cloud monitoring, weather forecasting, and environmental analysis."
    },
    {
      "id": "fc710629-d3b2-4537-be16-1ed1e03eeb69",
      "adv_source_id": "fc710629-d3b2-4537-be16-1ed1e03eeb69",
      "source_id": "6614f2e5-0751-45d0-9633-265eff0097aa",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS\" from arxiv.org:",
      "generation": " In this paper, we present an effective cloud detection and segmentation algorithm for satellite imagery based on a gradient-based approach. Our algorithm is designed to detect and segment clouds in satellite imagery, and it can be used to improve the performance of cloud detection and segmentation algorithms such as PERSIANN-CCS. We evaluate our algorithm on a dataset of satellite imagery and compare its performance to other state-of-the-art cloud detection and segmentation algorithms. Our results show that our algorithm is effective at detecting and segmenting clouds in satellite imagery, and it can improve the performance of PERSIANN-CCS. Furthermore, our algorithm is computationally efficient and can be implemented in a scalable manner. Compared to other state-of-the-art algorithms, our proposed method achieves higher accuracy and faster execution speed."
    },
    {
      "id": "9483a27a-794e-4df1-b5ec-7a9478b0fafc",
      "adv_source_id": "9483a27a-794e-4df1-b5ec-7a9478b0fafc",
      "source_id": "4f72dde4-9d71-4c2c-a9e9-19c831d3d2b4",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes",
      "prompt": "The following is the full text of the abstract for a research paper titled \"3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes\" from arxiv.org:",
      "generation": " We address the problem of 3D segmentation of objects with highly unbalanced object sizes in the context of point clouds. We introduce a new loss function, the exponential logarithmic loss, which compensates for the errors made by the model in predicting the sizes of the objects. We show that our method improves the performance of state-of-the-art methods for 3D segmentation on the ScanNet and S3DIS datasets, which contain a large variety of object sizes. Our code and models are available at https://github.com/autonomousvision/logloss."
    },
    {
      "id": "e97bccb8-2d83-4b75-a5a5-bf53f507685a",
      "adv_source_id": "e97bccb8-2d83-4b75-a5a5-bf53f507685a",
      "source_id": "4f72dde4-9d71-4c2c-a9e9-19c831d3d2b4",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes",
      "prompt": "The following is the full text of the abstract for a research paper titled \"3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes\" from arxiv.org:",
      "generation": " We present a 3D segmentation method for objects with highly unbalanced object sizes in the 3D point cloud. We address the challenge of segmenting objects with various sizes by proposing an exponential logarithmic loss (ELo) that captures the hierarchical structure of the input point cloud. The proposed method can effectively segment small and large objects while maintaining the accuracy of the segmentation results. We evaluate our method on the public benchmark SARS-CoV-2 DETECTION CHALLENGE and demonstrate its effectiveness in segmenting various-sized objects. Our code and model files are available at https://github.com/ABC-Learning-Cluster/ELo_3D_Segmentation."
    },
    {
      "id": "0acc4d2b-34af-4ed3-b643-acdc48010f69",
      "adv_source_id": "0acc4d2b-34af-4ed3-b643-acdc48010f69",
      "source_id": "a92f9691-f56a-409b-b928-42b7b872a42e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors\" from arxiv.org:",
      "generation": " In this paper, we present a time series graph cut image segmentation scheme for liver tumors. Our method combines the advantages of both the level set method and the graph cut method, and it is designed to handle the challenges of liver tumor segmentation in dynamic contrast enhanced CT (DCE-CT) images. The scheme is based on a region-based segmentation approach that utilizes a graph cut algorithm to optimize the segmentation results. The input to the graph cut algorithm is a time series of images, where each image is obtained by enhancing the contrast of the DCE-CT image using a different parameterization of the diffusion kernel. Our method has been evaluated on a public dataset containing DCE-CT images of liver tumors, and the experimental results show that our method can accurately segment the tumors while handling well the challenges of intensity variations and noise.\n\nAbstract: Image segmentation is a fundamental step in medical image analysis, and it is particularly challenging for liver tumor segmentation in DCE-CT images due to the intensity variations and noise. In this paper, we present a time series graph cut image segmentation scheme that combines the advantages of both the level set method and the graph cut method. Our scheme is designed to handle the challenges of liver tumor segmentation in DCE-CT images, and it has been evaluated on a public dataset containing DCE-CT images of liver tumors. The experimental results show that our method can accurately segment the tumors while handling well the challenges of intensity variations and noise. The proposed method has the potential to improve the accuracy of liver tumor segmentation in clinical practice, which may lead to better diagnosis and treatment of liver tumors."
    },
    {
      "id": "4e039010-0440-4042-8883-b14bd24635f0",
      "adv_source_id": "4e039010-0440-4042-8883-b14bd24635f0",
      "source_id": "a92f9691-f56a-409b-b928-42b7b872a42e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors\" from arxiv.org:",
      "generation": " In this paper, we present an automatic liver tumor segmentation method based on a time series graph cut. Unlike most existing methods that handle liver tumor segmentation as a binary segmentation problem, we model the tumor segmentation as a hierarchical segmentation problem where the tumor is segmented from the healthy liver tissues and then the tumor is segmented into different tumor regions. In our method, we utilize a time series of MR images acquired over the course of several months where the tumor grows in size. The time series information is incorporated in a graph cut framework where the affinity between pixels is computed using a time series weighted distance metric. We perform a comprehensive evaluation of our method on a public liver tumor segmentation dataset. The experimental results demonstrate the effectiveness of our method compared to nine state-of-the-art algorithms."
    },
    {
      "id": "18c80e63-cf09-4157-940e-afb0c3575841",
      "adv_source_id": "18c80e63-cf09-4157-940e-afb0c3575841",
      "source_id": "8ab94b1f-f372-4747-8c52-ec4f4f279997",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark",
      "prompt": "The following is the full text of the abstract for a research paper titled \"YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark\" from arxiv.org:",
      "generation": " YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark\\\nAbstract: \\\nWe introduce YouTube-VOS, a large-scale video object segmentation benchmark consisting of over 5.4 million video clips and more than 600 object classes. The dataset is collected from YouTube and contains a diverse range of objects, scenes, and motions. It is designed to be challenging and representative of real-world video understanding tasks. The benchmark includes a training and validation split, and we provide baseline results using state-of-the-art object detection and instance segmentation methods. We believe YouTube-VOS will be a useful resource for the research community working on video object segmentation and other video understanding tasks."
    },
    {
      "id": "9d61d034-cc9e-4e55-8539-896f465835a2",
      "adv_source_id": "9d61d034-cc9e-4e55-8539-896f465835a2",
      "source_id": "8ab94b1f-f372-4747-8c52-ec4f4f279997",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark",
      "prompt": "The following is the full text of the abstract for a research paper titled \"YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark\" from arxiv.org:",
      "generation": " YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark\\\nAbstract:\u00a0\u00a0We introduce YouTube-VOS, a large-scale video object segmentation dataset with more than 100,000 videos and 2.5 million annotated objects. The dataset is collected from YouTube and contains a wide variety of objects and scenes. The annotations are produced using a novel crowdsourcing algorithm that efficiently utilizes the temporal coherence in videos. We describe the creation and evaluation of the dataset, and show that it poses significant challenges for current state-of-the-art models. We believe that YouTube-VOS is a useful resource for training and evaluating video object segmentation algorithms."
    },
    {
      "id": "3d2500b5-0aa2-4ae6-8aa3-5f329236fd92",
      "adv_source_id": "3d2500b5-0aa2-4ae6-8aa3-5f329236fd92",
      "source_id": "b5146fec-a44a-41a4-bd1b-f4e87e0812cf",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera\" from arxiv.org:",
      "generation": " We address the problem of recognizing iris images with a database of iris images obtained in visible light using a smartphone camera. We show that the use of this type of camera, which acquires images in natural light, can be an alternative to near-infrared cameras, reducing the cost of iris recognition systems. Initially, we extract the iris region and eliminate the eyelids and eyelashes in order to have only the iris for recognition. Subsequently, we propose to use the Local Binary Pattern (LBP) histogram as a feature for recognition. First, we divide the iris into several zones, and for each zone, we compute the LBP histogram. Then, we use Fisher's linear discriminant analysis (FLDA) to reduce the dimensionality of the feature vector and classify the samples. We use two well-known iris databases, namely the iris database of the Institute of Automation of the Chinese Academy of Sciences (CASIA) and the iris database of the University of Notre Dame (UND), to evaluate our proposal. In order to assess its performance, we conduct a comparison with other recent works that have used these databases.\n\nIn summary, the proposed method is a fast and efficient iris recognition system, which is capable of detecting and recognizing the iris even when the input images present eyelids and/or eyelashes, in addition to being robust to occlusions, illuminations, and resolutions."
    },
    {
      "id": "51d5145d-0b38-427c-8419-8ff0415c255e",
      "adv_source_id": "51d5145d-0b38-427c-8419-8ff0415c255e",
      "source_id": "b5146fec-a44a-41a4-bd1b-f4e87e0812cf",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera\" from arxiv.org:",
      "generation": " We address the problem of recognizing iris in visible light using a smartphone camera. We show that it is possible to employ a conventional smartphone camera, requiring no additional optics, to capture iris images suitable for recognition, with a tolerant pose variation. Given that acquisition is performed in the visible spectrum, the database of iris images can be freely shared, which makes our approach suitable for internet-driven applications. We describe the whole process, from image capture in the device, through preprocessing and encoding of the iris image, to its comparison against enrolled iris codes. We evaluate the performance of the proposed approach through a public dataset, showing its feasibility. Moreover, we have implemented a proof-of-concept application, where iris images are uploaded to a cloud, and the user is identified based on the iris code."
    },
    {
      "id": "8912e1c9-2a4e-4871-bf78-6fc16c593376",
      "adv_source_id": "8912e1c9-2a4e-4871-bf78-6fc16c593376",
      "source_id": "f6cb3441-e401-4b16-a8bc-9e6b55f7717d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation of Microscopy Data for finding Nuclei in Divergent Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Segmentation of Microscopy Data for finding Nuclei in Divergent Images\" from arxiv.org:",
      "generation": " In this paper, we present an approach for the segmentation of microscopy data for the purpose of identifying and locating nuclei in breast tissue images. We utilize a combination of techniques including image enhancement, thresholding, and morphological operations as preprocessing steps, with the primary segmentation approach utilizing a variant of the watershed transform. The watershed transform, initialized using gradient vector flow, is applied to the enhanced image, and then refined using a gradient-based approach. The resulting segmentation is evaluated using both manual and automated measures. The automated measure utilizes a graph cut based approach to evaluate the segmentation in a similar manner to the ground truth annotation. Results of the segmentation are presented, as are comparisons to other approaches. The code and data related to this paper are available at: https://github.com/midn8-sfa/nuclei_segmentation."
    },
    {
      "id": "885b4d46-d9c8-4ee9-b1ce-567d53ca49a6",
      "adv_source_id": "885b4d46-d9c8-4ee9-b1ce-567d53ca49a6",
      "source_id": "f6cb3441-e401-4b16-a8bc-9e6b55f7717d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation of Microscopy Data for finding Nuclei in Divergent Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Segmentation of Microscopy Data for finding Nuclei in Divergent Images\" from arxiv.org:",
      "generation": " In this paper, we present an approach for the segmentation of microscopy data for the purpose of identifying and locating nuclei in breast tissue images. Microscopy images of histopathological breast tissue slides contain multiple objects of interest including epithelium, ducts, stroma and nuclei. Finding nuclei is of particular interest because nuclei are the fundamental repeating unit of ductal carcinoma, which is the most common type of breast cancer. Our approach utilizes a convolutional neural network (CNN) to segment histopathological images of breast tissue into epithelium, ducts, stroma and nuclei. Our proposed method achieves a DSC of 0.9152 and a Jaccard index of 0.8523."
    },
    {
      "id": "5701a6fa-226d-4995-bd12-b40bea3268b1",
      "adv_source_id": "5701a6fa-226d-4995-bd12-b40bea3268b1",
      "source_id": "f7da78e0-ee78-459a-bc61-93d7d103b66c",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concept Mask: Large-Scale Segmentation from Semantic Concepts",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Concept Mask: Large-Scale Segmentation from Semantic Concepts\" from arxiv.org:",
      "generation": " Recent works have shown that segmentation models benefit from explicit representation of semantic concepts in the form of concept modules. However, existing concept modules are constructed manually, which limits their generalization and scalability. In this paper, we propose a new framework called Concept Mask, which automatically discovers concept modules from raw image pixels without any human annotation. Specifically, we formulate the concept module discovery problem as a non-parametric binary classification problem, where each class corresponds to whether a certain region contains a specific concept. We utilize a diffusion model with an adaptive context to classify whether each pixel belongs to a given concept, and further employ an attention module to refine the predictions. Furthermore, we propose a novel online training strategy to optimize the model parameters towards accurate concept segmentation. Our method achieves top performance on the Cityscapes and COCO-Instance segmentation datasets, and our discovered concept modules are visually consistent with the ground truth."
    },
    {
      "id": "2b67a628-d1c5-4aa4-92b4-b7e56663a4cc",
      "adv_source_id": "2b67a628-d1c5-4aa4-92b4-b7e56663a4cc",
      "source_id": "f7da78e0-ee78-459a-bc61-93d7d103b66c",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concept Mask: Large-Scale Segmentation from Semantic Concepts",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Concept Mask: Large-Scale Segmentation from Semantic Concepts\" from arxiv.org:",
      "generation": " Recent works have shown that learning robust features from weakly-supervised web-scale image labels -- where labels are only present at the category level (e.g., \"bird\") as opposed to the instance level (e.g., \"robin\")-- is an effective way to tackle the demanding problem of image segmentation. However, even the best algorithms so far suffer from an important limitation: they produce segmentations that are too coarse, i.e., each region tends to contain pixels from many object instances. For example, an algorithm that produces segmentations at the level of \"bird\" will likely group pixels from many different bird instances within the same region. This limitation prevents such algorithms from being able to produce high quality segmentations of images in which the instances of objects are of interest. In this paper we propose a new algorithm, based on an explicit \"instance prior\", that is designed to produce much finer-grained segmentations in which each region tends to correspond to a single object instance. We show that our algorithm is able to significantly improve the quality of segmentations produced by existing state-of-the-art algorithms for weakly-supervised segmentation. We evaluate our algorithm on the full dataset of 20 million images from Google Images, and show that our approach is able to achieve substantial improvements over previous state-of-the-art approaches for weakly-supervised segmentation."
    },
    {
      "id": "5d7d3569-bef0-49a0-8809-a82686606c0a",
      "adv_source_id": "5d7d3569-bef0-49a0-8809-a82686606c0a",
      "source_id": "124e3778-cc90-4e40-bae4-1f7b655fa8bd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a 3D coarse-to-fine framework for volumetric medical image segmentation. Our method leverages a fully convolutional neural network (FCN) that is trained in a coarse-to-fine fashion, where the network produces an initial segmentation that is subsequently refined through iterative application of the network. We demonstrate the efficacy of our approach on the task of abdominal organ segmentation from T1-weighted MR images. In our experiments, we observe that our approach, when compared to a single-stage FCN, reduces the misclassification rate by up to 47.4% and decreases the mean distance to ground truth by up to 30.2%. We show that the proposed approach yields segmentations of greater accuracy and detail and allows for precise localization of abdominal organs."
    },
    {
      "id": "0e68d3de-25af-435d-9a5b-a4a5b3730a10",
      "adv_source_id": "0e68d3de-25af-435d-9a5b-a4a5b3730a10",
      "source_id": "124e3778-cc90-4e40-bae4-1f7b655fa8bd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation\" from arxiv.org:",
      "generation": " We present a 3D coarse-to-fine framework for volumetric medical image segmentation. Our method leverages multi-scale context propagation and 3D recursive partitioning. It starts by initializing using 3D watershed, which is followed by iterative refinement using geometric active contouring (GAC). We optimize the GAC energy by solving a sparse system, which can be solved efficiently using elementary operations. In order to further improve efficiency, we propose a 3D recursive partitioning strategy, such that the global optimization of the GAC energy can be performed on a sparse 3D grid, instead of the original volumetric image. In addition, we propose a multi-scale context propagation scheme to enforce the consistency of segmentations across different scales. Our framework is fully automatic and does not require any initialization. We evaluate our method on the cardiac image segmentation benchmark, and compare it with state-of-the-art methods. Both quantitative and qualitative results demonstrate the effectiveness of our approach."
    },
    {
      "id": "8b9d53ce-f099-4394-a996-dc37bbb3cc88",
      "adv_source_id": "8b9d53ce-f099-4394-a996-dc37bbb3cc88",
      "source_id": "92208120-efed-4ea8-8828-a80528a442d8",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A multi-contrast MRI approach to thalamus segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A multi-contrast MRI approach to thalamus segmentation\" from arxiv.org:",
      "generation": " Recent developments in MRI acquisition enable the acquisition of multiple contrasts confounding the manual delineation of the thalamus. We propose a method to segment the thalamus in MRI volumes acquired with diffusion tensor imaging (DTI) and magnetization transfer (MT) weighted imaging. Our approach is based on a multi-atlas segmentation framework leveraging the structural and the functional information available in those two contrasts. We used 50 manually annotated atlases built from synthetic brains generated with Diffusion Toolkit (DITK), and two atlases built from real MRI datasets to obtain an initial segmentation of the thalamus. Then, to combine the information provided by the atlases, we defined a cost function that quantifies the disagreement between the predicted segmentations and the ground truth labels in the atlases. We used a greedy algorithm to obtain the final segmentation where the cost is minimized. The proposed method is evaluated on a dataset of 20 subjects (10 healthy controls and 10 Parkinson's disease patients) with encouraging results."
    },
    {
      "id": "6dfab7fe-1361-4839-841a-96b0e4a9477e",
      "adv_source_id": "6dfab7fe-1361-4839-841a-96b0e4a9477e",
      "source_id": "92208120-efed-4ea8-8828-a80528a442d8",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A multi-contrast MRI approach to thalamus segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A multi-contrast MRI approach to thalamus segmentation\" from arxiv.org:",
      "generation": " In this paper, we present a multi-contrast magnetic resonance imaging (MRI) approach for segmenting the thalamus. The approach is based on the fusion of T1-weighted, T2-weighted, and diffusion-weighted MRI scans. The fusion is performed using a three-dimensional (3D) convolutional neural network (CNN). The CNN is trained on a large dataset of thalamus segments from different MRI contrasts. The resulting model is evaluated on a separate test dataset, and results show that the approach is able to accurately segment the thalamus. The segmented thalamus can then be used for further analysis, such as brain connectivity analysis.\n\nThis paper makes a contribution to the field of MRI-based brain segmentation by introducing a multi-contrast approach that leverages the complementary information from different MRI contrasts. The approach is evaluated on a large dataset and shows promise for accurate thalamus segmentation. The segmented thalamus can be used for various brain studies, including brain connectivity analysis. \n\n COPYRIGHT 2017 ARXIV.ORG. \n\n\n\nWIREs Data Mining and Knowledge Discovery, 2017;6:e17001. https://doi.org/10.1002/widm.17001 \n\n\n\nA multi-contrast MRI approach to thalamus segmentation\n\nAbhishek Kumar1*, Manohar B. Mirmira2, and Udayakumar M. Kuducheckal3\n\n1Neuroradiology and Nuclear Medicine, Institute of Neurosciences, Banaras Hindu University, Varanasi, India\n2Neuroradiology and Nuclear Medicine, Institute of Medical Sciences, Banaras Hindu University, Varanasi, India\n3Department of Computer Science and Engineering, National Institute of Technology, Tiruchirappalli, India\n\nKeywords: Brain segmentation, Thalamus, Magnetic resonance imaging, Convolutional neural network\n\n\nAbstract:\n\nThalamus is a vital brain structure that connects the brain to various other parts of the body. Magnetic \nresonance imaging (MRI) is a useful tool for studying the thalamus and other brain structures. In this paper, we present a multi-contrast MRI approach for segmenting the thalamus. The approach is based on the fusion of T1-weighted, T2-weighted, and diffusion-weighted MRI scans, performed using a three-dimensional (3D) convolutional neural network (CNN). The CNN is trained on a large dataset of thalamus segments from different MRI"
    },
    {
      "id": "fefb6d9d-b0d2-4d77-b4ce-ad9cb1e74dd9",
      "adv_source_id": "fefb6d9d-b0d2-4d77-b4ce-ad9cb1e74dd9",
      "source_id": "9e0eccf7-89ba-4900-9c48-e03ac204d395",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"UNet++: A Nested U-Net Architecture for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Abstract: UNet++ is an architecture extension of U-Net, using nested U-Net buildings for medical picture segmentation. By nesting U-Net models, we are able to make the most of the multi-scale characteristic throughout the total image without losing representation ability on any scale. In UNet++, the input image is first processed by a series of nested U-Net blocks, followed by global pooling and a fully connected layer for segmentation. The nested U-Net blocks effectively capture multi-scale context information without increasing the number of parameters or the computational complexity. We evaluate UNet++ on 3 publicly available medical picture segmentation benchmarks, and our model achieves state-of-the-art performance in 2 out of 3 datasets without any data augmentation or transfer learning. The code and models are available at https://github.com/uo0v/UNet-Plus-Plus."
    },
    {
      "id": "4a930896-cea9-4e5d-909d-7d38ffaf7b4b",
      "adv_source_id": "4a930896-cea9-4e5d-909d-7d38ffaf7b4b",
      "source_id": "9e0eccf7-89ba-4900-9c48-e03ac204d395",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"UNet++: A Nested U-Net Architecture for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Abstract: UNet++ is an architecture extension of U-Net, using nested U-Net buildings for medical picture segmentation. In this study, we introduce a new building block, the nested hourglass network (NH), which recursively concatenates U-Net into a single network. Our NH retains the advantages of U-Net while providing more parameters for the deeper layers. This is especially beneficial for segmenting small objects as it allows the network to learn more detailed features. Through extensive experiments on five public segmentation benchmarks, we show the advantages of our NH in both segmentation performance and efficiency. Specifically, our NH improves the state-of-the-art performance of U-Net and other competitive methods on three benchmarks; without any sacrifice in speed. The code and models are available at https://github.com/VOML/unetplusplus."
    },
    {
      "id": "d9a8734e-4534-464e-b22b-be9b3141b28b",
      "adv_source_id": "d9a8734e-4534-464e-b22b-be9b3141b28b",
      "source_id": "625ed860-7a58-4842-8055-b1da093316d0",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Data-Driven Segmentation of Post-mortem Iris Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Data-Driven Segmentation of Post-mortem Iris Images\" from arxiv.org:",
      "generation": " Automated iris recognition has been successfully used in many applications, but it typically relies on live iris images taken under controlled conditions. Post-mortem iris images, taken after death, present additional challenges that require new techniques for segmentation and recognition. In this paper, we propose a data-driven approach for the segmentation of post-mortem iris images. Our approach is based on a convolutional neural network (CNN) that has been trained on a large dataset of post-mortem iris images. The CNN learns to predict pixel-level segmentation labels for each input image, resulting in a binary mask that highlights the iris region. We evaluate our approach on a new publicly available dataset of post-mortem iris images and show that it achieves a high degree of accuracy compared to the state-of-the-art.\n\nOur approach is designed to address the specific challenges of post-mortem iris imaging, which include iris collapse, pupil misalignment, and image noise. To handle these challenges, we propose to use a two-stage approach. In the first stage, we use a CNN to predict a coarse segmentation mask, which is then used to refine the segmentation in the second stage. The refined segmentation mask is then used to extract the iris region from the image.\n\nWe evaluate our approach on a new publicly available dataset of post-mortem iris images, which we make available to the research community. Our results show that our approach achieves a high degree of accuracy compared to the state-of-the-art. We also perform a comprehensive analysis of our approach and show that it is effective in handling the challenges of post-mortem iris imaging.\n\nOur work is a step towards developing a fully automated system for the segmentation and recognition of post-mortem iris images. Such a system could potentially be used for identification purposes in situations where live iris images are not available, such as in mass disaster scenarios or when identifying remains in forensic investigations."
    },
    {
      "id": "c99b5556-8a2f-4b86-bab1-a3ddebc8c894",
      "adv_source_id": "c99b5556-8a2f-4b86-bab1-a3ddebc8c894",
      "source_id": "625ed860-7a58-4842-8055-b1da093316d0",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Data-Driven Segmentation of Post-mortem Iris Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Data-Driven Segmentation of Post-mortem Iris Images\" from arxiv.org:",
      "generation": " Deep learning methods have achieved outstanding performance in face image analysis, but hardly any research focuses on post-mortem iris image analysis. In this paper, we aim to develop a data-driven based approach to segment post-mortem iris images automatically into iris and pupila regions. Normally, the color and texture information of the iris region are quite different from those of the pupila and iris sclera. However, the appearance of the iris region in post-mortem images becomes degenerated and ambiguous due to the decay of the iris texture with damage and color change. To address this issue, we collect a large-scale dataset containing 700 images with ground truth marking the iris and pupila regions. A convolutional neural network based approach is then developed to automatically learn the segmentation mapping from input images to output segmentation masks. We evaluate our approach using seven widely used metrics on our dataset and compare our method with other state-of-the-art ones. The experimental results demonstrate the effectiveness of our approach."
    },
    {
      "id": "ad722a9a-b7e1-4ff1-aaf3-35acd0d394e2",
      "adv_source_id": "ad722a9a-b7e1-4ff1-aaf3-35acd0d394e2",
      "source_id": "921fbb2b-0995-4de5-adc1-87409d109abd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Function Norms and Regularization in Deep Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Function Norms and Regularization in Deep Networks\" from arxiv.org:",
      "generation": " We introduce a new class of layer-specific function norms for deep networks, which are inspired by classical regularization methods such as weight decay and dropout. These function norms are defined as the sum of the absolute values of the gradients of the activation functions with respect to the inputs, and they act as a regularizer by penalizing large activation functions. We show that our function norms lead to improved generalization performance compared to the commonly used weight decay and dropout regularization methods. We present a novel optimization algorithm that minimizes a upper-confidence bound (UCB) objective that includes our function norms, and demonstrate the effectiveness of our method on a variety of deep learning tasks, including image classification and object detection."
    },
    {
      "id": "4dd56592-a8eb-4f4d-be0e-c6270f685fb8",
      "adv_source_id": "4dd56592-a8eb-4f4d-be0e-c6270f685fb8",
      "source_id": "921fbb2b-0995-4de5-adc1-87409d109abd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Function Norms and Regularization in Deep Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Function Norms and Regularization in Deep Networks\" from arxiv.org:",
      "generation": " We consider the problem of optimizing the weights of a deep neural network via stochastic gradient descent (SGD). We analyze the statistical properties of SGD in this setting, and demonstrate that standard function norms, such as the $L_2$ and $L_1$ norms, are ill-suited for regularizing the weights of deep networks. This stems from the fact that the gradients of the loss function with respect to the weights, which drive SGD, exhibit certain statistical regularities in deep networks due to the chain rule of differentiation. We characterize these regularities and introduce a new norm, which we term the gradient norm, that is tailored to them. We show that this norm is more effective at regularizing deep networks than the standard norms, and present an algorithm that optimizes the network's weights using this norm."
    },
    {
      "id": "6628d4aa-c511-4ede-a262-4356b90eecff",
      "adv_source_id": "6628d4aa-c511-4ede-a262-4356b90eecff",
      "source_id": "71eaf1a6-f920-4a5c-b280-55c857c01a9f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model\" from arxiv.org:",
      "generation": " This paper proposes a fully-automatic method for myocardial segmentation of contrast echocardiography sequences. The method is based on random forests, using a shape model constructed with a dictionary of myocardial shapes. The shape model is used to produce an initial segmentation, which is then refined by a random forest to produce a final segmentation. The method was evaluated on a database of 25 contrast echocardiography sequences, and produced segmentations that were compared to manual segmentations produced by an experienced observer. The method achieved a mean distance of 11.4 pixels between the automatically segmented and manually segmented myocardiums, and 96.2% of the automatically segmented myocardiums were within 15 pixels of the manually segmented myocardiums. The method also achieved a mean distance of 8.7 pixels between the automatically segmented and manually segmented left ventricles, and 96.6% of the automatically segmented left ventricles were within 15 pixels of the manually segmented left ventricles. The results show that the method is able to produce accurate segmentations of the myocardium and left ventricle in contrast echocardiography sequences.\n\n```json {\u00a0 \u00a0 \u00a0 \u00a0 \u00a0\"title\" : \"Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model\", \u00a0 \u00a0\"authors\" : [\"Mazen Al-Daccak\"], \u00a0 \u00a0\"year\" : \"2023\", \u00a0 \u00a0\"month\" : \"1\", \u00a0 \u00a0\"day\" : \"1\", \u00a0 \u00a0\"abstract\" : \"This paper proposes a fully-automatic method for myocardial segmentation of contrast echocardiography sequences. The method is based on random forests, using a shape model constructed with a dictionary of myocardial shapes. The shape model is used to produce an initial segmentation, which is then refined by a random forest to produce a final segmentation. The method was evaluated on a database of 25 contrast echocardiography sequences, and the results show that the method is able to produce accurate segmentations of the myocardium and left ventricle in contrast echocardiography sequences.\"\u00a0 \u00a0 \u00a0}```"
    },
    {
      "id": "691603b4-3ebe-450d-9237-b76ff727cb34",
      "adv_source_id": "691603b4-3ebe-450d-9237-b76ff727cb34",
      "source_id": "71eaf1a6-f920-4a5c-b280-55c857c01a9f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model\" from arxiv.org:",
      "generation": " Detection and segmentation of the myocardium in echocardiographic sequences is a fundamental step for quantification of cardiac function. We present an innovative method for fully automatic myocardial segmentation that is guided by a shape model. Random forests are trained using a large number of features extracted from both the intensity histogram and gradient magnitude of the image. The proposed method has been validated on 400 frames from 16 different sequences, and compared with two state-of-the-art methods using the widely used dice similarity coefficient. Our method shows a large improvement in performance with an average dice coefficient of 0.88\u00b10.06, significantly (p<0.001) better than the baselines."
    },
    {
      "id": "ef54e83f-4cbd-41f3-8fdc-eea9337a3c26",
      "adv_source_id": "ef54e83f-4cbd-41f3-8fdc-eea9337a3c26",
      "source_id": "0a3de390-010c-4cad-be4f-a663ae4f4311",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Real-time Prediction of Segmentation Quality",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Real-time Prediction of Segmentation Quality\" from arxiv.org:",
      "generation": " We present a method for real-time prediction of segmentation quality while the network is training. Given a set of training images, we train a student-teacher network with an auxiliary segmentation branch whose sole purpose is to predict segmentation quality. Since the teacher network is defined in terms of the student network, the two networks can be trained jointly in an end-to-end fashion. During training, the predicted quality metric provides reinforcement signal to guide the training process of the student network and it could be used to protect the regions of interest. In the test phase, the segmentation quality predictor can be used for active learning and selective sampling. We demonstrate the effectiveness of our approach on the Cityscapes dataset."
    },
    {
      "id": "e3dfc2a8-6a8f-4fed-913c-65440b281de9",
      "adv_source_id": "e3dfc2a8-6a8f-4fed-913c-65440b281de9",
      "source_id": "0a3de390-010c-4cad-be4f-a663ae4f4311",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Real-time Prediction of Segmentation Quality",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Real-time Prediction of Segmentation Quality\" from arxiv.org:",
      "generation": " We present a method for the real-time prediction of segmentation quality while the algorithm is training. By comparing the label of a minority class to the predicted class of a pixel, we create an error map that highlights areas of misclassification. This error map can be used to determine the quality of the segmentation as the model is trained, giving an indication of how well the model is performing on the minority class. We show that our method provides a useful tool for guiding the training process, and can be used to produce segmentations with higher quality than a model trained without the guidance. In particular, we apply our method to the problem of segmenting lymph nodes in CT scans, where the rarity of lymph nodes poses a challenge for standard segmentation methods. Our method achieves a 10.7% improvement in F1 score over training without guidance, and a 4.3% improvement over the state-of-the-art."
    },
    {
      "id": "1aaceecc-42ef-458a-9d06-020ad2a08f0d",
      "adv_source_id": "1aaceecc-42ef-458a-9d06-020ad2a08f0d",
      "source_id": "fd27ce13-36b8-43c6-9083-350afddf9680",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\" from arxiv.org:",
      "generation": " We present a concurrent spatial and channel squeeze & excitation (CSSE) module for fully convolutional networks that concurrently performs channel-wise and spatial squeeze and excitation operations as a single layer, allowing for efficient network designs that achieve better performance and robustness compared to conventional architectures. The proposed CSSE module is composed of a spatial squeeze path and a channel squeeze path, which perform orthogonal transformations to capture different kinds of important features. We demonstrate the effectiveness of the proposed module through extensive experiments on four benchmark datasets: CIFAR-10, CIFAR-100, SVHN, and ImageNet. The proposed CSSE module achieves state-of-the-art performance on CIFAR-10, CIFAR-100, and SVHN, and improves over the state-of-the-art ResNet-50 model by 1.7% on ImageNet, surpassing the performance of many complex architectural variations."
    },
    {
      "id": "a91b3692-357f-4d42-bb2e-8c96e506512a",
      "adv_source_id": "a91b3692-357f-4d42-bb2e-8c96e506512a",
      "source_id": "fd27ce13-36b8-43c6-9083-350afddf9680",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\" from arxiv.org:",
      "generation": " We present a concurrent spatial and channel squeeze & excitation (SSA-SCE) block for fully convolutional networks that significantly improves the efficiency-accuracy trade-off. SSA-SCE first performs channel-wise squeeze & excitation on the hidden feature maps, and then a spatial squeeze & excitation on the modulated feature maps, followed by an expansion operation to scale up the resultant feature maps to the original size for further computation. During the training process, the proposed method simultaneously performs channel-wise and spatial squeeze & excitation, alongside an optimizer that iteratively trains the proposed SSA-SCE block to learn the time-consuming spatial modulation. We evaluate our method on CIFAR-10, CIFAR-100 and SVHN. Our method achieves state-of-the-art performance on CIFAR-10 and CIFAR-100, and improves over the state-of-the-art by a large margin on SVHN, surpassing the performance of SE-Net and Pyramid-SE-Net."
    },
    {
      "id": "6b443226-17c7-4c25-b37f-c695f0a0fc67",
      "adv_source_id": "6b443226-17c7-4c25-b37f-c695f0a0fc67",
      "source_id": "22391927-6f6e-4432-96f7-a86db2bc425d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CFCM: Segmentation via Coarse to Fine Context Memory",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CFCM: Segmentation via Coarse to Fine Context Memory\" from arxiv.org:",
      "generation": " Today's contextual bandit algorithms operate under the assumption that the context features are directly observable. However, in many real-world applications such as fraud detection or customer journey analysis, the contextual features are not directly observable, and we can only observe their aggregates (e.g., sum, count, average) over a sliding window of recent observations. In this paper, we close this gap by proposing a new algorithmic framework called CFCM: Segmentation via Coarse to Fine Context Memory. CFCM maintains a context memory to store recent observations and dynamically adjusts the context window size based on the underlying distribution. It then uses a segmentation algorithm to recover the fine-grained context features from the aggregated features stored in the context memory. We show that the proposed CFCM can achieve optimal contextual bandit performance (i.e., Bayes-optimal prediction and O(1)/O(1/e) regret) under certain mild conditions, and experimental results on real-world datasets demonstrate its efficacy and efficiency."
    },
    {
      "id": "79970e40-2e5f-41de-9f5b-b4ac66fbe5e5",
      "adv_source_id": "79970e40-2e5f-41de-9f5b-b4ac66fbe5e5",
      "source_id": "22391927-6f6e-4432-96f7-a86db2bc425d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CFCM: Segmentation via Coarse to Fine Context Memory",
      "prompt": "The following is the full text of the abstract for a research paper titled \"CFCM: Segmentation via Coarse to Fine Context Memory\" from arxiv.org:",
      "generation": " Today's memory-based point cloud segmentation methods are either too slow for scanning-scale point clouds or too inaccurate for segmenting fine-grained objects. To bridge this performance-accuracy gap, we propose CFCM: a novel approach that leverages a coarse-to-fine memory mechanism. We start by efficiently extracting point-wise features and use a memory mechanism to store and retrieve the most relevant features for object localization. Then, we fine-tune the remembered features using a segmentation-trained point transformer. Experiments on S3DIS and ScanNet show our approach outperforms the state-of-the-art in both speed and accuracy, often by a large margin. For instance, on S3DIS, our method achieves 9.65% mIOU on the test set with 50% memory-saving (0.43 s/scan vs. 0.86 s/scan of the fastest prior), and 55.5% mIOU on ScanNet with already +10% speedup (79.9 s vs. 89.3 s)."
    },
    {
      "id": "90249274-59b7-47e2-8c27-e40a0877790f",
      "adv_source_id": "90249274-59b7-47e2-8c27-e40a0877790f",
      "source_id": "f5d9f666-2531-4417-bb70-be7f381b36b9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Shape Segmentation via Shape Fully Convolutional Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"3D Shape Segmentation via Shape Fully Convolutional Networks\" from arxiv.org:",
      "generation": " We present a fully-automatic deep learning approach to 3D shape segmentation, and demonstrate the effectiveness of our approach on the ShapeNet part segmentation benchmark. Our approach is based on a fully convolutional network (FCN) that processes the input shape directly without requiring a voxelization or surface reconstruction step. To handle the variability and ambiguity of 3D shapes, we propose a novel FCN architecture that is capable of learning context-aware representations from multiple input viewpoints. We evaluate our method on the ShapeNet part segmentation benchmark, and show that our approach outperforms the state-of-the-art on both the part segmentation and chamfer distance metrics. Our source code and trained models are available at https://github.com/sshaoshuai/3D-Shape-Segmentation-FCN."
    },
    {
      "id": "fd348602-628e-4863-8f72-3478e5893c33",
      "adv_source_id": "fd348602-628e-4863-8f72-3478e5893c33",
      "source_id": "f5d9f666-2531-4417-bb70-be7f381b36b9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Shape Segmentation via Shape Fully Convolutional Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"3D Shape Segmentation via Shape Fully Convolutional Networks\" from arxiv.org:",
      "generation": " We present a fully convolutional network architecture for segmenting object shapes in 3D point clouds. Our approach naturally handles overlapping objects and is capable of segmenting objects in partially annotated point clouds. We leverage recent advances in 2D convolutional neural networks (CNNs) for image segmentation as a starting point for our 3D shape segmentation network. Specifically, we first utilize a shallow 2D CNN to predict an initial segmentation mask for each point based on point-wise features, and then apply a subsequent network that utilizes both point-wise and spatial features for refined segmentation. To enable end-to-end training, we propose an iterative training scheme that gradually unfreezes layers in the subsequent network. We demonstrate the effectiveness of our approach on the ShapeNet part segmentation benchmark."
    },
    {
      "id": "ba6d5d8f-97bd-4874-a23e-6c58a50c0454",
      "adv_source_id": "ba6d5d8f-97bd-4874-a23e-6c58a50c0454",
      "source_id": "b77f7de9-a9bb-40e6-9179-4d303a00b494",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention U-Net: Learning Where to Look for the Pancreas",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Attention U-Net: Learning Where to Look for the Pancreas\" from arxiv.org:",
      "generation": " Deep learning-based models for medical image segmentation are typically evaluated on the entire image slab, irrespective of the anatomical region of interest. In this work, we draw analogy from attention mechanisms in natural language processing to develop a method for automatically learning to attend to the region of interest (ROI). We propose an attention u-net architecture that learns a map of attention weights for each input slice to selectively attend to the region of interest while ignoring the background. We demonstrate the efficacy on pancreas segmentation in CT volumes from the NIH Pancreas Phantom and Pancreas Divisibility Challenge. Our method improves upon the baseline u-net model (FROC: 0.83 vs 0.81) and reduces the run-time by approximately 55%."
    },
    {
      "id": "c20f3472-fba4-49ea-8ea0-a1266d60c41c",
      "adv_source_id": "c20f3472-fba4-49ea-8ea0-a1266d60c41c",
      "source_id": "b77f7de9-a9bb-40e6-9179-4d303a00b494",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention U-Net: Learning Where to Look for the Pancreas",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Attention U-Net: Learning Where to Look for the Pancreas\" from arxiv.org:",
      "generation": " Deep learning-based models for radiotherapy treatment planning have become indispensable; however, these models cannot be directly applied to diagnostic radiology, as they tend to overfit the data and perform poorly on offline inference. To create a model that strikes a balance between underfitting and overfitting, we have developed a novel attention-based U-Net architecture, which outperforms the original U-Net in terms of interpolation and generalization performance. Our model learns to attend to discriminative areas of the image and leverages both appearance and geometry for pancreas segmentation. In this study, we demonstrate the performance of our model on the publicly available pancreatic segmentation challenge and show it achieves state-of-the-art performance."
    },
    {
      "id": "d947f950-a8f4-4472-b94c-3ee52ee683c6",
      "adv_source_id": "d947f950-a8f4-4472-b94c-3ee52ee683c6",
      "source_id": "412b173e-6cfb-446f-9818-48c8094e7caf",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning\" from arxiv.org:",
      "generation": " Recent progress in deep learning has led to significant advances in medical image segmentation. However, fully automated segmentation algorithms still face challenges in dealing with anatomical variation, lesion appearance, and similar tissue structures. While manual segmentation by domain experts is accurate and robust, it can be tedious, time-consuming, and suffers from intra- and inter-user variability. There has been a growing interest in interactive segmentation techniques that seek to combine the strengths of both fully automated and manual segmentation methods. In this paper, we propose a novel approach for interactive medical image segmentation that combines point-based interaction and sequential patch learning. Our method enables the user to quickly highlight points on the image to define an initial segmentation boundary. These user-defined points are then used to generate a initial segmentation mask, which is subsequently refined using a sequential patch learning algorithm. The patch learning algorithm utilizes a deep learning model to predict pixel-level labels for each patch within the mask, and iteratively updates the mask based on the predicted labels. We evaluate our method on a public dataset of abdominal CT scans and demonstrate that our approach achieves substantial improvement in segmentation accuracy compared to existing interactive segmentation methods."
    },
    {
      "id": "20cb8cd2-651a-42ff-8529-a778f7fcfc3e",
      "adv_source_id": "20cb8cd2-651a-42ff-8529-a778f7fcfc3e",
      "source_id": "412b173e-6cfb-446f-9818-48c8094e7caf",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning\" from arxiv.org:",
      "generation": " In this paper, we present an efficient and interactive medical image segmentation method via point-based interaction and sequential patch learning. Our method leverages a deep learning architecture, namely the convolutional neural network (CNN), to learn the underlying image representation effectively. In particular, we design a novel architecture that integrates point-based interaction and sequential patch learning to achieve precise and efficient segmentation. We introduce a point-based interaction module to facilitate the network learning the implicit representation of the image by utilizing a set of seed points provided by the user. Furthermore, we propose a sequential patch learning module that utilizes the recently proposed patch-based CNN to sequentially learn the image representation in a patch-by-patch manner. The entire segmentation process is formulated as a minimax optimization problem, in which the network is trained to optimize the segmentation while minimizing the user's intervention. We demonstrate the effectiveness of our approach on a public lung CT image dataset and show that our method can achieve precise segmentation with significantly reduced user interaction compared to the state-of-the-art methods."
    },
    {
      "id": "0ae435f8-8610-4467-b638-79556cd4b269",
      "adv_source_id": "0ae435f8-8610-4467-b638-79556cd4b269",
      "source_id": "6e8be1dc-7f31-4511-9832-b8e324403bfb",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation\" from arxiv.org:",
      "generation": " This paper describes the Matlab implementation of a machine vision algorithm for the evaluation of ballast degradation using image processing techniques. The algorithm was developed as a part of a larger project on ballast degradation evaluation and was implemented using the Matlab programming language. The implementation was done by first developing a set of image processing techniques that could be used to extract the necessary information from a series of images of ballast degradation. These techniques included methods for image enhancement, feature extraction, and image segmentation. Once the image processing techniques had been developed, they were then combined into a single algorithm that could be used to evaluate ballast degradation. The algorithm was implemented using a set of Matlab functions that were developed specifically for this purpose. The functions were designed to be easy to use and modular, so that they could be easily modified or extended in the future. The implementation was tested on a set of images of ballast degradation and was found to be effective in evaluating the degradation. The algorithm was able to extract the necessary information from the images and provide a measure of the degradation. The results of the testing were consistent with the expected results and demonstrated the effectiveness of the algorithm."
    },
    {
      "id": "3127ea20-36e4-4c8d-a5d2-fc702e298135",
      "adv_source_id": "3127ea20-36e4-4c8d-a5d2-fc702e298135",
      "source_id": "6e8be1dc-7f31-4511-9832-b8e324403bfb",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation\" from arxiv.org:",
      "generation": " This paper presents a Matlab implementation of a machine vision algorithm for the evaluation of ballast degradation using image processing techniques. The algorithm utilizes grayscale images of ballast to identify and quantify the erosion of the ballast surface due to the passage of rolling stock. The implementation consists of several image processing steps, including filtering, thresholding, and erosion/dilation, to extract the necessary features for evaluating ballast degradation. The extracted features are then used to compute a degradation metric that quantifies the erosion of the ballast surface. The algorithm was tested on a database of ballast images and the results show that it can effectively identify and quantify ballast degradation. The implementation can be used as a tool for conducting ballast maintenance planning and monitoring.\n\nThe cost of railway maintenance is one of the key factors affecting the economics of rail transport. Ballast degradation, caused by the dynamic loads of rolling stock, is a serious problem leading to high maintenance costs. Our innovative machine vision algorithm, for the automatic evaluation of ballast degradation, is aimed at reducing these costs. The proposed algorithm utilizes digital images of ballast to identify and quantify the erosion of the ballast surface due to the passage of rolling stock. The algorithm utilizes image processing techniques, such as filtering, thresholding, and erosion/dilation to extract the necessary features for evaluating ballast degradation. The extracted features are then used to compute a degradation metric that quantifies the erosion of the ballast surface. The proposed method has been tested using a large number of ballast images obtained from different railway sites, and the results show that it can effectively identify and quantify ballast degradation. The implementation of the proposed method can be used as a tool to assist in conducting ballast maintenance planning and monitoring.\n\nOur approach focuses on change detection in pixel-level features of ballast surface patterns, arising due to different degradation levels. Our machine vision algorithm will contribute to the automation of ballast inspection, thereby reducing the human effort and cost associated with manual inspection, while increasing the accuracy and efficiency of the process. The output of our algorithm in the form of a ballast degradation metric can be used by railway engineers to make informed decisions regarding the maintenance and replacement of ballast."
    },
    {
      "id": "94b70b48-5b14-4026-bd9b-3b0254593e73",
      "adv_source_id": "94b70b48-5b14-4026-bd9b-3b0254593e73",
      "source_id": "e5025a46-510c-4eab-9c95-3bfd0684ef89",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means\" from arxiv.org:",
      "generation": " We present an unsupervised image segmentation approach specifically designed for pathological whole-slide images (WSIs). Given a WSI, our goal is to segment the image into regions corresponding to tumor and healthy tissue. We employ a recently proposed representation learning approach, spherical k-means (SKM), to learn a low-dimensional feature representation for WSIs. Our approach is unsupervised, meaning it does not require labeled data for training. We demonstrate the performance of our approach on WSIs from a public dataset, the CAMELYON17 challenge dataset, which contains 16 cases (slides) with a total of 1,421 tiles. Our approach achieves a mean tile F1-score of 0.5139, which is comparable to recent supervised approaches that rely on labeled data. We also perform a detailed analysis of the proposed approach to understand its behavior and identify areas for improvement."
    },
    {
      "id": "9327ab83-4328-4482-8ad4-fae5b9b6f8e8",
      "adv_source_id": "9327ab83-4328-4482-8ad4-fae5b9b6f8e8",
      "source_id": "e5025a46-510c-4eab-9c95-3bfd0684ef89",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means\" from arxiv.org:",
      "generation": " We present an unsupervised image segmentation approach specifically designed for pathological whole-slide images (WSIs). Firstly, we adopt a recently proposed representation learning algorithm, spherical k-means (SKM), to learn a discriminative and rotationally invariant feature representation for WSIs. The proposed SKM is an unsupervised learning approach and can obtain rotationally invariant features in the WSI domain where the images are typically very large and suffer from strong intra-class variation. We then adopt a simple yet effective watershed based segmentation approach to segment the WSIs based on the learned features. The proposed approach was evaluated on the largest available dataset of WSIs, the ScanScope Express slides from the Indian Bio-medical Imaging Archive (IBIA) database, where the ground truth segmentation is also available. Our experiments show that the proposed approach achieves state-of-the-art performance in WSI segmentation."
    },
    {
      "id": "3f653747-75fc-4226-bfc8-3e05c300a369",
      "adv_source_id": "3f653747-75fc-4226-bfc8-3e05c300a369",
      "source_id": "641f6cbf-e9ce-462f-95f1-37c1755bc265",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound\" from arxiv.org:",
      "generation": " We present an approach for training deep learning models for multi-organ segmentation. Our method utilizes sample selection based on the relaxed upper confident bound (RUCB) algorithm. Given a limited number of training samples, we aim to select a subset that covers the distribution of all classes well, in order to reduce the model's generalization error. We show that our approach can achieve better performance compared to existing methods, such as cross-validation and random sampling, in terms of average precision and intersection over union. Our method is particularly useful for the training of deep learning models in medical imaging, where the available training data is often limited."
    },
    {
      "id": "9c79dc7c-0d07-4ad9-8295-8450ca78c3a2",
      "adv_source_id": "9c79dc7c-0d07-4ad9-8295-8450ca78c3a2",
      "source_id": "641f6cbf-e9ce-462f-95f1-37c1755bc265",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound\" from arxiv.org:",
      "generation": " We propose an effective training method for multi-organ segmentation, which addresses the dataset imbalance problem without requiring the ground-truth organs to be labeled. We formulate the multi-organ segmentation as a sampling problem, where the goal is to sample the similar organ instances into the same class. By measuring the similarity between instances, our method selects a confident and diverse set of positive samples for each class, and positive samples with large IoUs are given higher probabilities. Within each class, we further adapt online hard example mining to select hard positive samples, resulting in a relaxed upper confident bound (RUCB). The proposed method is evaluated on the abdominal CT volume of the Patient-CT dataset. With a single-scale model and without ground-truth organ labels, our method achieves 97.2% mean accuracy on five organs, outperforming the state-of-the-art by a large margin. When training a multi-scale model, the performance gain is smaller due to the limitation of the training data."
    },
    {
      "id": "50665cd3-8095-4de2-92bf-88dbe167097f",
      "adv_source_id": "50665cd3-8095-4de2-92bf-88dbe167097f",
      "source_id": "40ad0431-c84c-4f1c-b797-48a0c19c6ffd",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Compassionately Conservative Balanced Cuts for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Compassionately Conservative Balanced Cuts for Image Segmentation\" from arxiv.org:",
      "generation": " We present an approach for surgical instrument segmentation in intra-operative images that leverages a generative model of the deformable surgical instrument appearance, learned from synthetic data. Given a query image, the model predicts instrument appearances that explain the image pixel intensities. These predictions are used to induce a region proposal network that computes an initial segmentation. We introduce a novel overlap-based metric that compares segmentations with the ground truth, and use it to produce a final segmentation by iteratively fusing proposals in a process internally akin to watershed. We demonstrate the efficacy of our approach on a new publicly available dataset of intra-operative images."
    },
    {
      "id": "0f6da362-ea19-4952-8c15-02a311257e97",
      "adv_source_id": "0f6da362-ea19-4952-8c15-02a311257e97",
      "source_id": "40ad0431-c84c-4f1c-b797-48a0c19c6ffd",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Compassionately Conservative Balanced Cuts for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Compassionately Conservative Balanced Cuts for Image Segmentation\" from arxiv.org:",
      "generation": " A basic challenge in semantic image segmentation is establishing appropriate\nbalances between precision and coverage, especially in the presence of\nlimited training data. Classic approaches such assupervised\nlearning with a sigmoid classifier enforce balance by maximizing\nthe global F-measure. These are inherently conservative, but\ntheir conservativeness is not well-controlled and can often be too\naggressive, especially for difficult segments. We introduce a\nnovel, intuitively compelling metric, the Covered Area F-measure\n(CA-F), which uses the area of the intersection of the ground\ntruth and predicted segmentation, normalized by the area of the\nground truth, as an information-theoretically derived measure of\nprecision. Maximizing CA-F enforces balance by minimizing the\ncovered area loss, analogous to the popular IoU metric for\ndetection. In simulation and on real data, we show that CA-F\nmaximization can significantly (up to +0.4 dB) improve\nsegmentation accuracy over sigmoid maximizing F-measure, with\nonly a small (less than 1%) increase in conservativeness. We\nalso present an efficient, parallel implementation of the\noptimization problem based on recent advances in numerically\nefficientConcise Conservation Lawsfor deterministic optimal\ncontrol."
    },
    {
      "id": "659e22ad-da09-4e07-9791-d08bdafe18e6",
      "adv_source_id": "659e22ad-da09-4e07-9791-d08bdafe18e6",
      "source_id": "92e24d44-2ce3-4aa8-97be-7571af82aa59",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive strategy for superpixel-based region-growing image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Adaptive strategy for superpixel-based region-growing image segmentation\" from arxiv.org:",
      "generation": " Segmentation of anatomical structures in 3D MR images is a crucial step towards automated analysis of these images. We present an adaptive strategy for superpixel-based region growing to segment 3D MR images. In this approach, 3D MR images are first transformed into a stack of 2D images using orthogonal plane slices. Each 2D slice is segmented by employing a simple and efficient superpixel-based region growing algorithm. To handle the anatomical variation in the size of structures across different patients, we propose an adaptive strategy that estimates the optimal superpixel size by analyzing the gradient distribution of the 2D slices. The proposed adaptive strategy enables the algorithm to produce significantly more accurate segmentations compared to fixed-size superpixel approaches, while maintaining a practical computational complexity. The segmented 2D slices are then combined to generate a 3D segmentation of the anatomical structure of interest. The proposed method has been evaluated on manually annotated MR images of the knee joint, lower lumbar spine, and femur. The results show that the proposed method is capable of producing accurate segmentations of the different anatomical structures within these images."
    },
    {
      "id": "15ac7ae7-4c61-48bd-ab2f-6741062b339a",
      "adv_source_id": "15ac7ae7-4c61-48bd-ab2f-6741062b339a",
      "source_id": "92e24d44-2ce3-4aa8-97be-7571af82aa59",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive strategy for superpixel-based region-growing image segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Adaptive strategy for superpixel-based region-growing image segmentation\" from arxiv.org:",
      "generation": " Segmentation of structural elements in images, especially in histopathology images, remains a challenging task. In this paper, we propose a new superpixel-based region-growing (SPRG) algorithm for segmentation of structures in histopathology images. The proposed method adaptively refines the initial superpixel segmentation using region growing based on gradient and regional information. In the regional refinement process, the method uses a novel regional shaping factor (RSF) to improve the precision of the segmentation. We assess the performance of the proposed method using microwave images and tissue histopathology images, and compare the results with those of state-of-the-art methods. The experimental results show that the proposed method provides high quality segmentation results."
    },
    {
      "id": "7f2e64b2-03cb-4016-94dc-c985f9cfdb90",
      "adv_source_id": "7f2e64b2-03cb-4016-94dc-c985f9cfdb90",
      "source_id": "f5db7b0d-ca4e-4b11-8b07-41006bc8a59e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions",
      "prompt": "The following is the full text of the abstract for a research paper titled \"IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions\" from arxiv.org:",
      "generation": " Active contour models have been widely used in image segmentation due to their simplicity and efficiency. However, existing active contour models assume that the image to be segmented possesses homogeneity and smoothness in the intensity values of the image pixels. Such assumptions are not valid for many real world images where intensity inhomogeneities and discontinuities exist. In this paper, we propose a new active contour model, which we refer to as the Inhomogeneity Estimated by Orthogonal Primary Functions (IEOPF) model, to overcome this limitation. In the IEOPF model, the inhomogeneities in the image are estimated by using a set of orthogonal primary functions. The primary functions are optimized to minimize the cost function in the active contour model. The use of orthogonal primary functions enables us to decouple the optimization of the inhomogeneity estimate and the active contour model. The IEOPF model is capable of effectively handling intensity inhomogeneities and discontinuities in the image and significantly improving the segmentation accuracy. We demonstrate the effectiveness of our approach through experiments on synthetic and real world images."
    },
    {
      "id": "2d61910c-51e7-4f38-a91e-fb0d0eea883e",
      "adv_source_id": "2d61910c-51e7-4f38-a91e-fb0d0eea883e",
      "source_id": "f5db7b0d-ca4e-4b11-8b07-41006bc8a59e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions",
      "prompt": "The following is the full text of the abstract for a research paper titled \"IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions\" from arxiv.org:",
      "generation": " Active contour models have been successfully used for image segmentation, however, classical active contour models assume that intensities of image pixels are smooth and depend on the curve parameter in a known fashion. These assumptions hold reasonably well for images without inhomogeneities, noise, or texture. In this paper, we generalize the active contour model to allow for image inhomogeneities that can be described by a finite set of orthogonal primary functions. We estimate the primary function coefficients adaptively from the image itself. We show that the minimum energy boundary for our model provides a compact representation of the image and is equivalent to an optimal superposition of the primary functions. Unlike in classical active contour models, in our approach, the minimization of an energy functional determines both the curve boundary and the primary function coefficients. We show that our approach is related to Tikhonov regularization of ill-posed problems. We apply the proposed method to segmentation of pathological lymph node images that demonstrate pronounced texture and noise. In comparative experiments on synthetic images, we demonstrate that the proposed method outperforms both classical active contour models and a recently proposed active contour model that utilizes Tikhonov regularization with a fixed parameterization."
    },
    {
      "id": "eadb7a77-9b38-4a6e-bcf4-9990eeae3146",
      "adv_source_id": "eadb7a77-9b38-4a6e-bcf4-9990eeae3146",
      "source_id": "1a062307-d3f9-4d2f-921e-e4ab41966337",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey\" from arxiv.org:",
      "generation": " Knee joint magnetic resonance imaging (MRI) is important in the study of osteoarthritis as it is a noninvasive procedure. However, the manual segmentation of the knee joint MRI is tedious and time-consuming. In this paper, we present a survey on computer-aided knee joint MRI segmentation methods. We first introduce the problem and present the commonly used knee joint MRI datasets. Then, we survey the state-of-the-art segmentation methods. We also present an evaluation of these methods using two datasets. The results show that the performance of these methods is far from satisfactory. We then discuss the reasons and suggest possible improvements.\n"
    },
    {
      "id": "a31e0587-d6ac-477e-a892-4810695ff0be",
      "adv_source_id": "a31e0587-d6ac-477e-a892-4810695ff0be",
      "source_id": "1a062307-d3f9-4d2f-921e-e4ab41966337",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey\" from arxiv.org:",
      "generation": " Knee joint magnetic resonance imaging (MRI) is important in the study of osteoarthritis as it is the gold standard technique in the detection of this disease and is also used for the determination of disease progression. In our research, we survey the state-of-the-art in automated knee joint MRI segmentation. Our focus is on segmentation methods that utilize machine learning and/or statistical techniques. Our survey covers a total of 25 works, including detailed analyses of each of these works. We analyze the database used, the techniques adopted, the evaluation metrics used and the results obtained. We also include a comparison of the various methods according to the database used, the technique adopted, and the type of knee joint segmentation. Furthermore, we present a benchmark database that comprises 100 MRI volumes, and we evaluate the performance of eight state-of-the-art methods on this database.\n\nConclusion: \n\nWe have presented a survey of 25 recent works on automated knee joint MRI segmentation. The survey includes an analysis of the databases used, the techniques adopted, the evaluation metrics used, and the results obtained. We have also included a new benchmark database for knee joint segmentation and have evaluated eight state-of-the-art methods on this database. The results show that there is no single best method for knee joint segmentation and that the performance of each method depends on the database used and the evaluation metrics adopted.\n\n"
    },
    {
      "id": "5522aaa7-5ca6-4314-8495-8ecd533331b3",
      "adv_source_id": "5522aaa7-5ca6-4314-8495-8ecd533331b3",
      "source_id": "ce96623b-70a5-4e98-a000-88f14a9b1d6b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Iterative Spanning Forest Framework for Superpixel Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Iterative Spanning Forest Framework for Superpixel Segmentation\" from arxiv.org:",
      "generation": " An Iterative Spanning Forest Framework for Superpixel Segmentation\n\nAbstract\nWe present an effective image segmentation approach based on an iterative algorithm that combines the benefits of spectral clustering and greedy region-growing strategies. In each iteration, our algorithm iteratively constructs an optimal spanning forest using a graph-based segmentation, and then improves it by performing a spectral clustering that reduces the number of segments. In this way, we obtain a sequence of segmentations, where the segmentation obtained in each iteration is an optimal spanning forest in the sense that every region of the image is connected. Our framework is extremely fast, and it is able to produce very good segmentations for a large number of images, outperforming state-of-the-art algorithms in terms of objective evaluation metrics as well as visual inspection.\n\nIntroduction\nOne of the most fundamental problems in computer vision is image segmentation, i.e., the process of dividing an image into regions that correspond to physically meaningful objects. In many applications, these regions need to be compact and approximately spatially contiguous. In other words, we are interested in segmenting the image into so-called superpixels [1, 2, 4, 5, 9, 10, 12, 13, 15, 16, 17, 19, 25, 26, 27, 29, 30, 31, 33, 34, 37, 39]. In this paper, we introduce a new segmentation approach based on iterative optimization that explicitly aims at producing a good quality spanning forest.\n\nOur Approach\nGiven an image, we start by generating a cluttered graph, where each pixel is a node, and we only connect pixels that belong to the same object with an edge. In order to obtain a segmentation where each region is a connected component, we then find a spanning forest, i.e., a subgraph where each node has exactly one connected component. This can be seen as a graph-based segmentation, and various algorithms have been proposed to find optimal spanning forests [6, 7, 8, 18, 21, 22, 23, 28, 32, 35, 36, 38]. Our approach can be seen as an extension of spectral clustering [11, 14, 24, 42, 43], where we start with a graph-based segmentation and use spectral techniques to improve it. In particular, we use the spanning forest obtained from the graph-based segmentation as an initial guess, and we iteratively refine it by performing a spectral clustering that reduces the number of segments. Note that our approach is similar to [3], but it is"
    },
    {
      "id": "1df94d21-99f8-4a5e-904c-155d93c2396c",
      "adv_source_id": "1df94d21-99f8-4a5e-904c-155d93c2396c",
      "source_id": "ce96623b-70a5-4e98-a000-88f14a9b1d6b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Iterative Spanning Forest Framework for Superpixel Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Iterative Spanning Forest Framework for Superpixel Segmentation\" from arxiv.org:",
      "generation": " We present an iterative framework for superpixel segmentation based on spanning forests. Our algorithm starts from an initial guess of the superpixel segmentation and iteratively refines it. At each iteration, the algorithm selects an edge in the graph representation of the current segmentation that connects two superpixels, and then merges the two superpixels. This process is repeated until no more edges can be selected. We analyze the properties of the algorithm and discuss how it can be implemented. We demonstrate the effectiveness of our method on several benchmark images and compare it with other state-of-the-art superpixel segmentation algorithms."
    },
    {
      "id": "57c1d853-276b-4cb9-aa18-62c0bd9c82e0",
      "adv_source_id": "57c1d853-276b-4cb9-aa18-62c0bd9c82e0",
      "source_id": "88e5c1cd-2edc-4c4d-b72a-63ed078c3c5e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation\" from arxiv.org:",
      "generation": " We present TernausNet, a U-Net variant designed for efficient implementation, while retaining the ability to effectively segment instance levels of intra-class variability. The design includes a VGG11 encoder, pre-trained on ImageNet, that is modified for semi-supervised segmentation. We modify the encoder by adding a convolutional layer with a ReLU activation function and a convolutional layer with a sigmoid activation function. These modifications enable the encoder to produce segmentation masks with well-defined boundaries. We evaluate TernausNet on the Cityscapes and CamVid datasets, and achieve F1 scores of 95.4% and 92.1% on the val sets, respectively. Our code and model weights are available at https://github.com/autonomousvision/segmentation."
    },
    {
      "id": "c1617ef2-17b0-4fd8-af31-322e092d2640",
      "adv_source_id": "c1617ef2-17b0-4fd8-af31-322e092d2640",
      "source_id": "88e5c1cd-2edc-4c4d-b72a-63ed078c3c5e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation\" from arxiv.org:",
      "generation": " We present TernausNet, a deep learning model for image segmentation based on U-Net architecture with a VGG11 encoder pre-trained on ImageNet. In this work, we adapt the U-Net model to incorporate a pre-trained encoder in order to leverage the power of large pre-trained models. We show that by using a pre-trained encoder, our model outperforms the original U-Net in terms of segmentation performance on the Cityscapes dataset. Our model is able to achieve a mean accuracy of 92.5% on the Cityscapes test set, compared to the U-Net model, which achieves a mean accuracy of 89.6%."
    },
    {
      "id": "8a13c310-c191-4bc7-85fe-f087dba35a05",
      "adv_source_id": "8a13c310-c191-4bc7-85fe-f087dba35a05",
      "source_id": "00664552-af87-4295-a1bc-9149639bbe92",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions\" from arxiv.org:",
      "generation": " We present a new approach to semantic segmentation, a process that aims to assign semantic labels to each pixel in an input image. Our method is based on a convolutional neural network (CNN) with multiple soft cost functions. These cost functions are designed to encourage consistent predictions across neighboring pixels, as this is often a strong cue for object boundaries in images. Furthermore, we introduce a new type of layer, which we term the \"highly fused convolutional layer,\" that combines the responses of multiple filters in a single layer. This has the effect of making our network more resilient to noise and other sources of error. We evaluate our method on the public Cityscapes dataset and achieve a competitive result, with a mean accuracy of 82.1%."
    },
    {
      "id": "17aafb03-ccc7-4ada-99d7-6ec03d5df486",
      "adv_source_id": "17aafb03-ccc7-4ada-99d7-6ec03d5df486",
      "source_id": "00664552-af87-4295-a1bc-9149639bbe92",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions\" from arxiv.org:",
      "generation": " Semantic segmentation is a fundamental task in computer vision for scene understanding. In this paper, we propose a new framework to achieve accurate and robust semantic segmentation via a highly fused convolutional network. Unlike most existing works that use a single soft cost function to measure the similarity between the predicted and ground truth labels, we propose to use multiple soft cost functions to encode various prior knowledge for accurate segmentation. Specifically, we introduce three soft cost functions: similarity in feature space, spatial consistency, and appearance consistency. These functions are seamlessly integrated into a unified framework to improve the segmentation performance. We optimize the parameters of our framework in a supervised manner via an efficient online learning algorithm, which can converge reliably and fast. We validate our method on the NYU Depth V2 dataset and compare it with other state-of-the-art methods. The experimental results demonstrate the effectiveness of the proposed approach."
    },
    {
      "id": "9a617e68-8d16-4f7d-aea5-d41d05976dcb",
      "adv_source_id": "9a617e68-8d16-4f7d-aea5-d41d05976dcb",
      "source_id": "7980f1b3-1665-4905-a105-2a48c4204a67",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection\" from arxiv.org:",
      "generation": " Semantic image segmentation has gained increasing attention, as it alleviates the manual effort required for pixel-level labeling by automatically assigning a class label to each pixel in an image. Most existing works focus on accurately classifying pixels within the interior of objects, while the boundary pixels are often treated as a derivative of the interior pixels, resulting in suboptimal performance around object boundaries. In this paper, we argue that the boundary regions carry significant information that can be exploited to improve the segmentation performance. To this end, we propose a novel approach called Boundary-informed Classification (Boa), which explicitly models the boundary regions to capture the additional information. Boa first detects boundary pixels using a simple yet effective boundary detector, and then utilizes a boundary-informed loss function to enhance the classification of boundary pixels during training. In the inference stage, we develop an adaptive inference mechanism that can effectively utilize the boundary information for accurate segmentation. When extensively evaluated on four semantic image segmentation benchmarks, Boa achieves promising results, outperforming the state-of-the-art approaches, especially in terms of boundary accuracy."
    },
    {
      "id": "7a42982e-c183-4c9c-872b-713c5ad74a02",
      "adv_source_id": "7a42982e-c183-4c9c-872b-713c5ad74a02",
      "source_id": "7980f1b3-1665-4905-a105-2a48c4204a67",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection\" from arxiv.org:",
      "generation": " Semantic image segmentation has gained increasing attention, as it partitions an image into different object regions, which could assist subsequent understanding. Most existing works focus on accurately localizing the main object regions, while boundaries, which encode the object contours, are often ignored. In this work, we propose a simple yet effective approach to improving semantic image segmentation by explicitly detecting object boundaries. We find that the detected boundaries could provide additional information for pixel-level classification, and design an edge-oriented loss to train a more competent classifier that is able to reason the object boundaries. With the help of the proposed boundary detection module, our approach could effectively enhance the segmentation performance, especially in margin regions where errors mostly reside. We validate our approach on Cityscapes and COCO-stuff datasets, and achieve promising results compared to the state-of-the-art methods."
    },
    {
      "id": "fd553867-654a-4bc3-8f7e-7086cfd66b72",
      "adv_source_id": "fd553867-654a-4bc3-8f7e-7086cfd66b72",
      "source_id": "a766a7e6-c1bd-4449-8d5a-58a596462371",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient Evolutionary Based Method For Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Efficient Evolutionary Based Method For Image Segmentation\" from arxiv.org:",
      "generation": " An Efficient Evolutionary Based Method for Image Segmentation\n\nImage processing is the technique of converting an image into digital form and performing\nseveral techniques on it to extract information, alter the image, or extract information from it.\nIt is widely used in many applications to extract the features of an image. Image\nsegmentation is one of the most important techniques of image processing. It is used\nto break down an image into its constituent parts, or segments, which helps in\nunderstanding the image better. It is widely used in medical imaging, satellite\nimaging, and image editing. We use an efficient evolutionary-based method for\nimage segmentation. The proposed method initially starts with an initial\npopulation of segments. These segments are generated by dividing the image into\nequal parts. We then use a genetic algorithm to evolve the segments. The fitness\nfunction used for the evolution is the variance of the pixels in each segment. The\nvariance is calculated by comparing the red, green, and blue values of the pixels\nin each segment. The proposed method is tested on several images, and the results\nare compared with the results of other methods. The results show that the proposed\nmethod is efficient and effective in image segmentation.\n\nThe proposed method is innovative in the sense that it uses a genetic algorithm for\nimage segmentation. The use of genetic algorithms for image processing is not\nnew, but using them for image segmentation is innovative. The proposed method is\nalso efficient in terms of the time and space complexity. It is faster and requires\nless memory than other methods. The proposed method is also able to segment\nimages with complex shapes and structures."
    },
    {
      "id": "7854bd17-d054-46f9-b55b-55c26f393a5c",
      "adv_source_id": "7854bd17-d054-46f9-b55b-55c26f393a5c",
      "source_id": "a766a7e6-c1bd-4449-8d5a-58a596462371",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient Evolutionary Based Method For Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An Efficient Evolutionary Based Method For Image Segmentation\" from arxiv.org:",
      "generation": " An Efficient Evolutionary Based Method for Image Segmentation simultaneously optimizes millions of pixels in a large number of images, subject to a global constraint that enforces smooth boundaries between segments. The method can successfully segment image regions shaped by a wide variety of smooth and complex boundaries that are difficult to segment using existing methods. We achieve these results by evolving images using an efficient serial algorithm in which pixels evolve independently, but their evolution is guided by a potential function that enforces both segment homogeneity and smooth segment boundaries. In particular, our method can successfully evolve the binary masks for use in image segmentation; for each pixel, it decides to which segment it belongs. We evaluate the performance of the algorithm on several benchmark image segmentation problems from the synthetic SARVISE dataset and show that our method produces accurately segmented images that are as good as or better than those produced by interactive segmentation methods, and which are superior to images produced by existing non-interactive methods. Our method runs in linear CPU time and is several orders of magnitude faster than other evolutionary algorithms for this problem."
    },
    {
      "id": "9978f9ad-55ba-463f-a1aa-ed3b19966875",
      "adv_source_id": "9978f9ad-55ba-463f-a1aa-ed3b19966875",
      "source_id": "9596d503-ffc7-4359-ad4a-48aad4a13cdb",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks\" from arxiv.org:",
      "generation": " In this work, we aim to segment splenomegaly in ultrasound images, a segmentation task that has not been explored in the literature. We propose to use global convolutional kernels, which we name splenomegaly kernels (SMKs), that are designed to capture global appearance patterns of splenomegaly in ultrasound images. We also propose to use conditional generative adversarial networks (cGANS) to generate synthetic ultrasound images from which we can extract additional SMKs that capture a wider variety of appearance patterns. We show that our approach achieves a high segmentation performance, even when the number of training examples is relatively low."
    },
    {
      "id": "0ab012ac-0639-4e67-8b49-649bc0d54797",
      "adv_source_id": "0ab012ac-0639-4e67-8b49-649bc0d54797",
      "source_id": "9596d503-ffc7-4359-ad4a-48aad4a13cdb",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks\" from arxiv.org:",
      "generation": " In this work, we present an end-to-end trainable convolutional neural network that accurately detects and segments magnetic resonance imaging (MRI) based splenomegaly, which is a pronounced enlargement of the spleen. We use a pre-trained VGG16 model to extract a multi-scale splenomegaly descriptor and then use a conditional generative adversarial network (CGAN) to refine and classify the descriptor for final segmentation. Our experiments showed that our method produced segmentation results with higher quantitative accuracy and better visualization quality compared to the state-of-the-art FARSIGHT system on the Validation and Test sets of the 2018 ISBI Splenomegaly Segmentation Challenge."
    },
    {
      "id": "e176c865-da7d-4dd7-ad0b-551893a39aad",
      "adv_source_id": "e176c865-da7d-4dd7-ad0b-551893a39aad",
      "source_id": "60b73c57-6333-44c1-8fcf-eb3f65ffa16d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method\" from arxiv.org:",
      "generation": " Color image segmentation has always been an active research topic due to its wide applications in image processing area. In this paper, we present a color image segmentation method based on seeded region growing and merging technique. Specifically, we propose to use square elemental regions as the basic unit to extract the region information. In the proposed method, the color image first undergoes the Gaussian smoothing operation, and the smoothing result is followed by the binary thresholding process to produce the initial segmentation. Then, the seeded region growing and merging technique is applied to the binary image sequentially to produce the final segmentation result. In the proposed method, the region growing process is performed within each square elemental region, and the region merging process is performed among the adjacent square elemental regions. Therefore, the proposed method is named as square elemental region-based seeded region growing and merging method. Compared with the other seeded region growing and merging methods that use the pixel as the basic unit, the proposed method can achieve more satisfactory segmentation results. Experiments on a number of color images, including the Berkeley segmentation dataset, show the promising performance of the proposed method."
    },
    {
      "id": "7011697f-b9d9-44fa-80e2-0e494040ae1e",
      "adv_source_id": "7011697f-b9d9-44fa-80e2-0e494040ae1e",
      "source_id": "60b73c57-6333-44c1-8fcf-eb3f65ffa16d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method\" from arxiv.org:",
      "generation": " Color image segmentation has always been an active research topic due to its wide applications in image processing area. In this paper, we present a color image segmentation method based on a combination of a seeded region growing and merging method, which integrates the advantages of both approaches. In the proposed method, we first utilize a square elemental region (SER) to extract the regions with coherent color information, and then use a seeded region growing and merging method to achieve the final segmentation. In the experimental part, the performance of the proposed method is evaluated on Yale B database and it is compared with other seven state-of-the-art segmentation methods. The experimental results demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "25a8a4bc-fe38-4cf6-b182-fabb99635db9",
      "adv_source_id": "25a8a4bc-fe38-4cf6-b182-fabb99635db9",
      "source_id": "c568b816-b63e-4969-9a57-99d4c072e0b6",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "WAYLA - Generating Images from Eye Movements",
      "prompt": "The following is the full text of the abstract for a research paper titled \"WAYLA - Generating Images from Eye Movements\" from arxiv.org:",
      "generation": " We present a novel eye-based interaction technique for image editing. Given a rough sketch as input, our system uses gaze-tracking to collect a user's eye movements. These eye movements are then used to iteratively refine a generated image using a conditional generative adversarial network (CGAN). We demonstrate that our approach achieves state-of-the-art performance in terms of image quality, and provides a fun and intuitive interaction for image creation. In a user study, we show that our approach is preferred by users compared to alternative methods."
    },
    {
      "id": "222a87ce-d582-4632-821e-77dd3ab31f46",
      "adv_source_id": "222a87ce-d582-4632-821e-77dd3ab31f46",
      "source_id": "c568b816-b63e-4969-9a57-99d4c072e0b6",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "WAYLA - Generating Images from Eye Movements",
      "prompt": "The following is the full text of the abstract for a research paper titled \"WAYLA - Generating Images from Eye Movements\" from arxiv.org:",
      "generation": " Recent works have shown that eye movements contain rich information about the visual content of a scene. However, existing pose-based eye movement prediction models only utilize the presence or absence of an object in an image, disregarding the object's appearance, which limits their generalizability across different object instances. In this paper, we introduce a novel approach to synthesize images conditioned on eye movements by introducing a latent conditioned variation network (LCVN). Our model learns a joint distribution of images and eye movements, such that the posterior of the image given the eye movement is proportional to the joint distribution. We further present a new large-scale eye movement dataset, WAYLA, which provides ground truth images, eye movements, and bounding box annotations for 30,475 images. Experiments show that our model outperforms the state-of-the-art in predicting eye movements as well as generating images from eye movements. Codes are available at https://github.com/linzh optional/WAYLA."
    },
    {
      "id": "bf3e3ed7-e969-41e0-97ff-5942e7d29c15",
      "adv_source_id": "bf3e3ed7-e969-41e0-97ff-5942e7d29c15",
      "source_id": "5394c7c4-535b-4f88-9fdb-c89387b8b5db",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes\" from arxiv.org:",
      "generation": " Medical image segmentation plays a crucial role in medical image analysis. In this study, we focus on the segmentation of brain MRI (Magnetic Resonance Imaging) images and propose a method based on multi-modal convolutional neural network (MCNN). The proposed method combines T1-weighted, T2-weighted and fluid-attenuated inversion recovery (FLAIR) images for segmentation, using three different image fusion schemes: early fusion, late fusion and multi-scale late fusion. The objective is to investigate the effect of image fusion scheme on the performance of MCNN. Our results show that the multi-scale late fusion scheme provides the best segmentation performance in terms of dice similarity coefficient and intersection over union."
    },
    {
      "id": "bcce00ad-ab35-4715-a457-16cbae0c6aae",
      "adv_source_id": "bcce00ad-ab35-4715-a457-16cbae0c6aae",
      "source_id": "5394c7c4-535b-4f88-9fdb-c89387b8b5db",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes\" from arxiv.org:",
      "generation": " Medical image segmentation plays a crucial role in medical image analysis. In this paper, we focus on the segmentation of anatomical structures in sinus imaging. We propose a method based on a multi-modal convolutional neural network (MCNN) that combines information from multiple imaging modalities, namely computed tomography (CT) and magnetic resonance imaging (MRI). Our approach is designed to take advantage of the complementary information provided by these modalities, improving the segmentation accuracy compared to single-modal methods. We evaluate our method on a public dataset and compare it to state-of-the-art segmentation methods. Our results show that the proposed MCNN approach significantly improves segmentation performance, demonstrating the effectiveness of multi-modal fusion for medical image segmentation tasks."
    },
    {
      "id": "297e55cf-f637-4241-bed1-f72788b149b4",
      "adv_source_id": "297e55cf-f637-4241-bed1-f72788b149b4",
      "source_id": "8c0e30f7-a6f7-4360-908b-b484f52814ec",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks\" from arxiv.org:",
      "generation": " Recent progress in historical document image segmentation is reviewed. Then, a new approach is proposed which combines hierarchical segmentation with unsupervised learning. A deep neural network is initialized with a linear discriminant analysis classifier trained on manually labeled pixels that are easily separable. An image is segmented by iteratively labeling the most confident regions in a bottom-up fashion, starting from the end of the hierarchy, where the regions are smallest. The proposed method is evaluated on the Public Domain Data Set (PDDS) and the authors' dataset of historical Chinese documents, and compared with four recent state-of-the-art methods on the PDDS. Results show that the proposed method performs favorably against the other methods in terms of segmentation quality and speed."
    },
    {
      "id": "23e2b0c1-fa4d-423c-bc0b-81d626ce4749",
      "adv_source_id": "23e2b0c1-fa4d-423c-bc0b-81d626ce4749",
      "source_id": "8c0e30f7-a6f7-4360-908b-b484f52814ec",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks\" from arxiv.org:",
      "generation": " Historical document image segmentation is a fundamental step for content-based retrieval and understanding of historical documents. In this paper, we propose an approach that combines the strength of both shallow and deep features for accurate segmentation. Our approach initially segments the image using shallow models like variance and HLDT. These initial segments are then used to initialize deep neural network models, such as AlexNet and GoogleNet, for further segmentation. We employ the Latent Dirichlet Allocation (LDA) topic modeling technique to obtain the main themes from a set of training documents. The themes are then used to select the initial training images for the deep neural network models. Our approach was evaluated on the Public Domain Smithsonian Image Collection (PDSI). Without any fine-tuning of the deep neural network models, our approach achieved a RMSE of 0.266225 on the test set, outperforming the previous state-of-the-art methods."
    },
    {
      "id": "bc0c1808-be2f-44e7-8aec-1a595e226c18",
      "adv_source_id": "bc0c1808-be2f-44e7-8aec-1a595e226c18",
      "source_id": "b92ae04c-342d-4221-8ffc-cf81dae44f5b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm\" from arxiv.org:",
      "generation": " In this paper, we propose a novel FPGA based parallelized architecture of an efficient graph based image segmentation algorithm. The proposed algorithm is based on a hierarchical graph representation of the image, where the nodes of the graph represent the pixels of the image and the edges of the graph represent the similarities between the pixels. The algorithm consists of two main steps: a node classification step and an edge weighting step. The node classification step is used to classify the pixels of the image into different classes, based on the similarities between the pixels. The edge weighting step is used to assign weights to the edges of the graph, based on the similarities between the pixels and the class of the pixels. The proposed algorithm is implemented on an FPGA platform, which provides a high-performance and low-power implementation. The algorithm is parallelized to take advantage of the parallel processing capabilities of the FPGA. The experimental results show that the proposed algorithm achieves a high segmentation quality and a fast processing speed.\n\n```abstract\nDue to the ever increasing size of digital images, there is a need for efficient and fast image segmentation algorithms. In this paper we present a novel parallelized architecture for segmentation of large grayscale images using graph based segmentation techniques. The proposed architecture is based on a hierarchical graph representation of the image, where the nodes of the graph represent the pixels of the image and the edges of the graph represent the similarities between the pixels. The algorithm consists of two main steps: a node classification step and an edge weighting step. The node classification step is used to classify the pixels of the image into different segments, based on the similarities between the pixels. The edge weighting step is used to assign weights to the edges of the graph, based on the similarities between the pixels and the segment that the pixels belong to. The proposed architecture is designed to be implemented on a field programmable gate array (FPGA) platform, which can provide a high performance and low power implementation. The architecture is parallelized to take advantage of the parallel processing capabilities of the FPGA. We have implemented the proposed architecture on an FPGA development board and the experimental results show that the proposed algorithm can achieve a high segmentation quality and a fast processing speed.\n```"
    },
    {
      "id": "8c3c4207-83a0-4967-9c26-7146fa36473e",
      "adv_source_id": "8c3c4207-83a0-4967-9c26-7146fa36473e",
      "source_id": "b92ae04c-342d-4221-8ffc-cf81dae44f5b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm",
      "prompt": "The following is the full text of the abstract for a research paper titled \"FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm\" from arxiv.org:",
      "generation": " In this paper, we propose a novel FPGA based parallelized architecture of an efficient graph based image segmentation algorithm. The proposed algorithm is efficient in terms of both computation and memory usage. It is based on a graph cut algorithm which uses a boundary based approach to image segmentation. The algorithm is implemented on a FPGA platform using the parallelized architecture. The architecture is designed to take advantage of the FPGA's parallel processing capabilities. The algorithm is divided into several parallelized stages, with each stage processing a subset of the image data in parallel. The memory usage is also optimized by using a compressed representation of the image data. The proposed architecture is evaluated on a variety of images, and the results show that it is capable of achieving real-time processing speed with high accuracy.\n\nThe efficiency of the proposed algorithm comes from the combination of a graph cut algorithm and a boundary based approach. The graph cut algorithm provides an efficient way to compute the segmentation, while the boundary based approach ensures that the resulting segmentation is accurate and coherent. The proposed algorithm is also designed to be parallelized, which makes it possible to run on a FPGA platform. The parallelized architecture is designed to take advantage of the FPGA's parallel processing capabilities, which allows for real-time processing speed.\n\nThe proposed architecture is evaluated on a variety of images, and the results show that it is capable of achieving real-time processing speed with high accuracy. The evaluation results show that the proposed architecture is efficient in terms of both computation and memory usage, and is capable of achieving real-time processing speed with high accuracy.\n\nThe paper also discusses the possibility of using the proposed architecture for other image processing applications, such as object detection and tracking. The proposed architecture is flexible and can be adapted to different image processing tasks."
    },
    {
      "id": "1deb513f-ef45-42cb-bc5c-eb45fcbba281",
      "adv_source_id": "1deb513f-ef45-42cb-bc5c-eb45fcbba281",
      "source_id": "823822b8-9b81-4fd1-bdd1-0246093e110f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation\" from arxiv.org:",
      "generation": " DeepIGeoS is a deep learning framework for medical image segmentation, which introduces an interactive geodesic method based on the geodesic active contour without edges (GAC-WE) model. Unlike the original GAC-WE model, which optimizes the contour directly, our method optimizes the geodesic distance map instead, allowing for more intuitive user interactions. We employ a deep neural network to learn the geodesic distance map from the image, which can be trained in an unsupervised manner. During the segmentation process, the user can freely adjust the contour by editing the geodesic distance map, and the network will update the map according to the user's input. Our method has been evaluated on the MIT-BIH Arterial Tree andLiver CT images. Compared to the original GAC-WE model, our method achieved a similar performance with significantly fewer parameters and without any edge detection modules."
    },
    {
      "id": "fa528697-0807-4f16-ba00-96d148a850df",
      "adv_source_id": "fa528697-0807-4f16-ba00-96d148a850df",
      "source_id": "823822b8-9b81-4fd1-bdd1-0246093e110f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation\" from arxiv.org:",
      "generation": " DeepIGeoS is a deep learning framework for interactively segmenting anatomical structures in medical images. It leverages a geodesic-based approach to allow for intuitive and geodesically-constrained segmentation, which facilitates exploring and editing the segmentation results within the space of the considered anatomical structure. Our framework consists of a standalone geodesic distance module and a deep neural network (DNN) module for segmentation. We employ a supervised learning strategy, training the DNN on a large dataset of manually annotated images. The segmentation network takes as input the distance map provided by the geodesic module and returns a probability map over the structures of interest. We introduce a loss function that combines the Dice similarity coefficient with the Wasserstein geodesic distance to enforce agreement between the predicted segmentation and the ground truth segmentation while favoring predictions that are close in the geodesic space. We demonstrate the efficacy of our approach on three different datasets of abdominal CT scans and lung CT scans, and compare it with state-of-the-art methods on the public ISLES 2017 challenge dataset."
    },
    {
      "id": "9a77cbbc-2d31-4db6-a692-87d601e7bcdd",
      "adv_source_id": "9a77cbbc-2d31-4db6-a692-87d601e7bcdd",
      "source_id": "c0009c67-d594-4da3-af81-a06984e70ab5",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "4D Multi-atlas Label Fusion using Longitudinal Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"4D Multi-atlas Label Fusion using Longitudinal Images\" from arxiv.org:",
      "generation": " Recent studies in the analysis of neonatal brain MRI datasets have demonstrated the potential of using 4D atlases for labeling cerebrovasculature, however, label fusion across time-points in longitudinal MR studies has not been addressed so far. In this work, we present a label fusion approach to combine the label information from a 4D atlas across multiple time-points based on a previously proposed 4D atlas construction pipeline. In the context of a neonatal cerebrovascular study, we demonstrate the efficacy of our approach on a real-world dataset, showing that our method can yield more accurate labeling results compared to state-of-the-art approaches."
    },
    {
      "id": "32cd73e6-576e-4d25-b92d-aa9836d65053",
      "adv_source_id": "32cd73e6-576e-4d25-b92d-aa9836d65053",
      "source_id": "c0009c67-d594-4da3-af81-a06984e70ab5",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "4D Multi-atlas Label Fusion using Longitudinal Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"4D Multi-atlas Label Fusion using Longitudinal Images\" from arxiv.org:",
      "generation": " Recent studies on label fusion for segmentation of longitudinal images using atlases produce accurate results only in the registered atlas regions. Therefore, a missing label in an atlas can lead to a missing label in the fused output, making it difficult to reliably segment unseen structures. Additionally, the above methods produce a single fused label map for the entire longitudinal stack, making it difficult to adapt to the potentially varying appearances of structures across time. In this paper, we address the above issues by adapting label fusion to handle incomplete atlases and producing a separate fused output for each longitudinal image. We cast the label fusion problem as a sparse reconstruction problem on a 4-dimensional (4D) grid of label probabilities, with an explicit 4D dictionary learning step. We utilize the longitudinal nature of our data by exploiting the high structural similarity across nearby temporal slices to build a more complete fused output. We evaluate our method on the MRI Brain Imaging Archive and achieve state-of-the-art performance on three different longitudinal segmentation tasks."
    },
    {
      "id": "5cb71b59-3ae6-4e29-a74f-b9220873eaea",
      "adv_source_id": "5cb71b59-3ae6-4e29-a74f-b9220873eaea",
      "source_id": "7184b7e7-e328-4ad1-acd3-593389fce265",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation by Iterative Inference from Conditional Score Estimation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Image Segmentation by Iterative Inference from Conditional Score Estimation\" from arxiv.org:",
      "generation": " Recent progress in conditional score estimation (CSE) has enabled state-of-the-art performance on a variety of computer vision tasks. However, existing CSE methods are mainly designed for segmenting objects in images with simple shapes or with well-defined boundaries, and they often struggle to accurately segment objects with complex shapes or object instances with poor boundary contrast. In this paper, we propose a new approach to image segmentation based on iterative inference from CSE. Our approach iteratively refines the segmentation results by simultaneously estimating object boundaries and the conditional scores. Specifically, given initial guesses for object boundaries and conditional scores, we iteratively update them by minimizing a joint energy function that measures the dissimilarity between the predicted and ground truth segmentations. We also introduce an effective boundary refinement module that selectively refines object boundaries based on the confidence of the predicted boundaries. Our approach can handle instances with complex shapes and/or poor boundary contrast, and is efficient and easy to optimize. We demonstrate the effectiveness of our approach on the PASCAL VOC 2012 segmentation benchmark, where our method achieves competitive performance to the state-of-the-art methods while being much faster to optimize."
    },
    {
      "id": "5540f9c0-4b8a-42fb-a0ff-81816a28dd39",
      "adv_source_id": "5540f9c0-4b8a-42fb-a0ff-81816a28dd39",
      "source_id": "7184b7e7-e328-4ad1-acd3-593389fce265",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation by Iterative Inference from Conditional Score Estimation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Image Segmentation by Iterative Inference from Conditional Score Estimation\" from arxiv.org:",
      "generation": " Recent progress in conditional score estimation (CSE) has enabled state-of-the-art performance on image segmentation. However, existing methods rely on nested loops for inference, making them computationally expensive and difficult to parallelize. We address these issues by developing an efficient two-stage inference algorithm, which is based on the observation that for a fixed label configuration, CSE objective is convex with respect to the pixel intensities. Specifically, given the current labeling, we first estimate a parametric family of labelings that can achieve the global optimal objective value. Then, we perform a conditional inference on the estimated labelings to obtain the final segmentation. This two-stage algorithm simplifies the optimization procedure, and enables a parallel implementation. We derive an efficient approximation for the proposed algorithm, and demonstrate its efficacy on semantic segmentation. In particular, our algorithm runs at 36 fps on semantic segmentation benchmark Cityscapes with a single commodity GPU, which is 2-4\u00d7 faster than the state-of-the-art methods while maintaining comparable performance."
    },
    {
      "id": "0f0d388f-6584-4a4c-93a3-27d72d7cb3ea",
      "adv_source_id": "0f0d388f-6584-4a4c-93a3-27d72d7cb3ea",
      "source_id": "8a74b221-8794-4a05-b598-b856f58d6f5e",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection\" from arxiv.org:",
      "generation": " Recent progress in artificial intelligence has enabled the automation of various agriculture-related tasks, such as crop and weed detection. However, creating a large and diverse dataset for training deep learning models can be a time-consuming and laborious process, which can hinder the development of accurate and robust models. In this paper, we present a data generation framework that automatically creates a large and diverse dataset for training deep learning models to detect crops and weeds. Our framework utilizes a combination of image segmentation, edge detection, and geometric projection algorithms to generate synthetic training data that closely resembles real-world images. Furthermore, our framework can rapidly create datasets of various sizes and crop types, making it suitable for a wide range of applications. We demonstrate the effectiveness of our approach through extensive experiments, showing that our framework can significantly improve the accuracy and robustness of deep learning models for crop and weed detection."
    },
    {
      "id": "16d6ce08-ca44-405a-b42b-7e8fb57a4068",
      "adv_source_id": "16d6ce08-ca44-405a-b42b-7e8fb57a4068",
      "source_id": "8a74b221-8794-4a05-b598-b856f58d6f5e",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection\" from arxiv.org:",
      "generation": " Recent progress in machine learning and computer vision has enabled the development of automated techniques for various agricultural applications, such as crop and weed detection. However, these techniques often require large amounts of manually labeled training data, which can be time-consuming and expensive to acquire. In this paper, we present an approach for automatically generating a large and diverse dataset for crop and weed detection using a combination of satellite imagery and labeled example images. Our approach involves using a deep learning based model to synthesize new training samples by transforming existing samples using a combination of semantic and instance mapping. We show that our approach can generate a large and diverse dataset for crop and weed detection, and that the use of this dataset can improve the accuracy and speed of deep learning based models for crop and weed detection.\n\nField cropping is one of the major contributors to the global food production. Weeds are the major inhibitors for the growth of the crops, and manual weeding is a common activity farmers perform to remove unwanted weeds. It is a tedious and time consuming activity, and depending on the geographic location, it can be accomplished using human labor or animals (e.g., mules). Automated weed detection can assist farmers in maintaining their fields and increasing their productivity. However, creating training datasets for training machine learning models for automatic weed detection is a challenging task. It requires a lot of manual work, which is time consuming, tedious, and expensive. In this work, we present a data generation technique to create a large training dataset for weed detection. Our technique uses an iterative approach of generating training examples by transforming existing examples using a combination of semantic and instance mapping. Our technique can generate a large number of diverse training examples for training deep learning based models for accurate and fast crop and weed detection."
    },
    {
      "id": "05d4c6d8-3f3d-494c-ab04-d4aa9daf8d9a",
      "adv_source_id": "05d4c6d8-3f3d-494c-ab04-d4aa9daf8d9a",
      "source_id": "48cae5f6-0109-460d-9e7f-1ca679052e18",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Locally Adapting Technique for Boundary Detection using Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Locally Adapting Technique for Boundary Detection using Image Segmentation\" from arxiv.org:",
      "generation": " In this paper, a new boundary detection technique is proposed which adapts locally to the image noise and the object boundary regions in the image. It is based on an iterative algorithm the convergence of which is guaranteed. In each iteration, the algorithm updates the estimate of the boundary by pooling the information from the gradient magnitude and the gradient direction obtained from the image segmentation mask. Since the proposed technique is capable of local adaptation, the performance of the algorithm degrades slowly in the presence of noise compared to other existing techniques and the performance is significantly better than those techniques in low-contrast images. The proposed technique does not require any parameters to be tuned and therefore makes it easy to use in any type of application. Finally, the performance of the proposed technique is compared to other existing techniques using various evaluation metrics. The experimental results demonstrate the superiority of the proposed technique."
    },
    {
      "id": "ef19d607-2ca9-48da-8405-6465d607b8b1",
      "adv_source_id": "ef19d607-2ca9-48da-8405-6465d607b8b1",
      "source_id": "48cae5f6-0109-460d-9e7f-1ca679052e18",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Locally Adapting Technique for Boundary Detection using Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Locally Adapting Technique for Boundary Detection using Image Segmentation\" from arxiv.org:",
      "generation": " A technique for boundary detection in image processing is presented in this paper that adapts to local image structure. This technique, called the locally adapting technique (LAT), uses image segmentation to compute a boundary map. No prior knowledge of the image characteristics, such as smoothness or texture, is required. Therefore, the LAT has the ability to adapt to various types of images. The boundary map represents the image regions and is obtained by recursively segmenting the image using a threshold. To implement the LAT, we modify the watershed transformation to produce a hierarchical segmentation, and then we thin the resulting segmentation to obtain the boundary map. Compared with other techniques, the proposed approach is able to preserve boundaries in relatively noise-free regions and to detect boundaries in relatively noise-intensive regions. The proposed technique is also computationally efficient. This paper includes a performance comparison of several boundary detection techniques using both synthetic and real images. The experimental results demonstrate the effectiveness of the proposed technique."
    },
    {
      "id": "d74c4e0d-3b07-4a04-bd26-2600feb0a588",
      "adv_source_id": "d74c4e0d-3b07-4a04-bd26-2600feb0a588",
      "source_id": "71653c06-9860-47b9-9528-c636e6e14767",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Aerial Image Segmentation from Online Maps",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning Aerial Image Segmentation from Online Maps\" from arxiv.org:",
      "generation": " Deep learning has achieved great success in image segmentation, however, most existing works focus on the indoor scene which usually has a large number of training images. For the aerial image segmentation, the collection of a large amount of training data is costly and time-consuming, and the diversity of scenes is another challenge. Online maps provide a new perspective to acquire training data. In this paper, we propose a learning approach for aerial image segmentation from online maps, which has the advantages of low cost and fast training speed. We first extract the building footprint and road vector information from online maps, and then generate labeled images for training. Finally, we design a deep neural network to learn the segmentation model from the generated images. The experimental results on two public-available datasets show the effectiveness of our approach."
    },
    {
      "id": "c72ae2c8-7738-4301-9e06-3349eaefc76a",
      "adv_source_id": "c72ae2c8-7738-4301-9e06-3349eaefc76a",
      "source_id": "71653c06-9860-47b9-9528-c636e6e14767",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Aerial Image Segmentation from Online Maps",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning Aerial Image Segmentation from Online Maps\" from arxiv.org:",
      "generation": " Deep learning has achieved great success in image segmentation, however, existing works mainly focus on classifying pixels on the ground. In this paper, we aim to learn a segmentation model in the air, which is a typical scenario for UAV (Unmanned Aerial Vehicle) applications. We manually annotate 10,000 aerial images to build a dataset for training a deep neural network. Our model takes an aerial image as input and directly produces a binary segmentation mask where the foreground corresponds to buildings and the background represents the sky. We adopt a shallow fully convolutional network pre-trained on the imageNet dataset and fine-tune it on our own dataset. The proposed model is fast in testing and can run in real-time on a typical CPU. Furthermore, a systematic evaluation of several basic deep architectures is conducted to provide suggestions for choosing the network architecture in practical applications. Our model generally achieves promising segmentation performance and obtains a mean IOU of 88.15% on the testing set."
    },
    {
      "id": "c5b558d8-f9cf-48b7-bbac-f5a768041cc2",
      "adv_source_id": "c5b558d8-f9cf-48b7-bbac-f5a768041cc2",
      "source_id": "2180a38c-3528-420b-b712-f2cb27a3d6fa",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task",
      "prompt": "The following is the full text of the abstract for a research paper titled \"On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task\" from arxiv.org:",
      "generation": " We introduce a new architecture for 3D convolutional neural networks (CNNs) that aims to improve the representation power of 3D CNNs while maintaining a compact structure and efficient training. Our key idea is to factorize the 3D convolutional layer into a stack of 2D convolutional layers, allowing the network to utilize the powerful 2D CNN representations while retaining the 3D structure for handling longitudinal information. We show that this factorization leads to a compact and efficient network that achieves better performance on a variety of 3D CNN benchmarks. In addition, we demonstrate the effectiveness of our architecture on the challenging task of brain parcellation, where we achieve state-of-the-art performance.\n\nOur 3D CNN architecture consists of a stack of 2D CNNs, where each 2D CNN processes a slice of the input volume. This enables the network to utilize the powerful 2D CNN representations while retaining the 3D structure for handling longitudinal information. The 2D CNNs are connected via a 3D pooling layer that aggregates the representations from the 2D CNNs. We introduce a new 3D pooling layer that performs max-pooling along the third dimension, reducing the spatial dimensions of the representations while retaining the important information.\n\nWe evaluate our architecture on the challenging task of brain parcellation, where the goal is to segment the brain into different regions. We achieve state-of-the-art performance on the public Brain Parcellation challenge 2017 dataset, outperforming the previous state-of-the-art by a large margin. Our network is able to learn robust representations that capture the structural and anatomical properties of the brain, despite the limited amount of training data.\n\nOur work demonstrates the effectiveness of the proposed architecture in improving the representation power of 3D CNNs while maintaining a compact structure and efficient training. We believe that our architecture will pave the way for more powerful and efficient 3D CNNs that can tackle a wide range of 3D data processing tasks."
    },
    {
      "id": "3c17ced1-7e27-44ea-b651-44ada673ab31",
      "adv_source_id": "3c17ced1-7e27-44ea-b651-44ada673ab31",
      "source_id": "2180a38c-3528-420b-b712-f2cb27a3d6fa",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task",
      "prompt": "The following is the full text of the abstract for a research paper titled \"On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task\" from arxiv.org:",
      "generation": " We introduce a new architecture for volumetric convolutions that enables an understanding of 3D convolutional networks as multi-layer 2D convnets stacked along the third dimension. This new architecture improves efficiency through a combination of three methods: a new non-linear pooling operation, a subsidized isotropic convolution, and the use of 2D convnets to parametrize the networks. This new architecture performs on par with state-of-the-art methods on the challenging problem of Automatic Brain Parcellation on adult human brains, while using less than half the number of parameters. In a second step, we explore the space of 3D convolutional networks and show that our new architecture is unusually robust with respect to the choice of architecture and hyperparameters, thus providing a solid basis for further research on this topic."
    },
    {
      "id": "5fb71504-b449-4854-9f96-de8b18264057",
      "adv_source_id": "5fb71504-b449-4854-9f96-de8b18264057",
      "source_id": "f780a932-62e7-469a-97e8-c3f2d1ef3e95",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Tversky loss function for image segmentation using 3D fully convolutional deep networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Tversky loss function for image segmentation using 3D fully convolutional deep networks\" from arxiv.org:",
      "generation": " We propose a new loss function for image segmentation using 3D fully convolutional deep networks. Our loss function is based on the Tversky index, which is a measure of perceptual similarity between two images. We show that our loss function is able to improve the performance of state-of-the-art 3D fully convolutional deep networks for image segmentation. We evaluate our method on the Cityscapes and KITTI datasets, and show that our method achieves state-of-the-art performance on both datasets. Our code and models are available at https://github.com/junqiu/TverskyLossFunction."
    },
    {
      "id": "100e7647-54b5-4521-9c93-c7d3f8ea222d",
      "adv_source_id": "100e7647-54b5-4521-9c93-c7d3f8ea222d",
      "source_id": "f780a932-62e7-469a-97e8-c3f2d1ef3e95",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Tversky loss function for image segmentation using 3D fully convolutional deep networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Tversky loss function for image segmentation using 3D fully convolutional deep networks\" from arxiv.org:",
      "generation": " We propose a new loss function for image segmentation using 3D fully convolutional deep networks. Our loss function is based on the Tversky index, which is a measure of uncertainty derived from the theory of evidence. The Tversky index measures the overlap between two probability distributions, and we use it to evaluate the similarity between the predicted probability distribution of labels and the true distribution. We show that our loss function is able to improve the performance of state-of-the-art 3D fully convolutional deep networks on the challenging task of image segmentation. Furthermore, we demonstrate that our loss function is able to extract meaningful features from the data, and we show that it is compatible with different network architectures."
    },
    {
      "id": "a25f2e7d-8724-4569-8183-4bdca02cf89d",
      "adv_source_id": "a25f2e7d-8724-4569-8183-4bdca02cf89d",
      "source_id": "1fbea034-ec8e-4bf2-81b5-3b627f0a9b4f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images\" from arxiv.org:",
      "generation": " We present an efficient graph cuts based technique for segmenting kidneys in ultrasound (US) images. Graph cuts methods have been widely used in image processing and computer vision for image segmentation. However, most existing methods rely on the initial segmentation results to obtain the final segmentation. In this work, we develop an adaptive graph cuts technique that can simultaneously consider the global and local features of the kidneys in US images. It can be used for the initial segmentation or as a stand-alone technique. Furthermore, we integrate multiple feature maps, including edge, regional, and texture information, into the graph cuts framework to achieve more accurate segmentation results. The experimental results on US images obtained from patients with kidney stones indicate that the proposed method is effective in segmenting kidneys and yields promising results."
    },
    {
      "id": "61270574-fac3-4751-ab7f-045c707400a1",
      "adv_source_id": "61270574-fac3-4751-ab7f-045c707400a1",
      "source_id": "1fbea034-ec8e-4bf2-81b5-3b627f0a9b4f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images\" from arxiv.org:",
      "generation": " We present an efficient graph cuts based technique for segmenting kidneys in ultrasound (US) images. The key to the proposed method is a novel dynamic weighting strategy that judiciously integrates multiple feature maps via graph cuts. Feature maps from conventional texture, gradient, and contour detectors are combined to produce a more robust segmentation. Furthermore, a new feature map that exploits the 3D spatio-temporal relationship among the frames in a US video is introduced to improve performance. Finally, a post-processing step is employed to refine the segmentation results. The experimental results on a challenging US image sequence annotated by experts demonstrate the superiority of the proposed method compared to state-of-the-art techniques."
    },
    {
      "id": "f8c4ec4a-9898-4e2f-b3fc-d00547867968",
      "adv_source_id": "f8c4ec4a-9898-4e2f-b3fc-d00547867968",
      "source_id": "43bf4bad-9105-4bdf-bd78-d1ccdfa6dbfc",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
      "prompt": "The following is the full text of the abstract for a research paper titled \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\" from arxiv.org:",
      "generation": " DeepLab is a novel deep convolutional neural network architecture designed for semantic image segmentation. The key component of DeepLab is a densely connected conditional random field (CRF) module that is connected to a deep convolutional neural network (DCNN) in a end-to-end training manner. This CRF module leverages the spatial coherence prior in image segmentation and makes the network focus on hard examples for training. We show that the CRF module can be integrated with the DCNN in an efficient way without deteriorating the convergence property. Our network is trained by minimizing the conditional logistic likelihood of the CRF, which is an alternating optimization process. We further propose an atrous convolution technique to effectively increase the field of view of the convolutional layers and an adaptive convolution kernel size technique to improve the performance in the image boundary. Our network is applied to the PASCAL VOC 2012 segmentation challenge. Without any further tuning, our network outperforms the current state-of-the-art methods for this benchmark by a large margin of +5.7% in terms of pixel accuracy and +4.0% in terms of mean accuracy."
    },
    {
      "id": "2d759f74-4e4e-4a8f-b295-2bc3f4ad93a3",
      "adv_source_id": "2d759f74-4e4e-4a8f-b295-2bc3f4ad93a3",
      "source_id": "43bf4bad-9105-4bdf-bd78-d1ccdfa6dbfc",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
      "prompt": "The following is the full text of the abstract for a research paper titled \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\" from arxiv.org:",
      "generation": " DeepLab is a novel deep convolutional neural network architecture designed for semantic image segmentation, which is a problem of assigning semantic labels to every pixel in an input image. The key feature of DeepLab is the use of atrous convolution, which enables the network to utilize multi-scale information and capture complex context in the image with lightweight kernels. Atrous convolution is combined with a fully connected conditional random field (CRF) layer to refine the segmentation results and improve the accuracy. In addition, the use of a deep convolutional neural network architecture allows the network to learn robust features directly from the raw image data, without the need for explicit feature engineering. The overall architecture is comprised of a bottom-up pathway and a top-down pathway, which allows the network to integrate multi-scale information and refine the segmentation results in a hierarchical manner. The bottom-up pathway extracts low-level features and generates initial segmentation results, while the top-down pathway refines the segmentation results by incorporating high-level features and context information. The fully connected CRF layer is utilized in the top-down pathway to refine the segmentation results based on the learned features and context information.\n\nThe proposed approach was evaluated on the Cityscapes dataset, which is a large-scale dataset for semantic image segmentation consisting of 5,000 labeled images and 19 semantic classes. The network was trained using a cropped and resized version of the dataset, and the evaluation was performed on the full-size images. The proposed approach achieved a mIoU of 79.3% on the validation set, which is a significant improvement over the existing state-of-the-art methods.\n\nThe proposed approach demonstrates the effectiveness of deep convolutional neural networks and atrous convolution for semantic image segmentation, and shows the potential of this technique for applications such as autonomous driving and medical imaging."
    },
    {
      "id": "7d3cc5f3-71fe-41d2-9a8e-674f10ef719c",
      "adv_source_id": "7d3cc5f3-71fe-41d2-9a8e-674f10ef719c",
      "source_id": "771f8e8c-a41a-410b-bb61-2b0e70467c95",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Object Discovery via Cohesion Measurement",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Object Discovery via Cohesion Measurement\" from arxiv.org:",
      "generation": " Recent works have shown that deep neural networks (DNNs) can learn meaningful shape representations. However, existing methods require extensive training and suffer from the dataset imbalance problem, i.e., the scarcity of labeled objects with respect to the background. In this paper, we present a simple yet effective approach for object discovery via cohesion measurement. Given a group of points in the 3D space, our method can automatically discover the objects with coherent structures. We propose a novel cohesion measurement by exploring the intrinsic relationship between points, which captures the object boundaries in a learnable manner. We further develop an efficient algorithm to optimize the cohesion measurement with guaranteed convergence. The experimental results on both synthetic and real datasets demonstrate the effectiveness of the proposed approach for object discovery, outperforming the state-of-the-art methods."
    },
    {
      "id": "b359d9d8-fb06-4f99-904b-cd38046723e2",
      "adv_source_id": "b359d9d8-fb06-4f99-904b-cd38046723e2",
      "source_id": "771f8e8c-a41a-410b-bb61-2b0e70467c95",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Object Discovery via Cohesion Measurement",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Object Discovery via Cohesion Measurement\" from arxiv.org:",
      "generation": " Recent works have shown that deep neural networks (DNNs) can learn meaningful shape representations. However, existing works predominantly focus on part-based detection, and the discovery of holistic objects remains largely unaddressed. In this paper, we aim to fill this gap by casting object discovery as a direct detection problem: given a query shape, can we directly output a binary decision indicating the presence of an object? To this end, we propose a novel Architecture Search framework that automatically finds a direct object discovery architecture. Specifically, our architecture consists of two components: a region proposal network (RPN) that proposes regions and an object discovery network (ODN) that refines the proposals into objects. For the ODN, we propose a new cohesion-based loss that measures the similarity among pixels within an object and contrasts it with the background. Our method can be trained end-to-end and is completely automatic without any extra tuning or human effort. When compared with the state-of-the-art part-based methods on the PASCAL VOC 2007 dataset, our approach achieves a mAP of 68.4%, which demonstrates the effectiveness of our approach."
    },
    {
      "id": "9672c2d9-35f8-4b38-a640-995f3d6b184f",
      "adv_source_id": "9672c2d9-35f8-4b38-a640-995f3d6b184f",
      "source_id": "1afb7c7d-24f7-48d5-9f6a-aa755c67bca9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities\" from arxiv.org:",
      "generation": " Recent advances in deep learning have achieved remarkable performance in medical image segmentation in a single modality. However, in real clinical settings, medical images are often acquired in multiple modalities, each providing different information. In this paper, we present a multi-task learning framework for joint segmentation of anatomical structures in CT and MRI. Our framework learns a shared representation across different modalities from their corresponding annotated image pairs, and is based on a deep architecture that explicitly models anatomical correspondences between the modalities. We evaluate our method on multi-modal segmentation of the prostate, and show that it performs competitively to state-of-the-art single-modality segmentation methods while learning anatomical correspondences across modalities, thus demonstrating the promise of multi-task learning for medical image segmentation in multiple modalities."
    },
    {
      "id": "e975fb6e-3b83-4b75-97d5-d5235f79d44c",
      "adv_source_id": "e975fb6e-3b83-4b75-97d5-d5235f79d44c",
      "source_id": "1afb7c7d-24f7-48d5-9f6a-aa755c67bca9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities\" from arxiv.org:",
      "generation": " Recent advances in deep learning have achieved remarkable performance in medical image segmentation in a single modality. However, in real clinical settings, medical images are often acquired in multiple modalities, such as CT and MRI, which often require different segmentation models. In this paper, we present a novel approach for multi-task learning for medical image segmentation in multiple modalities. Our method leverages the recently proposed deep dual learning architecture, which enables the training of a single segmentation model for multiple modalities while maintaining high performance. We apply our method to the task of brain tumor segmentation in CT and MRI images and demonstrate its effectiveness compared to single-modality segmentation models. Our method achieves an average precision of 92.3% and 91.5% on the CT and MRI segments of the BRATS 2018 challenge, respectively, and achieves the best performance in the multimodal category."
    },
    {
      "id": "dd550fa9-c9c4-4228-9b8f-f9738eedf64f",
      "adv_source_id": "dd550fa9-c9c4-4228-9b8f-f9738eedf64f",
      "source_id": "26d1f313-f244-4df4-9929-c66886bb29b3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Structured Learning of Tree Potentials in CRF for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Structured Learning of Tree Potentials in CRF for Image Segmentation\" from arxiv.org:",
      "generation": " We present a novel approach for learning tree potentials in conditional random fields (CRFs) for image segmentation. Our method is based on structured output learning (SOL), which allows us to train the CRF using a linear split solver, such as L-BFGS. Our approach utilizes a tree-structured output representation, where each node represents an intermediate segmentation result and only the leaf nodes correspond to the final segmentation. We introduce an objective function that optimizes the tree potentials to enforce spatial coherence and a smooth transition from the intermediate to the final segmentation. We demonstrate the effectiveness of our approach on two public image segmentation benchmarks, and show that our method achieves state-of-the-art performance."
    },
    {
      "id": "b97f6f41-daa1-4336-a026-6f8c4dd78ff4",
      "adv_source_id": "b97f6f41-daa1-4336-a026-6f8c4dd78ff4",
      "source_id": "26d1f313-f244-4df4-9929-c66886bb29b3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Structured Learning of Tree Potentials in CRF for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Structured Learning of Tree Potentials in CRF for Image Segmentation\" from arxiv.org:",
      "generation": " We present a method for learning tree potentials in conditional random fields (CRFs) for image segmentation. Our approach leverages the structured output labels of image segmentation, using a tree-structured CRF model to reason about spatial context. We introduce an algorithm for learning these tree potentials from data, using a min-max optimization framework. Our approach is able to learn an expressive model of spatial context, even in the presence of noisy labels, by leveraging a robust optimization technique. We demonstrate the effectiveness of our approach on the Stanford Background Segmentation dataset, outperforming the state-of-the-art in terms of both precision and recall. Our code and trained models are available at this https URL."
    },
    {
      "id": "22c21825-5b56-4cd0-a19e-3c5fe8883992",
      "adv_source_id": "22c21825-5b56-4cd0-a19e-3c5fe8883992",
      "source_id": "722f13c4-cf25-4279-9191-a023b49e53ea",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge",
      "prompt": "The following is the full text of the abstract for a research paper titled \"k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge\" from arxiv.org:",
      "generation": " We present a method for skin lesion segmentation in dermoscopic images, tailored for the ISIC 2017 Skin Lesion Segmentation Challenge. The method is based on a convolutional neural network, trained on ISIC 2016 training and validation datasets. We use k-means clustering to select the most relevant features and ensemble of regressions to improve the segmentation map. We ranked 8th in ISIC 2017 Skin Lesion Segmentation Challenge."
    },
    {
      "id": "daf80033-8af5-4106-803a-40ae7c893011",
      "adv_source_id": "daf80033-8af5-4106-803a-40ae7c893011",
      "source_id": "722f13c4-cf25-4279-9191-a023b49e53ea",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge",
      "prompt": "The following is the full text of the abstract for a research paper titled \"k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge\" from arxiv.org:",
      "generation": " We present a method for the ISIC 2017 Skin Lesion Segmentation Challenge, where the goal is to create a segmentation map of skin lesions in dermoscopic images. The proposed method first applies a Gaussian mixture model (GMM) to the vectorized image patches (i.e. pixels). We then use the clusters as the regressions' features for an ensemble of regressions (ER), where each regression is a support vector machine (SVM) trained on a subset of the training data. Our method does not require any parameter tuning. We combine the GMM and ER in a simple yet effective way to achieve competitive results. We rank 9th in the challenge."
    },
    {
      "id": "951bebfe-9781-4cd2-961a-b8c2226344d2",
      "adv_source_id": "951bebfe-9781-4cd2-961a-b8c2226344d2",
      "source_id": "ce2077bf-e2d9-4ae6-b49b-25b586e4edf5",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation\" from arxiv.org:",
      "generation": " Medical image segmentation is typically formulated as an iterative estimation problem, with each iteration updating an initial guess for the segmentation through comparison with a reference (often a labeled image). In this paper we propose a simple method for learning the input to such an iterative process, by comparing the current segmentation with the reference using a normalized input representation. This learned input can then be used in place of the original image input in subsequent iterations, resulting in a significant improvement in segmentation accuracy. We demonstrate the effectiveness of our approach on three different segmentation problems: myocardial segmentation in cardiac MR images, tumor segmentation in MR breast images, and hippocampus segmentation in T1-weighted MR brain images. In all cases, we achieve substantial improvements over the baseline methods."
    },
    {
      "id": "db49afa7-097b-4e1b-af96-ea9c6b2b11db",
      "adv_source_id": "db49afa7-097b-4e1b-af96-ea9c6b2b11db",
      "source_id": "ce2077bf-e2d9-4ae6-b49b-25b586e4edf5",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation\" from arxiv.org:",
      "generation": " Medical image segmentation is typically formulated as an iterative estimation problem, with a labeled source domain and an unlabeled target domain. The quality of segmentation results varies widely across different anatomical structures and imaging modalities, largely because of the highly variable appearance of the structures of interest, the presence of noise and artifacts, and the lack of annotated training data. In this paper, we propose a simple yet effective approach to boost the performance of any segmentation algorithm in such iterative estimation frameworks. Our key observation is that, when the appearance of objects in the target domain is highly variable, a decent initial estimate can be obtained by learning a normalized input from the source domain, which effectively transforms the target image to a typical appearance of the structure of interest. Specifically, given source images with ground truth segmentations, we learn a transformation that preserves the structural appearance of the object of interest, such that after transformation, the object of interest in the source images have similar appearance to those in the target image. We then apply the learned transformation to the target image to obtain a normalized input, which is used for iterative estimation. We evaluate our method on multiple challenging segmentation problems, including brain tumor segmentation in T1-weighted magnetic resonance imaging (MRI), and pulmonary nodule segmentation in computed tomography (CT). Results show that our method can significantly improve the performance of any baseline segmentation algorithms in iterative estimation frameworks, with gains up to 6.7% in F1-score on brain tumor segmentation, and 8.0% in F1-score on pulmonary nodule segmentation."
    },
    {
      "id": "04f50755-cca4-44d7-b976-e5323a0daa84",
      "adv_source_id": "04f50755-cca4-44d7-b976-e5323a0daa84",
      "source_id": "679eac63-52d1-4c28-82bb-ce60718ab36b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation Using Overlapping Group Sparsity",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Image Segmentation Using Overlapping Group Sparsity\" from arxiv.org:",
      "generation": " We present a new algorithm for image segmentation based on grouping pixels together that belong to the same object. Our method incorporates a priori knowledge of object geometry in the form of an overlapping group sparsity model. This model is a generalization of the traditional group sparsity model, where groups of pixels can overlap, enabling better segmentation of objects that overlap in the image. The algorithm is based on minimizing an objective function that encourages sparsity in the group labels, as well as a fidelity term that compares the predicted pixel labels to the ground truth labels. We demonstrate the performance of our method on the public available Cityscapes dataset, where we achieve state-of-the-art segmentation results."
    },
    {
      "id": "aea2dda7-e57e-4f2e-96fd-a7343cedc193",
      "adv_source_id": "aea2dda7-e57e-4f2e-96fd-a7343cedc193",
      "source_id": "679eac63-52d1-4c28-82bb-ce60718ab36b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation Using Overlapping Group Sparsity",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Image Segmentation Using Overlapping Group Sparsity\" from arxiv.org:",
      "generation": " We present a new algorithm for image segmentation based on grouping pixels together using sparse representations. In contrast to related work on group sparsity, we allow the groups to overlap, resulting in significant improvements in accuracy. The basis for our algorithm is a variational framework in which we minimize the expected squared error averaged over all possible segmentations. This non-convex problem is shown to possess a unique solution, and we develop an efficient Expectation-Maximization algorithm for its computation. In addition, we introduce a new variant of the recently introduced Cityblock distance that is well-suited for our algorithm. Finally, we perform a comprehensive experimental evaluation of our method on a publicly available dataset, and show that it yields state-of-the-art results."
    },
    {
      "id": "cb070e4b-8760-4f42-a3d6-e44e34487941",
      "adv_source_id": "cb070e4b-8760-4f42-a3d6-e44e34487941",
      "source_id": "405fe838-cab2-4f7e-9833-4c045c1bbdf3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images\" from arxiv.org:",
      "generation": " We present a novel framework for automatic prostate segmentation in ultrasound (US) images. Our method is based on fine-grained recurrent neural networks (RNNs), which are designed to capture the complex anatomical structures and patterns in US images. The proposed method consists of two main components: a coarse segmentation network and a fine-grained segmentation network. The coarse segmentation network is based on a conventional RNN, while the fine-grained segmentation network is based on a long-short term memory (LSTM) RNN. We evaluate our method on a large dataset of US images and compare it to other state-of-the-art methods. Our results show that our method achieves competitive performance and is able to handle large variations in the appearance and shape of the prostate."
    },
    {
      "id": "af913e69-2d5b-4fa8-be19-b7f361d5333d",
      "adv_source_id": "af913e69-2d5b-4fa8-be19-b7f361d5333d",
      "source_id": "405fe838-cab2-4f7e-9833-4c045c1bbdf3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images\" from arxiv.org:",
      "generation": " We present a novel framework for automatic prostate segmentation in ultrasound (US) images. Our method is based on fine-grained recurrent neural networks (RNNs), which are designed to capture the intricate architecture of the prostate. The proposed network is composed of a sequence of RNN layers, each of which is specialized in identifying a specific anatomical feature of the prostate. The network is trained end-to-end on a large dataset of annotated US images. We evaluate our method on the publicly available CHaP-vessel dataset, which contains 68 US images. Our results show that the proposed method achieves an average precision of 92.0% and recall of 95.2% on the test set, which is significantly higher than the state-of-the-art approaches. Furthermore, the proposed method is able to generalize well to images that were not included in the training set. The code and models used in this work are publicly available at https://github.com/FCNN-Medical/US-Segmentation.\n\nObjective \u2014 Prostate cancer is the most common non-skin cancer in men and the second-leading cause of cancer death in American men. Ultrasound (US) imaging is commonly used for prostate cancer diagnosis due to its convenience, cost-effectiveness, and real-time capability. In this work, we develop a novel framework for automatic prostate segmentation in US images. Our method is based on fine-grained recurrent neural networks (RNNs), which are designed to capture the intricate architecture of the prostate.\n\nMethods \u2014 The proposed network is composed of a sequence of RNN layers, each of which is specialized in identifying a specific anatomical feature of the prostate. The network is trained end-to-end on a large dataset of annotated US images. We evaluate our method on the publicly available CHaP-vessel dataset, which contains 68 US images. The proposed method is compared to state-of-the-art approaches based on deep learning and traditional image processing techniques.\n\nResults \u2014 Our results show that the proposed method achieves an average precision and recall of 92.0% and 95.2% on the test set, which is significantly higher than the state-of-the-art approaches. Furthermore, the proposed method is able to generalize well to images that were not included in the training set.\n\nConclusion \u2014 In this work, we present a novel framework for automatic prostate segmentation in US images based on fine-grained RNNs. Our method achieves state-of-the-"
    },
    {
      "id": "6a8ec51e-0b6b-4786-b02c-49635b9b9efb",
      "adv_source_id": "6a8ec51e-0b6b-4786-b02c-49635b9b9efb",
      "source_id": "891c294c-58d9-4ba3-b085-b992f1ef2818",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors",
      "prompt": "The following is the full text of the abstract for a research paper titled \"MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\" from arxiv.org:",
      "generation": " We present a shape-sampler for image segmentation based on a nonparametric shape prior, built upon the monte carlo belief propagation framework. Our approach addresses the issue of combinatorial explosion by parameterizing a shape prior as a probabilistic distribution, and efficiently sampling from this distribution using MCMC methods. We demonstrate the effectiveness of our approach on a benchmark dataset, showing that our method compares favorably with state-of-the-art segmentation methods while providing a structured shape prior."
    },
    {
      "id": "50203423-1ea4-45b3-9367-1dec7a39c115",
      "adv_source_id": "50203423-1ea4-45b3-9367-1dec7a39c115",
      "source_id": "891c294c-58d9-4ba3-b085-b992f1ef2818",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors",
      "prompt": "The following is the full text of the abstract for a research paper titled \"MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\" from arxiv.org:",
      "generation": " We present a shape-driven Markov chain Monte Carlo (MCMC) sampling algorithm for image segmentation based on a nonparametric shape prior. Our approach combines a shape model, constructed from a set of training shapes via a Probabilistic Sister Embedding (PSE), with a variational EM algorithm for MCMC sampling. We introduce a new technique for efficient sampling of the PSE model, which we term Sequential Tail Sampling (STS). STS combines the efficiency of Importance Sampling with the accurate sampling of the tails provided by Direct Sampling. We demonstrate the efficacy of our approach on a set of 2D and 3D images, and compare to state-of-the-art segmentation algorithms."
    },
    {
      "id": "eca2d05e-6c63-44f5-b6e0-5f00d1664467",
      "adv_source_id": "eca2d05e-6c63-44f5-b6e0-5f00d1664467",
      "source_id": "ade3fd87-30b8-4664-91fa-61b2bb99c132",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Theoretical Analysis of Active Contours on Graphs",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Theoretical Analysis of Active Contours on Graphs\" from arxiv.org:",
      "generation": " We present a theoretical analysis of active contours on graphs. We show that the energy of an active contour on a graph is minimized when the contour is a geodesic, and we characterize the first and second variations of the energy. We show that the eigenvectors of the laplacian of the graph are parallel transported along geodesics, and we use this result to demonstrate the convergence of active contours to geodesics. Finally, we present a new algorithm for implementing active contours on graphs. Our results establish the mathematical foundation for active contours on graphs and provide a basis for their application to a variety of problems in image processing and computer vision."
    },
    {
      "id": "01d31f7b-21b0-487e-adcf-ff9e04b30267",
      "adv_source_id": "01d31f7b-21b0-487e-adcf-ff9e04b30267",
      "source_id": "ade3fd87-30b8-4664-91fa-61b2bb99c132",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Theoretical Analysis of Active Contours on Graphs",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Theoretical Analysis of Active Contours on Graphs\" from arxiv.org:",
      "generation": " We present a theoretical analysis of active contours on graphs. We show that the energy of an active contour can be written as the sum of a elasticity term and a diffusion term. The elasticity term is a potential energy that is defined on the space of curves and measures the curvature of the curve. The diffusion term is a kinetic energy that is defined on the space of curves and measures the length of the curve. The equality between the elasticity term and the diffusion term is a subtle balance between the curvature of the curve and its length. We also show that the minimizing curves of the energy are characterized by a dynamic programming equation that can be solved efficiently. Finally, we present several examples that demonstrate the behavior of the active contour model on graphs."
    },
    {
      "id": "8ac72746-a309-43e2-afbf-128732ad8655",
      "adv_source_id": "8ac72746-a309-43e2-afbf-128732ad8655",
      "source_id": "8bf02f83-d4a0-43dd-b7e5-e35d82337d9a",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combinatorial Energy Learning for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Combinatorial Energy Learning for Image Segmentation\" from arxiv.org:",
      "generation": " Recent works on semantic segmentation largely focus on designing models with better expressive power to leverage abundant annotated image data for training. However, these models are known to be susceptible to the inherent ambiguity of pixel-level annotations, and their complexity can make them difficult to optimize. In this paper, we take a different approach and treat energy learning as a combinatorial problem, where the goal is to design an energy function that can effectively encode segmentations for a specific task. Our key observation is that when the energy function is a composition of simple parts, the search for a good segmentation can be formulated as a combinatorial optimization problem, for which efficient algorithms exist. To learn a good composition of parts, we propose a novel framework called Combinatorial Energy Learning (CEL). Given a database of image segmentations, CEL learns a segmentation energy function as a weighted composition of a set of predefined atomic energies, where the weights are optimized via combinatorial optimization. We show that this approach can produce segmentation energies that are as effective as state-of-the-art models while being an order of magnitude faster to evaluate. In addition, by formulating the learning problem as a combinatorial optimization problem, CEL is able to bypass the need for annotated images during the learning process. We demonstrate the effectiveness of CEL on both indoor and outdoor segmentation benchmarks, where CEL achieves competitive performance to state-of-the-art models while being an order of magnitude faster to evaluate."
    },
    {
      "id": "d7ce0735-f134-4201-bb28-c084df677f53",
      "adv_source_id": "d7ce0735-f134-4201-bb28-c084df677f53",
      "source_id": "8bf02f83-d4a0-43dd-b7e5-e35d82337d9a",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combinatorial Energy Learning for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Combinatorial Energy Learning for Image Segmentation\" from arxiv.org:",
      "generation": " We present a fast combinatorial energy learning scheme for image segmentation, which is built upon coordinate descent method with box constraints. In contrast to existing methods that optimize energy functions with linear terms, our approach can handle non-linear energy terms, and thus can capture more complex and background-like object boundaries. Furthermore, the non-convexity of our energy function enables it to better adapt to the implicit background prior in image pixel labels. The resulting energy function is then optimized by coordinate descent within a linear programming (LP) relaxation, which can be solved efficiently with box constraints. We show that our approach is fast and effective in segmenting objects in images, and is especially useful for objects with complex boundaries and/or weak object boundaries.\n\nIllustration: The figure on the left shows an example input image. The middle figure shows the segmentation result by simply using a Mean Shift filter to segment objects without considering object boundaries. The right figure shows the segmentation result of our approach, where we can see that our approach is able to recover the object boundaries and segment the objects more accurately."
    },
    {
      "id": "d263a4a7-346d-44c2-bc74-a2da2e327138",
      "adv_source_id": "d263a4a7-346d-44c2-bc74-a2da2e327138",
      "source_id": "3ce9a084-3b0a-4e7c-aeab-acb4b02105b3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 4D Light-Field Dataset and CNN Architectures for Material Recognition",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A 4D Light-Field Dataset and CNN Architectures for Material Recognition\" from arxiv.org:",
      "generation": " We present a large-scale dataset for material recognition named MatNet, which contains over five thousand labeled samples of forty-eight common materials. To acquire this dataset, we built a multi-camera light-field capture system, and developed an effective pipeline to synthesize 4D light-field images from stereo pairs of images, given the depth information. To the best of our knowledge, MatNet is the first 4D light-field dataset for material recognition. We further contribute two CNN architectures designed for material recognition: a simple but effective CNN called MatNet-Conv, and a more complex CNN called MatNet-ResNet. Our extensive experiments show that our methods achieve top-tier performance on two public material recognition benchmarks, namely CAMEO and MatNet."
    },
    {
      "id": "39ec79ba-a2d8-4295-9851-3faaf793c1eb",
      "adv_source_id": "39ec79ba-a2d8-4295-9851-3faaf793c1eb",
      "source_id": "3ce9a084-3b0a-4e7c-aeab-acb4b02105b3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 4D Light-Field Dataset and CNN Architectures for Material Recognition",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A 4D Light-Field Dataset and CNN Architectures for Material Recognition\" from arxiv.org:",
      "generation": " We present a large-scale dataset for material recognition namedmatnet, which contains over five thousand labeled samples of sixty-eight material classes. Together with the dataset, we introduce a deep learning architecture, called MatNet, that is composed of two stream networks, a spatial pyramid network, and a temporal segment network, to effectively utilize the 4D light-field data. We evaluate different variations of the network in terms of the number of streams and the number of layers for the stream networks, the number of bins for the spatial pyramid network, and the number of frames for the temporal segment network. We compare the proposed methods to other state-of-the-art methods on two public material recognition benchmarks, namely CuRe and Shi et al., and show that our methods outperform the others in all cases. In particular, when the proposed method is applied to the matnet dataset, it achieves 92.08% accuracy."
    },
    {
      "id": "29088218-1f5d-412b-ad96-e2ad21a18fd7",
      "adv_source_id": "29088218-1f5d-412b-ad96-e2ad21a18fd7",
      "source_id": "eed27bff-8deb-44ec-ad7b-3c6316bd345d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in electron microscopy (EM) have made it possible to automate the process of imaging large cells and tissues with nanoscale resolution. However, manual analysis of EM images remains a bottleneck in understanding cell structure and organization. In this paper, we present a semi-supervised learning framework for segmentation of EM images by seeking a hierarchical merge tree of labeled pixels. Given an image with only a few user-provided labels, our approach seeks an accurate segmentation through a sparse merge tree of labels that respects a hierarchical organization of labels. We introduce a new graph neural network architecture tailored to our semi-supervised segmentation framework. We show that the proposed method performs favorably compared with state-of-the-art segmentation methods on both simulated and real EM images."
    },
    {
      "id": "396cdc15-14a8-4594-a4ba-ff9bb0805062",
      "adv_source_id": "396cdc15-14a8-4594-a4ba-ff9bb0805062",
      "source_id": "eed27bff-8deb-44ec-ad7b-3c6316bd345d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in electron microscopy (EM) has made it possible to automate the image acquisition process, leading to a surge in EM data available for 3D reconstruction. However, the lack of tools for EM image segmentation poses a major bottleneck for analyzing large EM datasets. In this paper, we present SSHMT, a semi-supervised learning framework for EM image segmentation. SSHMT is based on a hierarchical merge tree (HMT) architecture, which uses a recursive graph representation to encode the spatial coherence of the segmentation tasks. We propose a semi-supervised learning strategy that leverages unlabeled data to refine the HMT, improving segmentation accuracy. Our approach performs competitively against state-of-the-art methods on two public EM image segmentation benchmarks, including a dataset with significantly larger image size and increased noise levels."
    },
    {
      "id": "ceec80c4-4f2c-4910-9fd6-5b0ee1fda87a",
      "adv_source_id": "ceec80c4-4f2c-4910-9fd6-5b0ee1fda87a",
      "source_id": "54e0f95e-00d1-469e-ad5f-aee23ffdcc3d",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Superpixel Convolutional Networks using Bilateral Inceptions",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Superpixel Convolutional Networks using Bilateral Inceptions\" from arxiv.org:",
      "generation": " We present a new architecture for effective superpixel extraction, based on a shallow convolutional network, trained with a large collection of image segments. Our architecture is based on a series of bicubic-sampling operations, which we term bilateral inceptions, that explicitly model the pixel-edge correspondence. During training, we enforce a global consistency on the pixels and edges, while permitting local variations in the responses. This is achieved by a new loss function that is inspired by theedge-aware energy of the bilateral filter. Our approach is fast, and extracts superpixels on the CPU in linear time with respect to the number of pixels. We demonstrate the effectiveness of our approach on the 2015 Berkeley Segmentation Dataset, obtaining state-of-the-art results."
    },
    {
      "id": "62a926fe-7996-4d71-8225-7e120bf2284e",
      "adv_source_id": "62a926fe-7996-4d71-8225-7e120bf2284e",
      "source_id": "54e0f95e-00d1-469e-ad5f-aee23ffdcc3d",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Superpixel Convolutional Networks using Bilateral Inceptions",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Superpixel Convolutional Networks using Bilateral Inceptions\" from arxiv.org:",
      "generation": " We present a new architecture for robust semantic segmentation based on superpixels derived from a clustering of pixels. Superpixels provide an initial grouping of pixels that captures local structure, such that each superpixel can be treated as a single object. We propose a novel approach to incorporate this prior knowledge of object locations by learning a superpixel convolutional network (SCN) that directly operates on superpixel inputs. Our approach leverages the recently introduced Inception architecture for efficient computation of multi-scale information. We show that Inception features extracted from superpixels provide good performance on semantic segmentation, and we analyze types of information that are missed when going from pixels to superpixels. We then propose a modification of the classical bilateral filter that is applied to the raw pixel data prior to running our SCN. This modification adaptively enhances the information content of each pixel, providing a form of pixel-level attention. We show that this attention mechanism improves performance in semantic segmentation, probably by making our SCN more robust to the loss of information during the superpixel clustering step. Our approach achieves 75.8% mean accuracy on the CamVid dataset, a new record on this dataset, and 79.4% mean accuracy on the Stanford Background dataset."
    },
    {
      "id": "7fb22bc9-1d91-4579-a503-acabc51b6e92",
      "adv_source_id": "7fb22bc9-1d91-4579-a503-acabc51b6e92",
      "source_id": "fad4e6d3-4732-418a-9dcb-7722141430a3",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization\" from arxiv.org:",
      "generation": " We present a method for segmenting images based on the content of screens present in the image. Our method comprises two stages: in the first stage we identify regions in the image that contain screens, and in the second stage we segment the content displayed on those screens. Our proposed method is fully automatic, and is able to handle a variety of screen content, including text, images, and videos. We evaluate our method on a new dataset we created, as well as on publicly available datasets, and show that it performs better than state-of-the-art methods.\n\nOur screen content segmentation algorithm proceeds by modeling the image as a sparse combination of dictionary elements, where each dictionary element corresponds to a candidate screen content patch. We simultaneously identify regions containing screens, and segment the content on the screens by selecting the appropriate dictionary elements. Within a screen region, we further refine the segmentation by minimizing a total variation energy, which allows us to handle screens that are distorted, occluded, or viewed from a perspective. In order to effectively handle textual content, we introduce a text-specific dictionary learned from a set of handwritten digits, and show that it effectively improves performance on images with text.\n\nOur algorithm performs well on a variety of screen content, including natural images, textual content, and videos. We evaluate our method on a new dataset we created, which contains a variety of screen content, including natural images, text, and web-browsers. We also evaluate our method on two publicly available datasets. In all cases, we show that our method performs better than state-of-the-art screen content segmentation methods."
    },
    {
      "id": "e224b2e0-b0f8-4b77-9eb0-e64013613dc3",
      "adv_source_id": "e224b2e0-b0f8-4b77-9eb0-e64013613dc3",
      "source_id": "fad4e6d3-4732-418a-9dcb-7722141430a3",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization\" from arxiv.org:",
      "generation": " We present a method for the segmentation of images containing screens, based on the sparse decomposition of the image in convolution kernels. This decomposition, followed by a total variation minimization, produces a segmentation of the image into foreground (screen) and background components. We demonstrate the efficacy of our method on a dataset of images containing a variety of screens with different content, including text, and scenes containing multiple screens.\n\nOur method successfully segments the screens in these images, despite variations in lighting, appearance of screens, and background behind the screens. We demonstrate that our method successfully segments screens despite variations in screen content, and that it can handle screens that are partially visible or obscured by hands or other objects.\n\nOur algorithm consists of two main steps: sparse decomposition of the image, and segmentation based on total variation minimization. In the sparse decomposition step, we represent the image as a sum of convolutions with a set of learned kernels. These kernels capture both the appearance of screens and the variations in lighting and background. In the segmentation step, we minimize the total variation of the image, subject to the constraint that the image is a sum of convolutions with the learned kernels. This optimization problem can be solved efficiently using the alternating direction method of multipliers.\n\nOur experiments demonstrate that our method is effective at segmenting screens in images, and that it performs better than state-of-the-art methods on the UDHR dataset. Our method is also efficient, running in approximately one minute on a typical laptop."
    },
    {
      "id": "c20a0656-9833-4859-b318-61eadf125716",
      "adv_source_id": "c20a0656-9833-4859-b318-61eadf125716",
      "source_id": "32ad507a-fd83-458a-8a3d-0d4f8c6b5b47",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating prior knowledge in medical image segmentation: a survey",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Incorporating prior knowledge in medical image segmentation: a survey\" from arxiv.org:",
      "generation": " Medical image segmentation is a fundamental step towards computerized\ndiagnosis extraction. In this survey, we focus on the prior knowledge\nincorporated in medical image segmentation. We first categorize\nintegrating prior knowledge into a medical image segmentation framework\ninto three categories: (1) low-level prior, (2) mid-level prior, and\n(3) high-level prior. For each category, we survey the state-of-the-art\nmethods. We compare the methods based on the characteristics of\nthe incorporated prior knowledge, the performance, the computational\ncomplexity, and the flexibility of the methods. We also provide a\nbrief overview of the popular prior knowledge sources for medical\nimage segmentation. Finally, we discuss the advantages and limitations\nof the surveyed methods. Our goal is to provide guidance for choosing\nthe proper method for a specific clinical application."
    },
    {
      "id": "61c175ca-2f84-4835-8ed9-8563f4cc6aa2",
      "adv_source_id": "61c175ca-2f84-4835-8ed9-8563f4cc6aa2",
      "source_id": "32ad507a-fd83-458a-8a3d-0d4f8c6b5b47",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating prior knowledge in medical image segmentation: a survey",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Incorporating prior knowledge in medical image segmentation: a survey\" from arxiv.org:",
      "generation": " Medical image segmentation is a vital step in advanced quantitative analysis and computer-aided diagnosis. The recent progress in learning-based segmentation methods significantly improved the accuracy; however, those methods usually assume the images are acquired under ideal conditions, i.e., no noisy, no partial occlusion, no intensity inhomogeneity, etc. In practice, the assumption does not hold and the learned models could fail. In this paper, we survey the recent efforts on incorporating prior knowledge into learning-based medical image segmentation to overcome these challenges. We group the prior knowledge into three categories: (1) prior on the appearance of objects, (2) prior on the acquisition process, and (3) prior on the relationship between objects. We further categorize the incorporation approaches into two groups: (1) explicit modeling, and (2) implicit modeling. For each category and each group, we first give the intuition of the approach, then review the state-of-the-art methods with detailed explanations. We finally provide a comprehensive comparison to summarize the advantages and disadvantages of each approach."
    },
    {
      "id": "d9586ce4-3f35-494c-9589-b579e0681984",
      "adv_source_id": "d9586ce4-3f35-494c-9589-b579e0681984",
      "source_id": "04284051-7a7a-4104-bd93-0f44b50ed616",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient piecewise training of deep structured models for semantic segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Efficient piecewise training of deep structured models for semantic segmentation\" from arxiv.org:",
      "generation": " Efficient piecewise training of deep structured models for semantic segmentation\n\nSemantic segmentation is the task of assigning semantic labels to each pixel in an input image. While deep convolutional neural networks (CNNs) have achieved remarkable performance in this task, training such models is still a computationally expensive task, especially for large datasets and models. In this paper, we present a method for training deep structured models for semantic segmentation that is more efficient than existing methods. Our method is based on the idea of training the model in a piecewise fashion, where the model is trained on a subset of the data and then the trained model is used to predict the labels for the remaining data. This allows us to train the model on a much smaller subset of the data, which reduces the computational cost of training. Our experiments show that our method is able to achieve comparable performance to existing methods while reducing the computational cost of training by up to 50%."
    },
    {
      "id": "883fe4a6-0e7a-44f3-ab29-9f1cb94e9d07",
      "adv_source_id": "883fe4a6-0e7a-44f3-ab29-9f1cb94e9d07",
      "source_id": "04284051-7a7a-4104-bd93-0f44b50ed616",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient piecewise training of deep structured models for semantic segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Efficient piecewise training of deep structured models for semantic segmentation\" from arxiv.org:",
      "generation": " Efficient piecewise training of deep structured models for semantic segmentation\n\nReliable and robust semantic segmentation requires powerful features learned by\ndeep architectures. However, training such networks is challenging due to the\ncurse of initialization and the lack of sufficient training data. In this\npaper we propose a piecewise training approach for deep structured models\nwhere we jointly optimize for both the network architecture and the\ninitialization. This is performed by training the model on several\nsubsets of the training data with architectures containing different\nnumbers of layers, determined by a genetic algorithm. We apply our\napproach to semantic segmentation on the Cityscapes and CamVid datasets\nwith promising results, achieving better performance compared to\ntraining on the full dataset with a fixed architecture and\n initialization. The proposed method is extremely general and can be\napplied to any semantic segmentation architecture.\n\nKeywords: Semantic segmentation, deep learning, training data efficiency,\ntransfer learning"
    },
    {
      "id": "d4cf700e-f133-47c8-8262-0aa34ece1b7f",
      "adv_source_id": "d4cf700e-f133-47c8-8262-0aa34ece1b7f",
      "source_id": "1091bb0b-6cb0-4667-b117-58ee4136ee94",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bridging Category-level and Instance-level Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bridging Category-level and Instance-level Semantic Image Segmentation\" from arxiv.org:",
      "generation": " Recent progresses in deep learning have advanced semantic image segmentation towards accuracy in categorizing individual pixels in an image and recognizing objects in the real world. However, most existing semantic segmentation algorithms treat categorization and recognition as two separate tasks, regardless of their tight relationship in semantics. In this paper, we bridge the gap between category-level and instance-level semantic segmentation with a unified framework, called simultaneous category-instance segmentation (SCIS). Specifically, we formulate category-level and instance-level segmentation as a multi-task learning problem, such that the mutual interactions between category-level and instance-level segmentation can be effectively exploited for performance improvement. A novel architecture, named Feature Pyramid Network with Grouping (FPN-G), is developed to effectively fuse multi-scale contextual information and decode the segmentation results. Extensive experiments on the PASCAL VOC 2012 and Cityscapes datasets demonstrate the effectiveness of our approach in bridging the gap between category-level and instance-level segmentation. Without using any instance-level annotated data, our approach achieves 83.1% in terms of F-measure on the PASCAL VOC 2012 dataset, outperforming the state-of-the-art fully-supervised instance-level segmentation methods. Moreover, the category-level performance of our approach is only a 1.6% drop compared with DenseCap, which benefits from instance-level annotations. Code is available at: https://github.com/YxyangTao/SCIS."
    },
    {
      "id": "c5b36980-84e3-4a0a-97aa-0637aff7d87e",
      "adv_source_id": "c5b36980-84e3-4a0a-97aa-0637aff7d87e",
      "source_id": "1091bb0b-6cb0-4667-b117-58ee4136ee94",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bridging Category-level and Instance-level Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bridging Category-level and Instance-level Semantic Image Segmentation\" from arxiv.org:",
      "generation": " Recent progresses in deep learning have driven the development of semantic image segmentation, which aims to assign semantic labels to every pixel in an image. The state-of-the-art approaches tend to rely on large amounts of labeled images to train deep convolutional neural networks (CNNs). However, collecting and annotating large-scale semantic segmentation datasets are expensive and time-consuming. In this paper, we aim to bridge the gap between category-level and instance-level semantic image segmentation, i.e., reducing the cost of collecting and annotating images for training while maintaining high segmentation performance. Specifically, we propose a Unifying Framework of Cascaded Refinement (UFCR), which leverages the massive amount of image-level labels and the limited amount of pixel-level labels to train a deep CNN for semantic image segmentation. We formulate the problem of semantic image segmentation as a refinement learning problem in which the category-level predictions produced by the upper layers of a deep CNN are refined by the instance-level predictions produced by the bottom layers. Furthermore, we propose an instance-aware loss function to train the deep CNN in a end-to-end manner. The proposed method has been evaluated on three popular semantic image segmentation benchmarks, and experimental results demonstrate the effectiveness of the proposed approach."
    },
    {
      "id": "df73633b-dca4-4bcd-80b4-8bbe003484d0",
      "adv_source_id": "df73633b-dca4-4bcd-80b4-8bbe003484d0",
      "source_id": "fe96becb-05e7-47fa-bf1f-41acbedb8636",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Semantic Image Segmentation with High Order Context and Guided Filtering",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Semantic Image Segmentation with High Order Context and Guided Filtering\" from arxiv.org:",
      "generation": " Semantic image segmentation has achieved remarkable performance on various benchmarks,\ndriven by deep learning with ever-growing model complexity. However,\ntraining these complex models is computationally expensive, and they\ntend to overfit the specific training datasets, thereby hindering their\nperformance on new datasets or in realistic applications. In this\npaper, we investigate methods to reduce model complexity, both\nduring training and inference, while maintaining high segmentation\nquality. We introduce a context module that captures high-order\ncontext with a small number of parameters, and show that it\nimproves performance over representative baselines. We then\ncapitalize on the guidance property of the context module to\ndevelop an adaptive inference strategy that selectively applies\nfiltering to improve accuracy with minimal computational overhead.\nOur method achieves state-of-the-art performance on the Cityscapes\ndataset with a model that is an order of magnitude smaller\nthan the current leaders. In an extreme case, our method\nachieves 86.6% mIOU on Cityscapes with a model size under 5M\nparameters, about 15\u00d7 smaller than the current state of the\nart. The code is available at\nthis https URL."
    },
    {
      "id": "23903a14-aa85-464b-8064-deca000e1243",
      "adv_source_id": "23903a14-aa85-464b-8064-deca000e1243",
      "source_id": "fe96becb-05e7-47fa-bf1f-41acbedb8636",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Semantic Image Segmentation with High Order Context and Guided Filtering",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Semantic Image Segmentation with High Order Context and Guided Filtering\" from arxiv.org:",
      "generation": " Recent progresses in deep learning have achieved great performance in image segmentation, especially with the pre-trained models such as PSPNet, DeepLabv3, and EncNet. However, these methods focus on the low-level details and often fail to capture the high-order context, which is crucial for accurate segmentation. In this paper, we propose a novel approach to combine both low-level and high-order features for  semantic image segmentation. We introduce a high-order context module that can capture the long-range dependencies in an image, and combine it with the recent guided filtering technique to refine the segmentation results. Our experiments on the VOC 2012 and Cityscapes datasets show that our method achieves state-of-the-art performance. Our implementation is based on PyTorch and will be made publicly available."
    },
    {
      "id": "d755a60c-0141-4ca8-8f9a-75a46d060890",
      "adv_source_id": "d755a60c-0141-4ca8-8f9a-75a46d060890",
      "source_id": "bd1160c4-4533-4102-b1bb-1c5b3304ff5f",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Classifier-guided Approach for Top-down Salient Object Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Classifier-guided Approach for Top-down Salient Object Detection\" from arxiv.org:",
      "generation": " Recent results on salient object detection take a top-down approach, in which the model first detects the foreground and then refines it to obtain the final salient regions. Most existing works focus on designing effective refinement modules to improve the quality of the predicted saliency maps. In this work, we argue that a key problem of current top-down methods is the quality of the initial foreground map, which is usually obtained by thresholding or clamping the output of an encoder. To address this problem, we propose a classifier-guided approach that refines the initial foreground map with a classifier. Specifically, we train a support vector machine (SVM) classifier with image-level labels to predict whether a pixel is in the foreground or not. Since the classifier is trained with image-level labels, it is able to exploit global information and capture the underlying object-like structures. Extensive experiments on the salient object detection benchmark datasets validate the effectiveness of our approach compared to the state-of-the-art methods."
    },
    {
      "id": "36420c18-8426-43ef-8e39-a1c0f89a0311",
      "adv_source_id": "36420c18-8426-43ef-8e39-a1c0f89a0311",
      "source_id": "bd1160c4-4533-4102-b1bb-1c5b3304ff5f",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Classifier-guided Approach for Top-down Salient Object Detection",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Classifier-guided Approach for Top-down Salient Object Detection\" from arxiv.org:",
      "generation": " Recent results on salient object detection take a top-down approach, where the focus is on predicting saliency map instead of objectness map. However, unlike objectness map, a saliency map usually represents a soft confidence score and thus is more challenging to train. Previously proposed solutions for training a saliency map mainly rely on ad-hoc loss functions such as Euclidean distance or Wasserstein distance. In this paper, we argue that these loss functions are not adequate for training a good saliency map because they do not take into account the prediction confidence of the model. To address this issue, we propose a classifier-guided approach that leverages a pre-trained classifier as a guide. Specifically, we train a top-down salient object detector by minimizing the divergence between the predicted probability distribution and the ground-truth distribution, where the ground-truth distribution is defined by the classifier. We demonstrate the effectiveness of our approach on both salient object detection and single-shot detection. Without bells and whistles, our approach outperforms the state-of-the-art on four benchmark datasets for salient object detection (Saliency 350, Satellite, PASCAL VOC 2012, and CityScapes), and achieves competitive results on two datasets for single-shot detection (COCO 2017 and VisualGenome)."
    },
    {
      "id": "c4411aa6-5d4e-4514-beba-74ace1303ff7",
      "adv_source_id": "c4411aa6-5d4e-4514-beba-74ace1303ff7",
      "source_id": "fce72cc8-d3d3-4ff3-973d-9371dceab939",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully-Automatic Synapse Prediction and Validation on a Large Data Set",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fully-Automatic Synapse Prediction and Validation on a Large Data Set\" from arxiv.org:",
      "generation": " We present a new large dataset of electron microscopy images of synapses, along with a novel method for automatically identifying synapses in these images. We trained a deep learning model on a large dataset of images manually annotated by human experts, and used it to predict the presence of synapses in a new dataset of images. The model achieved a high accuracy, with an F1 score of 0.9369 on the test set. We also present a new method for validating the accuracy of our model, which we used to evaluate the performance of our model on a separate validation set. Finally, we present a quantitative analysis of the characteristics of the synapses in our dataset, and compare them to previous studies."
    },
    {
      "id": "8fd92add-114f-4cde-bdb3-12eb769ff2e6",
      "adv_source_id": "8fd92add-114f-4cde-bdb3-12eb769ff2e6",
      "source_id": "fce72cc8-d3d3-4ff3-973d-9371dceab939",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully-Automatic Synapse Prediction and Validation on a Large Data Set",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fully-Automatic Synapse Prediction and Validation on a Large Data Set\" from arxiv.org:",
      "generation": " We introduce a new large dataset of electron microscopy images of synapses, together with a completely automated pipeline for detecting synapses in these images. By avoiding manual labor and multiple choices during the pipeline, the presented dataset provides an objective ground truth, which we use to evaluate dozens of state-of-the-art deep learning models. We find that the best models still fail at detecting synapses in EM images, but with a significant improvement over previous works. Furthermore, we show the possibility of creating new effective models using only a small number of training examples. Finally, we use the winning models to provide a quantitative characterization, for the first time, of the full heterogeneity of synapses in a large brain region, thereby paving the way to a full understanding of the complexity of brain connectivity."
    },
    {
      "id": "9dbb57a8-ce34-4c3e-887f-4f7be3001979",
      "adv_source_id": "9dbb57a8-ce34-4c3e-887f-4f7be3001979",
      "source_id": "8a1ace59-3d8c-483d-a8dc-7a15e898f310",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation\" from arxiv.org:",
      "generation": " Traditional fuzzy C-means (FCM) clustering algorithms suffer from the occurrence of local minima in iterative optimization of the objective function during the clustering process. We propose a kernelized weighted SUSAN based FCM clustering algorithm to segment images affected by Gaussian noise. Noisy images degrade the quality of feature vectors for traditional FCM algorithms that are extracted from the original images. In the proposed algorithm, a kernelization technique is employed during the feature extraction to mitigate the effects of noise. Then, to further improve the quality of feature vectors, a weighted strategy is introduced where the weights are determined by using the SUSAN (symmetric unambiguous structures and anomalies) test. The SUSAN based weights can effectively reduce the influence of outliers in the feature vectors. In addition, the proposed algorithm utilizes the watershed algorithm to overcome the difficulty of determining the number of clusters for traditional FCM algorithms. The proposed algorithm is evaluated using two benchmark noisy image datasets. The experimental results demonstrate the effectiveness of the proposed algorithm compared to four state-of-the-art FCM algorithms."
    },
    {
      "id": "7c7222a9-ee03-46e9-a8fe-011a3a1868c4",
      "adv_source_id": "7c7222a9-ee03-46e9-a8fe-011a3a1868c4",
      "source_id": "8a1ace59-3d8c-483d-a8dc-7a15e898f310",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation\" from arxiv.org:",
      "generation": " Traditional fuzzy C-means (FCM) clustering algorithm is affected by noise, and the weighted FCM is sensitive to the choice of weights. This paper proposes a kernelized weighted SUSAN based FCM (KWFCM) for image segmentation. The proposed method utilizes the SUSAN filter to obtain a noise-resistant feature and a kernelization technique is employed to scale the elements of the feature vector, which can adapt to different noise levels. The proposed method is evaluated on three standard databases, and the results show that the proposed method yields more accurate segmentation results compared to the conventional FCM and the weighted FCM.\n\n\u6ce8(Note): This abstract may be abridged. Visit the journal home page for the full abstract. KWSUSAN refers to kernelized weighted SUSAN."
    },
    {
      "id": "aeda1635-7698-4a10-9aa5-aca0eb487879",
      "adv_source_id": "aeda1635-7698-4a10-9aa5-aca0eb487879",
      "source_id": "33990fb9-7202-4e37-8d52-b1df461a4951",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\" from arxiv.org:",
      "generation": " Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\n\nWhile neural factorization models can express complex distributed computations,\nmost inference algorithms for structural SVM relax the modular structure\nimposed by these models, resulting in a factor graph that is not\nnecessarily sparse or tree-like. In this paper, we propose a new\nalgorithm for exact inference that respects the given factorization\nstructure, and show that the resulting graph has a tree-like structure;\ni.e., can be written as an oriented tree. We then use this inference\nalgorithm as the basis for a new algorithm for learning the weights of\nthe factors in a neural factorization machine. The experimental results\nshow that our method compares favorably with state-of-the-art\nalgorithms for learning neural factorization machines.\n\nKeywords: Neural Factorization Machines, Structural SVM, Graphical Models,\nTree-based Learning."
    },
    {
      "id": "b70e42f3-7077-4c53-9199-a455d5efbd1e",
      "adv_source_id": "b70e42f3-7077-4c53-9199-a455d5efbd1e",
      "source_id": "33990fb9-7202-4e37-8d52-b1df461a4951",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\" from arxiv.org:",
      "generation": " Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\n\nWhile neural factorization models have demonstrated strong performance in a variety of supervised learning tasks, they generally require labeled information to learn the neural biases that factorize the input data. In this paper, we overcome this limitation by introducing an algorithmic formulation of structural SVM that integrates inference and learning of neural biases within a single optimization procedure, even in the absence of labels. In particular, our approach leverages the structural SVM framework, which optimizes an inference network to directly predict the latent factors of the data; learns a neural factorization model that approximates the inference network; and performs online learning of the neural biases in the factorization model. We demonstrate the effectiveness of our approach on a series of benchmark datasets, and show that it can improve upon the state of the art in neural factorization models across a variety of tasks, including image restoration, autoencoding, and super-resolution."
    },
    {
      "id": "ec0999d5-a295-427d-a6f2-8aad05f0d79c",
      "adv_source_id": "ec0999d5-a295-427d-a6f2-8aad05f0d79c",
      "source_id": "df3be76c-8027-4379-a2d3-1f92e94a7842",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Graph entropies in texture segmentation of images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Graph entropies in texture segmentation of images\" from arxiv.org:",
      "generation": " Graph entropies: recent progress in estimating graph entropies from data and applications in image processing is surveyed. Graph entropy is a measure of randomness in graph structures. Various entropy estimates are reviewed, including the spectral entropy, the distance-based entropy (also known as cut-entropy), and the recently developed interference-based entropy. Applications of graph entropies in image processing, such as image segmentation and edge-preserving image filtering, are discussed. In particular, a new edge-preserving smoothing filter based on the estimated cut-entropy is presented. The new filter is shown to be effective in reducing the noise in the image while preserving the edges. The relationship between the graph entropy and the graph Laplacian is also discussed.\n\n Image segmentation remains a challenging problem in image processing, despite the considerable progress made in the last few decades [1, 2]. No single segmentation algorithm works well for all types of images. Most segmentation algorithms assume that the image pixels are related to each other in a smooth and gradual way, and that the boundaries of the objects in the image can be described by a set of smooth curves. However, this assumption is not always valid, and there are many images where the boundaries of the objects are abrupt and noisy, and the pixels are not related to each other in a smooth way. For such images, traditional segmentation algorithms often fail to produce satisfactory results.\nIn recent years, there has been considerable interest in the use of entropy measures in image processing [3-5]. Entropy measures can be used to quantify the randomness or uncertainty in an image, and this information can be used to improve the performance of image segmentation algorithms. In [6], it is shown that the entropy of an image can be estimated directly from the image data, without any prior knowledge of the image or the objects in it. This estimate of the entropy is based on the observation that the entropy of an image is equal to the average entropy of the pixels in the image. The entropy of a pixel is defined as the average entropy of the pixels in its neighborhood. This neighborhood entropy can be estimated from the image data, using a distance-based approach. The distance-based approach is based on the observation that the entropy of a pixel is proportional to the average distance between the pixel and its neighbors. A fast algorithm for estimating the neighborhood entropy is presented, based on the use of a binary tree. This algorithm is shown to be effective in reducing the computation time required for estimating the entropy of an image."
    },
    {
      "id": "97ab1947-6d8a-498c-b644-133badeb2e93",
      "adv_source_id": "97ab1947-6d8a-498c-b644-133badeb2e93",
      "source_id": "df3be76c-8027-4379-a2d3-1f92e94a7842",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Graph entropies in texture segmentation of images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Graph entropies in texture segmentation of images\" from arxiv.org:",
      "generation": " In this paper, we propose a new approach for texture segmentation of images based on graph entropies. Our method combines the advantages of both texture and entropy, and it is applicable to a wide range of images. We utilize the concept of graph entropy, which is a measure of the randomness or uncertainty in an image, to capture the texture information in an image. Our approach is based on the observation that images with high graph entropy are characterized by a high degree of randomness or uncertainty, which is often associated with texture. We propose a novel algorithm that efficiently computes the graph entropy of an image, and we demonstrate that our approach is effective for texture segmentation of images. Our experimental results show that our method outperforms the state-of-the-art approaches for texture segmentation.\n\nFurthermore, our approach is computationally efficient and suitable for real-time applications. Our experimental results show that our method is significantly faster than the state-of-the-art approaches for texture segmentation while maintaining a high level of accuracy.\nWe believe that our approach has the potential to revolutionize the field of texture segmentation and it has many practical applications in areas such as image processing, computer vision, and pattern recognition."
    },
    {
      "id": "66753da0-d183-4046-b5a4-07a997eaf0be",
      "adv_source_id": "66753da0-d183-4046-b5a4-07a997eaf0be",
      "source_id": "630ef64e-3295-4eaf-b366-69b602196016",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation\" from arxiv.org:",
      "generation": " In this paper, we present a new image segmentation method based on active contours without edges. Unlike the original active contour without edges model, we propose to use a regional active contour model, which can handle the ambiguity of the initial contour placement in the image without edges. Our method is based on the variational level set framework, in which we introduce a new energy functional combining both the regional and the traditional smoothness terms. The regional term encourages the contour to partition the image into spatially coherent regions, while the smoothness term enforces the contour to be smooth and compact. To optimize the energy functional, we propose to use a new gradient descent algorithm which is specifically designed to handle the regional term. In order to improve the efficiency of our method, we introduce a machine learning component to approximate the regional term in the energy functional. In particular, we train a fully connected neural network to predict the regional term from the gradient of the smoothed image. We show that our method is able to handle various types of images, and can produce spatially coherent segmentations. Our method is evaluated on the public available Cityscapes dataset, and compared with four state-of-the-art image segmentation methods. The experimental results show the advantages of our method over the competitors in terms of segmentation quality."
    },
    {
      "id": "17852138-5323-434f-906a-86f06aefd036",
      "adv_source_id": "17852138-5323-434f-906a-86f06aefd036",
      "source_id": "630ef64e-3295-4eaf-b366-69b602196016",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation\" from arxiv.org:",
      "generation": " In this paper, we present a new approach for image segmentation based on active contours without edges. The energy of the system is defined by a regional term that keeps the regions created by the algorithm soft and fuzzy, and avoids the generation of erroneous hard regions. The regional term is combined with a variational level set method that efficiently propagates the contours in the image, and with a machine learning term that takes advantage of the properties of modern loss functions to impose constraints on the solutions. The approach is evaluated on the Berkeley Segmentation Dataset, obtaining promising results when compared with the state of the art."
    },
    {
      "id": "ce5d26a3-d16f-427b-aee6-f3087323d325",
      "adv_source_id": "ce5d26a3-d16f-427b-aee6-f3087323d325",
      "source_id": "56166659-ec7a-41d0-9396-fd6abfdf7861",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deeply Learning the Messages in Message Passing Inference",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deeply Learning the Messages in Message Passing Inference\" from arxiv.org:",
      "generation": " The \"message passing\" paradigm is widely used for inference in graphical models, but the messages themselves have usually been treated as auxiliary variables rather than first-class citizens. In this paper, we show that the messages carry predictive information that can be leveraged by a neural network to improve inference. In particular, we propose a \"message passing neural network\" (MPNN) that combines the strengths of graphical models and deep learning. Our MPNN uses the messages from a graphical model as input to a neural network that performs inference in a different, potentially larger, graphical model. Since the MPNN can be trained end-to-end, it is able to \"learn\" good messages that improve performance. We demonstrate the effectiveness of our approach on two canonical inference problems: graphical model inference and natural language inference. Our results show that the MPNN achieves state-of-the-art performance on a variety of datasets."
    },
    {
      "id": "168ff38c-7ccc-4d92-9238-f231ed98642a",
      "adv_source_id": "168ff38c-7ccc-4d92-9238-f231ed98642a",
      "source_id": "56166659-ec7a-41d0-9396-fd6abfdf7861",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deeply Learning the Messages in Message Passing Inference",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deeply Learning the Messages in Message Passing Inference\" from arxiv.org:",
      "generation": " The \"message passing\" inference algorithm unfolds recursively in discrete time steps as a dynamic programming algorithm, with the message-passing queries at each time step depending on the current \"messages\" from all other variables. We show that each recursive iteration can be formulated as a discrete-time dynamical system of nonlinear recursive equations, with an attractor that characterizes the posterior distribution. This dynamical systems view offers a new interpretation of the recurrent \"belief propagation\" algorithm, and new insights into the deeply learning approaches recently proposed for distributed inference."
    },
    {
      "id": "cda3974a-6b68-43f7-938b-76ce765e60cb",
      "adv_source_id": "cda3974a-6b68-43f7-938b-76ce765e60cb",
      "source_id": "36db5def-75bc-48cc-9d66-1e673e563773",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Approach Towards Clustering Based Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Novel Approach Towards Clustering Based Image Segmentation\" from arxiv.org:",
      "generation": " Clustering based image segmentation is an important problem in digital image processing. In this paper, we propose a novel approach towards clustering based image segmentation. Our approach is based on a new clustering algorithm that we call the Generalized Median Clustering (GMC). The GMC is a variation of the K-Means clustering algorithm. It is designed to work well with high-dimensional data such as images. We evaluate the performance of our approach using a variety of images. The results show that our approach is effective in segmenting images.\n\nWe also present a variation of our approach that is based on a new clustering algorithm called the Median-Ball Clustering (MBC). The MBC is a variation of the GMC. It is designed to work well with sparse data such as medical images. We evaluate the performance of our sparse data variation using a variety of medical images. The results show that our sparse data variation is effective in segmenting medical images."
    },
    {
      "id": "00371802-d04b-446e-854e-9812ccf32f7b",
      "adv_source_id": "00371802-d04b-446e-854e-9812ccf32f7b",
      "source_id": "36db5def-75bc-48cc-9d66-1e673e563773",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Approach Towards Clustering Based Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Novel Approach Towards Clustering Based Image Segmentation\" from arxiv.org:",
      "generation": " Improved image segmentation techniques are necessary to achieve better medical imaging analysis. In this paper, we propose a novel approach towards clustering based image segmentation. Image segmentation is a process of grouping pixels that belong to the same object in an image. This paper focuses on the segmentation of cardiac images. Our approach is to utilize an entropy based technique along with a clustering algorithm for the purpose of image segmentation. Our proposed method utilizes the concept of entropy, which is a measure of the randomness or uncertainty of a signal. The proposed method is evaluated using simulated as well as real world images. The performance of the proposed method is compared to existing methods using metrics such as accuracy, precision, recall and F1 score. The experimental results demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "df4fceb0-097a-47ec-a617-22a68e62c9d0",
      "adv_source_id": "df4fceb0-097a-47ec-a617-22a68e62c9d0",
      "source_id": "5bd44b68-d473-4224-82a7-b1023155c1ba",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Learning for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Deep learning has shown promising results for medical image segmentation, but adopting deep learning models in clinical practice remains challenging. In this paper, we survey the state-of-the-art deep learning techniques specific to medical image segmentation and discuss the challenges and opportunities for clinical adoption. We first provide an overview of the most commonly used deep learning architectures, including fully convolutional networks (FCNs), U-Net, and Mask R-CNN. We then survey the most commonly used loss functions, including pixel-wise loss, contour loss, and shape prior loss. We then discuss the challenges of deep learning for medical image segmentation, including data scarcity, annotation difficulty, model size, and computational requirements. We conclude by discussing the opportunities for future research, including multi-modal segmentation, semi-supervised learning, and domain generalization."
    },
    {
      "id": "86c9ca7f-4637-4914-a719-94fa655a052e",
      "adv_source_id": "86c9ca7f-4637-4914-a719-94fa655a052e",
      "source_id": "5bd44b68-d473-4224-82a7-b1023155c1ba",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Medical Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Deep Learning for Medical Image Segmentation\" from arxiv.org:",
      "generation": " Recent advances in deep learning have led to significant improvements in the field of medical image segmentation. These improvements have paved the way for the development of accurate and efficient segmentation algorithms that can assist doctors in various tasks such as diagnosis, treatment planning, and image-guided surgery.\n\nIn this paper, we review the recent advances in deep learning for medical image segmentation and discuss the advantages and limitations of these methods. We also present a comprehensive overview of the most commonly used deep learning architectures for medical image segmentation, including fully connected networks, convolutional neural networks, and recurrent neural networks.\n\nFurthermore, we explore the application of deep learning for the segmentation of medical images in various domains, such as radiology, pathology, and surgery. We also discuss the challenges and opportunities for the future development of deep learning algorithms for medical image segmentation.\n\nOur goal is to provide a comprehensive overview of the current state of the art in deep learning for medical image segmentation and to highlight the potential of these methods for assisting doctors in various medical tasks."
    },
    {
      "id": "bf4eeb2a-0144-4960-bca4-6a88a4a992dd",
      "adv_source_id": "bf4eeb2a-0144-4960-bca4-6a88a4a992dd",
      "source_id": "00bc54c0-632d-49a1-afec-0b813da77a46",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Evolving Fuzzy Image Segmentation with Self-Configuration",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Evolving Fuzzy Image Segmentation with Self-Configuration\" from arxiv.org:",
      "generation": " Abstract\nTraditional image segmentation methods produce binary images, which\nmay not reflect the fuzzy regions in the original image.\nFuzzy segmentation methods can produce more accurate\nsegmentation results, but are often computationally expensive\nand difficult to use. In this paper, we present a new method\nfor fuzzy image segmentation that is both efficient and easy\nto use. Our method is based on a self-configuring fuzzy\nsegmentation algorithm that can evolve to produce a good\nsegmentation of the image. The efficiency of the algorithm\ncomes from the fact that it only produces the necessary number\nof segments, rather than segmenting the entire image. The\nease of use of our method comes from the fact that it is a\nfully automatic algorithm that does not require any user\ninteraction. We present results showing that our method\ncan produce good segmentations of a variety of images, and\nthat it is faster than other fuzzy segmentation methods.\n\nIntroduction\nImage segmentation is the process of dividing an image into\nmultiple segments. Traditional image segmentation methods\nproduce binary images, where each pixel is either classified\nas belonging to a segment or not. Fuzzy segmentation methods\nproduce images where each pixel is classified as belonging to a\nsegment with a certain degree of confidence.\n\nFuzzy segmentation methods have the potential to produce more\naccurate segmentations than traditional binary segmentation\nmethods, as they can capture the fuzzy regions in the image\nthat may not be well-defined by a binary segmentation. However,\nfuzzy segmentation methods can be computationally expensive\nand difficult to use.\n\nIn this paper, we present a new method for fuzzy image\nsegmentation that is both efficient and easy to use. Our method\nis based on a self-configuring fuzzy segmentation algorithm\nthat can evolve to produce a good segmentation of the image.\nThe efficiency of our algorithm comes from the fact that it\nonly produces the necessary number of segments, rather than\nsegmenting the entire image. The ease of use of our method\ncomes from the fact that it is a fully automatic algorithm\nthat does not require any user interaction.\n\nOur method consists of two main steps: fuzzy clustering and\nsegmentation. In the fuzzy clustering step, we divide the image\ninto a number of clusters based on the similarity of the pixels\nin the image. In the segmentation step, we use a segmentation\nalgorithm to divide the image into a number of segments based\non the fuzzy clustering.\n\nThe self-configuration"
    },
    {
      "id": "8f7d9e7f-381e-4d0b-abd7-5ab669a2263b",
      "adv_source_id": "8f7d9e7f-381e-4d0b-abd7-5ab669a2263b",
      "source_id": "00bc54c0-632d-49a1-afec-0b813da77a46",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Evolving Fuzzy Image Segmentation with Self-Configuration",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Evolving Fuzzy Image Segmentation with Self-Configuration\" from arxiv.org:",
      "generation": " Abstract\u2014 This paper presents a novel fuzzy image segmentation method that evolves morphological components endowed with fuzzy logic output, namely, fuzzy pixels (FPs). The components evolve in a double dynamic process toward stable configurations: iterative feedback loops of segmentation and self-configuration. In each iteration, the evolving process updates the fuzzy logic output of FPs based on neighborhood relations of the components, making them more coherent. The self-configuration process acts as a top-down influence on the segmentation, providing regions with prior knowledge that can be dynamically modified throughout the evolutionary process. The proposed method is evaluated on a database of crowded images, and results show the advantages of the proposed approach compared to state-of-the-art techniques in terms of segmentation quality and speed."
    },
    {
      "id": "d09398ec-5901-4c78-88a7-ee4c52672606",
      "adv_source_id": "d09398ec-5901-4c78-88a7-ee4c52672606",
      "source_id": "1e2a9ff9-abdb-4a71-b656-3dcefbf0faab",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bethe Learning of Conditional Random Fields via MAP Decoding",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bethe Learning of Conditional Random Fields via MAP Decoding\" from arxiv.org:",
      "generation": " We consider the problem of learning conditional random fields (CRFs) via the Bethe approximation. Previous work has shown that the Bethe free energy has a simple expression in terms of marginals of the CRF, and that these marginals can be efficiently computed via the forward-backward algorithm. However, computing the free energy still requires performing summations over the entire CRF, which can be computationally expensive. In this paper, we propose an alternative expression for the Bethe free energy based on the concept of message passing, and show that it can be efficiently computed via the map decoding algorithm. Our approach is based on the observation that the Bethe free energy can be viewed as the difference between the marginal likelihoods of two CRFs, one in which all variables are conditioned on, and one in which a subset of variables is conditioned on. We show that the map decoding algorithm can be used to efficiently compute these marginal likelihoods, and that the difference between them corresponds to the Bethe free energy. Our experiments show that on a standard CRF benchmark, our approach is faster and more accurate than the previously proposed method based on the forward-backward algorithm."
    },
    {
      "id": "b642bfbd-0b1a-4e4b-a875-4eba1bfe91a0",
      "adv_source_id": "b642bfbd-0b1a-4e4b-a875-4eba1bfe91a0",
      "source_id": "1e2a9ff9-abdb-4a71-b656-3dcefbf0faab",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bethe Learning of Conditional Random Fields via MAP Decoding",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Bethe Learning of Conditional Random Fields via MAP Decoding\" from arxiv.org:",
      "generation": " We consider the problem of learning conditional random fields (CRFs) via the Bethe approximation. The Bethe algorithm approximates the probability of observed events using a belief propagation algorithm on a factor graph. We derive a new expression for the error made by the Bethe approximation in terms of a gap between the marginal distributions of the CRF and its Bethe approximation. This gap can be made arbitrarily small by increasing the number of iterations of the Bethe algorithm. However, in the context of maximum a posteriori (MAP) decoding, only a finite number of iterations is performed. We derive an upper bound on the error made by the MAP decoder in the finite iteration regime, and show that the error vanishes as the number of iterations goes to infinity. The result implies that the MAP decoder can be viewed as the limit of the Bethe algorithm as the number of iterations goes to infinity, and is therefore guaranteed to converge to the global optimum. Moreover, we show that the gradient of the marginal distributions can be efficiently computed, which enables the use of gradient-based optimization methods such as L-BFGS or Newton's method. The proposed method is evaluated on a number of benchmark CRFs, and is shown to achieve significantly better performance than the popular MRF-EM algorithm."
    },
    {
      "id": "30859e5a-d74f-4a24-87c4-8cbf390e3ad8",
      "adv_source_id": "30859e5a-d74f-4a24-87c4-8cbf390e3ad8",
      "source_id": "0efbebca-d6fa-4e22-848c-d880b538be06",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Constraint Propagation for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Constraint Propagation for Image Segmentation\" from arxiv.org:",
      "generation": " We present a fast constraint propagation algorithm for image segmentation based on label fusion of watershed segments. The approach is motivated by the shape prior of watershed segments, which reflects the local connectivity of the image pixels. The proposed algorithm efficiently computes a score for each pixel based on the constraints from the watershed segmentation and fuses the scores of neighboring segments to obtain the final segmentation. We demonstrate the effectiveness of the proposed approach on a variety of images, and show that it produces a segmentation that is more accurate and coherent compared to existing algorithms. The proposed algorithm is fast enough to be used in real-time applications.\n\nIntroduction: \nImage segmentation involves assigning a label to each pixel in an image, thereby separating the image into distinct regions. A number of techniques have been proposed for image segmentation, but most of them require some prior knowledge of the image to produce good results. In this paper, we present a novel approach to image segmentation that is fast, automatic, and requires no prior knowledge of the image. The proposed method is based on the watershed transformation, which has been widely used for image segmentation. However, the watershed transformation can be slow and may produce over-segmentation. The proposed approach addresses these issues by combining the watershed transformation with a fast constraint propagation algorithm. The constraint propagation algorithm is based on the local connectivity of pixels, and it efficiently computes a score for each pixel based on the constraints from the watershed segmentation. The scores of neighboring segments are then fused to obtain the final segmentation. The proposed algorithm is fast enough to be used in real-time applications, and it produces a segmentation that is more accurate and coherent compared to existing algorithms.\n\nApproach: \nThe approach is based on the watershed transformation, which is a local segmentation method that assigns labels to pixels based on their local maxima. The watershed transformation can be slow and may produce over-segmentation, which can be addressed by combining it with a fast constraint propagation algorithm. The constraint propagation algorithm is based on the local connectivity of pixels, and it efficiently computes a score for each pixel based on the constraints from the watershed segmentation. The scores of neighboring segments are then fused to obtain the final segmentation. The proposed algorithm is compared to existing algorithms on a variety of images, and it is shown to produce a segmentation that is more accurate and coherent. The proposed algorithm is also fast enough to be used in real-time applications.\n\nResults: \nThe proposed algorithm is evaluated on a variety of images, and it is shown to produce a segmentation that is more accurate"
    },
    {
      "id": "6bb7dba6-ea8a-4c0d-885c-79eb0d39db90",
      "adv_source_id": "6bb7dba6-ea8a-4c0d-885c-79eb0d39db90",
      "source_id": "0efbebca-d6fa-4e22-848c-d880b538be06",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Constraint Propagation for Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Constraint Propagation for Image Segmentation\" from arxiv.org:",
      "generation": " We present a fast algorithm for constraint propagation in image segmentation. Our algorithm is based on a novel residual term formulation of the continuous bilevel optimization framework. This formulation permits the efficient solution of a simplified optimization problem, from which the constraint propagation algorithm easily derives. We demonstrate the efficacy of our approach on multiple segmentation benchmarks, achieving speeds of more than 4x and improving boundary precision over the state-of-the-art."
    },
    {
      "id": "55be508b-12b0-436e-b7ac-a7040c62f5bc",
      "adv_source_id": "55be508b-12b0-436e-b7ac-a7040c62f5bc",
      "source_id": "5c337fac-1159-4849-a09e-aad39334e431",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks\" from arxiv.org:",
      "generation": " This paper proposes an unsupervised image segmentation method based on global and local criteria optimization. It consists in minimizing an energy function which takes into account both gradient and similarity terms through a Bayesian network. This network models the spatial dependence between pixels, and allows the use of various criteria at different scales. We perform an iterative optimization of the energy function, which alternately minimizes a gradient descent and a similarity term. The optimization of the latter is performed by propagating evidence through the Bayesian network. This method does not require any initialization or parameter tuning, and allows to obtain competitive results compared to state-of-the-art unsupervised segmentation methods, while being simpler and faster."
    },
    {
      "id": "80930551-a261-4b47-857f-40fb703f571b",
      "adv_source_id": "80930551-a261-4b47-857f-40fb703f571b",
      "source_id": "5c337fac-1159-4849-a09e-aad39334e431",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks\" from arxiv.org:",
      "generation": " We propose an unsupervised image segmentation method based on Bayesian networks. Firstly, a segmentation mask is obtained by optimizing local criteria functions using a genetic algorithm. Secondly, the initial segmentation mask is modified based on a global criterion, which encourages the segmentation regions to have a coherent size and location. Finally, the resulting segmentation is refined by an edge optimization process to achieve more accurate object boundaries. The experiments show that the proposed method produces promising results even when compared with other state-of-the-art supervised segmentation algorithms."
    },
    {
      "id": "9d04b342-898c-4033-b8d3-7949922278c4",
      "adv_source_id": "9d04b342-898c-4033-b8d3-7949922278c4",
      "source_id": "a29d1548-e47d-4a63-b18d-706267efdf14",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification\" from arxiv.org:",
      "generation": " We present a technique for the forward and backward propagation of convolutional neural networks that is optimized for pixelwise classification tasks. Our method achieves a significant speedup compared to standard techniques, with a negligible reduction in accuracy. We present results showing that our method can speed up the training process by as much as a factor of two, with no loss in accuracy. We also show that our method can be used to improve the performance of existing convolutional neural networks, resulting in a significant increase in accuracy with only a small increase in training time. Our method is based on a simple yet effective technique for optimizing the forward and backward propagation of convolutional neural networks. We believe that our method will be useful for many applications, including the training of large-scale convolutional neural networks."
    },
    {
      "id": "7920dc2f-ddb0-42fa-9340-d8971c2d8d8e",
      "adv_source_id": "7920dc2f-ddb0-42fa-9340-d8971c2d8d8e",
      "source_id": "a29d1548-e47d-4a63-b18d-706267efdf14",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification\" from arxiv.org:",
      "generation": " We present a technique to reduce the computational requirements of convolutional neural networks (CNNs) for pixel-wise prediction tasks. Our method forms the basis of a tool that can quickly process HD images in less than 100 ms. This makes it applicable for the interactive visualization of CNN-based results in contexts such as image editing, thereby opening new avenues for creative image manipulation. We achieve this speed by performing forward and backward propagation in a highly efficient manner, allowing for a factor of more than 200 speedup compared to state-of-the-art deep image editing systems. In this work, we present a thorough analysis of the memory access patterns during inference, which reveals that a significant speedup can be obtained by rearranging the memory access pattern using cached circular buffering. We present extensive experiments with various CNN architectures, including VGG, GoogLeNet, and AlexNet. Our source code is available at https://github.com/skiy/fastconv."
    },
    {
      "id": "f3b37b81-b454-426d-8995-a6bc85b2524e",
      "adv_source_id": "f3b37b81-b454-426d-8995-a6bc85b2524e",
      "source_id": "a99184a4-6066-4778-99ed-4f3c9d98ba83",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Edge Detection Using Structured Forests",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Edge Detection Using Structured Forests\" from arxiv.org:",
      "generation": " Fast Edge Detection Using Structured Forests\n\nEdge detection is a fundamental step in many computer vision applications, but it is also a challenging task. In this paper, we present a new edge detection method based on structured forests. Our approach is fast, accurate, and easy to use. We achieve this by combining the advantages of both the structured and the random forests. The structured forest is used to extract the edges from the image, while the random forest is used to remove the noise and to improve the accuracy of the edge detection. Our method is fast because it uses a parallel implementation of the structured forest, which allows us to process multiple images at once. We demonstrate the performance of our method on a variety of images, and show that it is competitive with state-of-the-art edge detection methods."
    },
    {
      "id": "f850f6a9-6ac7-43fe-a2d8-034ce0da5c9a",
      "adv_source_id": "f850f6a9-6ac7-43fe-a2d8-034ce0da5c9a",
      "source_id": "a99184a4-6066-4778-99ed-4f3c9d98ba83",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Edge Detection Using Structured Forests",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Fast Edge Detection Using Structured Forests\" from arxiv.org:",
      "generation": " Fast Edge Detection Using Structured Forests\n\nWe present an efficient image-based edge detection method that benefits from structural information included in the definition of a structured forest. We exploit the rotation invariant robustness of the Harris corner detection operator together with a preprocessing step based on a Gaussian pyramid to build a strong edge detector based on the notion of integral images. The resulting detector is extremely fast, and is shown to compete with other state-of-the-art techniques, while providing better results. Validation is done quantitatively and visually on two standard datasets, showing the efficiency of the proposed method."
    },
    {
      "id": "321e60d7-df2d-4908-a6b9-e7af72bf4a52",
      "adv_source_id": "321e60d7-df2d-4908-a6b9-e7af72bf4a52",
      "source_id": "7fdb0b4a-1cf4-4a4d-8ab5-c221c8e1ada9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Open-set Person Re-identification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Open-set Person Re-identification\" from arxiv.org:",
      "generation": " Person re-identification (ReID) has achieved remarkable progress in recent years with the advancement of deep learning. However, the existing ReID methods are typically trained and tested under a closed-set scenario, where all the persons in the training set also appear in the test set, which restricts their applicability in real-world scenarios. In this paper, we tackle the open-set person ReID problem, where the test set may contain persons never seen during training. Specifically, we propose a novel open-set aware feature embedding module (OFEM) to augment existing ReID models for open-set ReID. In OFEM, we adopt an effective dropout technique to enhance the generalization ability of ReID models. In addition, we propose a novel open-set ranking loss to explicitly enforce the model to learn the open-set information. Furthermore, a simple yet effective ensemble method is proposed to boost the performance of open-set ReID. We evaluate OFEM on three open-set person ReID benchmarks, and achieve superior performance over the state-of-the-art."
    },
    {
      "id": "23703251-7751-4236-97bc-35a25e4ad6b2",
      "adv_source_id": "23703251-7751-4236-97bc-35a25e4ad6b2",
      "source_id": "7fdb0b4a-1cf4-4a4d-8ab5-c221c8e1ada9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Open-set Person Re-identification",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Open-set Person Re-identification\" from arxiv.org:",
      "generation": " Person re-identification (ReID) has achieved remarkable progress in recent years with the development of deep learning. However, existing ReID methods focus on recognizing persons across different cameras in a closed-set scenario, where only a fixed set of persons of interest is given. In this work, we tackle the open-set person re-identification (OpenSet-ReID) problem, where the set of persons in the gallery set is not restricted to the training set, and persons never seen during training can appear in the gallery set. This setting is more practical as it can handle scenarios where the identity information of persons is not available during training. To address the OpenSet-ReID problem, we propose a novel triplet-based metric learning framework with a discriminative embedding quality (DEQ) loss. The DEQ loss can effectively mitigate the gap between the distributions of person images in the training and gallery sets, which is induced by the open-setness. Moreover, we introduce a tri-linear refinement network to obtain more accurate and robust representations for OpenSet-ReID. Our approach achieves state-of-the-art performance on three benchmark datasets for open-set person re-identification."
    },
    {
      "id": "21202de0-3b7e-4b61-b4ed-4f013046f01b",
      "adv_source_id": "21202de0-3b7e-4b61-b4ed-4f013046f01b",
      "source_id": "59dd570d-3eda-49b1-85aa-fa94abaabaec",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation\" from arxiv.org:",
      "generation": " We present a method for fast pathological lung segmentation directly in thoracic CT scans, based on optimal sampling of keypoints. Our method is tailored to the typically large amount of noise and artefacts in pathological scans. At its core is a new algorithm for near-optimal keypoint sampling, i.e., approximately minimizing an upper bound on the expected segmentation error. This is achieved by a greedy algorithm that iteratively selects the most informative keypoint to sample, i.e., the one that guarantees the largest decrease in the upper bound. We show that our approach is fast, typically requiring under 5 minutes of computation time on a single core, thereby allowing for interactivity. We demonstrate through extensive experiments on real patient data that our method is accurate, successfully segmenting 95% of lung voxels on average at an under 5% false positive rate, and outperforming state-of-the-art segmentation methods."
    },
    {
      "id": "a7b6d967-2f61-485b-9c37-2825e043ac38",
      "adv_source_id": "a7b6d967-2f61-485b-9c37-2825e043ac38",
      "source_id": "59dd570d-3eda-49b1-85aa-fa94abaabaec",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation\" from arxiv.org:",
      "generation": " We present a method for fast pathological lung segmentation directly in inked chest radiographs (ICRs), which are routinely generated for lung cancer screening. Unlike previous methods that rely on additional information like lung segmentation in raw chest radiographs (CRCs) or ICRs, our approach works directly on ICRs and achieves near-optimal segmentation results. Our key insight is that, given a pre-defined set of keypoints, the optimal segmentation can be obtained by assigning the label of the dominant component of each keypoint region. Thus, we frame the segmentation as a binary labeling problem in which each keypoint region is labeled based on the dominant component in the region. To enable near-optimal segmentation, we propose an efficient keypoint sampling strategy that generates a sparse set of keypoints that closely approximate the optimal set, and a computational efficient algorithm to solve the resulting labeling problem. When evaluated on 50 ICRs from the publicly available NLST dataset, our method produces segmentation results with an average approximation ratio of 0.019 and an average runtime of 68.4s per case."
    },
    {
      "id": "c39e1b2c-7db6-45da-a95c-e81769caf279",
      "adv_source_id": "c39e1b2c-7db6-45da-a95c-e81769caf279",
      "source_id": "51d16092-4f0c-4c73-9d5e-13fc713cb941",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating Near-Infrared Information into Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Incorporating Near-Infrared Information into Semantic Image Segmentation\" from arxiv.org:",
      "generation": " Semantic image segmentation is the process of classifying each pixel in an image into one of a pre-defined set of classes, such as sky, road, car, pedestrian. We introduce a novel approach for semantic image segmentation that incorporates near-infrared (NIR) information into the process. Our approach uses a deep learning architecture called a U-Net, which is designed to take advantage of the multi-scale contextual information in the image. We train our U-Net on a large dataset of images that includes both visible wavelength and NIR images, and we evaluate its performance on the Cityscapes dataset. Our results show that incorporating NIR information into the segmentation process can improve the accuracy of the segmentation, especially in cases where the visible wavelength images are unclear or ambiguous. We also find that our approach is able to generalize well to images that were not in our training dataset, demonstrating the effectiveness of our approach in real-world applications."
    },
    {
      "id": "2cc604aa-acc4-4a0f-827f-85ca6578bd05",
      "adv_source_id": "2cc604aa-acc4-4a0f-827f-85ca6578bd05",
      "source_id": "51d16092-4f0c-4c73-9d5e-13fc713cb941",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating Near-Infrared Information into Semantic Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Incorporating Near-Infrared Information into Semantic Image Segmentation\" from arxiv.org:",
      "generation": " In this paper, we present a semantic image segmentation approach that utilizes near-infrared (NIR) information. We propose a simple yet effective way to incorporate NIR information into a fully convolutional network (FCN) by concatenating the NIR channel with the red, green, and blue (RGB) channels. Additionally, we introduce a novel NIR-weighted smoothing module that selectively smoothes predictions in poorly-segmented regions while retaining boundaries in well-segmented regions. This module effectively utilizes the NIR channel to highlight regions that require special attention. We evaluate our method on the Cityscapes and CamVid datasets, and demonstrate that our approach can improve segmentation performance, especially in challenging cases. Our code is made publicly available at https://github.com/HMADUS/NIR- Semantic-Segmentation."
    },
    {
      "id": "9c774562-25f7-4096-8563-c68bde6d8fed",
      "adv_source_id": "9c774562-25f7-4096-8563-c68bde6d8fed",
      "source_id": "5c9dc6c5-d804-48a3-8309-5c671883d770",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Variational Image Segmentation Model Coupled with Image Restoration Achievements",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Variational Image Segmentation Model Coupled with Image Restoration Achievements\" from arxiv.org:",
      "generation": " In this paper, we propose a variational image segmentation model coupled with image restoration achievements. Firstly, we adopt a variational model to achieve image segmentation, and the model is based on the Mumford-Shah functional. Secondly, we couple the proposed model with image restoration achievements for improving the segmentation performance. Thirdly, we propose a fast algorithm to solve the proposed model efficiently. Finally, we perform comprehensive experiments on synthetic and real-world images to validate the effectiveness of the proposed model. The results demonstrate that the proposed model is able to produce high-quality segmentation results."
    },
    {
      "id": "5bccdfbf-2501-47a6-8092-8daa6a7e5090",
      "adv_source_id": "5bccdfbf-2501-47a6-8092-8daa6a7e5090",
      "source_id": "5c9dc6c5-d804-48a3-8309-5c671883d770",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Variational Image Segmentation Model Coupled with Image Restoration Achievements",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Variational Image Segmentation Model Coupled with Image Restoration Achievements\" from arxiv.org:",
      "generation": " An image segmentation method is proposed in this paper, which aims to segment an image with uneven illumination and complex background. In order to solve these problems, we propose a variational model which contains an adaptive region expansion and a region-based image restoration term. The restoration term can not only make the segmentation result more satisfactory, but also make the uneven illumination of the image uniform. Compared with the state-of-the-art image segmentation methods, the proposed method can effectively segment the image with complex background and uneven illumination. Moreover, in order to get the optimal solution of the proposed model, we adopt an efficient algorithm to solve it alternatively, which consists of a gradient descent algorithm and a Gauss-Seidel iteration algorithm. Finally, we perform comprehensive experimental evaluations on a number of benchmark images to demonstrate the effectiveness of the proposed approach."
    },
    {
      "id": "d5e3083d-8bbd-4b3b-ae44-fa6924606a70",
      "adv_source_id": "d5e3083d-8bbd-4b3b-ae44-fa6924606a70",
      "source_id": "34276620-b1c3-4c30-97d5-2f222f881076",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies\" from arxiv.org:",
      "generation": " We consider the problem of making a minimum number of moves to transform a given initial configuration of objects into a target configuration, where each move may involve flipping an edge between two objects (e.g., swapping the two objects). This may model, for example, inserting a barrier between two objects in a network, or moving them closer together. We seek a solution that minimizes an energy function that generalizes the submodular pairwise energies, and is defined using a symmetric and monotone set function that is not necessarily submodular. This energy may model, for example, the total length of the edges involved in the moves.\n\nWe first present a polynomial-time algorithm that outputs an optimal solution when the energy function is submodular. This algorithm extends a recent algorithm of Feige, Krause, Naor, and Ron for the special case in which the energy function is the sum of edge weights. Our algorithm is based on a novel technique for handling the general submodular set function in an efficient way, by using a greedy tiered approach that iteratively adds subsets of nodes to the solution.\n\nWe then present an algorithm for the general case that has a approximation guarantee of $(1 - 1/e)$, by reducing the problem to a convex optimization problem. This approximation guarantee is the best possible in the non-submodular case, and nearly matches the approximation guarantee of the known algorithm for the submodular case."
    },
    {
      "id": "6e17ab82-d7d2-4e21-b300-b4e8b30dbb29",
      "adv_source_id": "6e17ab82-d7d2-4e21-b300-b4e8b30dbb29",
      "source_id": "34276620-b1c3-4c30-97d5-2f222f881076",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies\" from arxiv.org:",
      "generation": " We consider the setting where one wants to solve a minimization problem involving a general non-submodular energy function, subject to linear constraints. We present an algorithm that computes an approximate solution, and we provide a theoretical analysis of its performance in terms of the approximation quality and the computational complexity. The algorithm is based on a novel move-making technique, tailored to non-submodular energies, which explores a certain hierarchy of relaxed problems. The aforementioned approximation scheme is rooted in a strategy where, at each step of the hierarchy, one fixes some variables and reverses the optimization direction of others, with the aim of keeping a fraction of the original objective function unchanged. This strategy leads to a performance guarantee for the computed solution, in terms of the difference with the optimal one. We assess the effectiveness of our approach through an extensive experimental evaluation."
    },
    {
      "id": "7d4f767e-5aa5-474f-8d01-56a96b06b47d",
      "adv_source_id": "7d4f767e-5aa5-474f-8d01-56a96b06b47d",
      "source_id": "4b1ca80e-60c9-4567-b437-90fc7c4545af",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm\" from arxiv.org:",
      "generation": " In this paper a new algorithm is presented for clustering categorical data. This algorithm is an extension of the Fuzzy C-Means (FCM) algorithm for continuous vector data. In contrast to the FCM algorithm, which requires the specification of a single parameter defining the granularity of clustering, the new algorithm incorporates a degree of uncertainty, or fuzziness, in the specification of the cluster membership, making it more flexible and able to capture the inherent uncertainty in clustering. This algorithm has been developed to perform clustering on a new data type, referred to as vector membership (VM) data. VM data combines categorical and vector data in a single data representation. The proposed algorithm partitions VM data according to the belonging degrees of the constituent vectors to different clusters. An application of this algorithm is demonstrated in the context of image segmentation. The algorithm is applied to segment images based on the VM data obtained from the MPEG-7 descriptor, which represents shape and motion information in a local spatio-temporal neighbourhood."
    },
    {
      "id": "526463af-32f4-45e8-9c66-3fb6f580397a",
      "adv_source_id": "526463af-32f4-45e8-9c66-3fb6f580397a",
      "source_id": "4b1ca80e-60c9-4567-b437-90fc7c4545af",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm\" from arxiv.org:",
      "generation": " In this paper, we present a novel extension of the fuzzy C-means (FCM) algorithm for vector-valued data. In contrast to the original FCM algorithm, which works with single-valued data and produces fuzzy clusters with possibility distributions, our extension, which we call vector membership (VM), works with vector-valued data and produces fuzzy clusters with vector-valued possibility distributions. This extension is useful for a number of applications, including image processing, anomaly detection, and texture analysis. We present a detailed theoretical analysis of the VM algorithm and show that it is related to, but different from, other extensions of the FCM algorithm, such as fuzzy co-clustering and multi-modal clustering. We present experimental results on a number of datasets, including synthetic datasets and a dataset of handwritten digits from the UCI Machine Learning Repository, and show that the VM algorithm outperforms the original FCM algorithm in terms of the quality of the produced clusters."
    },
    {
      "id": "def12f73-50ab-48a1-8b22-ad165c5a6ff6",
      "adv_source_id": "def12f73-50ab-48a1-8b22-ad165c5a6ff6",
      "source_id": "d491aa7c-3468-4ac9-acd1-d72353da723b",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient Energy Minimization for Enforcing Statistics",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Efficient Energy Minimization for Enforcing Statistics\" from arxiv.org:",
      "generation": " Recent advances in machine learning have spurred great hopes for artificial intelligence to make a lasting impact on society, akin to other scientific advancements such as quantum mechanics or biotechnology. However, machine learning systems that are trained to excel at natural language processing, for instance, require large amounts of data to be labeled, which is often collected in a non-transparent fashion without user consent. This has led to an ongoing debate about the ethical implications of machine learning, and recent research has shown that the collection of data can be harmful to individuals and society. In this paper, we propose a new method for enforcing statistical properties of data that mitigates the need to collect large amounts of data. Our method is based on a convex optimization formulation that minimizes the difference between the empirical distribution of the data and a specified target distribution. We show that our approach can be used to enforce statistical properties of data, such as the mean or the variance, and that it can be used to reduce the impact of outliers or noise in the data. Our method is computationally efficient and can be applied to a wide range of problems in machine learning and statistics. We demonstrate the performance of our method on a number of examples, including a simulation study and a real-world dataset, and compare it to existing methods for enforcing statistics."
    },
    {
      "id": "fcf1f9d2-edf3-4738-9240-b1d996ae75fb",
      "adv_source_id": "fcf1f9d2-edf3-4738-9240-b1d996ae75fb",
      "source_id": "d491aa7c-3468-4ac9-acd1-d72353da723b",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient Energy Minimization for Enforcing Statistics",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Efficient Energy Minimization for Enforcing Statistics\" from arxiv.org:",
      "generation": " Recent advances in machine learning have revived interest in statistics-based pruning, which leverages statistical considerations to prune zeros in coefficients of a model. Although shows promise, generating statistically-sound predictions necessitates the use of expensive operations, such as sorting and finding the median. In this work, we aim to minimize the energy required for satisfying statistical requirements of prediction. We find that even though considering statistics can reduce the energy consumption, the approach in MNM can be extremely inefficient, due to redundant computations. We propose an energy-aware scheme that minimizes the redundant computations by reusing computed statistics, and presents an adaptive algorithm deployment strategy such that the prediction energy is minimized while still satisfying the statistical requirements. Our scheme reduces the energy consumption by up to 43.7% compared to the existing scheme in MNM, and reduces the energy-delay product by up to 25.3% and 19.2% compared to the two best schemes in MNM, respectively."
    },
    {
      "id": "a58c5217-db89-45f3-8731-cd2c3a390dd4",
      "adv_source_id": "a58c5217-db89-45f3-8731-cd2c3a390dd4",
      "source_id": "39b84587-7fd4-4a2b-a566-aaad6d196df1",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Live-wire 3D medical images segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Live-wire 3D medical images segmentation\" from arxiv.org:",
      "generation": " Live-wire 3D medical images segmentation is a challenging problem in image processing. In this paper, we propose a novel method for live-wire 3D medical images segmentation based on a combination of geometric and topological features. Our method utilizes the geometric feature of the live-wire algorithm, which can effectively capture the local structure of the object, and the topological feature of the 3D medical images, which can describe the global structure of the object. By combining these two features, our method can achieve accurate segmentation results. We evaluate our method on a public 3D medical images segmentation dataset, and the results show that our method can achieve better performance compared to the state-of-the-art methods."
    },
    {
      "id": "7e4ad8c5-0724-4a5d-a862-a6f64b917d71",
      "adv_source_id": "7e4ad8c5-0724-4a5d-a862-a6f64b917d71",
      "source_id": "39b84587-7fd4-4a2b-a566-aaad6d196df1",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Live-wire 3D medical images segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Live-wire 3D medical images segmentation\" from arxiv.org:",
      "generation": " Live-wire 3D medical images segmentation is a challenging problem in image processing. In this paper, we propose a novel method for live-wire 3D medical images segmentation based on a new algorithm called fused conditional random fields (FCRF). Our method combines the advantages of live-wire and 3D active contour models (ACM) with the powerful capabilities of CRF. We use a 3D live-wire algorithm to obtain an initial contour, and then we refine it using a 3D ACM. Finally, we apply a FCRF algorithm to segment the medical image. Our method has been tested on synthetic and real 3D medical images, and the results show that our method can accurately segment the images.\n\nOur FCRF algorithm is a novel combination of live-wire and 3D ACM with CRF. It has several advantages over other live-wire 3D medical images segmentation methods. First, our method can handle large variations in the size and shape of objects in the images. Second, our method is robust to noise and small details in the images. Third, our method is fast and efficient.\n\nWe believe that our method is a promising approach to live-wire 3D medical images segmentation. It can be used to assist doctors in diagnosing and treating medical conditions. In addition, our method can be used to segment other types of images, such as satellite images and aerial photographs."
    },
    {
      "id": "2a829264-9459-4ab9-a5e5-b4f6bc9e9cf8",
      "adv_source_id": "2a829264-9459-4ab9-a5e5-b4f6bc9e9cf8",
      "source_id": "c7755a04-f926-4698-9639-a3613011e9f9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Image Segmentation in Low Depth Of Field Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Robust Image Segmentation in Low Depth Of Field Images\" from arxiv.org:",
      "generation": " Recent works on image segmentation often assume that all pixels in an image are relatively evenly focused, and that blur is either constrained or appropriately modeled. However, real-world images often include significant variations in focus, which can adversely impact the performance of segmentation algorithms. In this paper, we seek to address this deficiency by developing a robust segmentation algorithm that operates under significant defocus. Towards this goal, we first characterize the effects of defocus on pixel labels, and then propose a new energy function that mitigates the impact of defocus. We demonstrate the effectiveness of our approach on a benchmark dataset, showing that our method achieves state-of-the-art performance even when operating in significantly defocused images."
    },
    {
      "id": "2c2dae77-e2c3-49dd-9549-6b1c060768c2",
      "adv_source_id": "2c2dae77-e2c3-49dd-9549-6b1c060768c2",
      "source_id": "c7755a04-f926-4698-9639-a3613011e9f9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Image Segmentation in Low Depth Of Field Images",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Robust Image Segmentation in Low Depth Of Field Images\" from arxiv.org:",
      "generation": " Capturing a high depth-of-field image is an important task in computer vision. It substantially simplifies the following image processing tasks, such as object detection and segmentation. Unfortunately, increasing the DOF results in a decrease of image quality, causing the fine texture details to be blurred, and the segmented regions to have inaccurate boundaries. In this paper, we address the image segmentation task, and propose a fully automated and parameter-free approach to obtain segmentation of objects in low DOF images. Our key observation is that although the boundaries of objects are blurred in a low DOF image, the appearance of the object itself is preserved through the lens. Based on this observation, we develop an approach that uses a deep neural network to estimate the blur kernel, and then uses a segmentation network to segment the image. Since the estimation of the blur kernel is a challenging task, we propose to use a variational framework that is tailored to this specific problem. We demonstrate the performance of our method on both semantic segmentation and instance segmentation benchmarks."
    },
    {
      "id": "52eab3a4-ae41-4753-b4ed-fe4aca923dd0",
      "adv_source_id": "52eab3a4-ae41-4753-b4ed-fe4aca923dd0",
      "source_id": "5995c43a-4d3e-4323-b562-9617d4756b35",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Gray Image extraction using Fuzzy Logic",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Gray Image extraction using Fuzzy Logic\" from arxiv.org:",
      "generation": " Extraction of gray image from a color image is very important and it is the first step of extraction process in any image processing application. A new approach is proposed to extract the gray image using fuzzy logic. In this paper, RGB components of color image are considered as input for fuzzy system. These inputs are fuzzified and processed by fuzzy logic to obtain a single output which is the gray value corresponding to that pixel. The proposed method is tested on various images and results are also compared with existing methods. The proposed method produced better results in terms of contrast and brightness."
    },
    {
      "id": "f2065326-5b1b-428e-97ba-5db20524512a",
      "adv_source_id": "f2065326-5b1b-428e-97ba-5db20524512a",
      "source_id": "5995c43a-4d3e-4323-b562-9617d4756b35",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Gray Image extraction using Fuzzy Logic",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Gray Image extraction using Fuzzy Logic\" from arxiv.org:",
      "generation": " Extraction of gray image from a color image is very important and it is the first step of extraction process in any image processing activity. In this paper, a new method is proposed to extract the gray image using fuzzy logic. In the proposed method, RGB components are taken to extract the gray image. Here, each RGB component is treated as a fuzzy variable. By considering the fuzzy variables following fuzzy rules are made. By applying these rules on RGB components, a gray image is extracted. To extract the gray image from color image, proposed method is compared with the existing methods. Results show that the proposed method extracts gray image fast and accurately."
    },
    {
      "id": "60d00f48-260e-43a7-829e-1315989c2e0a",
      "adv_source_id": "60d00f48-260e-43a7-829e-1315989c2e0a",
      "source_id": "b78ac52e-93ca-4ed2-9b68-32f6cf1f4617",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Model Assisted Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"3D Model Assisted Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in image segmentation is significantly advanced by the 3D model assisted approach, which utilizes a 3D model to provide prior knowledge for the segmentation task. Given a query image and a set of 3D models, the goal is to find the optimal 3D model which best explains the query image. However, the existing 3D model assisted methods usually assume that the surface of the 3D model is composed of a finite number of segments, which is not suitable for scanning objects with complex surfaces, where significant texture variations and geometric structures usually exist in a single segment. In this paper, we propose a new 3D model assisted image segmentation method to overcome the above limitations. The key idea is to formulate the segmentation problem as a non-parametric binary labeling problem on the surface of the 3D model, and then solve it using a conditional random field (CRF) model. To efficiently compute the CRF energy, we propose a new algorithm called the multi-scale graph cut algorithm, which approximates the CRF energy by a graph cut energy on a multi-scale graph structure. The experimental results on both synthetic and real-world data sets demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "d7c85962-6510-42a7-9997-faad426f18c1",
      "adv_source_id": "d7c85962-6510-42a7-9997-faad426f18c1",
      "source_id": "b78ac52e-93ca-4ed2-9b68-32f6cf1f4617",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Model Assisted Image Segmentation",
      "prompt": "The following is the full text of the abstract for a research paper titled \"3D Model Assisted Image Segmentation\" from arxiv.org:",
      "generation": " Recent progress in image segmentation is significantly advanced by the 3D model assisted approach, which utilizes a 3D model for explicit surface representation as the intermediate during image segmentation. In this paper, we first present an overview of the 3D model assisted image segmentation approaches, where we categorize these approaches into two groups: the geometry-based and the appearance-based methods. Next, we analyze the assumptions, limitations and advantages of different approaches. We argue that the appearance-based methods are more robust and scalable compared with the geometry-based ones, because the appearance-based methods avoid the construction of an explicit surface representation and the resulting parameterization issues. In addition, we also study the impact of the 3D model generation on the segmentation performance. Finally, we present a comprehensive evaluation of the state-of-the-art methods on the public Sinai dataset. The evaluation results demonstrate the advantages and limitations of different approaches."
    },
    {
      "id": "b0857074-c63d-43e5-9342-e997e213369e",
      "adv_source_id": "b0857074-c63d-43e5-9342-e997e213369e",
      "source_id": "2dca91ca-3683-4870-92c8-e9172f837fa9",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A linear framework for region-based image segmentation and inpainting involving curvature penalization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A linear framework for region-based image segmentation and inpainting involving curvature penalization\" from arxiv.org:",
      "generation": " In this paper, we propose a linear framework for image segmentation and inpainting that involves a curvature penalty. Our approach is based on the minimization of a functional that consists of an edge-stopping term, a region-size regularization term, and a curvature penalty term. The edge-stopping term ensures that the solution remains close to the initial image, the region-size regularization term encourages the solution to have a prescribed number of regions, and the curvature penalty term helps to preserve the curvature of the initial image. The resulting functional is minimized via a linear programming algorithm. We demonstrate the effectiveness of our approach on a variety of image segmentation and inpainting problems, and show that our method produces results that are comparable to, or better than, state-of-the-art methods. \n\nOur approach offers several advantages over existing methods for image segmentation and inpainting. First, our method is simple and efficient, as it involves the minimization of a linear functional. This makes it easier to implement and faster to compute than methods that involve the minimization of non-linear functionals. Second, our method is able to produce results that are comparable to, or better than, state-of-the-art methods for image segmentation and inpainting. This is because our method incorporates a curvature penalty, which helps to preserve the curvature of the initial image and produces more accurate results. Third, our method is flexible and can be used to segment and inpaint images of different types, including medical images, satellite images, and images of objects in the real world. This is because our method involves a region-size regularization term, which allows us to prescribe the number of regions that we want the solution to have."
    },
    {
      "id": "455b5f2d-5988-42cb-9b54-970f3b2e6ef2",
      "adv_source_id": "455b5f2d-5988-42cb-9b54-970f3b2e6ef2",
      "source_id": "2dca91ca-3683-4870-92c8-e9172f837fa9",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A linear framework for region-based image segmentation and inpainting involving curvature penalization",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A linear framework for region-based image segmentation and inpainting involving curvature penalization\" from arxiv.org:",
      "generation": " In this paper, we propose a linear framework for image segmentation and inpainting that combines region-based segmentation with a curvature penalization term. Together with an image inpainting technique, our approach produces realistic fills for fragments of objects missing from the image. An over-segmentation is first produced via a region-based algorithm; subsequently, the inpainting algorithm is applied to the borders of the regions, using the gradient of the curvature penalization term as a guidance for filling in the gaps. We demonstrate the efficacy of our method on a variety of images."
    },
    {
      "id": "f476e49e-9943-40ba-8fea-03cb0579ce1d",
      "adv_source_id": "f476e49e-9943-40ba-8fea-03cb0579ce1d",
      "source_id": "8c9cafc4-fe7b-4b95-9ed7-ac1e5f2cafea",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-environment model estimation for motility analysis of Caenorhabditis Elegans",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Multi-environment model estimation for motility analysis of Caenorhabditis Elegans\" from arxiv.org:",
      "generation": " A new multi-environment model (MEM) is presented for estimating the motility of individual Caenorhabditis elegans worms from images. The approach estimates the worm's pose, size, and shape for each video frame, and additionally, estimates a set of parameters that characterize the worm's motility. These motility parameters facilitate comparison across different worms, and moreover, they can be used to detect changes in a single worm's behavior over time. The parameters include a crawling velocity, a muscle contraction velocity, and a set of parameters that characterize the worm's compliance to external forces. This last set of parameters can be used to detect changes in the worm's elasticity and its tendency to buckle. The approach is evaluated on a new dataset that contains images of C. elegans worms collected in a variety of environments (liquid, solid, and sand) and a variety of orientations. In these new experiments, the worms are sometimes out of focus, and they sometimes crawl on the bottom of the dish, which can produce ambiguous appearances. In addition, the evaluation compares the new motility parameters to other features that have been used for C. elegans motility analysis, namely joint angles and silhouette curvature."
    },
    {
      "id": "93c61366-38a4-49fd-b52d-75be3fc296b5",
      "adv_source_id": "93c61366-38a4-49fd-b52d-75be3fc296b5",
      "source_id": "8c9cafc4-fe7b-4b95-9ed7-ac1e5f2cafea",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-environment model estimation for motility analysis of Caenorhabditis Elegans",
      "prompt": "The following is the full text of the abstract for a research paper titled \"Multi-environment model estimation for motility analysis of Caenorhabditis Elegans\" from arxiv.org:",
      "generation": " A novel algorithm is presented for tracking the movement of each of the ~300 body segments of Caenorhabditis elegans using active illumination and automated image analysis. This algorithm fits a model to each image in a sequence where the model is a linear combination of a small number of biologically-inspired motion primitives that capture the recursive coil-stretch-coil motility of C. elegans. The parameters of the model are estimated using a novel multi-environment expectation-maximization algorithm that is particularly well-suited to this problem because it excels at fitting parameters in the presence of large amounts of noise, partial occlusions, and varying environments. This algorithm can track the complete spatiotemporal dynamics of C. elegans, including twitching events that appear to be irregular and induce observable deformations in the animal's body. This enables a quantitative analysis of the twitching event, revealing that the amplitude of a twitching event decreases with the duration of the preceding static period. Our algorithm provides a valuable tool for quantifying the effects of genetic mutations and drug treatments on C. elegans motility."
    },
    {
      "id": "51ef578e-f286-463b-b933-9e095da5d205",
      "adv_source_id": "51ef578e-f286-463b-b933-9e095da5d205",
      "source_id": "4c4676d5-c143-448c-b058-9c360b04d900",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Topological derivative based image segmentation for sign language recognition system using isotropic filter",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Topological derivative based image segmentation for sign language recognition system using isotropic filter\" from arxiv.org:",
      "generation": " In this paper, we propose a topological image segmentation technique based on the topological derivative for automatic recognition of sign language. In order to segment the input hand gesture sequences from the video sequences, we adopt the watershed transformation to obtain the topological derivative of the gradient magnitude of the video frames. Since the gradient magnitude does not satisfy the assumption of the topological derivative, we apply an isotropic filter to obtain a suitable input for the watershed transformation. The use of isotropic filter prevents the gradient from being dominated by few directions and makes the watershed based segmentation technique more effective and robust. The segmented regions are then subjected to a classification based on the statistical features extracted from the normalized RGB histogram of the regions. In the next stage, a feature selection based on the Fisher score is applied to reduce the dimensionality and to obtain a set of features that are most relevant to the classification task. Finally, the k-NN classifier is used as the classifier for the classification of the gesture sequences. The proposed method has been tested on Texas Instrument DLP Sign Language Gesture Database and has achieved a recognition rate of 95.6%."
    },
    {
      "id": "8b64fc77-112b-4562-88d7-9f5e0e0077e0",
      "adv_source_id": "8b64fc77-112b-4562-88d7-9f5e0e0077e0",
      "source_id": "4c4676d5-c143-448c-b058-9c360b04d900",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Topological derivative based image segmentation for sign language recognition system using isotropic filter",
      "prompt": "The following is the full text of the abstract for a research paper titled \"A Topological derivative based image segmentation for sign language recognition system using isotropic filter\" from arxiv.org:",
      "generation": " In this paper, we propose a new approach for segmentation of video images of sign language gestures for recognition using topological derivative and an isotropic filter. Segmentation is a prerequisite for any pattern recognition system. The topological derivative is a measure of the deviation of a function from a sinusoidal smoothness. The proposed method is evaluated using ultrasound videos and the KTH sign language dataset. The proposed method is compared with four different methods, namely, Active Contour Without Edge Detection (ACWD), Active Contour With Edge Detection (ACWE), Snake, and Watershed. Both subjective and objective evaluation has been performed for the comparison. The results demonstrate the effectiveness of the proposed method."
    },
    {
      "id": "9571e125-6dc5-493a-94df-1381a852e97d",
      "adv_source_id": "9571e125-6dc5-493a-94df-1381a852e97d",
      "source_id": "dfae1272-4190-42eb-bc08-706538d93519",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An information-theoretic derivation of min-cut based clustering",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An information-theoretic derivation of min-cut based clustering\" from arxiv.org:",
      "generation": " We show that the min-cut based clustering framework recently introduced by us, as well as a variety of other graph clustering algorithms, can be viewed in a new information- theoretic light, as special cases of a common algorithm for extracting independent signal from a noisy graph via spectral partitioning in the Stieltjes-Wasserstein (SW) space. Our analysis provides new insights into the behavior of these algorithms on graphs with certain types of spectra, and suggests a direction for future research on graph clustering beyond the widely studied graph Laplacian setting."
    },
    {
      "id": "b869efeb-368b-40e1-b5ba-ecc72798a6bd",
      "adv_source_id": "b869efeb-368b-40e1-b5ba-ecc72798a6bd",
      "source_id": "dfae1272-4190-42eb-bc08-706538d93519",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An information-theoretic derivation of min-cut based clustering",
      "prompt": "The following is the full text of the abstract for a research paper titled \"An information-theoretic derivation of min-cut based clustering\" from arxiv.org:",
      "generation": " We present a novel information-theoretic approach for clustering binary data, which is derived from the minimal cut problem in network flows. Our method partitions the data points into clusters in such a way that the number of edges connecting points in different clusters is minimized. Up to constant factors our approach is analogous to minimizing the ratio of cut size to the total number of edges in the graph, thus generalizing the well-known spectral clustering algorithm based on the graph laplacian. We derive our approach using a recent information-theoretic interpretation of the minimal cut problem, and show that it is related to a maximal flow problem in a transformed network. Our experiments on synthetic and real-world data indicate that the new algorithm is competitive with leading clustering algorithms in terms of robustness and running time."
    },
    {
      "id": "09a7ceb8-0c22-41ff-b8d8-1e1a433728ce",
      "adv_source_id": "09a7ceb8-0c22-41ff-b8d8-1e1a433728ce",
      "source_id": "106a77e0-cf20-44a6-bfa9-6ffd08551882",
      "model": "cohere",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients",
      "prompt": "The following is the full text of the abstract for a research paper titled \"$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients\" from arxiv.org:",
      "generation": " In this paper, we study $L^2$ well-posedness of boundary value problems for parabolic systems with measurable coefficients. We show that, for suitable function spaces, the existence and uniqueness of a solution can be obtained by using a fixed point theorem. Furthermore, we establish a priori estimates for the solution in terms of the boundary data and the initial data."
    },
    {
      "id": "b877024a-970a-40f6-a122-c8e64aaec61a",
      "adv_source_id": "b877024a-970a-40f6-a122-c8e64aaec61a",
      "source_id": "106a77e0-cf20-44a6-bfa9-6ffd08551882",
      "model": "cohere",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients",
      "prompt": "The following is the full text of the abstract for a research paper titled \"$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients\" from arxiv.org:",
      "generation": " Let $\\Omega\\subset \\mathbb R^n$ be a domain with a bounded boundary $\\partial \\Omega$. We consider nonlinear parabolic systems of the form $u_t - \\mathcal A(x) u = 0$ in $\\Omega_T:=\\Omega\\times(0,T)$ for a given $T>0$, where $\\mathcal A$ is a real-valued measurable coefficient matrix with linear growth. We study $L^2$ well-posedness of associated initial-boundary value problems in the case that the matrix $\\mathcal A$ is independent of $x$, and show that in this case virial identities play an important role in obtaining optimal $L^2$ well-posedness results."
    }
  ],
  "chat": [
    {
      "id": "76625368-9695-487d-85c6-c51af78f7250",
      "adv_source_id": "76625368-9695-487d-85c6-c51af78f7250",
      "source_id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging",
      "prompt": "Write the abstract for the academic paper titled \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\".",
      "generation": "Abstract:\nAs artificial intelligence (AI) increasingly impacts medical imaging, it is essential to ensure the development and use of trustworthy AI systems. The lack of widely accepted principles and consensus recommendations for developing and regulating AI in medical imaging may hinder the advancement of the field and the safe, effective, and appropriate use of these technologies. This paper outlines a set of guiding principles and consensus recommendations for the development and use of trustworthy AI in future medical imaging. These principles and recommendations are based on a multidisciplinary workshop involving experts from the fields of medical imaging, AI, ethics, and regulation. The paper aims to provide a foundation for future research and development in the field and to guide the safe and effective use of AI in medical imaging."
    },
    {
      "id": "b18e7066-dbfb-4213-bfb8-bbdc771af1dd",
      "adv_source_id": "b18e7066-dbfb-4213-bfb8-bbdc771af1dd",
      "source_id": "e5e058ce-be2b-459d-af36-32532aaba5ff",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging",
      "prompt": "Write the abstract for the academic paper titled \"FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Future Medical Imaging\".",
      "generation": "Artificial intelligence (AI) has the potential to revolutionize medical imaging, but it is crucial to ensure that it is used in a safe, effective, and trustworthy manner. The objective of this paper is to provide a comprehensive overview of the current state of AI in medical imaging and to develop a set of guiding principles and consensus recommendations for the use of AI in future medical imaging. \n\nThe paper begins by discussing the current state of AI in medical imaging, including the types of AI algorithms and systems that are currently available, as well as their potential benefits and limitations. The paper then reviews the existing literature on the use of AI in medical imaging, including research on the performance of AI algorithms, the development of standards and guidelines for their use, and the identification of key challenges and concerns. \n\nThe paper concludes by developing a set of guiding principles and consensus recommendations for the use of AI in future medical imaging. These principles and recommendations are designed to ensure that AI is used in a safe, effective, and trustworthy manner and to address the key challenges and concerns that have been identified in the existing literature. Overall, this paper provides a comprehensive overview of the current state of AI in medical imaging and a set of guiding principles and consensus recommendations for its use in future medical imaging."
    },
    {
      "id": "965bd3e3-19bb-4b2d-a2f7-2941392268cb",
      "adv_source_id": "965bd3e3-19bb-4b2d-a2f7-2941392268cb",
      "source_id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow",
      "prompt": "Write the abstract for the academic paper titled \"EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow\".",
      "generation": "Here is the abstract for the academic paper titled \"EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow\":\n\nEdgeFlow is a new approach to interactive segmentation that combines the strengths of edge-based and region-based segmentation methods. It uses edge-guided flow to achieve accurate and efficient segmentation, even for large and complex images. The edge-guided flow is a novel technique that uses edge information to guide the flow of segmentation, ensuring that the segmentation is both accurate and efficient. The method is fast, allowing for real-time segmentation, and is also able to handle large and complex images. The results show that EdgeFlow is a promising approach to interactive segmentation, outperforming existing methods in terms of both accuracy and efficiency."
    },
    {
      "id": "e639ccf8-c59b-4c64-913d-7fcd65498619",
      "adv_source_id": "e639ccf8-c59b-4c64-913d-7fcd65498619",
      "source_id": "f95b107b-d176-4af5-90f7-4d0bb20caf93",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow",
      "prompt": "Write the abstract for the academic paper titled \"EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow\".",
      "generation": "EdgeFlow: Achieving Practical Interactive Segmentation with Edge-Guided Flow\n\nIn this paper, we present EdgeFlow, a practical system for real-time, interactive image segmentation. Our system combines the benefits of edge-based and region-based segmentation, while avoiding their respective drawbacks. Edge-based segmentation often produces fragmented results due to its sensitivity to noise and small details. On the other hand, region-based segmentation techniques can be slow and require a significant amount of user interaction to obtain satisfactory results. To address these issues, we propose a new edge-guided flow algorithm that smoothly merges edge-based and region-based segmentation. Our algorithm leverages the benefits of both approaches to achieve a robust and efficient segmentation system.\n\nWe demonstrate the effectiveness of our approach on a variety of images, showing that EdgeFlow produces accurate and coherent segmentations with minimal user interaction. Our system is able to handle challenging cases, including images with complex shapes, textures, and noise. We also present a user study to evaluate the performance of our algorithm in comparison to existing techniques.\n\nOur work provides a practical solution for real-time, interactive image segmentation, overcoming the limitations of existing techniques. We believe that our approach will find applications in various fields, including medical imaging, remote sensing, and image editing."
    },
    {
      "id": "d7807d09-56dd-44a6-814c-e987b7553ec0",
      "adv_source_id": "d7807d09-56dd-44a6-814c-e987b7553ec0",
      "source_id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation\".",
      "generation": "Abstract:\nTraining deep learning models for medical image segmentation is challenging due to the limited availability of annotated data. We propose a semi-supervised learning approach for training contrastive learning (CL) models with fewer labels. Our approach leverages unlabeled data and a small amount of labeled data to improve the segmentation performance. We introduce a novel loss function that combines contrastive loss with supervised loss, allowing the model to learn from both labeled and unlabeled data. Our experimental results on three different medical image segmentation datasets show that our approach significantly improves segmentation performance compared to supervised CL models and other state-of-the-art semi-supervised learning methods. Furthermore, our approach is more label-efficient than traditional fully supervised learning methods, making it more practical for medical image segmentation tasks where labeled data is scarce."
    },
    {
      "id": "4d436340-0c6c-4bbe-ac50-c3c8906654f9",
      "adv_source_id": "4d436340-0c6c-4bbe-ac50-c3c8906654f9",
      "source_id": "856d8972-9e3d-4544-babc-0fe16f21e04d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised Contrastive Learning for Label-efficient Medical Image Segmentation\".",
      "generation": "Abstract:\nTraining deep learning models for medical image segmentation is challenging due to the limited availability of labeled data. We address this issue by developing a semi-supervised learning framework that utilizes unlabeled data to enhance the model's performance with a reduced amount of labeled data. Our approach incorporates contrastive learning and utilizes a label-efficient loss function to maximize the agreement between the model's predictions and the ground truth labels on labeled data while minimizing the discrepancy between the model's predictions on labeled and unlabeled data. We evaluate our method on three different medical image segmentation tasks, and our results show that our approach significantly improves the performance of the model compared to supervised and self-supervised learning methods. Our approach shows that it is possible to train deep learning models for medical image segmentation with a reduced amount of labeled data while still achieving state-of-the-art performance."
    },
    {
      "id": "522f405a-57eb-4a5f-8aa0-fd116214c293",
      "adv_source_id": "522f405a-57eb-4a5f-8aa0-fd116214c293",
      "source_id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation\".",
      "generation": "An important problem in medical imaging is multi-organ segmentation of CT scans, which determines the contours of multiple organs in a given CT scan. The segmentation of organs in medical images is typically based on labeled training data, but the annotation of such data is expensive and labor-intensive, particularly for CT scans. As a result, the available training data tends to be imbalanced, with far more images for some organs (e.g., the aorta) than others (e.g., the kidneys), and this imbalance carries over to the segmentation performance. We propose a method for multi-organ segmentation that combines multiple loss functions for different organs to handle both input and output imbalance. We propose a new loss function that directly addresses the imbalance by weighing the loss for each organ based on its prevalence in the training data. Combining multiple loss functions, each specialized for a different organ, achieves better performance than using a single loss function for all organs. We evaluate our method on the publicly available abdominal CT scans from the abdominal connectome challenge."
    },
    {
      "id": "ba657aa9-5004-440d-99b3-67a2f4137961",
      "adv_source_id": "ba657aa9-5004-440d-99b3-67a2f4137961",
      "source_id": "fbc8a5ea-90fa-47b8-8fa7-73dd954f1524",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Combo Loss: Handling Input and Output Imbalance in Multi-Organ Segmentation\".",
      "generation": "An important problem in medical imaging is multi-organ segmentation of CT scans, which detects and segments multiple organs in the CT scan. The difficulty of the problem lies in the variability of the appearance of organs, the complexity of their shape, and the presence of artifacts. However, recent advancements in deep learning have achieved remarkable performance in this area. These methods usually consist of two steps: a localization step followed by a segmentation step. However, these works do not address the imbalanced distribution of organs. That is, some organs (like the lungs) can take up to tens of percentages of the total volume of the CT scan, while some others, such as the brain, might take less than 1%. This leads to an imbalanced dataset where most of the training labels are coming from a small subset of the classes, and the rest remain underrepresented. This may negatively impact the performance of the model on the underrepresented classes. In this work, we introduce a new loss function that incentivizes the model to learn the underrepresented classes better. We show that our method improves the performance of the model on all classes and leads to more robust predictions across different organs."
    },
    {
      "id": "2058db71-694a-4139-830b-1aa1a1e6efbb",
      "adv_source_id": "2058db71-694a-4139-830b-1aa1a1e6efbb",
      "source_id": "72c41b8d-0069-4886-b734-a4000ffca286",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels",
      "prompt": "Write the abstract for the academic paper titled \"Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels\".",
      "generation": "In this paper, we present a novel approach for training a 3D seismic fault segmentation model using only a few 2D slice labels. Our approach utilizes attention mechanisms to weigh the importance of different slices in the 3D volume and adaptively combine the information from multiple slices to generate the final segmentation output. We demonstrate the effectiveness of our approach on a challenging seismic fault segmentation task, where only a small number of 2D slice labels are available for training. Our method achieves state-of-the-art performance in terms of both accuracy and efficiency, and has the potential to significantly reduce the amount of manual labeling required for training 3D segmentation models."
    },
    {
      "id": "3420faf6-5cfd-4d30-a9bf-64c8fcb8c623",
      "adv_source_id": "3420faf6-5cfd-4d30-a9bf-64c8fcb8c623",
      "source_id": "72c41b8d-0069-4886-b734-a4000ffca286",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels",
      "prompt": "Write the abstract for the academic paper titled \"Attention-Based 3D Seismic Fault Segmentation Training by a Few 2D Slice Labels\".",
      "generation": "In this paper, we present a novel approach for training a 3D seismic fault segmentation model using only a few 2D slice labels. Our method utilizes attention mechanisms to weigh the importance of different input features and adaptively combine information from multiple slices to produce accurate 3D predictions. We demonstrate the effectiveness of our approach on real-world seismic data, achieving state-of-the-art performance in terms of both accuracy and efficiency. Our method has the potential to significantly reduce the labor-intensive labeling process, making 3D seismic fault segmentation more practical and accessible."
    },
    {
      "id": "8bafbbae-166b-41e3-8456-a8385863fc00",
      "adv_source_id": "8bafbbae-166b-41e3-8456-a8385863fc00",
      "source_id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmenter: Transformer for Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Segmenter: Transformer for Semantic Segmentation\".",
      "generation": "Semantic segmentation is a fundamental computer vision task, with many applications in the realm of autonomous driving, medical imaging, and robotics. In this paper, we present a novel approach to semantic segmentation, based on transformer architecture. Our method, which we call Segmenter, is designed to take advantage of the recent advancements in pre-trained language models, and to leverage the large amount of image-text pairs available on the web. Our experimental results show that our method can achieve state-of-the-art performance on the semantic segmentation task, outperforming existing methods based on convolutional neural networks. We believe that our approach has the potential to revolutionize the field of semantic segmentation, and to pave the way for a new generation of computer vision algorithms."
    },
    {
      "id": "70eba479-de9c-4328-9050-54f191639709",
      "adv_source_id": "70eba479-de9c-4328-9050-54f191639709",
      "source_id": "72fe360b-cce6-4daf-b66a-1d778f5964f8",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmenter: Transformer for Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Segmenter: Transformer for Semantic Segmentation\".",
      "generation": "The paper proposes a novel approach for semantic segmentation, a task in computer vision that aims to assign a semantic label to each pixel in an image. The proposed method utilizes a transformer architecture, a type of neural network architecture that has shown great success in natural language processing tasks. The transformer architecture is adapted and specialized for the semantic segmentation task, and the paper presents experimental results showing that the proposed method outperforms state-of-the-art approaches on several benchmark datasets. The paper's contributions are threefold: (1) it proposes a new architecture for semantic segmentation, (2) it shows that this architecture is effective and efficient for the task, and (3) it provides a new set of benchmark results for future research."
    },
    {
      "id": "38217254-4019-4747-aac1-51882edcd7a3",
      "adv_source_id": "38217254-4019-4747-aac1-51882edcd7a3",
      "source_id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mining Contextual Information Beyond Image for Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Mining Contextual Information Beyond Image for Semantic Segmentation\".",
      "generation": "Abstract:\nTraditional semantic segmentation models rely on image-level annotations, which are expensive to collect and may not fully capture the contextual information relevant to the segmentation task. In this paper, we address the problem of mining contextual information beyond the image for semantic segmentation. We propose a novel approach that utilizes the power of language models to extract contextual information from the image's corresponding caption. Our approach combines the strengths of both image and language understanding to enhance the performance of semantic segmentation. We demonstrate the effectiveness of our method on the Cityscapes and COCO-Stuff datasets, achieving state-of-the-art results. Our approach has the potential to revolutionize the field of semantic segmentation by leveraging contextual information from language, opening up new possibilities for accurate and robust segmentation in complex scenes."
    },
    {
      "id": "3e0e5510-2221-4510-b5b3-a75b120e0f86",
      "adv_source_id": "3e0e5510-2221-4510-b5b3-a75b120e0f86",
      "source_id": "df594cf4-9a0c-4488-bcb3-68f41e2d5a16",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mining Contextual Information Beyond Image for Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Mining Contextual Information Beyond Image for Semantic Segmentation\".",
      "generation": "Abstract:\nThe field of computer vision has recently seen significant advances in the development of semantic segmentation algorithms. These algorithms aim to assign a semantic label to each pixel in an image, relying on the visual appearance of the objects in the image. However, the performance of these algorithms often suffers when dealing with images that contain complex scenes, objects that are partially obscured, or when there is a lack of training data. In this paper, we propose a novel approach to semantic segmentation that goes beyond the image itself and mines contextual information from related web pages. Our approach leverages the power of state-of-the-art pre-trained language models to extract relevant information from the surrounding context, which is then combined with visual features extracted from the image using a supervised learning framework. We demonstrate the effectiveness of our approach on the Cityscapes and Mapillary Vistas datasets, showing that our method significantly improves the performance of semantic segmentation compared to existing approaches."
    },
    {
      "id": "0f0bd472-4b00-4f7a-9015-5a376cdca43c",
      "adv_source_id": "0f0bd472-4b00-4f7a-9015-5a376cdca43c",
      "source_id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Comprehensive Multi-Modal Interactions for Referring Image Segmentation\".",
      "generation": "An emerging research direction in computer vision is to localize object regions in images given verbal descriptions about the objects. Referring image segmentation (RIS) is a challenging task that requires joint understanding of both image and language. In this paper, we comprehensively study a wide range of multi-modal fusion strategies that integrate both visual and linguistic information for RIS. We consider a variable-length and uncertain textual input, where a sentence may describe multiple objects with different levels of specificity, resulting in ambiguous region proposals for segmentation. To overcome these challenges, we propose a modular and flexible approach for RIS that is based on a tri-encoder architecture. Specifically, we design a region proposal encoder (RPE) that generates an initial set of region proposals, a segment encoder (SE) that refines the proposals at segment-level, and a referring encoder (RE) that selects the final referring segments. To fuse multi-modal information, we propose a set of fusion strategies at each encoder, including cross-attention, cross-modality gate, and cross-modality interaction. We evaluate our approach on the ReferItSeg dataset, and our method outperforms the previous state-of-the-art approaches."
    },
    {
      "id": "e8ce1fa0-b9d7-4988-b9a3-c23e476f6522",
      "adv_source_id": "e8ce1fa0-b9d7-4988-b9a3-c23e476f6522",
      "source_id": "853c0e51-7dd5-4bb5-8286-e4aa8820173b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Comprehensive Multi-Modal Interactions for Referring Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Comprehensive Multi-Modal Interactions for Referring Image Segmentation\".",
      "generation": "Introducing a new comprehensive multi-modal interaction framework for referring image segmentation. This new framework aims to address the gap between low-level image primitives and high-level semantic referring expressions in images. We combine both the referring expression and the image to generate a referring segment. Furthermore, we introduce a novel referring expression generation module that leverages the power of pre-trained language models and a segmentation-based loss function to produce more accurate referring expressions. We evaluate our approach on the public ReferItDigest dataset and achieve state-of-the-art performance."
    },
    {
      "id": "1c835b2a-9115-4f90-8d12-fc512f2a7664",
      "adv_source_id": "1c835b2a-9115-4f90-8d12-fc512f2a7664",
      "source_id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-Shot Segmentation with Global and Local Contrastive Learning",
      "prompt": "Write the abstract for the academic paper titled \"Few-Shot Segmentation with Global and Local Contrastive Learning\".",
      "generation": "Few-shot segmentation with global and local contrastive learning is a powerful approach to tackle the challenge of training accurate segmentation models with limited training data. This paper proposes a novel framework that combines global and local contrastive learning to improve the performance of few-shot segmentation. Global contrastive learning is used to generate a generic segmentation model, while local contrastive learning is used to adapt the model to specific image domains. The proposed framework is evaluated on the Cityscapes and CamVid datasets, and results show that it significantly improves the performance of few-shot segmentation compared to existing methods."
    },
    {
      "id": "41824110-0103-4ae2-af14-486234ee1748",
      "adv_source_id": "41824110-0103-4ae2-af14-486234ee1748",
      "source_id": "1649f195-8f98-4c79-92b6-54a5ca9261fa",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-Shot Segmentation with Global and Local Contrastive Learning",
      "prompt": "Write the abstract for the academic paper titled \"Few-Shot Segmentation with Global and Local Contrastive Learning\".",
      "generation": "Few-shot segmentation with global and local contrastive learning is a powerful approach to tackle the challenge of segmenting objects in few-shot images. This approach combines the global contrastive learning with the local contrastive learning to achieve accurate and robust segmentation. In this paper, we propose a novel framework that utilizes the global and local contrastive learning to improve the segmentation performance. Our approach focuses on learning a global representation of the image, as well as local representations of the image patches. By doing so, we are able to capture both the global and local context of the image, which is crucial for accurate segmentation. We evaluate our approach on the VOC2012 and Cityscapes datasets, and demonstrate that our method achieves state-of-the-art performance in few-shot segmentation."
    },
    {
      "id": "54355a2b-f153-4d5f-af12-16f326f6bf74",
      "adv_source_id": "54355a2b-f153-4d5f-af12-16f326f6bf74",
      "source_id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images",
      "prompt": "Write the abstract for the academic paper titled \"Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images\".",
      "generation": "Interactive segmentation is a powerful technique for correcting mispredictions in medical images, but existing methods can be time-consuming and require significant user interaction. This paper introduces a new, efficient, and generic interactive segmentation framework that reduces the time and effort required for correction. The framework utilizes a fully-automatic segmentation algorithm as a starting point, which is then refined through user-guided editing. This approach strikes a balance between efficiency and accuracy, reducing the time and effort required for correction while maintaining a high level of precision. The framework is evaluated on a variety of medical images, including CT scans and MRIs, and is shown to be effective in correcting mispredictions in a variety of anatomical structures. The results demonstrate that the proposed framework can significantly reduce the time and effort required for interactive segmentation, while maintaining a high level of accuracy."
    },
    {
      "id": "12c3b8d9-2e71-4af7-adf7-81c33622b8c4",
      "adv_source_id": "12c3b8d9-2e71-4af7-adf7-81c33622b8c4",
      "source_id": "5e23ab14-b85f-48e8-9aa3-15452e73524e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images",
      "prompt": "Write the abstract for the academic paper titled \"Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images\".",
      "generation": "Interactive segmentation is a powerful technique for correcting mispredictions in medical images, but existing methods can be time-consuming and require significant expertise. This paper presents a new, efficient, and generic interactive segmentation framework that is designed to correct mispredictions during clinical evaluation of medical images. The framework utilizes a fully automated initialization, which is followed by efficient and effective interactive segmentation with single-button correction. We evaluate our method on a variety of medical image datasets, including liver and lung CT scans, and demonstrate that it significantly reduces the time and expertise required for interactive segmentation, while still producing accurate results. The proposed method has the potential to improve the efficiency and accuracy of clinical evaluation of medical images."
    },
    {
      "id": "b4543641-48d6-4566-9fbd-2eddfe028e85",
      "adv_source_id": "b4543641-48d6-4566-9fbd-2eddfe028e85",
      "source_id": "ddcb207c-a790-4e16-a053-4aced58d7c15",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Distribution-Free, Risk-Controlling Prediction Sets",
      "prompt": "Write the abstract for the academic paper titled \"Distribution-Free, Risk-Controlling Prediction Sets\".",
      "generation": "Abstract:\nWe propose a new prediction set algorithm that enjoys both distribution-free and risk-controlling properties. Distribution-free means that the proposed algorithm is not based on any assumption about the data-generating distribution of the predictive variables. Risk-controlling implies that the proposed algorithm inherently controls prediction error on a given hold-out dataset. The proposed algorithm is based on a data-driven transformation of the original variables, which makes the algorithm applicable to high-dimensional data. We provide an theoretical analysis of the algorithm and demonstrate its performance via a simulation study and an application to a real-world dataset."
    },
    {
      "id": "6200a5d1-7fc7-4057-a475-ac770a4ae755",
      "adv_source_id": "6200a5d1-7fc7-4057-a475-ac770a4ae755",
      "source_id": "ddcb207c-a790-4e16-a053-4aced58d7c15",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Distribution-Free, Risk-Controlling Prediction Sets",
      "prompt": "Write the abstract for the academic paper titled \"Distribution-Free, Risk-Controlling Prediction Sets\".",
      "generation": "Abstract:\nWe propose a new prediction set method that amends the certainty equivalence principle by considering a risk measure and the forecaster's confidence in the prediction interval. We apply the prediction set to distribution-free setups, where the data do not follow any known distribution. We establish the asymptotic properties of the prediction set under suitable regularity conditions. Finally, we examine the performance of the prediction set through a simulation study and an application to real data. The software implementation is provided."
    },
    {
      "id": "35bf3c91-26b5-473c-8104-bed5dd08d728",
      "adv_source_id": "35bf3c91-26b5-473c-8104-bed5dd08d728",
      "source_id": "b00bf7dc-4de9-4ab4-9962-a16e0b5f4628",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation\".",
      "generation": "Synthetic Aperture Sonar (SAS) images are commonly segmented to aid in the analysis of seafloor topography and in the interpretation of seafloor imagery. Current techniques for segmentation are either operator-assisted or operator-dependent and are therefore subjective, tedious, and not generally scalable. In this paper, we propose a novel, unsupervised approach to segmentation of SAS images. Our method is based on a deep learning architecture, specifically a U-Net, that has been previously used for medical image segmentation. We train our U-Net using an iterative process of subject-matter expert feedback and evaluation on a large dataset of SAS images. Our approach has been shown to be effective in segmenting SAS images and could potentially be used for automating a tedious and time-consuming process."
    },
    {
      "id": "e12990d0-8d05-4824-af22-a7f236178487",
      "adv_source_id": "e12990d0-8d05-4824-af22-a7f236178487",
      "source_id": "b00bf7dc-4de9-4ab4-9962-a16e0b5f4628",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Iterative, Deep, and Unsupervised Synthetic Aperture Sonar Image Segmentation\".",
      "generation": "The paper proposes an iterative, deep, and unsupervised synthetic aperture sonar image segmentation method that utilizes a U-Net neural network architecture. The method is designed to segment grayscale images of underwater scenes, which are often challenging to segment due to the presence of noise, clutter, and ambiguous boundaries. The proposed method utilizes an iterative approach, where the network is trained and tested on a sequence of images, and the segmentation results are used to refine the network's parameters. The method also utilizes a deep architecture, with multiple layers of convolutional and pooling layers, to capture complex spatial patterns and improve the segmentation accuracy. The method is unsupervised, meaning that it does not require any manual labeling of the training data, which can be expensive and time-consuming to obtain. Instead, the method relies on a self-supervised learning approach, where the network is trained using a contrastive loss function that measures the similarity between the predicted and ground truth segmentations. The proposed method is evaluated on a large dataset of synthetic aperture sonar images and compared to other state-of-the-art segmentation methods. The results show that the proposed method achieves a high segmentation accuracy and is able to handle a wide range of challenging images."
    },
    {
      "id": "5f7ac4be-fc60-477e-9c46-6cb6fb9bc628",
      "adv_source_id": "5f7ac4be-fc60-477e-9c46-6cb6fb9bc628",
      "source_id": "04d3809c-0abe-4bee-b1d2-9787af95362f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP",
      "prompt": "Write the abstract for the academic paper titled \"Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP\".",
      "generation": "Abstract:\nImage segmentation is a fundamental computer vision task with numerous applications in multimedia content analysis, but it remains a challenging problem in uncontrolled real-world scenarios. We present a novel approach to semantic image segmentation based on generative adversarial networks (GANs) and contrastive loss for image pre-training (CLIP). Our method leverages the power of GANs to extract robust features from images and the ability of CLIP to learn meaningful representations from unlabeled data. We train a GAN using a dataset of images with corresponding segmentation masks and use the generated images as a pseudo-labeled training set for CLIP. Our experiments show that the proposed method achieves state-of-the-art performance on the Cityscapes segmentation benchmark, outperforming other unsupervised methods by a large margin. We also demonstrate the generalization ability of our approach on the ACDC dataset, which contains a wider variety of scenes and object instances. Our code and models are publicly available at this https URL."
    },
    {
      "id": "04fe9d6d-b89c-466a-b10a-5f324741069a",
      "adv_source_id": "04fe9d6d-b89c-466a-b10a-5f324741069a",
      "source_id": "04d3809c-0abe-4bee-b1d2-9787af95362f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP",
      "prompt": "Write the abstract for the academic paper titled \"Segmentation in Style: Unsupervised Semantic Image Segmentation with Stylegan and CLIP\".",
      "generation": "Abstract:\nImage segmentation is a fundamental task in computer vision with a wide range of applications. Most existing methods rely on supervised learning with extensive labeled data, which is difficult to obtain for semantic segmentation. In this paper, we present a new approach to unsupervised image segmentation using generative adversarial networks (GANs) and contrastive learning. Our method utilizes the recently introduced StyleGAN, a GAN architecture designed for image synthesis, to generate a diverse set of segmentations for each input image. By leveraging the CLIP (Contrastive Language-Image Pre-training) model, we learn a universal segmentation representation that is invariant to image style. We show that our approach achieves state-of-the-art performance on the Cityscapes segmentation benchmark without using any labeled data."
    },
    {
      "id": "b4949909-bf82-42c4-82eb-820bfb320e21",
      "adv_source_id": "b4949909-bf82-42c4-82eb-820bfb320e21",
      "source_id": "06bffeb2-bea0-4b0b-b60d-767ba9b660a7",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Double Similarity Distillation for Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Double Similarity Distillation for Semantic Image Segmentation\".",
      "generation": "Semantic image segmentation is the process of labeling pixels in an image with specific semantic meanings, requiring robust feature representation and accurate model training. In this paper, we introduce a new approach for semantic image segmentation based on double similarity distillation (DSD). Our method leverages a teacher-student learning framework, where the teacher model is trained using a modified version of the popular SimCLR algorithm, which captures deep feature representations using a contrastive loss function. We then train the student model to match the feature representations of the teacher model while maintaining segmentation accuracy. Our proposed method significantly improves the segmentation performance of the student model, and we demonstrate its effectiveness on the Cityscapes and VOC2012 segmentation datasets, achieving competitive results with state-of-the-art methods."
    },
    {
      "id": "63b5f770-c599-4cf0-b2eb-041816373cd5",
      "adv_source_id": "63b5f770-c599-4cf0-b2eb-041816373cd5",
      "source_id": "06bffeb2-bea0-4b0b-b60d-767ba9b660a7",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Double Similarity Distillation for Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Double Similarity Distillation for Semantic Image Segmentation\".",
      "generation": "Semantic image segmentation has made significant progress in recent years with the advancement of deep learning techniques. However, training robust models with fine-grained segmentation categories remains challenging due to the scarcity of labeled training data. In this paper, we address this issue by proposing a double similarity distillation (DSD) approach for semantic image segmentation. Our key insight is to exploit the intrinsic hierarchical structure of segmentation datasets and the transitive property of similarity. Specifically, we first construct a similarity graph among the pixels of the images in the training dataset, and then we build a second-order similarity graph based on the first-order similarity graph. By jointly optimizing the parameters of the segmentation model and the maps representing the similarity relationships, our proposed approach effectively leverages the abundant unlabeled data and transfers the knowledge from the pixels with similar labels, leading to improved performance in semantic image segmentation. We validate the effectiveness of our approach on three popular semantic image segmentation benchmarks, achieving state-of-the-art performance with significant gains over the prior methods."
    },
    {
      "id": "97ced386-86d7-4421-844c-0810c3578115",
      "adv_source_id": "97ced386-86d7-4421-844c-0810c3578115",
      "source_id": "5eb88a59-eb5a-49ea-8304-f67efe338921",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation\".",
      "generation": "Abstract:\nTraditional self-supervised learning approaches have shown promise for unsupervised medical image segmentation, but they tend to miss spatial information. In this paper, we propose a novel spatial guided self-supervised clustering network (SGSCN) that incorporates spatial information into the self-supervised learning process. The SGSCN utilizes a weighted graph to guide the clustering process, ensuring that the resulting clusters are spatially coherent. Additionally, we propose a novel atrous spatial pyramid pooling (ASPP) module that extracts multi-scale spatial features from the input images. The ASPP module is designed to capture global and local spatial information, which is crucial for accurate segmentation. We evaluate the SGSCN on the ISBI 2017 segmentation challenge dataset and achieve competitive results compared to other state-of-the-art self-supervised methods, while improving the spatial coherence of the segmentation output."
    },
    {
      "id": "af5ac5fd-5d5f-4ceb-a72e-63ac9c0964ab",
      "adv_source_id": "af5ac5fd-5d5f-4ceb-a72e-63ac9c0964ab",
      "source_id": "5eb88a59-eb5a-49ea-8304-f67efe338921",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Spatial Guided Self-supervised Clustering Network for Medical Image Segmentation\".",
      "generation": "Abstract:\nTraditional self-supervised learning approaches fail to fully exploit the spatial information of medical images for segmentation tasks. In this paper, we propose a spatial guided self-supervised clustering network (SGSCN) to address this issue. SGSCN leverages a novel spatial clustering loss combined with a segmentation loss for self-supervised learning. The spatial clustering loss encourages the learned features to cluster the medical images based on their spatial structures, while the segmentation loss encourages the features to capture the underlying tissue structures. The two losses are jointly optimized to enable the network to learn robust features for segmentation. Our approach significantly improves the state-of-the-art performance for medical image segmentation. Furthermore, our method is flexible and generalizable to various medical image segmentation tasks."
    },
    {
      "id": "6f062901-6c87-4826-bbc0-76f477a632fd",
      "adv_source_id": "6f062901-6c87-4826-bbc0-76f477a632fd",
      "source_id": "1389aa64-25fb-4e56-9358-ef34143bfea9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Robust General Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Towards Robust General Medical Image Segmentation\".",
      "generation": "Towards Robust General Medical Image Segmentation\n\nPre-clinical and clinical imaging play an important role in the initial detection, diagnosis, and treatment of disease. In particular, the accurate segmentation of anatomical structures within medical images is essential for reliable quantitative measurements and the proper correlation of morphological changes over time. However, the manual segmentation of medical images is a time-consuming and tedious process, and the variability in manual segmentations between observers can hinder the clinical interpretation of imaging data. Therefore, the development of automated segmentation methods for medical images is of utmost importance.\n\nThis thesis makes three contributions towards the robust segmentation of medical images. First, a new method for the automated segmentation of lymph nodes in whole-body PET/CT images is presented. This method utilizes a fully-automated approach for both the initial segmentation and the final segmentation based on graph cuts. Second, a novel method for the segmentation of the liver and its vasculature in contrast-enhanced CT images is introduced. This method utilizes a combination of supervised learning and graph cuts for the segmentation of the liver, and a new method for the segmentation of the vasculature based on a distance regularized level set. Third, a new method for the segmentation of the heart and its vasculature in cine MR images is presented. This method utilizes a combination of supervised learning and graph cuts for the segmentation of the heart, and a new method for the segmentation of the vasculature based on a velocity field regularized level set.\n\nThe validation of these methods using clinical data shows that they are comparable to, if not better than, the current state-of-the-art."
    },
    {
      "id": "be588c4e-bcc5-4b0e-bdc6-8bb55e1ca31d",
      "adv_source_id": "be588c4e-bcc5-4b0e-bdc6-8bb55e1ca31d",
      "source_id": "1389aa64-25fb-4e56-9358-ef34143bfea9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Robust General Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Towards Robust General Medical Image Segmentation\".",
      "generation": "Towards Robust General Medical Image Segmentation\n\nTraining robust deep learning models for medical image segmentation is a challenging task due to the limited amount of annotated data and the variability in the appearance of medical images. In this paper, we present a novel approach to address these challenges. We propose a hybrid segmentation model that combines a coarse segmentation network with a refinement network. The coarse segmentation network is trained on a large dataset of medical images, while the refinement network is trained on a smaller dataset of annotated images. The proposed approach enables us to leverage the power of deep learning models while reducing the need for a large amount of annotated data. We evaluate our approach on a variety of medical image segmentation tasks, including liver, lung, and tumor segmentation. Our results show that the proposed approach is effective in improving the robustness of deep learning models for medical image segmentation."
    },
    {
      "id": "2d863a91-7eab-45c5-a397-685b16690a10",
      "adv_source_id": "2d863a91-7eab-45c5-a397-685b16690a10",
      "source_id": "d0064195-c22e-4550-a265-6b372deea3e0",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Left Atrium Segmentation with Mutual Consistency Training",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised Left Atrium Segmentation with Mutual Consistency Training\".",
      "generation": "Here is the abstract for the academic paper titled \"Semi-supervised Left Atrium Segmentation with Mutual Consistency Training\":\n\nAbstract\u2014 Left atrium segmentation is a fundamental step in cardiac magnetic resonance imaging analysis, but it is still challenging in 3D images due to the resemblance between the left atrium and the right atrium, and the overlapping of the left atrium with the pulmonary veins. In this paper, we propose a semi-supervised left atrium segmentation method with mutual consistency training to address this issue. We randomly select a subset of the training samples as the labeled samples and use the remaining samples as the unlabeled samples. We train a convolutional neural network with the labeled samples and fine-tune it with the unlabeled samples. The proposed method takes advantage of the large amount of unlabeled samples to improve the segmentation performance. Furthermore, we introduce a mutual consistency loss to enforce the consistency between the predicted segmentation and the ground truth segmentation of the labeled samples. The experimental results on the public dataset MIT-BIH show that our method achieves a competitive performance compared to the state-of-the-art fully-supervised methods."
    },
    {
      "id": "a31f1415-0ab0-4171-a3b8-dd23f8ffdac7",
      "adv_source_id": "a31f1415-0ab0-4171-a3b8-dd23f8ffdac7",
      "source_id": "d0064195-c22e-4550-a265-6b372deea3e0",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Left Atrium Segmentation with Mutual Consistency Training",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised Left Atrium Segmentation with Mutual Consistency Training\".",
      "generation": "The abstract for the academic paper titled \"Semi-supervised Left Atrium Segmentation with Mutual Consistency Training\" could be:\n\nThis paper presents a novel approach for semi-supervised left atrium segmentation using mutual consistency training. We propose a two-stage framework that utilizes a sparse-priors network and a conditional random field (CRF) to improve the accuracy and efficiency of the segmentation. In the first stage, the sparse-priors network is trained using a small amount of labeled data and a large number of unlabeled data to learn a low-dimensional representation of the left atrium. In the second stage, the CRF is trained using the learned representation and a larger amount of labeled data to refine the segmentation results. Our approach achieves state-of-the-art performance in terms of accuracy and efficiency, and shows the potential of mutual consistency training for semi-supervised segmentation tasks."
    },
    {
      "id": "57ffd621-058b-4fe5-a9c7-416813006d17",
      "adv_source_id": "57ffd621-058b-4fe5-a9c7-416813006d17",
      "source_id": "417afaa2-2d21-4df1-953b-768647de9980",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search",
      "prompt": "Write the abstract for the academic paper titled \"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search\".",
      "generation": "Abstract:\nTracking objects in videos requires models that can adapt to appearance changes and object motion. We present an approach for learning such models without requiring an abundance of labeled training data. Our approach consists of a cell-based architecture created by composing building blocks that are automatically discovered by differentiable neural architecture search. Each building block is an encoder-decoder network that transforms a sub-region of the input patch to its pixel-level representation. The architecture is automatically discovered in a way that ensures the decoder networks in each building block specialize in recognizing and locating objects within their corresponding sub-regions. We implement our approach on top of a state-of-the-art tracking framework and show that it significantly improves the accuracy and robustness of the tracker on popular benchmarks."
    },
    {
      "id": "70b693fc-926a-44a3-bf9b-e6759c199319",
      "adv_source_id": "70b693fc-926a-44a3-bf9b-e6759c199319",
      "source_id": "417afaa2-2d21-4df1-953b-768647de9980",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search",
      "prompt": "Write the abstract for the academic paper titled \"CHASE: Robust Visual Tracking via Cell-Level Differentiable Neural Architecture Search\".",
      "generation": "Abstract\nWe present an effective solution to the visual tracking problem based on a cell-level differentiable neural architecture search (C-D2NAS). Given a pre-defined tracking framework, C-D2NAS iteratively optimizes the tracker architecture by jointly searching for the best cell structures and their corresponding weights. In this way, C-D2NAS can effectively leverage the power of differentiable architecture search to find a robust tracker architecture within a reasonable amount of computational resources. In contrast to the existing differentiable NAS methods that search the entire network structure including the number and depth of the layers, etc., C-D2NAS focuses on searching merely the cell structures within a pre-defined network architecture, which significantly reduces the search space and computational resources required for the search. We implement the proposed C-D2NAS on a state-of-the-art tracking framework, and run the search under various settings. The experimental results show that C-D2NAS can successfully find effective tracker architectures that outperform the state-of-the-art methods on three commonly used visual tracking benchmarks VT2021, VOT2020, and OTB100."
    },
    {
      "id": "69b789aa-bdfb-422f-8681-34b4aed6b4b7",
      "adv_source_id": "69b789aa-bdfb-422f-8681-34b4aed6b4b7",
      "source_id": "ce898c28-428f-446f-975e-a1265942f2da",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Inter Extreme Points Geodesics for Weakly Supervised Segmentation\".",
      "generation": "Geodesic active contours are widely used in image processing for over two decades. Energy minimization of geodesics depends on the considered energy functional. The energy functional of the geodesic active contours is defined through the evolution of the curve itself and the appearance term driving the evolution of the curve. The appearance term is usually based on a weakly supervised term defined by the prior knowledge of the expert user on the application of the segmentation. In this paper, we consider the weakly supervised term as the extreme points of the gradient of the image to be segmented. Hence, the evolution of the geodesic active contour depends on the extreme points of the gradient of the image. We show that the proposed method is competitive with state-of-the-art segmentation methods."
    },
    {
      "id": "9ab33596-f2b3-4bfe-9aa0-009e2421177e",
      "adv_source_id": "9ab33596-f2b3-4bfe-9aa0-009e2421177e",
      "source_id": "ce898c28-428f-446f-975e-a1265942f2da",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Inter Extreme Points Geodesics for Weakly Supervised Segmentation\".",
      "generation": "Geodesic active appearance models (GAAMs) have successfully been used for a variety of tasks including segmenting objects in images. We extend the idea of GAAMs to weakly supervised settings, where the appearance model is learned from a set of images that are only weakly annotated, e.g., through image level labels. Our main contribution is a novel distance, the inter extreme points geodesic (IEPG), that measures the dissimilarity between two appearance models. We show both theoretically and empirically that the IEPG is more discriminative for segmentation tasks compared to the classical geodesic appearance distance (GAD). Furthermore, we propose an efficient optimization algorithm for learning the IEPG model in a weakly supervised setting. We demonstrate the efficacy of our approach on the PASCAL VOC 2012 segmentation benchmark. Our method achieves a MAP of 92.3% on the test set, outperforming the state-of-the-art weakly supervised methods."
    },
    {
      "id": "03d3b702-0e9f-427b-bc92-7db34563c8b4",
      "adv_source_id": "03d3b702-0e9f-427b-bc92-7db34563c8b4",
      "source_id": "380cd71d-3300-422c-9cde-8a63e71f2797",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "K-Net: Towards Unified Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"K-Net: Towards Unified Image Segmentation\".",
      "generation": "K-Net is a novel deep learning model for image segmentation. It utilizes a unique combination of a convolutional neural network (CNN) and a fully connected neural network (FNN) to achieve state-of-the-art performance on image segmentation tasks. The CNN component captures local spatial information, while the FNN component learns global context information. This approach enables K-Net to effectively segment images with complex and diverse object structures.\n\nIn this paper, we evaluate K-Net on the Cityscapes and CamVid datasets, which contain a large number of images with complex object structures. Our results show that K-Net significantly outperforms existing image segmentation models in terms of both accuracy and speed. Furthermore, we analyze the performance of K-Net on different image regions and demonstrate its effectiveness in segmenting objects with complex shapes and textures.\n\nWe believe that K-Net's ability to learn global context information and its performance on complex images make it a strong candidate for image segmentation tasks. We plan to further improve K-Net by exploring new techniques to capture long-range dependencies and to adapt it to new datasets."
    },
    {
      "id": "1e703a1f-e320-4cbc-a030-482323770530",
      "adv_source_id": "1e703a1f-e320-4cbc-a030-482323770530",
      "source_id": "380cd71d-3300-422c-9cde-8a63e71f2797",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "K-Net: Towards Unified Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"K-Net: Towards Unified Image Segmentation\".",
      "generation": "K-Net is a novel deep learning model for image segmentation. It combines the strengths of fully convolutional networks (FCNs) and fully connected networks (FCNs), while overcoming their limitations. K-Net uses a weighted sum of the features from the two networks to produce a final segmentation map. The weights are learned during the training process, allowing the network to focus on the most relevant features for each pixel.\n\nOur proposed method improves the state-of-the-art in image segmentation. It achieves a higher performance than other methods on the Cityscapes and VOC 2012 segmentation benchmarks. The network is also more efficient, requiring less computation and memory than other state-of-the-art models.\n\nThe K-Net model is a powerful new tool for image segmentation. It combines the strengths of FCNs and FNs to produce a final segmentation map that is more accurate and efficient than other state-of-the-art models."
    },
    {
      "id": "84ab6328-39ee-4c8c-8121-996b9072c307",
      "adv_source_id": "84ab6328-39ee-4c8c-8121-996b9072c307",
      "source_id": "c093400c-2bd2-4e0d-a732-f99d499d58a9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation\".",
      "generation": "Abstract:\nComBiNet is a compact convolutional Bayesian neural network (CBNN) for image segmentation. It combines the advantages of convolutional neural networks (CNNs) and Bayesian neural networks (BNNs) to achieve state-of-the-art performance with computational efficiency.\n\nComBiNet utilizes a series of convolutional layers to extract hierarchical features from the input image, followed by a Bayesian layer that models the uncertainty of the segmentation output. The Bayesian layer is implemented using a Gaussian process, which enables Bayesian inference and uncertainty estimation.\n\nThe proposed network is trained using a novel online learning algorithm that updates the network parameters based on the predicted segmentation and the ground truth labels. This allows for efficient training and convergence to the optimal solution.\n\nWe evaluate ComBiNet on the Cityscapes and CamVid datasets, and demonstrate that it achieves competitive performance compared to other state-of-the-art methods while having a lower computational cost. Our results show that ComBiNet is a powerful and efficient approach for image segmentation, particularly in scenarios with limited computational resources."
    },
    {
      "id": "4a06812c-5bbe-4046-bcf8-ca1ad057b82d",
      "adv_source_id": "4a06812c-5bbe-4046-bcf8-ca1ad057b82d",
      "source_id": "c093400c-2bd2-4e0d-a732-f99d499d58a9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation\".",
      "generation": "Abstract:\nComBiNet is a compact convolutional Bayesian neural network (CBNN) for image segmentation that integrates Bayesian inference with a depth-separable convolutional neural network (CNN). By exploiting the spatial redundancy in images, ComBiNet achieves a high level of compression compared to conventional CBNNs while maintaining accurate segmentation performance. The key innovation in ComBiNet is the use of a parameter-efficient depth-separable CNN architecture that reduces the number of parameters compared to conventional CNNs. This parameter efficiency enables the application of Bayesian inference techniques, such as Markov chain Monte Carlo (MCMC) sampling, to efficiently estimate the posterior distribution of the segmentation labels. The experimental results demonstrate that ComBiNet achieves state-of-the-art performance on the Cityscapes and CamVid datasets while being more than an order of magnitude faster than conventional CBNNs in terms of inference time."
    },
    {
      "id": "a5cc475e-04f0-4224-80de-070ff9a07236",
      "adv_source_id": "a5cc475e-04f0-4224-80de-070ff9a07236",
      "source_id": "05f40b6d-67cf-4a6e-ad2f-cfe003b7917e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images",
      "prompt": "Write the abstract for the academic paper titled \"Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images\".",
      "generation": "This paper proposes a bilateral awareness network (BAN) that combines the strengths of convolutional and transformer architectures for semantic segmentation of very fine-resolution urban scene images. Our approach effectively leverages the global and local contextual information within the image to accurately predict semantic segmentation masks. We introduce a novel bilateral attention module that computes attention weights based on both the global and local features, enabling the network to focus on relevant regions and capture long-range dependencies effectively. Furthermore, we propose a stacked hourglass network that incorporates multi-scale context aggregation and feature reuse to improve performance. Our approach achieves state-of-the-art performance on the Cityscapes dataset, with a mean accuracy of 92.5% on the fine-grained level and 89.1% on the coarse-grained level."
    },
    {
      "id": "105a0291-57e6-4930-9015-4dec3fc1f1b9",
      "adv_source_id": "105a0291-57e6-4930-9015-4dec3fc1f1b9",
      "source_id": "05f40b6d-67cf-4a6e-ad2f-cfe003b7917e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images",
      "prompt": "Write the abstract for the academic paper titled \"Transformer Meets Convolution: A Bilateral Awareness Net-work for Semantic Segmentation of Very Fine Resolution Ur-ban Scene Images\".",
      "generation": "Semantic segmenta-tion of very fine resolution urban scene images is a fundamental task in the field of urban computing, where the goal is to automatically as-sign pixel-level labels to each fine-grained urban object in im-ages. We present a bilateral awareness network (BA-Net) that achieves state-of-the-art performance on this task. BA-Net is composed of a stack of dilated convolutions and at-tention modules, which are able to capture global and local fea-tures respectively. The key innovation of BA-Net is the introduction of a bilateral grid structure, which enables the network to extract features in a location-aware manner. This means that when the network is parsing an image, it not only considers the global and local features within a given pixel, but also the global and local features of its neighboring pixels. We evaluate BA-Net on the recently published Cityscapes dataset [24], which contains fine-grained urban scene images annotated with twenty-six different classes. Our re-sults show that BA-Net achieves state-of-the-art performance on this dataset, with a per-formance improvement of over 35% relative to the previous state-of-the-art."
    },
    {
      "id": "d10ee8de-dc51-4de7-a483-73482abc4223",
      "adv_source_id": "d10ee8de-dc51-4de7-a483-73482abc4223",
      "source_id": "c404dbd3-7cec-41f5-8b70-968e9212ed7c",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Large-scale image segmentation based on distributed clustering algorithms",
      "prompt": "Write the abstract for the academic paper titled \"Large-scale image segmentation based on distributed clustering algorithms\".",
      "generation": "An efficient algorithm for large-scale image segmentation based on a distributed clustering approach is proposed in this paper. The algorithm is designed to handle large datasets and to improve the accuracy of image segmentation. The proposed algorithm consists of two main steps: a pre-processing step and a segmentation step. In the pre-processing step, the image is divided into a set of overlapping regions, and a feature vector is extracted for each region. In the segmentation step, a distributed clustering algorithm is applied to the feature vectors to obtain a set of clusters. The clusters are then used to segment the image. The proposed algorithm is evaluated on a large dataset and compared to a state-of-the-art image segmentation algorithm. The results show that the proposed algorithm is able to handle large datasets and to improve the accuracy of image segmentation."
    },
    {
      "id": "911f0447-c1c9-4800-87dd-fafa5d714fa5",
      "adv_source_id": "911f0447-c1c9-4800-87dd-fafa5d714fa5",
      "source_id": "c404dbd3-7cec-41f5-8b70-968e9212ed7c",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Large-scale image segmentation based on distributed clustering algorithms",
      "prompt": "Write the abstract for the academic paper titled \"Large-scale image segmentation based on distributed clustering algorithms\".",
      "generation": "An image segmentation algorithm based on distributed clustering algorithms is implemented to enable the segmentation of large-scale images. The algorithm is designed to work efficiently on distributed computing systems, allowing for the segmentation of large images in a scalable and efficient manner. A novel approach for grouping pixels based on their similarity is proposed which shows promising results in terms of segmentation quality and speed. The algorithm is evaluated on a large dataset of images and compared to other state-of-the-art image segmentation algorithms. The results show that the proposed algorithm is able to achieve high segmentation quality while maintaining efficiency in terms of running time and memory usage."
    },
    {
      "id": "d778a0b9-f318-4ad0-af3b-dbe5377ed216",
      "adv_source_id": "d778a0b9-f318-4ad0-af3b-dbe5377ed216",
      "source_id": "bc43fe33-d378-4bc3-b666-038cbacbdbc8",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Trilateral Attention Network for Real-time Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Trilateral Attention Network for Real-time Medical Image Segmentation\".",
      "generation": "The paper presents a novel approach for real-time medical image segmentation based on a trilateral attention network. The network utilizes three types of attention mechanisms: self-attention, cross-attention, and collective attention. The self-attention mechanism allows the network to focus on relevant regions of the input image, while the cross-attention mechanism enables the network to learn contextual relationships between different parts of the image. The collective attention mechanism helps the network to capture global information and improve the overall segmentation performance.\n\nThe proposed method was evaluated on the publicly available MICCAI-BraTS 2019 dataset, which contains magnetic resonance imaging (MRI) scans of brain tumors. The experimental results show that the trilateral attention network achieves state-of-the-art performance in terms of segmentation accuracy and runtime, making it suitable for real-time applications.\n\nThe paper makes the following contributions: (1) it proposes a novel trilateral attention network architecture for medical image segmentation; (2) it introduces three different attention mechanisms that work in tandem to improve the segmentation performance; and (3) it demonstrates the effectiveness of the proposed method on the challenging MICCAI-BraTS 2019 dataset."
    },
    {
      "id": "c22c9715-23fc-4c28-89e5-bc7f23684cff",
      "adv_source_id": "c22c9715-23fc-4c28-89e5-bc7f23684cff",
      "source_id": "bc43fe33-d378-4bc3-b666-038cbacbdbc8",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Trilateral Attention Network for Real-time Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Trilateral Attention Network for Real-time Medical Image Segmentation\".",
      "generation": "The paper presents a novel approach for real-time medical image segmentation based on a trilateral attention network. The network utilizes attention mechanisms to weigh the importance of different features and improve the segmentation accuracy. The proposed method is evaluated on a public dataset and compared to state-of-the-art algorithms, demonstrating its effectiveness in segmenting medical images in real-time. The paper concludes by discussing the potential applications of the proposed method in clinical practice and future research directions."
    },
    {
      "id": "9bd4bfcd-7b0e-4ccc-b182-54f2fbcf3df3",
      "adv_source_id": "9bd4bfcd-7b0e-4ccc-b182-54f2fbcf3df3",
      "source_id": "eff4a017-daa1-4b5d-84ac-927333aaa900",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery",
      "prompt": "Write the abstract for the academic paper titled \"Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery\".",
      "generation": "The paper proposes a new algorithm for real-time stereo matching, a technique used in minimally invasive surgery to obtain a 3D view of the surgical area. The algorithm is based on the Bayesian dense inverse searching approach, which uses a probabilistic model to find the best match between two images. The algorithm is designed to be efficient and to handle large amounts of data, and it is evaluated on a publicly available dataset. The results show that the algorithm is effective at finding good matches between images and is suitable for real-time stereo matching in minimally invasive surgery. \n\nThe abstract should include the following:\n1. The proposed algorithm is based on Bayesian dense inverse searching.\n2. It is designed for real-time stereo matching in minimally invasive surgery.\n3. It uses a probabilistic model to find the best match between two images. \n4. The algorithm is evaluated on a publicly available dataset, and the results show that it is effective at finding good matches between images.\n5. Finally, the abstract should conclude with a brief summary of the algorithm's potential applications in minimally invasive surgery."
    },
    {
      "id": "cd01c216-1b5c-4861-92fd-d34d9b0c206c",
      "adv_source_id": "cd01c216-1b5c-4861-92fd-d34d9b0c206c",
      "source_id": "eff4a017-daa1-4b5d-84ac-927333aaa900",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery",
      "prompt": "Write the abstract for the academic paper titled \"Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery\".",
      "generation": "The paper proposes a new algorithm for real-time stereo matching, a technique used to reconstruct 3D scenes in minimally invasive surgery. The algorithm is based on the Bayesian dense inverse searching approach, which utilizes a probabilistic model to estimate the depth of the scene. The algorithm is designed to be efficient and accurate, with the ability to handle large amounts of data and complex scenes. The paper discusses the algorithm's performance on a variety of stereo matching datasets, and demonstrates that it outperforms existing state-of-the-art algorithms in terms of accuracy and efficiency. The algorithm has the potential to improve the safety and effectiveness of minimally invasive surgery by providing a more accurate and efficient way to reconstruct 3D scenes."
    },
    {
      "id": "007860a9-77a4-446e-bcc9-44c233312baf",
      "adv_source_id": "007860a9-77a4-446e-bcc9-44c233312baf",
      "source_id": "92bcf142-f39d-48c7-a63e-0b770803fc81",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation\".",
      "generation": "Abstract:\nConnectomics is a field of neuroscience that studies the connectome, which is a map of all the connections in the brain. This paper introduces RLCorrector, a reinforced proofreading approach to correct segmentation errors in connectomics images. Our method leverages a fully convolutional neural network (FCN) and a graphical model to iteratively improve segmentation accuracy. We demonstrate the effectiveness of our approach on a public dataset, where we achieve significant improvements in segmentation accuracy over the state-of-the-art. We also show that our method is efficient and scalable, capable of handling large datasets and complex architectures. RLCorrector is a valuable tool for accurate connectomics image segmentation, enabling detailed analysis of neural connectivity."
    },
    {
      "id": "d706eae3-f61c-4a16-9eaa-6d1d7fc32050",
      "adv_source_id": "d706eae3-f61c-4a16-9eaa-6d1d7fc32050",
      "source_id": "92bcf142-f39d-48c7-a63e-0b770803fc81",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"RLCorrector: Reinforced Proofreading for Connectomics Image Segmentation\".",
      "generation": "Abstract:\nConnectomics is a field of neuroscience that relies on the analysis of electron microscope images of the brain to study the intricate web of neural connections. The segmentation of these images into individual neurons and their branches is a painstaking process, prone to errors that can lead to misidentifications of neural connections. We present an approach to reinforced proofreading of connectomics image segmentation that improves accuracy and efficiency. Our method, RLCorrector, combines reinforcement learning with corrective feedback from expert anatomists to identify and correct segmentation errors. We demonstrate the effectiveness of our approach on a dataset of 20 images, achieving a mean accuracy improvement of 16.7% over the initial segmentations. We further show that our method learns to correct errors in a manner that generalizes across images, and improves with increasing amounts of training data. This work provides a framework for improved Connectomics analysis, and can serve as a foundation for fully-automated segmentation approaches."
    },
    {
      "id": "ce0d51d5-290a-498e-afd4-48a4d3c49a9b",
      "adv_source_id": "ce0d51d5-290a-498e-afd4-48a4d3c49a9b",
      "source_id": "93029efb-6d75-45e7-bcfa-6ccd61690b3f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks",
      "prompt": "Write the abstract for the academic paper titled \"Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks\".",
      "generation": "We introduce Points2Polygons, an approach for training a geometric context-based segmentation model using weak labels. Points2Polygons utilizes a fully convolutional network that operates directly on point clouds to produce a segmentation mask for each point. We train our network using only a set of sparse labels that indicate bounding-box level object annotations, but contain no pixel-level segmentation labels. To this end, we propose an adversarial loss that leverages a strong auxiliary classifier network to guide the segmentation network training. Our approach naturally handles noise, outliers, and missing points, and does not require any form of point-cloud preprocessing such as surface reconstruction. We demonstrate the efficacy of our approach on the task of semantic segmentation of indoor scenes from LiDAR scans."
    },
    {
      "id": "eee48914-616e-47ee-ab84-3ee39de455fd",
      "adv_source_id": "eee48914-616e-47ee-ab84-3ee39de455fd",
      "source_id": "93029efb-6d75-45e7-bcfa-6ccd61690b3f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks",
      "prompt": "Write the abstract for the academic paper titled \"Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks\".",
      "generation": "We introduce Points2Polygons, an approach for training a segmentation model using weak labels alone, without needing strong labels. Unlike prior work on weakly supervised segmentation, Points2Polygons does not rely on a pre-defined objectness measure computed over the weak labels. Our approach instead minimizes an adversarial objective that encourages the model to output a segmentation that agrees with the weak labels, while being discriminative with respect to the context. This allows the model to discover objectness and detailed object boundaries based on contextual cues, without needing a separate objectness measure. We show that our approach matches the performance of strong supervision while using only weak labels, and even surpasses it in cases where the weak labels are easier to obtain."
    },
    {
      "id": "acff5cf5-a8e6-4681-89e5-de1faaa55849",
      "adv_source_id": "acff5cf5-a8e6-4681-89e5-de1faaa55849",
      "source_id": "0818a486-cec9-4668-bef1-e22e2f92b912",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy",
      "prompt": "Write the abstract for the academic paper titled \"A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy\".",
      "generation": "In this paper, we present a thorough view of exact inference in graphs from the degree-4 sum-of-squares hierarchy. This hierarchy is a generalization of the degree-3 sum-of-squares hierarchy, which has been widely used for exact inference in graphs. The degree-4 sum-of-squares hierarchy allows for more expressive models and faster computation, making it a powerful tool for exact inference in graphs. We provide a comprehensive review of the degree-4 sum-of-squares hierarchy, including its definition, properties, and applications. We also present a set of tools for working with the degree-4 sum-of-squares hierarchy, including algorithms for computing the partition function and the marginals. Finally, we provide a set of examples to demonstrate the use of the degree-4 sum-of-squares hierarchy for exact inference in graphs."
    },
    {
      "id": "c5acf955-faa0-4fd4-91d4-d0ae19ed808e",
      "adv_source_id": "c5acf955-faa0-4fd4-91d4-d0ae19ed808e",
      "source_id": "0818a486-cec9-4668-bef1-e22e2f92b912",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy",
      "prompt": "Write the abstract for the academic paper titled \"A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy\".",
      "generation": "In this paper, we present a thorough view of exact inference in graphs from the degree-4 sum-of-squares hierarchy. This hierarchy is a generalization of the degree-3 sum-of-squares hierarchy, which has been widely used for exact inference in graphs. The degree-4 sum-of-squares hierarchy allows for more expressive models and faster computation, making it a powerful tool for exact inference in graphs. We provide a comprehensive review of the degree-4 sum-of-squares hierarchy and its applications to exact inference in graphs. We also discuss the advantages and limitations of this hierarchy and provide a comparison to other methods for exact inference in graphs. Our goal is to provide a thorough understanding of the degree-4 sum-of-squares hierarchy and its applications to exact inference in graphs."
    },
    {
      "id": "57d4e99f-5012-43c3-b435-20602eab6e46",
      "adv_source_id": "57d4e99f-5012-43c3-b435-20602eab6e46",
      "source_id": "1e89cdc5-f7ba-4f0b-a2f3-22285fade7a5",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection",
      "prompt": "Write the abstract for the academic paper titled \"High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection\".",
      "generation": "In this paper, we propose a high-resolution segmentation algorithm for tooth root fuzzy edges based on polynomial curve fitting and landmark detection. Our algorithm utilizes the curvature information of the tooth surface to accurately segment the tooth root fuzzy edge. The proposed method takes into account the local structure of the tooth root and adapts to the varying shapes and sizes of the teeth. We evaluate the performance of our algorithm on a dataset of dental images and compare it with state-of-the-art methods. The results show that our algorithm significantly improves the accuracy and robustness of tooth root segmentation, thus providing a valuable tool for dental image analysis and diagnosis."
    },
    {
      "id": "d358c87f-4454-4dbd-a0ee-92dc864ff1ea",
      "adv_source_id": "d358c87f-4454-4dbd-a0ee-92dc864ff1ea",
      "source_id": "1e89cdc5-f7ba-4f0b-a2f3-22285fade7a5",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection",
      "prompt": "Write the abstract for the academic paper titled \"High-Resolution Segmentation of Tooth Root Fuzzy Edge Based on Polynomial Curve Fitting with Landmark Detection\".",
      "generation": "The tooth root fuzzy edge segmentation of dental panoramic images is a crucial step for identifying the location of the tooth roots and computing the geometric parameters of the teeth. In this paper, we proposed a high-resolution segmentation approach based on polynomial curve fitting and landmark detection. The proposed method starts with extracting the dental panoramic image features such as curve length, curvature, and intensity. Then, the polynomial curve fitting is performed between pre-selected points to obtain the optimal segmentation result. The proposed method was evaluated on a dataset of dental panoramic images. The experimental results demonstrate the superiority of the proposed method over the state-of-the-art approaches in terms of precision and recall."
    },
    {
      "id": "83edf62c-9192-4e5c-a666-dc119e6b2e6a",
      "adv_source_id": "83edf62c-9192-4e5c-a666-dc119e6b2e6a",
      "source_id": "162114f5-d069-46bf-a0c6-b5d9cf465c61",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Active Contours Using Locally Controlled Distance Vector Flow",
      "prompt": "Write the abstract for the academic paper titled \"Deep Active Contours Using Locally Controlled Distance Vector Flow\".",
      "generation": "The paper presents a new framework for active contour models that introduces a locally controlled distance vector flow. This allows the model to adapt in an efficient manner to the local characteristics of the image region inside which the contour is evolving. The framework is instantiated using a deep neural network architecture that is specifically designed to predict the locally controlled vector field. This network is trained using a new loss function that encourages the vector field to follow the image gradient while remaining locally controlled. The resulting model is demonstrated to perform exceptionally well in a variety of challenging image segmentation tasks. In particular, the model is shown to be able to accurately segment irregular and noisy contours that are difficult for standard active contour models to handle."
    },
    {
      "id": "b2468cc7-40f9-4d40-91fb-d60e14d736de",
      "adv_source_id": "b2468cc7-40f9-4d40-91fb-d60e14d736de",
      "source_id": "162114f5-d069-46bf-a0c6-b5d9cf465c61",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Active Contours Using Locally Controlled Distance Vector Flow",
      "prompt": "Write the abstract for the academic paper titled \"Deep Active Contours Using Locally Controlled Distance Vector Flow\".",
      "generation": "The paper presents a new algorithm for active contour models that incorporates local control of the contour evolution. The proposed method, called Deep Active Contours using Locally Controlled Distance Vector Flow (DAC-LCDVF), combines ideas from active contour models and deep learning. The DAC-LCDVF algorithm utilizes a deep neural network to learn a distance function between the evolving contour and the desired object boundary. The learned distance function is then used to guide the contour evolution by means of a vector flow field. The vector flow field is defined by the locally controlled distance vector flow algorithm, which ensures that the contour evolution is guided by local information only. The proposed method is evaluated on a number of benchmark datasets and the results show that it outperforms the state-of-the-art in active contour models."
    },
    {
      "id": "00c94b84-21aa-4460-96f1-ea721ab1fcc7",
      "adv_source_id": "00c94b84-21aa-4460-96f1-ea721ab1fcc7",
      "source_id": "2b623feb-b86c-48f8-84df-83ac9d36359f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-Modal Progressive Comprehension for Referring Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Cross-Modal Progressive Comprehension for Referring Segmentation\".",
      "generation": "Abstract:\nReferring segmentation is the task of decomposing a sentence into referring expressions that refer to entities in the context given. However, current referring segmentations lack context awareness, which limits their ability in understanding referring expressions in a broader context. In this paper, we present a cross-modal progressive comprehension approach to referring segmentation that incorporates both textual and visual information. Our approach leverages the power of pre-training and progressive comprehension to generate referring segments that are more accurate and context-aware. We evaluate our approach on the referring expression comprehension task, where our model outperforms the current state-of-the-art in referring segmentation.\n\nOur Approach:\nOur approach consists of two main components: (1) a pre-trained language model (LM) and (2) a progressive comprehension module. The pre-trained LM is used to generate referring segments by conditioning on the referring expressions in the input sentence. The progressive comprehension module then refines these referring segments by considering the broader context of the input, including visual information. This module uses a progressive approach, where the referring segments are refined iteratively, with each iteration considering more context.\n\nResults:\nWe evaluate our approach on the referring expression comprehension task, which consists of a referring expression generation and a referring segmentation subtask. Our model achieves state-of-the-art performance on both subtasks, with improvements of 2.2% and 2.6% respectively."
    },
    {
      "id": "9d538abc-7cf8-40c2-b8ab-4b6d925f6f18",
      "adv_source_id": "9d538abc-7cf8-40c2-b8ab-4b6d925f6f18",
      "source_id": "2b623feb-b86c-48f8-84df-83ac9d36359f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-Modal Progressive Comprehension for Referring Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Cross-Modal Progressive Comprehension for Referring Segmentation\".",
      "generation": "Abstract:\nReferring expressions are coherent descriptions of objects in an image used to establish a ground truth reference for the object. Most prior work on referring expression comprehension focuses on text-to-image tasks, where a textual description is provided and the goal is to identify the region in the image that the description refers to. However, in many practical applications, the information available is not solely textual but is provided across multiple modalities, such as images, videos, and language. We introduce a cross-modal progressive comprehension approach to referring expression comprehension, in which we initially leverage the referring expressions themselves to obtain a coarse initial segmentation and then refine it progressively by iteratively incorporating additional modalities. Our approach significantly improves upon the state-of-the-art referring region segmentation approaches on the challenging ReferItSeg benchmark, particularly when the number of referring expressions is limited."
    },
    {
      "id": "677355bc-7997-4c06-a455-ef2605b6b57c",
      "adv_source_id": "677355bc-7997-4c06-a455-ef2605b6b57c",
      "source_id": "ef61c066-14f3-413e-a266-afb9742903d9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Large-Scale Benchmark for Food Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Large-Scale Benchmark for Food Image Segmentation\".",
      "generation": "Abstract:\nWe introduce a large-scale benchmark for food image segmentation, consisting of 1,000 images divided into four categories. The images were collected from real-world scenarios, including restaurants, supermarkets, and kitchens. We aim to bridge the gap between academic research and practical applications by providing a dataset that is challenging and representative of the food image segmentation task. Our benchmark is designed to evaluate the performance of different algorithms and to encourage the development of new techniques. We will make the dataset publicly available, allowing researchers to compare their methods and contribute to the advancement of food image segmentation."
    },
    {
      "id": "6c229e5c-aaca-491e-a924-67f215f022c8",
      "adv_source_id": "6c229e5c-aaca-491e-a924-67f215f022c8",
      "source_id": "ef61c066-14f3-413e-a266-afb9742903d9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Large-Scale Benchmark for Food Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Large-Scale Benchmark for Food Image Segmentation\".",
      "generation": "Abstract:\nWe introduce a large-scale benchmark for food image segmentation, titled FoodSeg. The benchmark consists of a total of 5,452 images from six different categories of food items. The images are manually segmented and labeled by experts, and the dataset is split into a training set (5,024 images) and a testing set (428 images) in a ratio of 8:1. The goal of the benchmark is to promote the development of accurate and robust food image segmentation algorithms. We plan to make the benchmark available to the research community to facilitate further research in this field."
    },
    {
      "id": "d71f29a7-4ef1-4536-92b4-e998319cf487",
      "adv_source_id": "d71f29a7-4ef1-4536-92b4-e998319cf487",
      "source_id": "27891e8c-5a91-4bbc-a1a8-1de8c44fe219",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian Logistic Shape Model Inference: application to cochlea image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Bayesian Logistic Shape Model Inference: application to cochlea image segmentation\".",
      "generation": "In this paper, we present a novel approach for cochlea image segmentation based on a Bayesian logistic shape model. The proposed method utilizes a probabilistic model that describes the spatial distribution of pixels in the image, allowing for more accurate and robust segmentation compared to traditional thresholding techniques. The Bayesian inference algorithm is employed to estimate the model parameters and infer the most likely segmentation. The experimental results show that the proposed method significantly outperforms existing approaches in terms of accuracy and running time."
    },
    {
      "id": "74b94f78-4e90-4a89-88b6-2d24d264bd52",
      "adv_source_id": "74b94f78-4e90-4a89-88b6-2d24d264bd52",
      "source_id": "27891e8c-5a91-4bbc-a1a8-1de8c44fe219",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bayesian Logistic Shape Model Inference: application to cochlea image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Bayesian Logistic Shape Model Inference: application to cochlea image segmentation\".",
      "generation": "In this paper, we present a novel approach for cochlea image segmentation based on a Bayesian logistic shape model. The proposed method utilizes a probabilistic model that describes the spatial distribution of pixels in the image, allowing for more accurate and robust segmentation compared to traditional thresholding techniques. The Bayesian framework enables efficient inference, resulting in improved segmentation performance. We evaluate the proposed method on a dataset of cochlea images and demonstrate its effectiveness compared to state-of-the-art techniques."
    },
    {
      "id": "f48f00e8-0bdc-4dc9-b5f8-40b927b32803",
      "adv_source_id": "f48f00e8-0bdc-4dc9-b5f8-40b927b32803",
      "source_id": "59635421-a84f-4293-be66-323c49fca652",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation\".",
      "generation": "An active contour model with two fronts is introduced, which propagates through the image based on an external energy and a internal energy. This general model is derived from an optimal control formulation in which the propagation of fronts is governed by a velocity field. The velocity field, in turn, is obtained by minimizing a functional that couples both external and internal energies. This model offers more flexibility than classical active contour models, as it can accommodate various external energies (e.g., edge detectors, curvature detectors) and various internal energies (e.g., Gaussian, mean-shift). We show that our active contour model can be derived as a special case of the level set method when the Gaussian energy is used. The proposed model is evaluated on several images, and comparisons with a classical active contour model are presented. Additionally, we demonstrate the use of our active contour model for image segmentation."
    },
    {
      "id": "52639c1e-1435-48c3-b3fd-499c34c062a5",
      "adv_source_id": "52639c1e-1435-48c3-b3fd-499c34c062a5",
      "source_id": "59635421-a84f-4293-be66-323c49fca652",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Generalized Asymmetric Dual-front Model for Active Contours and Image Segmentation\".",
      "generation": "An active contour model with two fronts is generalized so that the two fronts can have different initial shapes, different propagation rules, and different goals. This generalized asymmetric model can be applied to more interesting binary segmentation problems where the two objects in the image have significantly different properties, such as intensity, texture, or shape. The effectiveness of the model is demonstrated via a variety of examples, including an image of a cow and a tractor, and a difficult binary segmentation problem involving a cluttered image of a garage full of cars."
    },
    {
      "id": "912fb857-b4b4-4099-bba6-5dd35e53647b",
      "adv_source_id": "912fb857-b4b4-4099-bba6-5dd35e53647b",
      "source_id": "a957f241-6622-4b33-9d53-932c2c3ac2b6",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Rethinking BiSeNet For Real-time Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Rethinking BiSeNet For Real-time Semantic Segmentation\".",
      "generation": "Abstract:\nWe address the problem of designing an efficient BiSeNet architecture for real-time semantic segmentation. Our approach is to simplify the recently proposed BiSeNet architecture while maintaining its performance. We achieve this by removing the depth-wise convolutional layers in the original design, which reduces the computational cost of the network. Our experiments show that our simplified architecture performs on par with the original BiSeNet in terms of accuracy but has a lower computational cost. We also show that our architecture is faster than the original BiSeNet by a factor of 2, while maintaining high accuracy. Our code and models are available at https://github.com/zzzz1993/BiSeNet.\n\nConclusion:\nWe have presented a simplified version of the BiSeNet architecture for real-time semantic segmentation. Our design removes the depth-wise convolutional layers in the original BiSeNet, which reduces the computational cost of the network. Our experiments show that our simplified architecture performs on par with the original BiSeNet in terms of accuracy but has a lower computational cost. We also show that our architecture is faster than the original BiSeNet by a factor of 2, while maintaining high accuracy. We believe that our work can serve as a strong baseline for future research on real-time semantic segmentation."
    },
    {
      "id": "a8602a64-5307-4073-846d-a7e89e3c1418",
      "adv_source_id": "a8602a64-5307-4073-846d-a7e89e3c1418",
      "source_id": "a957f241-6622-4b33-9d53-932c2c3ac2b6",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Rethinking BiSeNet For Real-time Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Rethinking BiSeNet For Real-time Semantic Segmentation\".",
      "generation": "Abstract:\nWe address the problem of designing an efficient BiSeNet (Bidirectional Feature Sequences for Real-time Semantic Segmentation) by exploring alternative convolutional block structures and optimizing the parameters for targeted platforms, namely GPUs and AI-accelerated GPUs (AI-GPU). Previous works have shown the benefit of bidirectional feature sequencing in reducing memory overhead and computational complexity. We build upon the recently proposed BiSeNet-aux and reshape its encoder-decoder architecture to improve its efficiency. In particular, we propose to replace the auxiliary classifiers with a new lightweight decoder architecture that reduces the parameters by 38.9\\% and the inference time by 23\\% while maintaining the same level of performance. Furthermore, we explore the appropriate parameters for both GPUs and AI-GPUs. Our experimental results show that our proposed method achieves 79.3\\% mean accuracy on the Cityscapes dataset with only 1.83\\ ms of inference time on an AI-GPU, which is significantly faster than the state-of-the-art performance."
    },
    {
      "id": "1cfbd864-19e1-43eb-b343-6d14a3421af3",
      "adv_source_id": "1cfbd864-19e1-43eb-b343-6d14a3421af3",
      "source_id": "59abcb55-9a95-4a34-9479-70cc7198c288",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement",
      "prompt": "Write the abstract for the academic paper titled \"GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement\".",
      "generation": "An increasingly important yet challenging problem in AI is learning object representations from images. While significant progress has been made for ordered objects (e.g., arrangeable objects in a scene), far less focus has been given to unordered objects (e.g., identifyable objects in an image), where an object's pose and occlusion significantly complicate the learning process. To address this, we propose GENESIS-V2, a novel approach for inferring unordered object representations without iterative refinement, which is a commonly used technique in existing state-of-the-art models. \n\nGENESIS-V2 is a single-stage approach that uses a transformer-based architecture to directly embed images into a high-dimensional space, where the distance between embeddings reflects the similarity between images. Our approach is the first to use a contrastive loss that compares embeddings of different images rather than same-image embeddings, which allows GENESIS-V2 to infer unordered object representations without iterative refinement. We show that GENESIS-V2 is able to outperform the current state-of-the-art on the popular CIFAR-100 and SVHN datasets with a significant margin."
    },
    {
      "id": "bc6cbb05-cf48-4fdb-b56a-b94db14f14fd",
      "adv_source_id": "bc6cbb05-cf48-4fdb-b56a-b94db14f14fd",
      "source_id": "59abcb55-9a95-4a34-9479-70cc7198c288",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement",
      "prompt": "Write the abstract for the academic paper titled \"GENESIS-V2: Inferring Unordered Object Representations without Iterative Refinement\".",
      "generation": "An increasingly important yet challenging problem in AI is learning object representations from images. In this paper we present an approach to this problem, Genesys-V2, that is designed to learn such representations without significant manual intervention. Specifically, our approach is able to learn a hierarchy of object classes from a set of labeled images without any iterative refinement or greedy search over a space of learned representations. Our approach combines a top-down, generative, and discriminative approach to object recognition, and is based on a new architecture called the Generative Object Recognition Network, or GORN. We show that GORN is able to learn representations that are highly discriminative for a wide variety of object classes, and that these representations are easily interpretable by humans."
    },
    {
      "id": "10cba980-9fb0-45b1-a591-525e5cb0f351",
      "adv_source_id": "10cba980-9fb0-45b1-a591-525e5cb0f351",
      "source_id": "bb5299ed-bebd-4210-b65b-6e466c875342",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization",
      "prompt": "Write the abstract for the academic paper titled \"Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization\".",
      "generation": "Abstract:\nGenerative models, such as generative adversarial networks (GANs) and variational autoencoders (VAEs), have demonstrated promising results for image synthesis and recognition. In this paper, we explore the use of generative models for semantic segmentation, a task that involves assigning semantic labels to each pixel in an image. We propose a novel approach that combines the strengths of supervised learning and generative modeling to improve the performance of semantic segmentation. Our method utilizes a generative model to synthesize additional training data and learn a more robust segmentation model. Furthermore, we explore the use of semi-supervised learning techniques to leverage the unlabeled data and further improve the segmentation performance. Our experiments show that our method achieves state-of-the-art performance on the Cityscapes and CamVid datasets, and demonstrates strong out-of-domain generalization."
    },
    {
      "id": "b005f2b0-0e66-44cc-90df-f13855cc6ae3",
      "adv_source_id": "b005f2b0-0e66-44cc-90df-f13855cc6ae3",
      "source_id": "bb5299ed-bebd-4210-b65b-6e466c875342",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization",
      "prompt": "Write the abstract for the academic paper titled \"Semantic Segmentation with Generative Models: Semi-Supervised Learning and Strong Out-of-Domain Generalization\".",
      "generation": "Abstract:\nGenerative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs), have demonstrated promising results for image synthesis and recognition. In this paper, we explore the use of generative models for semantic segmentation, a task that involves assigning semantic labels to each pixel in an image. We propose a new framework that combines the strengths of generative models and semi-supervised learning to improve the performance of semantic segmentation. Our approach utilizes a generative model to generate synthetic data for training, which is particularly useful for domains with limited training data. We further explore the use of a semi-supervised loss function that leverages the generative model to improve the segmentation performance on both the labeled and unlabeled data. Furthermore, we propose a strong out-of-domain generalization (SODG) loss that encourages the model to learn more robust representations that generalize well to unseen domains. Our experimental results show that our approach significantly improves the performance of semantic segmentation, particularly in the presence of limited training data and when dealing with out-of-domain data."
    },
    {
      "id": "a8bb45ef-00d8-4042-9884-d3437477cd21",
      "adv_source_id": "a8bb45ef-00d8-4042-9884-d3437477cd21",
      "source_id": "2003dcc1-7c5b-4c86-9263-cdcda09a1655",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation",
      "prompt": "Write the abstract for the academic paper titled \"Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation\".",
      "generation": "Here is an abstract for the academic paper titled \"Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for Polyp Localisation\":\n\nPolyp localisation is a challenging task in medical imaging. In this paper, we propose a method for polyp localisation using ensemble learning and comprehensive learning particle swarm optimisation (PSO). We use a set of classifiers, each trained on a different feature, to produce a final prediction. The classifiers are combined using PSO, which optimises the weights of each classifier based on their performance. Our method was tested on a dataset of endoscopic images and achieved a high accuracy rate. The results show that our method is effective in localising polyps and can be used to assist in medical imaging tasks. \n\nWould you like me to generate the introduction section of the academic paper titled \"Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation\"?"
    },
    {
      "id": "ee5a6ae5-98c6-4abd-8f2a-fa8dfe5edc2d",
      "adv_source_id": "ee5a6ae5-98c6-4abd-8f2a-fa8dfe5edc2d",
      "source_id": "2003dcc1-7c5b-4c86-9263-cdcda09a1655",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation",
      "prompt": "Write the abstract for the academic paper titled \"Ensemble Learning based on Classifier Prediction Confidence and Comprehensive Learning Particle Swarm Optimisation for polyp localisation\".",
      "generation": "Abstract\u2014\n\nPolyp localisation is an important first step towards automated polyp detection in colorectal endoscopic images. In this paper, we propose an ensemble learning scheme for polyp localisation based on classifier prediction confidence and comprehensive learning particle swarm optimisation. First, a set of features is extracted from the pre-trained deep learning-based polyp detectors. Then, we design a classifier prediction confidence-based ensemble learning framework, where the final classification result is obtained by combining the predictions of base classifiers. Besides, a comprehensive learning particle swarm optimisation algorithm is employed to optimise the ensemble weights. The proposed method has been evaluated on a public dataset, and the results show that our method achieves an F1-score of 0.79, outperforming the state-of-the-art approaches."
    },
    {
      "id": "c625d6bd-167c-4797-9e8c-a4e6f0ac4061",
      "adv_source_id": "c625d6bd-167c-4797-9e8c-a4e6f0ac4061",
      "source_id": "764ed713-72bd-43c1-85da-9b71c62bd873",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax",
      "prompt": "Write the abstract for the academic paper titled \"Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax\".",
      "generation": "Below is an example of what an abstract for this academic paper could be:\n\nAbstract:\n\nPneumothorax is a common medical condition that occurs when air leaks into the space between a person's lungs and chest wall. It is difficult to diagnose based on a single X-ray image, as it may appear similar to other conditions such as tuberculosis or cancer. In this paper, we present a method of segmenting pneumothorax on X-ray images using a deep learning approach. Our model is based on a fully convolutional neural network and has been trained on a large dataset of X-ray images with and without pneumothorax. The model performs well on both the test and validation datasets, with an F1 score of 0.91 and 0.88, respectively. We believe that this model could be used to assist in the diagnosis of pneumothorax, potentially reducing the time and cost of diagnosis."
    },
    {
      "id": "91955594-9c90-4936-bd54-422180cb288a",
      "adv_source_id": "91955594-9c90-4936-bd54-422180cb288a",
      "source_id": "764ed713-72bd-43c1-85da-9b71c62bd873",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax",
      "prompt": "Write the abstract for the academic paper titled \"Pneumothorax Segmentation: Deep Learning Image Segmentation to predict Pneumothorax\".",
      "generation": "Below is an example of what an abstract for this academic paper could be:\n\nAbstract:\n\nPneumothorax is a common medical condition that occurs when air leaks into the space between a patient's lungs and chest wall. It is difficult for radiologists to identify pneumothorax in chest radiographs as visual characteristics of pneumothorax vary according to the pneumothorax size and its location. In this paper, we present a state-of-the-art deep learning image segmentation model that segments pneumothorax from chest radiographs. Our model is based on a U-Net architecture with an attention module that focuses on the regions of interest in the image. We trained our model on a large dataset of chest radiographs with and without pneumothorax and evaluated its performance on a separate test dataset. Our results show that our model is effective at segmenting pneumothorax and could be used as a tool to help radiologists diagnose this condition."
    },
    {
      "id": "5464df8a-db84-4a15-9ff7-175618f4c98b",
      "adv_source_id": "5464df8a-db84-4a15-9ff7-175618f4c98b",
      "source_id": "d4f93d77-d6d9-4ee1-91d8-58ce8c7fe216",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation\".",
      "generation": "In this paper, we present a Feedback Attention Network (FANet) for improved biomedical image segmentation. FANet utilizes a unique attention module that highlights areas of interest and suppresses background noise. This attention module is trained using a feedback loop, allowing the network to learn to attend to the most relevant features. We evaluate FANet on three public biomedical image segmentation datasets, and achieve state-of-the-art performance on all three. Our results show that FANet is a effective approach for biomedical image segmentation, improving upon previous methods in both accuracy and efficiency."
    },
    {
      "id": "270ca014-3e5d-46f5-af36-38ddaf5a745b",
      "adv_source_id": "270ca014-3e5d-46f5-af36-38ddaf5a745b",
      "source_id": "d4f93d77-d6d9-4ee1-91d8-58ce8c7fe216",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"FANet: A Feedback Attention Network for Improved Biomedical Image Segmentation\".",
      "generation": "In this paper, we present a Feedback Attention Network (FANet) for improved biomedical image segmentation. FANet utilizes a unique attention module that highlights areas of interest within an image and feeds this information back into the network. This feedback mechanism allows FANet to selectively focus on relevant image content and suppress irrelevant information, leading to improved segmentation performance. We evaluate FANet on three different biomedical image segmentation datasets, and our results demonstrate that it significantly outperforms the state-of-the-art in terms of both accuracy and efficiency."
    },
    {
      "id": "54e983df-ffa4-4522-8202-fdeb92bc87a4",
      "adv_source_id": "54e983df-ffa4-4522-8202-fdeb92bc87a4",
      "source_id": "e9899aaa-4e38-408c-bc2c-7f1fb3cf934c",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Self-Guided and Cross-Guided Learning for Few-Shot Segmentation\".",
      "generation": "Few-shot segmentation is a challenging problem in which the goal is to segment new classes given only a few training examples. In this paper, we propose a self-guided learning approach that utilizes a reference image to adapt a pre-trained segmentation model to new classes. We further propose a cross-guided learning approach that utilizes a collection of reference images from multiple classes to adapt the model. Our approaches are based on a novel adaptation module that can be easily integrated into existing segmentation models. We demonstrate the effectiveness of our approaches on the VOC2012 and Cityscapes segmentation datasets, where we achieve state-of-the-art performance. Our code and models are available at this https URL."
    },
    {
      "id": "a08e471d-de1d-44ea-96e5-9716a909bff2",
      "adv_source_id": "a08e471d-de1d-44ea-96e5-9716a909bff2",
      "source_id": "e9899aaa-4e38-408c-bc2c-7f1fb3cf934c",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Guided and Cross-Guided Learning for Few-Shot Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Self-Guided and Cross-Guided Learning for Few-Shot Segmentation\".",
      "generation": "Few-shot segmentation refers to the task of segmenting objects in images when training on fewer than 1000 images per class. Although the dataset is reduced, the model needs to generalize well to new images. We propose a self-guided learning (SGL) framework to Few-shot segmentation that encourages the model to find informative and generic regions across different classes for better generalization. We also propose a cross-guided learning (CGL) framework that utilizes the correlation between source and target domains to assist the learning process. Our experimental results show that our approaches outperform existing state-of-the-art methods on two benchmark datasets."
    },
    {
      "id": "c1947468-9187-4b5d-bc6e-dcd2d9995302",
      "adv_source_id": "c1947468-9187-4b5d-bc6e-dcd2d9995302",
      "source_id": "24f2ca6c-1cb0-4ff0-a98f-e4c86496f942",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates",
      "prompt": "Write the abstract for the academic paper titled \"Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates\".",
      "generation": "Below is an abstract for the academic paper titled, \"Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates\":\n\nRecent progress in semantic segmentation has mainly focused on fully supervised settings, requiring large amounts of manually annotated pixel-level labels. This is labor-intensive and expensive, especially for fine-grained and instance-specific segmentation. To overcome this bottleneck, we target the setting of unsupervised segmentation, where only object instance bounding boxes are annotated. Our approach learns to segment object instances in images given only scribbles on the bounding boxes as weak supervision. A multi-scale adversarial attention gate is proposed to focus on the most relevant information for segmentation at multiple scales. We show that our approach significantly improves over the state-of-the-art on the cityscapes segmentation benchmark with only bounding box annotations."
    },
    {
      "id": "e4d25b4c-4c22-45d8-a8da-cce98fd4adbe",
      "adv_source_id": "e4d25b4c-4c22-45d8-a8da-cce98fd4adbe",
      "source_id": "24f2ca6c-1cb0-4ff0-a98f-e4c86496f942",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates",
      "prompt": "Write the abstract for the academic paper titled \"Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates\".",
      "generation": "Below is an example of what an abstract for the academic paper titled, \"Learning to Segment from Scribbles using Multi-scale Adversarial Attention Gates,\" could look like:\n\nAbstract: We present a novel approach for image segmentation based on scribbles. Our method leverages the use of multi-scale adversarial attention gates to effectively learn from scribble annotations. We propose a novel architecture that integrates attention mechanisms at various scales to capture both local and global context. This enables our model to accurately segment objects in images, even with limited and imperfect scribble annotations. We validate the effectiveness of our approach through extensive experiments on public datasets, showing that our method outperforms the state-of-the-art in image segmentation. Our method is particularly useful in scenarios where scribble annotations are preferred over pixel-level annotations due to their simplicity and speed of creation."
    },
    {
      "id": "aa34fcd1-ec6d-488c-850c-eeb6181a096d",
      "adv_source_id": "aa34fcd1-ec6d-488c-850c-eeb6181a096d",
      "source_id": "a7ce4b85-f4ec-45a2-b1f4-5219923dbbd2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images",
      "prompt": "Write the abstract for the academic paper titled \"Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images\".",
      "generation": "Abstract\nDeep learning has achieved great success in image segmentation tasks, yet the mechanisms that lead to their performance remain unclear. We investigate this question in the context of pixel-level segmentation of fluorescence microscopy images. We train deep neural networks to segment images of cells into foreground (cell) and background (background). We find that despite having access to millions of training images, the networks converge quickly and with a high degree of similarity across different trainings. In order to gain insight into the networks' decisions, we propose a novel method to visualize the attention weights of a attention-based segmentation model. From these visualizations, we make three main discoveries. First, the networks learn to focus on pixels that are most salient for segmentation, and largely ignore pixels that are cluttered or ambiguous. Second, despite being trained on images of cells, the networks tend to first detect foreground objects that are similar in size and appearance to cells, but are not cells, and only then proceed to detect cells. Third, the networks are able to generalize to unlabeled cell types not seen during training, but performance degrades as the cell type deviates further from the trained cell types. Based on these findings, we discuss how deep neural networks segment images, and how they generalize to new cell types."
    },
    {
      "id": "f7e406c1-303d-4379-90df-82fcb10b003d",
      "adv_source_id": "f7e406c1-303d-4379-90df-82fcb10b003d",
      "source_id": "a7ce4b85-f4ec-45a2-b1f4-5219923dbbd2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images",
      "prompt": "Write the abstract for the academic paper titled \"Deep Neural Networks Learn Meta-Structures to Segment Fluorescence Microscopy Images\".",
      "generation": "Abstract:\nTraditional computer vision techniques tend to fail in the context of segmenting fluorescence microscopy images (FMI). FMI possess challenging characteristics such as large intensity variations, high noise levels, and complicated structural hierarchies. In this paper we explore the use of deep neural networks for addressing this problem. We propose a novel architecture called Deep Analog Component (DAC) which is based on the principles of analog computation in the brain. In particular, our model is inspired by the retinal ganglion cells which are known to perform edge detection and suppression in a massively parallel fashion. We show that our proposed network, without using any dropout components, is capable of learning meta-structures which help in segmenting FMI. Our experimental results show that the proposed method achieves state-of-the-art performance on two publicly available FMI segmentation benchmarks while being an order of magnitude faster than the existing approaches."
    },
    {
      "id": "6f23e593-5473-4db6-83d4-c5ac4c3739ae",
      "adv_source_id": "6f23e593-5473-4db6-83d4-c5ac4c3739ae",
      "source_id": "53e6c118-bd9b-4e37-9670-7c4ee5621a66",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Contrastive Registration for Unsupervised Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Contrastive Registration for Unsupervised Medical Image Segmentation\".",
      "generation": "Abstract\nWe present a novel approach for unsupervised segmentation of medical images using registration induced contrast. Unlike previous methods that use registration for segmentation, our approach does not rely on a template or a prior model of the anatomy. Instead, we use a multi-scale registration algorithm to align the image with itself at different scales, and then use the resulting deformations to create an image pyramid with augmented contrast between different structures. This initial segmentation is then used to constrain a subsequent iteration of the registration algorithm, further improving the segmentation. We demonstrate the efficacy of our approach on 2D and 3D images from a variety of modalities, including MRI, CT, and ultrasound. Our method shows significant improvement over state-of-the-art unsupervised segmentation methods."
    },
    {
      "id": "7770ba5f-b521-4917-9c23-228716ee9dcf",
      "adv_source_id": "7770ba5f-b521-4917-9c23-228716ee9dcf",
      "source_id": "53e6c118-bd9b-4e37-9670-7c4ee5621a66",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Contrastive Registration for Unsupervised Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Contrastive Registration for Unsupervised Medical Image Segmentation\".",
      "generation": "Abstract.\nWe present a novel approach for unsupervised registration of medical images, which enables the transfer of annotations from a reference image to a target image without requiring ground truth segmentation. Our approach leverages the inherent anatomical structure of medical images to establish a robust geometric alignment between the reference and target images, facilitating accurate segmentation of the target image. We introduce a contrastive loss function that optimizes the registration by minimizing the discrepancy between the predicted segmentation and the ground truth segmentation in the reference image, while simultaneously learning a robust registration model. Our approach significantly improves upon existing state-of-the-art methods for unsupervised medical image segmentation, resulting in accurate and robust segmentation of medical images."
    },
    {
      "id": "d3b5e59c-3bb3-4584-9289-39603c75b604",
      "adv_source_id": "d3b5e59c-3bb3-4584-9289-39603c75b604",
      "source_id": "07dc10ce-198b-4398-901a-872cd727f5e9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation",
      "prompt": "Write the abstract for the academic paper titled \"Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation\".",
      "generation": "Here is the abstract for the academic paper titled \"Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation\":\n\nThis paper proposes a method for the automatic estimation of food portion from monocular images. The proposed method employs a cross-domain feature adaptation approach, where features learned from a source domain of images are adapted to a target domain of images. The adaptation is performed by minimizing the divergence between the feature distributions of the two domains. The proposed method was evaluated on two public datasets, and the results show that the adaptation approach significantly improves the performance of the feature learning algorithm.\n\nIs there anything else I may assist you with?"
    },
    {
      "id": "7a41e29d-5c31-4808-bf72-ed5e5af5c525",
      "adv_source_id": "7a41e29d-5c31-4808-bf72-ed5e5af5c525",
      "source_id": "07dc10ce-198b-4398-901a-872cd727f5e9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation",
      "prompt": "Write the abstract for the academic paper titled \"Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation\".",
      "generation": "Here is an abstract for the academic paper* titled \"Towards Learning Food Portion From Monocular Images With Cross-Domain Feature Adaptation\":\n\nNowadays, knowing the amount of food in dishes is useful for several applications, including healthy eating, fitness, and weight loss. However, this information is often not available unless the dish is carefully measured, which is both time-consuming and changes the appearance of the dish. In this paper, we aim to learn a food portion estimation model to estimate the amount of food in a dish from a single image without destroying the dish. We collected a large-scale dataset with food images from different domains, including home-cooked and restaurant-cooked meals. Our model is based on a convolutional neural network that is pre-trained on a large-scale image dataset and then fine-tuned on our dataset. We propose a cross-domain feature adaptation method to enhance the model's performance. Our model outperforms the state-of-the-art in food portion estimation and can be used to estimate the amount of food in a dish from a single image.\n\n*Please note that I did not read the academic paper this abstract is based on, and it may not fully represent the academic paper's findings or conclusions."
    },
    {
      "id": "9dfb987d-c2d4-404c-9209-fae9e8d67ec5",
      "adv_source_id": "9dfb987d-c2d4-404c-9209-fae9e8d67ec5",
      "source_id": "427b7a84-0ebe-4169-b118-abdaed57d65f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space",
      "prompt": "Write the abstract for the academic paper titled \"FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space\".",
      "generation": "Abstract\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\nIn this paper, we propose FedDG, a model for federated domain generalization (FDG) on medical image segmentation. FedDG leverages episodic learning in the continuous frequency space, enabling the model to acquire domain knowledge from a sparse set of labeled samples and generalize well across different domains. Our proposed approach incorporates a meta-learning framework, where a single model is first trained on a large number of domains using only unsupervised domain adaptation and then fine-tuned on a smaller set of labeled samples. We evaluate our model on four different medical image segmentation datasets, and our results show that FedDG significantly improves domain generalization performance compared to existing state-of-the-art models.\n\nIntroduction\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\nDomain generalization (DG) is a technique in which a machine learning model is trained on a diverse set of domains to improve its performance on new domains. This is particularly useful for medical image segmentation, where data is often limited and domain-specific. However, training a model on a diverse set of domains can be challenging, as it requires a large amount of data and cross-domain expertise.\n\nTo overcome these challenges, we propose FedDG, a model for FDG on medical image segmentation. FedDG leverages episodic learning in the continuous frequency space, enabling the model to acquire domain knowledge from a sparse set of labeled samples and generalize well across different domains. Our proposed approach incorporates a meta-learning framework, where a single model is first trained on a large number of domains using only unsupervised domain adaptation and then fine-tuned on a smaller set of labeled samples.\n\nBackground and Related Work\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\nDomain generalization is a technique in which a machine learning model is trained on a diverse set of domains to improve its performance on new domains [1]. This is particularly useful for medical image segmentation, where data is often limited and domain-specific. However, training a model on a diverse set of domains can be challenging, as it requires a large amount of data and cross-domain expertise.\n\nThere are several approaches to domain generalization, including meta-learning [2], transfer learning [3], and domain adaptation [4]. Meta-learning is a technique where a model is first trained on a large number of domains"
    },
    {
      "id": "6d649755-60e1-485f-bca7-c1702d1c2088",
      "adv_source_id": "6d649755-60e1-485f-bca7-c1702d1c2088",
      "source_id": "427b7a84-0ebe-4169-b118-abdaed57d65f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space",
      "prompt": "Write the abstract for the academic paper titled \"FedDG: Federated Domain Generalization on Medical Image Segmentation via Episodic Learning in Continuous Frequency Space\".",
      "generation": "Abstract\n\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\nDomain generalization (DG) aims to boost the performance of a model trained on a limited dataset by leveraging knowledge from other relevant domains. Most existing DG works focus on image domains with a similar imaging condition, e.g., natural images. In this work, we tackle the problem of DG in medical image segmentation, which typically consists of images from different hospitals and institutions, potentially captured by various scanners and sensors, as well as with distinct preprocessing pipelines. To this end, we propose FedDG, a simple yet effective federated learning (FL) framework that performs DG in medical image segmentation. FedDG leverages episodic learning, i.e., training the model on a short episode of a domain (e.g., a hospital) and performs FL across episodes from multiple institutions. Our key insight is that, instead of using the raw images, which may have very different imaging conditions and preprocessing pipelines, FedDG operates in the continuous frequency space of the pre-processed images, i.e., the spectrograms of the images, which effectively \u201caverages away\u201d the variations caused by different imaging conditions and preprocessing pipelines. We demonstrate the effectiveness of FedDG on four public medical image segmentation datasets."
    },
    {
      "id": "4c9fff07-ef71-40f3-b434-0fd93d1821fe",
      "adv_source_id": "4c9fff07-ef71-40f3-b434-0fd93d1821fe",
      "source_id": "823ab1a3-8a1d-4d4b-bab4-92dcb32285a2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Medical Image Segmentation through Dual-task Consistency",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised Medical Image Segmentation through Dual-task Consistency\".",
      "generation": "The paper proposes a method for semi-supervised medical image segmentation that combines the use of a fully-supervised segmentation network with a semi-supervised segmentation network. The fully-supervised network is used to provide a baseline segmentation, which is then refined by the semi-supervised network. The semi-supervised network is trained using a dual-task loss function that combines the segmentation loss with a consistency loss, which ensures that the predicted segmentations are consistent with the baseline segmentation. The proposed method was evaluated on the ISBI 2017 segmentation challenge dataset and achieved competitive results compared to other semi-supervised segmentation methods."
    },
    {
      "id": "2b845e5f-900a-4b3d-adc3-72eeaef96f32",
      "adv_source_id": "2b845e5f-900a-4b3d-adc3-72eeaef96f32",
      "source_id": "823ab1a3-8a1d-4d4b-bab4-92dcb32285a2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised Medical Image Segmentation through Dual-task Consistency",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised Medical Image Segmentation through Dual-task Consistency\".",
      "generation": "Here is the abstract for the academic paper titled \"Semi-supervised Medical Image Segmentation through Dual-task Consistency\":\n\nAbstract\u2014 Medical image segmentation is crucial for the diagnosis and analysis of various diseases by assisting doctors in locating the affected regions in an image. However, manual segmentation is tedious, costly, and time-consuming, limiting its applications for a large number of images. Therefore, automatic segmentation techniques are highly desirable. Nevertheless, these techniques require a significant amount of labeled data for training, which is scarce and expensive to obtain in the medical domain. To address this problem, we propose a semi-supervised segmentation framework that leverages the power of unlabeled data to improve segmentation performance. Our framework consists of two components: a semi-supervised segmentation network and a dual-task consistency loss. The segmentation network is trained on a small amount of labeled data and a large amount of unlabeled data. The dual-task consistency loss is designed to ensure that the segmentation network outputs consistent segmentations for the labeled and unlabeled data. Our experiments show that our framework significantly improves segmentation performance compared to state-of-the-art semi-supervised segmentation methods."
    },
    {
      "id": "5be6ae3c-1a64-422a-9a64-28e564191529",
      "adv_source_id": "5be6ae3c-1a64-422a-9a64-28e564191529",
      "source_id": "285debed-9666-4c9c-b086-1d1ecbbcfff6",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation\".",
      "generation": "Here is the abstract for the academic paper titled \"Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation\":\n\nAbstract: Biomedical image segmentation is challenging due to the domain shift between the training and test datasets, which can significantly degrade the performance of deep learning models. In this paper, we propose an unsupervised domain adaptation method that leverages the category-centric prototype alignment (CCPA) technique to address this issue. Our approach consists of two main components: a domain adaptation network (DAN) and a CCPA module. The DAN uses a convolutional neural network (CNN) to learn a domain-invariant feature representation, while the CCPA module aligns the category-specific prototypes of the source and target domains. We evaluate our method on three publicly available biomedical image segmentation datasets, and our results show that our approach significantly improves the performance of the baseline method in all cases. Our method achieves state-of-the-art performance on two of the datasets and competitive results on the third. We also perform a comprehensive analysis to understand the impact of the CCPA module on the performance of our method. Our work demonstrates the effectiveness of the CCPA technique in unsupervised domain adaptation for biomedical image segmentation."
    },
    {
      "id": "afb3de4b-2ba4-4212-a2fd-27f5a92ec5e9",
      "adv_source_id": "afb3de4b-2ba4-4212-a2fd-27f5a92ec5e9",
      "source_id": "285debed-9666-4c9c-b086-1d1ecbbcfff6",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation\".",
      "generation": "Here is the abstract for the academic paper titled, \"Unsupervised Domain Adaptation Network with Category-Centric Prototype Aligner for Biomedical Image Segmentation\":\n\nRecent progress in semantic segmentation has accelerated the development of image analysis tools in biomedical imaging. However, the performance of these models heavily depends on the amount and diversity of available training data in an assumed homogeneous target domain. In this work, we tackle the problem of domain adaptation for accurate segmentation of biomedical images acquired across multiple sites with incompatible protocols and acquisition modalities. Our key insight is that, for each category, a domain-invariant prototype exists in an high-level feature space, which is aligned to the source domain by minimizing an optimal transport distance. Therefore, we propose a Category-Centric Prototype Aligner (CCPA) to learn a joint embedding in an unsupervised manner. In this embedding, each category has a unique representation that is aligned across the domains. We further present an unsupervised Domain Adaptation Network (DAN) that leverages the CCPA to adapt the source model to a target domain. DAN applies the proposed CCPA as a domain-invariant regularization on the feature representations of the source model. Our experimental results on four public datasets for retinal image segmentation demonstrate the superiority of the proposed method against prior art on multiple evaluation metrics."
    },
    {
      "id": "1d5b9154-2885-4fd0-9511-d46686843534",
      "adv_source_id": "1d5b9154-2885-4fd0-9511-d46686843534",
      "source_id": "27db1b2d-f18b-492a-afb6-0ef17287a61e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models",
      "prompt": "Write the abstract for the academic paper titled \"Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models\".",
      "generation": "Deep learning approaches have proven effective in medical image segmentation tasks, improving the state-of-the-art in various modalities, including MRI, CT, and ultrasound. However, fully supervised training requires a significant amount of manually annotated images, which are expensive to acquire. This work provides an overview of recent advances in deep network models that reduce the amount of required supervision for medical image segmentation. We review semi-supervised and self-supervised approaches that leverage additional unlabeled data, transfer learning from related tasks or domains, and multi-task learning to improve segmentation performance with less labeled data. We also discuss the limitations and potential of these methods and suggest future research directions. The review is concluded with a summary of open challenges in applying limited supervision strategies to medical image segmentation problems."
    },
    {
      "id": "f0c3c7f8-4a82-4ff5-98b5-012a8cb8bd58",
      "adv_source_id": "f0c3c7f8-4a82-4ff5-98b5-012a8cb8bd58",
      "source_id": "27db1b2d-f18b-492a-afb6-0ef17287a61e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models",
      "prompt": "Write the abstract for the academic paper titled \"Medical Image Segmentation with Limited Supervision: A Review of Deep Network Models\".",
      "generation": "Deep learning approaches have proven effective in medical image segmentation tasks, benefiting from large annotated datasets and powerful computing resources. However, annotation of medical images remains a challenging and expensive task, and available annotated datasets are often limited in size. In this paper, we provide an overview of deep learning models that have been applied to medical image segmentation with limited supervision. We discuss the challenges and opportunities of training deep networks with limited supervision, and review recent advances in the field. We also discuss the potential of these approaches for improving the accuracy and efficiency of medical image segmentation tasks."
    },
    {
      "id": "d0a7592a-3b63-4fb3-83ab-4b9fcc31fa7b",
      "adv_source_id": "d0a7592a-3b63-4fb3-83ab-4b9fcc31fa7b",
      "source_id": "3f74bd45-21ac-4446-8591-88f32baafbaf",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks",
      "prompt": "Write the abstract for the academic paper titled \"EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks\".",
      "generation": "Abstract:\nAs the impact of Covid-19 decreases and the world becomes more stable, it is important to note the significant impact this virus had on the environment. Previous studies on the ecological consequences of the Covid-19 pandemic mainly concentrated on the direct effects of the lack of tourists and human activities on specific habitats, such as beaches, oceans, and grasslands. However, the influence of the Covid-19 pandemic on environmental microorganisms is yet to be explored thoroughly. In this study, we created the EMDS-5 dataset to facilitate the study of the environmental microorganisms' spatial-temporal variations caused by the Covid-19 pandemic. This dataset contains over 10 million images from various environments such as airports, hospitals, and public transit stations. We released EMDS-5 as a benchmark for environmental microorganism analysis, including classification, segmentation, and detection tasks. The dataset will help researchers understand the changes in the distribution and abundance of environmental microorganisms during the Covid-19 pandemic."
    },
    {
      "id": "83d02936-202e-4353-8658-22e7fa91d44c",
      "adv_source_id": "83d02936-202e-4353-8658-22e7fa91d44c",
      "source_id": "3f74bd45-21ac-4446-8591-88f32baafbaf",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks",
      "prompt": "Write the abstract for the academic paper titled \"EMDS-5: Environmental Microorganism Image Dataset Fifth Version for Multiple Image Analysis Tasks\".",
      "generation": "Abstract:\nThis paper presents EMDS-5, the fifth version of the Environmental Microorganism Image Dataset, for multiple image analysis tasks. EMDS-5 consists of a diverse set of images of environmental microorganisms, including bacteria, viruses, and fungi, as well as samples of non-microbial particles. The images in EMDS-5 are of high quality and are curated to cover a wide range of shapes, sizes, and textures. The dataset is designed to be useful for multiple image analysis tasks, such as object detection, segmentation, and classification. In this paper, we describe the creation and curation process of EMDS-5, as well as provide a comprehensive evaluation of the dataset using state-of-the-art image analysis methods. We believe that EMDS-5 will be a valuable resource for researchers and practitioners working on environmental microorganism image analysis."
    },
    {
      "id": "47702b01-450e-4f83-aea9-a37f20fe07c1",
      "adv_source_id": "47702b01-450e-4f83-aea9-a37f20fe07c1",
      "source_id": "256bd567-328d-4cf4-a87c-7d3226f89ef6",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Saliency-Aware Class-Agnostic Food Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Saliency-Aware Class-Agnostic Food Image Segmentation\".",
      "generation": "Abstract:\nTraditional image segmentation approaches tend to perform poorly on food images, as they generally lack the ability to adapt to the variability and complexity of food objects. In this paper, we present a saliency-aware approach for segmenting class-agnostic food images. Our approach leverages the recently proposed Deep Graphical Feature Learning (DGFL) framework to extract robust and generalizable features from food images. We then use a fully connected conditional random field (CRF) to refine the segmentation results and improve the accuracy. Furthermore, we propose a saliency-aware module that highlights the regions of interest in the image and guides the segmentation process. Our approach is evaluated on the publicly available Food-101 and Breakfast-102 datasets, and the results show that our approach significantly outperforms the state-of-the-art.\n\nOur saliency-aware approach effectively segments food images by extracting robust and generalizable features using the DGFL framework and refining the segmentation results with a fully connected CRF. The saliency-aware module highlights the regions of interest and guides the segmentation process, resulting in significantly improved performance compared to the state-of-the-art."
    },
    {
      "id": "55bc4592-7cc0-4188-a99d-aee6ada2ec3a",
      "adv_source_id": "55bc4592-7cc0-4188-a99d-aee6ada2ec3a",
      "source_id": "256bd567-328d-4cf4-a87c-7d3226f89ef6",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Saliency-Aware Class-Agnostic Food Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Saliency-Aware Class-Agnostic Food Image Segmentation\".",
      "generation": "Here is an abstract for the academic paper titled \"Saliency-Aware Class-Agnostic Food Image Segmentation\":\n\nThis paper presents a novel approach to segmenting images of food based on a combination of deep learning and semantic segmentation techniques. The proposed method utilizes a pre-trained deep learning model to extract features from the images, followed by a semantic segmentation model that is trained on a large dataset of food images. The semantic segmentation model is designed to identify and segment the main object in the image, which in this case is food. The proposed method is evaluated on a dataset of food images and compared to state-of-the-art image segmentation methods. The results show that the proposed method is able to achieve high accuracy and efficiency in segmenting food images, while being general to different kinds of food.\n\nWould you like me to generate the introduction or conclusion sections of the academic paper?"
    },
    {
      "id": "7303adf4-d515-44b9-9b80-a4efeaa2fc84",
      "adv_source_id": "7303adf4-d515-44b9-9b80-a4efeaa2fc84",
      "source_id": "a511beab-f399-4434-aa76-f484c25fb23e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Co-Seg: An Image Segmentation Framework Against Label Corruption",
      "prompt": "Write the abstract for the academic paper titled \"Co-Seg: An Image Segmentation Framework Against Label Corruption\".",
      "generation": "Abstract:\nLabel corruption, caused by inconsistent, inaccurate, or noisy labeling, is a common challenge in image segmentation. It can significantly degrade the performance of image segmentation models. We introduce Co-Seg, a novel image segmentation framework that leverages the strengths of multiple models to overcome label corruption. Our approach combines a pre-trained image segmentation model with a corruption detection model to selectively refine the predictions on potentially corrupted regions. The corruption detection model accurately identifies regions with a high likelihood of label corruption, while the image segmentation model provides initial predictions. We train the corruption detection model using a contrast set, which consists of examples from the same distribution as the training data but with known labels. Our approach is flexible and can be applied to any pre-trained image segmentation model. We demonstrate the effectiveness of our approach on two public datasets, demonstrating significant performance improvements over the state-of-the-art."
    },
    {
      "id": "817a0db2-db30-4be3-871c-5980ae059b72",
      "adv_source_id": "817a0db2-db30-4be3-871c-5980ae059b72",
      "source_id": "a511beab-f399-4434-aa76-f484c25fb23e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Co-Seg: An Image Segmentation Framework Against Label Corruption",
      "prompt": "Write the abstract for the academic paper titled \"Co-Seg: An Image Segmentation Framework Against Label Corruption\".",
      "generation": "Abstract:\nLabel corruption, induced by gross errors or vague boundaries in the labeled image, is a common challenge faced during the training of deep learning-based image segmentation models. In this paper, we present Co-Seg, a novel image segmentation framework that leverages the strengths of both global and local information to effectively mitigate the impact of label corruption. Our approach combines a global segmentation model, which captures the overall structure of the image, with a local refinement model that refines the predictions at difficult regions. We introduce a new loss function that not only minimizes the segmentation errors but also maximizes the mutual information between the predicted and ground truth segmentations. Furthermore, we implement a label cleaning module to selectively remove the erroneous labels and retain the reliable ones. Experimental results on three benchmark datasets, including VOC 2012, Cityscapes, and Mapillary Vistas, demonstrate the effectiveness of our approach in terms of both accuracy and efficiency."
    },
    {
      "id": "857f796a-01ce-4b8e-9c55-bb4274fbd413",
      "adv_source_id": "857f796a-01ce-4b8e-9c55-bb4274fbd413",
      "source_id": "b4300583-85ba-4980-8b29-8b312ef44492",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Network-Agnostic Knowledge Transfer for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Network-Agnostic Knowledge Transfer for Medical Image Segmentation\".",
      "generation": "Abstract:\nPre-trained models for medical image segmentation have demonstrated promising results but are often limited to specific networks and do not generalize well to new datasets. This work introduces a method for the network-agnostic transfer of knowledge from a source domain to a target domain. The method utilizes a variant of the generalized gradient descent (GGD) algorithm to minimize the difference in feature representations between the source and target domains. The GGD algorithm is adapted to the segmentation task by using a Dice coefficient as the loss function. The proposed method is evaluated on a public dataset for abdominal aortic aneurysm (AAA) segmentation and compared to a state-of-the-art pre-trained model. The results show that the proposed method achieves a Dice coefficient of 0.88, outperforming the pre-trained model by 0.07. The source domain is a publicly available AAA segmentation dataset, and the target domain is a private AAA segmentation dataset. The proposed method achieves a Dice coefficient of 0.83 on the private dataset, outperforming the pre-trained model by 0.04. The results show that the proposed method is capable of transferring knowledge from a source domain to a target domain, despite the domains having different characteristics. The proposed method is effective in segmenting medical images and outperforms the state-of-the-art pre-trained models."
    },
    {
      "id": "3589ec5e-09b4-4e12-8c4a-a4c47d138bae",
      "adv_source_id": "3589ec5e-09b4-4e12-8c4a-a4c47d138bae",
      "source_id": "b4300583-85ba-4980-8b29-8b312ef44492",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Network-Agnostic Knowledge Transfer for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Network-Agnostic Knowledge Transfer for Medical Image Segmentation\".",
      "generation": "Abstract:\nTraditional approaches to medical image segmentation typically rely on large amounts of manually annotated training data, which can be difficult to obtain for rare diseases or new imaging modalities. In this paper, we investigate the use of knowledge transfer to address this issue. We propose a network-agnostic knowledge transfer approach that utilizes a pre-trained segmentation model for a related task and fine-tunes it for the target task using only a small amount of labeled data. Our approach is evaluated on a public dataset for liver tumor segmentation in contrast-enhanced CT scans. We compare our method to a strong baseline and show that knowledge transfer improves performance, even when only a small fraction of the data is labeled. We also show that our approach generalizes well to data from different institutions, indicating the potential for successful knowledge transfer across sites."
    },
    {
      "id": "694f2cab-18c6-420e-befa-d95c6313f351",
      "adv_source_id": "694f2cab-18c6-420e-befa-d95c6313f351",
      "source_id": "b7c181d9-7c1a-4cbc-80ec-ff4b60ec7f3e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Nonparametric clustering for image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Nonparametric clustering for image segmentation\".",
      "generation": "This paper presents a nonparametric clustering method for image segmentation. The proposed method divides an image into multiple segments based on the similarity of pixels. The similarity is measured by a distance function that considers the intensity and spatial relationships of pixels. The clustering is performed by a hierarchical algorithm that iteratively merges and splits clusters until a specified number of segments is obtained. The proposed method is evaluated on a variety of images, and the results show that it is competitive with other state-of-the-art image segmentation methods."
    },
    {
      "id": "4560de71-8d18-4272-b0f1-25e2269810a4",
      "adv_source_id": "4560de71-8d18-4272-b0f1-25e2269810a4",
      "source_id": "b7c181d9-7c1a-4cbc-80ec-ff4b60ec7f3e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Nonparametric clustering for image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Nonparametric clustering for image segmentation\".",
      "generation": "An emerging research topic in image processing is image segmentation, which allows for the identification of different objects in an image. Clustering, a fundamental technique in statistical data analysis, has been widely used to segment images non-parametrically. The aim of this paper is to provide an overview of the state-of-the-art research on nonparametric clustering for image segmentation. After discussing the fundamental principles of nonparametric clustering, various techniques will be described, including hierarchical clustering, k-means clustering, and Gaussian mixture models. The paper will then explore the application of these techniques to image segmentation. Finally, the paper will discuss the advantages and disadvantages of each technique and identify areas for future research. This comprehensive review of the literature will provide researchers and practitioners with insights into the current state of nonparametric clustering for image segmentation."
    },
    {
      "id": "aa1babcb-870d-4de2-956e-c4fac2296356",
      "adv_source_id": "aa1babcb-870d-4de2-956e-c4fac2296356",
      "source_id": "15ed2c14-ddcf-45ef-93a7-3d2a9485dff4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds",
      "prompt": "Write the abstract for the academic paper titled \"Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds\".",
      "generation": "An innovative and effective approach for semantic segmentation of point clouds is described in this paper. The proposed method employs boundary-aware geometric encoding (BAGE) to enhance the representation of point cloud data. BAGE captures the geometric structure of point clouds while emphasizing the boundaries of objects, which proves crucial for accurate and reliable segmentation. A comprehensive evaluation on public datasets demonstrates the superiority of the proposed method compared to existing state-of-the-art techniques. The results show improved performance in terms of segmentation accuracy and efficiency. The code and trained models are made publicly available to facilitate further research and development in the field of point cloud segmentation."
    },
    {
      "id": "232537d5-0996-4fe0-990b-c1a62a5833c0",
      "adv_source_id": "232537d5-0996-4fe0-990b-c1a62a5833c0",
      "source_id": "15ed2c14-ddcf-45ef-93a7-3d2a9485dff4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds",
      "prompt": "Write the abstract for the academic paper titled \"Boundary-Aware Geometric Encoding for Semantic Segmentation of Point Clouds\".",
      "generation": "An innovative boundary-aware geometric encoding (BAGE) method is proposed for effective semantic segmentation of point clouds. This novel encoding scheme takes advantage of both local and global point distributions to capture underlying object structures. In particular, we introduce a boundary encoding module that enhances the representation of point cloud regions with complex shapes and interfaces. A stacked hourglass network coupled with skip connections is utilized to efficiently aggregate boundary and core information across different scales. We further propose a multi-scale training strategy that iteratively refines the segmentation results from coarse to fine levels. Extensive evaluations on three public datasets, including Semantic3D, S3DIS, and ScanNet, demonstrate the superiority of our approach against the state-of-the-art in terms of segmentation quality and efficiency."
    },
    {
      "id": "9715188a-82f9-40cb-828c-deb9c24a01e9",
      "adv_source_id": "9715188a-82f9-40cb-828c-deb9c24a01e9",
      "source_id": "bf86b311-04cb-4e22-9749-ff8c962eafb9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation\".",
      "generation": "Semantic segmentation of hip joint bone from MRI has a critical role in computer-aided diagnosis and surgical planning. However, existing methods are usually trained and evaluated on a single dataset, resulting in poor performance when deployed on other datasets. The intra- and cross-modality semantic consistency for unsupervised domain adaptation (ICMSC) is a new method that addresses this issue. Our method is based on a self-supervised learning framework, where a generative adversarial network (GAN) is used to learn a domain-invariant feature representation for semantic segmentation. The GAN is trained on a pair of domains, and the domain-invariant feature representation is then used to train a segmentation network for the target domain. Our method provides a new way to address the domain shift problem in medical image segmentation and can be applied to other semantic segmentation tasks."
    },
    {
      "id": "b9df5d61-6c6d-4947-834c-30e5ecc48bea",
      "adv_source_id": "b9df5d61-6c6d-4947-834c-30e5ecc48bea",
      "source_id": "bf86b311-04cb-4e22-9749-ff8c962eafb9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"ICMSC: Intra- and Cross-modality Semantic Consistency for Unsupervised Domain Adaptation on Hip Joint Bone Segmentation\".",
      "generation": "Semantic segmentation of hip joint bone from multimodal computed tomography (CT) and magnetic resonance (MR) images is a newly proposed problem in medical imaging, where the distribution difference between the source domain (CT) and target domain (MR) is very large. Most existing domain adaptation methods assume the similarity between the source and target domains, which makes them unsuitable for this problem. Furthermore, there is no labeled source data available for this task. Consequently, a self-supervised approach is needed to learn a transferable representation from the source domain to the target domain. In this paper, we propose a self-supervised approach for learning a transferable representation from CT to MR for hip joint bone segmentation. We name our method Intra- and Cross-modality Semantic Consistency (ICMSC), which consists of a Cycle Consistent Adversarial Network (CycleCAN) and a Multi-Modal Consistency (MMC) module. Specifically, we simultaneously learn intra-modality and cross-modality consistency within and across CT and MR domains, respectively. We show that our method is capable of capturing shared information between the source and target domains, leading to significant segmentation performance improvement on the target domain."
    },
    {
      "id": "83d140bd-2b4c-4986-88c6-5830d3a3fa20",
      "adv_source_id": "83d140bd-2b4c-4986-88c6-5830d3a3fa20",
      "source_id": "24ad0185-90a9-47b6-a029-30b1d4f3c650",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks",
      "prompt": "Write the abstract for the academic paper titled \"CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks\".",
      "generation": "The abstract for the academic paper titled \"CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks\" would be:\n\nChannel-wise Automatic KErnel Shrinking (CAKES) enables efficient 3D convolutional neural networks (CNNs) by exploiting the inherent structure of spatio-temporal volumes. It proposes a channel-wise kernel-shrinking algorithm that automatically identifies and removes redundant channels in a 3D CNN, resulting in a compact and more computationally efficient network. The algorithm works by iteratively selecting channels for removal based on their importance to the network's performance, as measured by a channel importance score. The score is calculated using a variation of the mean squared error (MSE) loss function that considers the channel-wise contributions to the network's output. The CAKES algorithm can be applied to any 3D CNN architecture and is particularly effective for networks with a large number of channels. It is shown to significantly improve the computational efficiency of 3D CNNs without sacrificing performance."
    },
    {
      "id": "f2ec1c30-2ea8-4915-bed7-ebe1b1cd8f3e",
      "adv_source_id": "f2ec1c30-2ea8-4915-bed7-ebe1b1cd8f3e",
      "source_id": "24ad0185-90a9-47b6-a029-30b1d4f3c650",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks",
      "prompt": "Write the abstract for the academic paper titled \"CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks\".",
      "generation": "The abstract for the academic paper titled \"CAKES: Channel-wise Automatic KErnel Shrinking for Efficient 3D Networks\" would be:\n\nAbstract: Large model size and high computational cost hinder the deployment of deep learning models on edge devices. In this paper, we introduce CAKES: a Channel-wise Automatic KErnel Shrinking approach for efficient 3D Networks. Given a 3DNet, we first apply a depth-wise traversal to convert it into a channel-wise decision tree, in which each internal node represents a channel and each leaf node represents a channel-wise kernel. We then apply an iterative two-phase pruning approach to aggressively eliminate the computations contributed by the redundant channels. Given a pre-defined threshold, we first apply a bottom-up pruning pass to remove the leaf nodes that contribute less than the threshold. We then perform a top-down pruning pass to merge the pruned channels and corresponding computations back to the parent layers. Our experimental results show that, given the same accuracy, CAKES can reduce the model size by 47% and the computational cost by 38% on the popular ResNet-50, comparing to the state-of-the-art pruning approaches."
    },
    {
      "id": "1f7a86ea-250b-49a7-a3f8-61e3e505280a",
      "adv_source_id": "1f7a86ea-250b-49a7-a3f8-61e3e505280a",
      "source_id": "c9883f32-e69f-4725-b09b-327291b0391b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding",
      "prompt": "Write the abstract for the academic paper titled \"Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding\".",
      "generation": "The paper presents a novel approach to few-shot medical image segmentation using a global correlation network with discriminative embedding. The proposed method aims to address the challenge of training effective segmentation models using a limited number of labeled examples. A global correlation network is utilized to capture the global context and relationships between pixels in an image, while a discriminative embedding is used to learn robust representations for few-shot learning. The experimental results demonstrate the effectiveness of the proposed method in terms of segmentation accuracy and few-shot learning capability. The paper concludes by discussing the potential applications of the proposed method in medical image analysis and the limitations of the current approach."
    },
    {
      "id": "c536f19e-37cd-40e1-a545-1f6c3b093382",
      "adv_source_id": "c536f19e-37cd-40e1-a545-1f6c3b093382",
      "source_id": "c9883f32-e69f-4725-b09b-327291b0391b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding",
      "prompt": "Write the abstract for the academic paper titled \"Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding\".",
      "generation": "Here is the abstract for the academic paper titled \"Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding\":\n\nThis paper proposes a few-shot medical image segmentation method that uses a global correlation network with discriminative embedding. The proposed method takes a set of few training images and a set of test images as input, and produces a set of segmented images as output. The goal of this work is to address the issue of limited training data in medical image segmentation tasks. The proposed method first uses a discriminative embedding to learn a low-dimensional representation of the input images. The low-dimensional representation is then used to train a global correlation network, which learns to segment the input images based on the correlations between the pixels in the images. The proposed method is evaluated on a set of medical image segmentation benchmarks, and the results show that the proposed method achieves state-of-the-art performance."
    },
    {
      "id": "c520121b-4872-4f95-9aa8-db7842a7b878",
      "adv_source_id": "c520121b-4872-4f95-9aa8-db7842a7b878",
      "source_id": "8169f839-943c-4c76-831f-1cba9e363b8d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity",
      "prompt": "Write the abstract for the academic paper titled \"Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity\".",
      "generation": "Abstract:\nInfrared images are widely used in various applications such as thermography, military surveillance, and astronomy. However, intensity inhomogeneity, which is caused by the non-uniform distribution of temperature or other factors, is a common issue in infrared images. This paper proposes a multi-feature driven active contour segmentation model to segment infrared images with intensity inhomogeneity. The proposed model combines edge detection, region expansion, and intensity equalization to achieve accurate and robust segmentation. The edge detection module extracts the edges of the objects in the image, while the region expansion module ensures that the objects are properly separated. The intensity equalization module compensates for the non-uniform distribution of intensity. The proposed model was evaluated on a public dataset and compared to state-of-the-art algorithms. The experimental results show that the proposed model significantly improves the segmentation accuracy and robustness.\n\nKeywords: Infrared image segmentation, active contour modeling, intensity inhomogeneity, edge detection, region expansion."
    },
    {
      "id": "4059de2c-64fb-4356-8ab2-4b58655dcdf3",
      "adv_source_id": "4059de2c-64fb-4356-8ab2-4b58655dcdf3",
      "source_id": "8169f839-943c-4c76-831f-1cba9e363b8d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity",
      "prompt": "Write the abstract for the academic paper titled \"Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity\".",
      "generation": "Here is an abstract for the academic paper titled \"Multi-feature driven active contour segmentation model for infrared image with intensity inhomogeneity\":\n\nAbstract:\nWe present a multi-feature driven active contour segmentation model for infrared images with intensity inhomogeneity. Our approach combines the benefits of both global and local features to overcome the challenges of intensity variations and noise in infrared images. The proposed method utilizes a snaking algorithm that adapts to the local intensity variations, enabling accurate segmentation of the object of interest. Additionally, we employ an intensity inhomogeneity correction scheme to enhance the quality of the segmentation results. The experimental results demonstrate the effectiveness of the proposed method in accurately segmenting objects in challenging infrared images."
    },
    {
      "id": "2e0e0e74-49c3-4077-91c6-47a956d25254",
      "adv_source_id": "2e0e0e74-49c3-4077-91c6-47a956d25254",
      "source_id": "9eda5549-966a-4eb7-bdbb-572dc585a4d3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation\".",
      "generation": "The paper proposes a novel approach for 3D medical image segmentation based on bidirectional recurrent neural networks (RNNs). The method is designed to address the challenge of few-shot learning, where only a limited number of labeled examples are available. By utilizing bidirectional RNNs, the model can capture both past and future contextual information, improving its ability to learn from limited data. The proposed method demonstrates state-of-the-art performance on 3D medical image segmentation tasks, achieving superior accuracy and efficiency compared to existing approaches. The paper contributes to the field by introducing a powerful and effective few-shot learning technique for 3D medical image segmentation, which has the potential to revolutionize the diagnosis and treatment of medical conditions."
    },
    {
      "id": "47dcccd3-9232-48db-8ff7-f8e8424b51e1",
      "adv_source_id": "47dcccd3-9232-48db-8ff7-f8e8424b51e1",
      "source_id": "9eda5549-966a-4eb7-bdbb-572dc585a4d3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation\".",
      "generation": "The abstract for the academic paper titled \"Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation\" could be:\n\nThis paper proposes a novel approach for 3D medical image segmentation based on bidirectional recurrent neural networks (BiRNNs) and few-shot learning. We develop a fully-automated system that can accurately segment medical images from a limited number of training examples. Our approach utilizes a bidirectional RNN (BiRNN) architecture, which processes input sequences in both forward and backward directions. This allows the model to capture long-range dependencies and improve its performance on challenging segmentation tasks. We evaluate our method on the task of brain tumor segmentation in magnetic resonance (MR) images, and demonstrate that it can achieve state-of-the-art results even when trained with only a few shots of labeled data. The code and dataset used in this work are publicly available for future research."
    },
    {
      "id": "fe76039a-812c-4837-9891-a19519800d7c",
      "adv_source_id": "fe76039a-812c-4837-9891-a19519800d7c",
      "source_id": "8054ca70-ccbe-43ec-9597-802980689d39",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution",
      "prompt": "Write the abstract for the academic paper titled \"w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution\".",
      "generation": "The abstract for the academic paper titled \"w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution\" would be:\n\nw-Net is a novel approach to medical image segmentation that combines the strengths of supervised and unsupervised learning. The model utilizes a multi-dimensional attention mechanism to capture the complex relationships between features in medical images. Additionally, a cascade of multi-scale convolutions is used to capture the hierarchical structure of medical images. The model is trained using a dual-supervised learning approach, where both supervised and unsupervised learning are used to optimize the model parameters.\n\nThe w-Net model was evaluated on the ISBI 2017 Cardiac Segmentation Challenge dataset, which consists of short-axis magnetic resonance imaging (MRI) scans of the heart. The model achieved a dice coefficient of 0.83, outperforming the previous state-of-the-art by a significant margin.\n\nThe paper concludes by discussing the potential of the w-Net model for medical image segmentation tasks beyond cardiac MRI, as well as the possibility of combining the model with other machine learning techniques to improve performance."
    },
    {
      "id": "d3e98e2a-c19b-479f-9a3b-a527851443f8",
      "adv_source_id": "d3e98e2a-c19b-479f-9a3b-a527851443f8",
      "source_id": "8054ca70-ccbe-43ec-9597-802980689d39",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution",
      "prompt": "Write the abstract for the academic paper titled \"w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution\".",
      "generation": "The abstract for the academic paper titled \"w-Net: Dual Supervised Medical Image Segmentation Model with Multi-Dimensional Attention and Cascade Multi-Scale Convolution\" would be:\n\nw-Net is a novel approach to medical image segmentation that combines the benefits of supervised and unsupervised learning. The model utilizes a multi-dimensional attention mechanism to capture the complex relationships between features in the image, and a cascade of multi-scale convolutions to capture the fine details. The model is trained on a dual-supervised learning scheme, where both the segmentation and attention maps are learned simultaneously. This allows the model to learn from both the intensity and spatial information in the image, resulting in improved performance compared to traditional single-supervised methods.\n\nThe w-Net model was evaluated on the ISBI 2017 Cardiac Image Segmentation Challenge dataset, and achieved state-of-the-art performance. The model was able to correctly segment the left and right ventricles, as well as the myocardium and background, with high accuracy. The attention maps generated by the model were also found to be informative, and could be used for downstream analysis such as regional segmentation.\n\nThe paper concludes by discussing the potential applications of the w-Net model in medical image analysis, and how the model could be extended to other domains such as lung and liver segmentation. The authors believe that the model's ability to learn from both supervised and unsupervised learning, as well as its use of multi-dimensional attention and cascade multi-scale convolutions, make it a powerful tool for medical image segmentation."
    },
    {
      "id": "3f4a899b-9eb9-4fb7-96dc-0a1e51350ed0",
      "adv_source_id": "3f4a899b-9eb9-4fb7-96dc-0a1e51350ed0",
      "source_id": "b879a41c-044d-4adf-8404-796a1f66bc0f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Disentangle, align and fuse for multimodal and semi-supervised image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Disentangle, align and fuse for multimodal and semi-supervised image segmentation\".",
      "generation": "Abstract:\nImage segmentation is a fundamental computer vision task with numerous applications. However, it remains a challenging problem, especially for multimodal and semi-supervised segmentation. In this paper, we propose a novel approach that leverages the power of disentangled representation learning, alignment, and fusion to address these challenges. Our method disentangles the underlying factors of variation in the data, aligns the different modalities, and fuses the information from both labeled and unlabeled data to improve segmentation performance. We demonstrate the effectiveness of our approach on a variety of multimodal and semi-supervised segmentation benchmarks, achieving state-of-the-art results. Our method's versatility and robustness make it a valuable tool for segmenting images in a variety of real-world applications."
    },
    {
      "id": "06d578ba-5116-4b57-b84d-cd2a8e852564",
      "adv_source_id": "06d578ba-5116-4b57-b84d-cd2a8e852564",
      "source_id": "b879a41c-044d-4adf-8404-796a1f66bc0f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Disentangle, align and fuse for multimodal and semi-supervised image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Disentangle, align and fuse for multimodal and semi-supervised image segmentation\".",
      "generation": "Abstract:\nImage segmentation is a fundamental task in medical imaging and has been actively researched for decades. In recent years, deep learning models have shown promising results in image segmentation. However, a significant number of medical images available in the real world are labeled partially or not at all. These images are called semi-supervised or unsupervised images, respectively. Meanwhile, some images can be in multiple modalities, such as computed tomography (CT) and magnetic resonance imaging (MRI). Therefore, the image segmentation task should be performed under the semi-supervised and multimodal settings. In this paper, we propose a novel approach for image segmentation under the semi-supervised and multimodal settings. We disentangle the foreground and background of the image using a U-Net-based encoder-decoder architecture. Then, we align the features of different modalities using a shared encoder and fuse them into a single vector using a weights-learned fusion strategy. Finally, we segment the image by classifying the fused features using a conditional random field (CRF) classifier. We evaluate our approach on the BRATS dataset, which is a commonly used dataset for brain tumor segmentation. The experimental results show that our approach achieves competitive performance compared to the state-of-the-art approaches on the BRATS dataset."
    },
    {
      "id": "051c6215-8c49-4995-bfbf-0e8ff517e63b",
      "adv_source_id": "051c6215-8c49-4995-bfbf-0e8ff517e63b",
      "source_id": "c840fcd3-26ff-40d9-8054-692fa48b1d37",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision",
      "prompt": "Write the abstract for the academic paper titled \"A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision\".",
      "generation": "In this paper, we present a teacher-student framework for semi-supervised medical image segmentation from mixed supervision. The proposed method utilizes a teacher model pre-trained on a large dataset of labeled images and a student model pre-trained on a small dataset of labeled images. The two models are trained together in a semi-supervised manner, with the teacher model providing soft labels for the student model. The student model is also trained on a mixed dataset of labeled and unlabeled images, with the unlabeled images providing additional information for the segmentation task. The proposed method has been evaluated on the ISBI 2017 segmentation challenge dataset and has achieved competitive results compared to other state-of-the-art methods."
    },
    {
      "id": "c4edef29-9ead-4d9b-bc0c-376278bf0d33",
      "adv_source_id": "c4edef29-9ead-4d9b-bc0c-376278bf0d33",
      "source_id": "c840fcd3-26ff-40d9-8054-692fa48b1d37",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision",
      "prompt": "Write the abstract for the academic paper titled \"A Teacher-Student Framework for Semi-supervised Medical Image Segmentation From Mixed Supervision\".",
      "generation": "In this paper, we present a teacher-student framework for semi-supervised medical image segmentation from mixed supervision. We propose to learn a teacher model from a small amount of labeled data and then use the teacher model to regularize and improve the segmentation performance of the student model on a large amount of unlabeled data. Our framework is specifically designed to handle mixed supervision, which is a common scenario in medical image segmentation where the labeled data is a mixture of different modalities (e.g., MRI, CT, and PET) and labels (e.g., tumor, healthy). We demonstrate the effectiveness of our framework on three different medical image segmentation datasets, showing that our method can significantly improve the segmentation performance compared to the state-of-the-art."
    },
    {
      "id": "62989647-7c2a-4f3f-827f-178999422641",
      "adv_source_id": "62989647-7c2a-4f3f-827f-178999422641",
      "source_id": "ca413b45-4e39-4c31-a986-9d8f38c02a9f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network",
      "prompt": "Write the abstract for the academic paper titled \"HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\".",
      "generation": "Here is an abstract for the academic paper \"HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\":\n\n\"HS-ResNet is a novel hierarchical-split block on convolutional neural networks that aims to improve the performance of deep learning models. The hierarchical-split block is designed to address the problem of vanishing gradient during training, which is a common issue in deep learning models. The block is composed of two sub-blocks, the first sub-block is a standard convolutional block that performs a series of convolutions and pooling operations, while the second sub-block is a hierarchical-split block that performs a series of convolutions and pooling operations with a hierarchical-split layer. The hierarchical-split layer is designed to split the input tensor into multiple branches, which allows for the training of deep networks with a shallower depth. The HS-ResNet model is evaluated on the CIFAR-10 and CIFAR-100 datasets, and the results show that the model outperforms the previous state-of-the-art models in terms of accuracy and efficiency. The model is also evaluated on the ImageNet dataset, and the results show that the model is on par with the previous state-of-the-art models in terms of accuracy.\""
    },
    {
      "id": "3b9b67b7-f0e1-4f81-8a38-fccbdbcf11eb",
      "adv_source_id": "3b9b67b7-f0e1-4f81-8a38-fccbdbcf11eb",
      "source_id": "ca413b45-4e39-4c31-a986-9d8f38c02a9f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network",
      "prompt": "Write the abstract for the academic paper titled \"HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\".",
      "generation": "Here is an abstract for the academic paper \"HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\":\n\nHS-ResNet is a novel hierarchical-split block on convolutional neural networks that enhances network performance. The hierarchical-split block allows the network to learn hierarchical representations of features, enabling it to capture more complex and abstract representations. This is achieved by splitting the input into multiple branches, each of which learns a different level of abstraction. The branches are then merged to produce a final representation.\n\nHS-ResNet has been shown to improve the performance of convolutional neural networks on a variety of tasks, including image classification, object detection, and semantic segmentation. It is a simple and efficient method that can be easily integrated into existing networks.\n\nThe paper will discuss the details of the hierarchical-split block, including the various architectures and the training process. It will also present experimental results showing the performance of HS-ResNet on a number of standard datasets."
    },
    {
      "id": "ddbe7623-239e-484b-b0dd-46dea8723412",
      "adv_source_id": "ddbe7623-239e-484b-b0dd-46dea8723412",
      "source_id": "9459e172-bc0f-48bf-b238-eab238e95ac2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps",
      "prompt": "Write the abstract for the academic paper titled \"MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps\".",
      "generation": "Semantic segmentation is a fundamental task in computer vision with a wide range of applications in robotics and autonomous driving. However, annotating large amounts of pixel-level labels is expensive and time-consuming, especially for high-resolution images. To overcome this issue, active learning can be used, where the model queries the user for labels only for the most informative regions, which are selected based on some criteria. In this paper, we propose a new active learning method for semantic segmentation called MetaBox+, which uses priority maps to select the most informative regions. Our method is simple, efficient, and effective, and can be easily integrated into any existing semantic segmentation framework. We evaluate our method on the Cityscapes and CamVid datasets, and show that it significantly reduces the amount of annotation required while maintaining high accuracy."
    },
    {
      "id": "a28f1bfa-aa45-4aa1-9050-5b8373ff93e0",
      "adv_source_id": "a28f1bfa-aa45-4aa1-9050-5b8373ff93e0",
      "source_id": "9459e172-bc0f-48bf-b238-eab238e95ac2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps",
      "prompt": "Write the abstract for the academic paper titled \"MetaBox+: A new Region Based Active Learning Method for Semantic Segmentation using Priority Maps\".",
      "generation": "MetaBox+ is a new, effective active learning framework for semantic segmentation. It introduces the use of priority maps to relabeling training data and adjusts the proposals of difficult boxes. A new joint-training scheme is also introduced to learn the proposed priority maps. We evaluate MetaBox+ on VOC 2012 Segmentation Challenge dataset and achieve promising results, outperforming the previous state-of-the-art approach in terms of intersection over union (IoU) metric for segmentation."
    },
    {
      "id": "ca1b27d0-c414-4ad6-b1a9-8430b6472913",
      "adv_source_id": "ca1b27d0-c414-4ad6-b1a9-8430b6472913",
      "source_id": "256e9cc8-7739-48cc-aba4-30a8840090f0",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images",
      "prompt": "Write the abstract for the academic paper titled \"Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images\".",
      "generation": "Interactive segmentation techniques allow users to annotate objects of interest in an image, and automatically propagate labels to fully segment the image. In many real-world applications, the objects of interest have specific structural properties, like periodicity or similarity in a series of images. We introduce a framework that uses a content-based propagation strategy that leverages the periodic nature of the objects of interest in a family of images, in addition to the user markings. Our framework significantly reduces the amount of manual labor required from the user to obtain a fully segmented image, and is specifically geared towards images with patterned content, like tiles in architectural photographs or cells in microscopy images. We present an evaluation of our method compared to state-of-the-art interactive segmentation techniques on a dataset of microscopy images of fluorescently stained tissues, and show that our method requires significantly fewer clicks to obtain a segmentation."
    },
    {
      "id": "db9efca3-0ff2-4246-b683-836ed1103803",
      "adv_source_id": "db9efca3-0ff2-4246-b683-836ed1103803",
      "source_id": "256e9cc8-7739-48cc-aba4-30a8840090f0",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images",
      "prompt": "Write the abstract for the academic paper titled \"Content-based Propagation of User Markings for Interactive Segmentation of Patterned Images\".",
      "generation": "Interactive segmentation techniques allow users to annotate objects in an image by drawing rough contours around them. The goal of the outlined propagation framework is to improve these segmentation results by selectively propagating user markings from one image to another, based on their content. This paper describes a technique for doing so, which operates in the image transform domain, and exploits image structure to avoid distortions that might be caused by inappropriate content-based propagation. Furthermore, the algorithm also takes into account the uncertainty in user markings. We demonstrate the effectiveness of the proposed approach on a wide range of images, showing that the algorithm is able to successfully propagate user markings from one image to another, while maintaining the integrity and accuracy of the segmentation."
    },
    {
      "id": "75fb6a29-30cc-4f5e-890b-c44658ab3415",
      "adv_source_id": "75fb6a29-30cc-4f5e-890b-c44658ab3415",
      "source_id": "e84fe950-80c8-4cab-ac0a-f2d2232477b3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to segment microscopy images with lazy labels",
      "prompt": "Write the abstract for the academic paper titled \"Learning to segment microscopy images with lazy labels\".",
      "generation": "Here is an abstract for the academic paper \"Learning to segment microscopy images with lazy labels\":\n\nSegmenting cells in microscopy images is important for biological research. We propose a method to learn a segmentation model from images with lazy labels, which are labels that only highlight the cells of interest, ignoring the rest. Our approach extends a fully convolutional network with a spatial attention module that can be trained with weak supervision from lazy labels. We train the proposed model on a dataset of microscopy images with lazy labels, and show that it can accurately segment cells. Our approach is able to learn a general cell segmentation model that can be applied to new images without requiring expensive, manual labeling. \n\nWould you like me to generate a full draft of the manuscript for the academic paper titled \"Learning to segment microscopy images with lazy labels\"?"
    },
    {
      "id": "30d93f6c-e3d8-49da-a0bd-6e6104bf63e3",
      "adv_source_id": "30d93f6c-e3d8-49da-a0bd-6e6104bf63e3",
      "source_id": "e84fe950-80c8-4cab-ac0a-f2d2232477b3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning to segment microscopy images with lazy labels",
      "prompt": "Write the abstract for the academic paper titled \"Learning to segment microscopy images with lazy labels\".",
      "generation": "Here is an abstract for the academic paper titled \"Learning to segment microscopy images with lazy labels\":\n\nArtificial intelligence techniques for segmentation of microscopy images have become an increasingly active area of research, with applications in various fields such as pathology and neuroscience. In this paper, we present a novel approach to segmentation of microscopy images using a combination of supervised and unsupervised learning techniques. Our approach makes use of a large dataset of microscopy images with ground truth segmentations, as well as a set of unlabeled images. By training a convolutional neural network on the labeled images and then using the unlabeled images as a validation set, we are able to achieve state-of-the-art performance in segmentation of microscopy images. Our approach is particularly useful in cases where the acquisition of labeled images is costly or time-consuming, and we believe it has the potential to revolutionize the field of digital pathology."
    },
    {
      "id": "71682426-afcd-446c-a969-bf8a2c466f8d",
      "adv_source_id": "71682426-afcd-446c-a969-bf8a2c466f8d",
      "source_id": "bf7d13ac-ed40-46b5-a0a4-fae8f7e8e6b0",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mask-guided sample selection for Semi-Supervised Instance Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Mask-guided sample selection for Semi-Supervised Instance Segmentation\".",
      "generation": "Here is the abstract for the academic paper titled \"Mask-guided sample selection for Semi-Supervised Instance Segmentation\":\n\nRecent progress in instance-based segmentation is largely driven by the advancement of two-stage detectors, especially those using a per-instance classifier for the second stage. However, training a large number of instance classifiers is time-consuming and requires a large amount of labeled data. In this paper, we aim to reduce the reliance on labeled data and focus on improving the quality of the labeled data for training instance classifiers, which is a more challenging aspect in the semi-supervised setting. We propose Mask-guided Sample Selection (MSS), a simple and effective method to select informative samples for annotating instance classifiers. Specifically, given a mask of an object, our method selects the most informative pixels to label, which can guide the training of an instance classifier. We formulate the selection as a pixel-wise loss minimization problem and solve it with an efficient algorithm. We demonstrate the effectiveness of MSS on both synthetic and real datasets, where MSS can select twice more informative pixels compared to the baselines while keeping the annotation time similar. Furthermore, MSS can improve the mAP of instance classifiers on VOC and Cityscapes by 3.0 and 2.7 points, respectively."
    },
    {
      "id": "3bbb563f-6c0d-41f7-85c2-388bcbed927c",
      "adv_source_id": "3bbb563f-6c0d-41f7-85c2-388bcbed927c",
      "source_id": "bf7d13ac-ed40-46b5-a0a4-fae8f7e8e6b0",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mask-guided sample selection for Semi-Supervised Instance Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Mask-guided sample selection for Semi-Supervised Instance Segmentation\".",
      "generation": "Here is an abstract for the academic paper titled \"Mask-guided sample selection for Semi-Supervised Instance Segmentation\":\n\nRecent progress in instance-based convolutional neural networks (InstaNs) has advanced the research on instance-level segmentation. However, training an InstaNs model still requires a large amount of labeled data, which is expensive to obtain. In this paper, we tackle the problem of training InstaNs models with fewer labeled data. We propose a novel mask-guided sample selection (MGSS) approach to select training samples based on their similar appearance with the given mask. Given a mask of an object, our method first estimates the probability distribution of the image pixels belonging to the object. Then, we select the training samples that have high probabilities of belonging to the object. We evaluate our method on the Cityscapes dataset and compare it with other state-of-the-art semi-supervised approaches. Our results show that our method can achieve 78.4% mIoU on the Cityscapes dataset, outperforming the previous state-of-the-art methods by 3.2%."
    },
    {
      "id": "87e506f2-ac94-43ba-855b-23c6a205f9a4",
      "adv_source_id": "87e506f2-ac94-43ba-855b-23c6a205f9a4",
      "source_id": "f9bd0e9d-1fba-4d46-a478-4c128430e1a3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data",
      "prompt": "Write the abstract for the academic paper titled \"PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data\".",
      "generation": "In this paper, we present a novel approach for jointly reconstructing and segmenting the cardiac walls in 3D from CT data. Our method, which we refer to as PC-U Net, is based on a fully convolutional neural network architecture. We train the network using a weakly-supervised learning strategy, in which a single network is trained to perform both reconstruction and segmentation tasks. During training, the network is provided with ground truth segmentation and reconstruction data, and it learns to predict the segmentation and reconstruction outputs for new input data.\n\nWe evaluate the performance of our method on a public dataset of cardiac CT scans. Our results show that PC-U Net is able to accurately reconstruct and segment the cardiac walls, outperforming existing state-of-the-art methods in terms of both segmentation accuracy and reconstruction quality. We also perform a comprehensive ablation study to evaluate the contribution of each component of our network architecture.\n\nOur work demonstrates the potential of deep learning for cardiac imaging and suggests new directions for research in this field."
    },
    {
      "id": "7ae197d6-c6c3-4f7b-9cdb-7c17cb983307",
      "adv_source_id": "7ae197d6-c6c3-4f7b-9cdb-7c17cb983307",
      "source_id": "f9bd0e9d-1fba-4d46-a478-4c128430e1a3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data",
      "prompt": "Write the abstract for the academic paper titled \"PC-U Net: Learning to Jointly Reconstruct and Segment the Cardiac Walls in 3D from CT Data\".",
      "generation": "Abstract:\nWe present a novel approach for jointly reconstructing and segmenting the cardiac walls in 3D from CT data. Our method, called PC-U Net, leverages a fully convolutional neural network architecture with an attention module to accurately localize and segment the cardiac walls. The attention module enables the network to focus on relevant regions and ignore irrelevant information, improving the segmentation accuracy. We evaluate our method on a public dataset and compare it to state-of-the-art approaches. Our results show that PC-U Net significantly improves the segmentation accuracy and is able to recover the 3D shape of the cardiac walls with high fidelity."
    },
    {
      "id": "3d17b460-2ec3-4136-ae44-9bb4d888f3e5",
      "adv_source_id": "3d17b460-2ec3-4136-ae44-9bb4d888f3e5",
      "source_id": "ce9b46ec-d192-4df9-9131-0a0851f72029",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image segmentation via Cellular Automata",
      "prompt": "Write the abstract for the academic paper titled \"Image segmentation via Cellular Automata\".",
      "generation": "This paper presents a novel approach to image segmentation using cellular automata. Our method utilizes the dynamic nature of cellular automata to extract meaningful features from images and partition them into distinct regions. We propose a hybrid approach that combines the advantages of both deterministic and stochastic cellular automata to achieve accurate and efficient segmentation. Our method is evaluated on a variety of images, and the results show that it can achieve state-of-the-art performance in terms of segmentation quality and speed. We believe that our approach has the potential to revolutionize the field of image segmentation and pave the way for a new generation of algorithms."
    },
    {
      "id": "4f8f2c12-eb9e-41bb-aa07-c6fc33212278",
      "adv_source_id": "4f8f2c12-eb9e-41bb-aa07-c6fc33212278",
      "source_id": "ce9b46ec-d192-4df9-9131-0a0851f72029",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image segmentation via Cellular Automata",
      "prompt": "Write the abstract for the academic paper titled \"Image segmentation via Cellular Automata\".",
      "generation": "This paper discusses the use of cellular automata for image segmentation. We propose a novel method based on cellular automata for segmenting images. Our method utilizes the inherent parallelism and simplicity of cellular automata to efficiently segment images. We compare our method to existing techniques and show that it is comparable in performance while being more efficient. Our method has the potential to be used for a variety of applications including image editing, medical imaging, and satellite imaging. \n\nThe paper will proceed as follows:\n1. Introduction: A description of the problem being solved and a review of related work.\n2. Methodology: A description of our proposed method and the algorithms used.\n3. Results: A discussion of the experimental results and a comparison to related work.\n4. Conclusion: A summary of the work done and possible future improvements."
    },
    {
      "id": "196171c9-b748-4306-8ec5-830309e2634e",
      "adv_source_id": "196171c9-b748-4306-8ec5-830309e2634e",
      "source_id": "c0292110-f6e8-4a0f-a23c-1baafe82481c",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field",
      "prompt": "Write the abstract for the academic paper titled \"Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field\".",
      "generation": "Abstract:\nWe propose a novel polarimetric SAR image semantic segmentation algorithm based on 3D Discrete Wavelet Transform (DWT) and Markov Random Field (MRF). The 3D DWT is utilized to extract the spatial and polarimetric information from the polarimetric SAR image, and the MRF is employed to refine the segmentation results. The proposed algorithm has been evaluated on the University of Illinois at Urbana-Champaign (UIUC) polarimetric SAR image dataset, and the experimental results demonstrate that our algorithm achieves superior performance compared to existing algorithms.\n\nIntroduction:\nPolarimetric synthetic aperture radar (SAR) image segmentation is a fundamental step in the analysis and interpretation of polarimetric SAR images. It involves the decomposition of the image into distinct regions, each corresponding to a different material or surface. The accuracy and reliability of the segmentation results are crucial for subsequent analysis and interpretation of the polarimetric SAR image.\n\nNumerous algorithms have been proposed for polarimetric SAR image segmentation, including expert systems, fuzzy logic, and artificial neural networks. These algorithms have demonstrated varying degrees of success in different applications. However, there is still a need for developing more effective and efficient algorithms for polarimetric SAR image segmentation.\n\nIn this paper, we propose a novel algorithm for polarimetric SAR image segmentation based on 3D Discrete Wavelet Transform (DWT) and Markov Random Field (MRF). The 3D DWT is utilized to extract the spatial and polarimetric information from the polarimetric SAR image, and the MRF is employed to refine the segmentation results. The proposed algorithm has been evaluated on the University of Illinois at Urbana-Champaign (UIUC) polarimetric SAR image dataset, and the experimental results demonstrate that our algorithm achieves superior performance compared to existing algorithms.\n\nBackground:\nSynthetic Aperture Radar (SAR) is a remote sensing technology that employs radar to produce high-resolution images of the Earth's surface. SAR utilizes the motion of an aircraft or satellite to simulate a large synthetic aperture, which enables the production of high-resolution images of the Earth's surface.\n\nPolarimetric SAR is a specialized form of SAR that utilizes polarimetry to extract additional information from the radar signals. Polarimetry measures the polarization of the radar signals, which can provide information about the material properties of the Earth's surface. This information can be used to distinguish between different types of materials, such as soil, water, and vegetation.\n\nSegmentation"
    },
    {
      "id": "cdab8ff6-7450-4969-a7ea-8329e692f4ca",
      "adv_source_id": "cdab8ff6-7450-4969-a7ea-8329e692f4ca",
      "source_id": "c0292110-f6e8-4a0f-a23c-1baafe82481c",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field",
      "prompt": "Write the abstract for the academic paper titled \"Polarimetric SAR Image Semantic Segmentation with 3D Discrete Wavelet Transform and Markov Random Field\".",
      "generation": "Abstract:\nWe propose a novel polarimetric SAR image semantic segmentation algorithm based on 3D discrete wavelet transform (DWT) and Markov random field (MRF). Our approach utilizes the polarization information of the SAR image, which provides additional texture and structural details that are useful for accurate segmentation. The 3D DWT is employed to extract multi-scale and multi-orientation features from the polarimetric SAR image, and the MRF is used to model the spatial dependencies between pixels and refine the segmentation results. We evaluate our algorithm on a publicly available dataset and compare it with state-of-the-art algorithms. Our results show that our approach significantly improves the segmentation accuracy, especially in challenging areas such as boundaries and texture-less regions.\n\nHighlights:\n- We propose a novel polarimetric SAR image semantic segmentation algorithm based on 3D discrete wavelet transform and Markov random field.\n- Our approach utilizes the polarization information of the SAR image, which provides additional texture and structural details for accurate segmentation.\n- The 3D DWT is employed to extract multi-scale and multi-orientation features from the polarimetric SAR image, and the MRF is used to model the spatial dependencies between pixels and refine the segmentation results.\n- We evaluate our algorithm on a publicly available dataset and compare it with state-of-the-art algorithms, and our results show that our approach significantly improves the segmentation accuracy."
    },
    {
      "id": "d309f449-86e3-4ce8-812d-34c68628e899",
      "adv_source_id": "d309f449-86e3-4ce8-812d-34c68628e899",
      "source_id": "b34196e2-ce17-4cc3-bddf-9dbec580bda5",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep learning for image segmentation: veritable or overhyped?",
      "prompt": "Write the abstract for the academic paper titled \"Deep learning for image segmentation: veritable or overhyped?\".",
      "generation": "Abstract:\nThe field of medical imaging has recently seen the introduction of deep learning techniques for image segmentation. While these techniques have shown promise in various applications, it is important to assess whether they are a veritable improvement over traditional methods or merely a hype. In this paper, we conduct a comprehensive review of the current state of deep learning for image segmentation in medical imaging. We examine the strengths and limitations of these techniques and compare them to the more established segmentation methods. We find that while deep learning techniques can achieve state-of-the-art performance in certain cases, they also suffer from limitations such as the need for large amounts of labeled data and the potential for overfitting. Overall, we conclude that deep learning for image segmentation in medical imaging is a promising but not yet mature technology that requires further research and development to reach its full potential."
    },
    {
      "id": "ab75bbbe-c32f-41c4-ad99-1f01932cd5a1",
      "adv_source_id": "ab75bbbe-c32f-41c4-ad99-1f01932cd5a1",
      "source_id": "b34196e2-ce17-4cc3-bddf-9dbec580bda5",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep learning for image segmentation: veritable or overhyped?",
      "prompt": "Write the abstract for the academic paper titled \"Deep learning for image segmentation: veritable or overhyped?\".",
      "generation": "Abstract:\nThe field of medical imaging has recently seen the introduction of deep learning techniques for image segmentation. This new approach has shown promising results, but it has also been met with some skepticism. In this paper, we aim to evaluate the effectiveness of deep learning for image segmentation. We compare the performance of deep learning models with that of traditional, manually segmented images. Our results show that deep learning models can indeed achieve superior performance to manual segmentation, but only when the right algorithms are used in the right contexts. We conclude that deep learning for image segmentation is a powerful tool, but it is not a cure-all. The choice of segmentation algorithm still plays an important role in achieving the best results."
    },
    {
      "id": "ba27bfd5-ea3d-45df-8ec0-fc2b03a4695f",
      "adv_source_id": "ba27bfd5-ea3d-45df-8ec0-fc2b03a4695f",
      "source_id": "656f2cf4-7ba8-466e-bd2b-85c1a64fc56b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images",
      "prompt": "Write the abstract for the academic paper titled \"Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images\".",
      "generation": "Abstract:\nTraditional 3D semantic segmentation methods treat all voxels in a 3D medical image uniformly when predicting labels, despite the significant structural differences in the shapes of different organs. In this paper, we propose a shape-aware semi-supervised approach for 3D semantic segmentation of medical images. We introduce a shape-aware loss function that utilizes shape information in the form of a signed distance function to guide the prediction of pixel-level labels. Our approach leverages the rich shape information provided by the signed distance function to improve the performance of 3D semantic segmentation. We demonstrate the effectiveness of our approach on a challenging dataset of abdominal CT scans and show that our method achieves state-of-the-art performance in 3D semantic segmentation."
    },
    {
      "id": "4bbe2468-4afe-4ef8-a04d-78282cdb7100",
      "adv_source_id": "4bbe2468-4afe-4ef8-a04d-78282cdb7100",
      "source_id": "656f2cf4-7ba8-466e-bd2b-85c1a64fc56b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images",
      "prompt": "Write the abstract for the academic paper titled \"Shape-aware Semi-supervised 3D Semantic Segmentation for Medical Images\".",
      "generation": "Abstract:\nTraditional semi-supervised learning (SSL) methods tend to ignore the intrinsic shape information of objects, which is crucial for 3D semantic segmentation (3DSS) of medical images. In this paper, we present a shape-aware semi-supervised 3D semantic segmentation (SA-SS3D) method that incorporates shape information into the SSL framework. We introduce a shape-aware loss function that utilizes shape information to regularize the SSL objective function. Our method also utilizes a shape-aware encoder that incorporates shape information into the feature extraction process. We demonstrate the effectiveness of our method on 3D semantic segmentation of abdominal computed tomography (CT) scans, and show that our method outperforms existing SSL methods in terms of segmentation accuracy."
    },
    {
      "id": "e5f4c382-f43f-46f9-9d58-cbf3fd4d0c9d",
      "adv_source_id": "e5f4c382-f43f-46f9-9d58-cbf3fd4d0c9d",
      "source_id": "57782d19-642c-44e0-8ebc-1408904b770e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Image Labelling at Pixel Level",
      "prompt": "Write the abstract for the academic paper titled \"Automatic Image Labelling at Pixel Level\".",
      "generation": "Abstract:\nThe paper presents a method for automatic image labeling at the pixel level. The proposed method employs a deep learning architecture called Convolutional Neural Network (CNN) to classify pixels in an image. The CNN is trained on a large dataset of images and their corresponding labels. The trained model is then used to classify pixels in a new image. The pixel-level classification results are combined using a graph cut algorithm to produce a final label for each pixel. The proposed method achieves state-of-the-art performance on the Stanford Background Segmentation Dataset and the Berkeley Segmentation Dataset. The source code and trained models are made publicly available for future research."
    },
    {
      "id": "019a6093-e423-4f16-8c3d-c4650780414b",
      "adv_source_id": "019a6093-e423-4f16-8c3d-c4650780414b",
      "source_id": "57782d19-642c-44e0-8ebc-1408904b770e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Image Labelling at Pixel Level",
      "prompt": "Write the abstract for the academic paper titled \"Automatic Image Labelling at Pixel Level\".",
      "generation": "Abstract:\nThe paper introduces a method for automatic image labeling at the pixel level. The proposed method takes an image as input and assigns a label to each pixel in the image. The method uses a deep learning approach to train a model on a large dataset of images and their corresponding pixel-level labels. The trained model is then used to predict the labels for new images. The proposed method achieves state-of-the-art performance on several benchmark datasets and is shown to be effective in various applications such as image segmentation, object detection, and image classification."
    },
    {
      "id": "e629edf5-7564-4f52-9fe1-ac364cbcd52d",
      "adv_source_id": "e629edf5-7564-4f52-9fe1-ac364cbcd52d",
      "source_id": "400c2edc-21b5-42aa-a700-335c3fce8264",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis",
      "prompt": "Write the abstract for the academic paper titled \"An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis\".",
      "generation": "Manufacturing is undergoing a revolution with the adoption of Industry 4.0 technologies that are poised to enhance the productivity and efficiency of production processes. However, the intermittent failures of these advanced systems could hinder their widespread adoption. To address this challenge, we propose a novel uncertainty-based human-in-the-loop system for tool wear analysis in the manufacturing domain. Our system employs a Bayesian deep learning model that can detect tool wear and predict its impact on the production process. To address the issue of uncertainty in the model's predictions, we incorporate a human-in-the-loop mechanism that leverages the domain expertise of manufacturing experts. Our system has been demonstrated to improve the accuracy of tool wear predictions, leading to improved productivity and efficiency in manufacturing."
    },
    {
      "id": "7b8e7918-4854-4081-9233-3939ca960f8f",
      "adv_source_id": "7b8e7918-4854-4081-9233-3939ca960f8f",
      "source_id": "400c2edc-21b5-42aa-a700-335c3fce8264",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis",
      "prompt": "Write the abstract for the academic paper titled \"An Uncertainty-based Human-in-the-loop System for Industrial Tool Wear Analysis\".",
      "generation": "Wear analysis of industrial tools is crucial for improving productivity and product quality in manufacturing industries. In this paper, we present a human-in-the-loop system for tool wear analysis based on uncertainty reasoning. The system utilizes a Bayesian network to model the uncertainty in tool wear data and incorporates expert knowledge through a decision-theoretic approach. A user-friendly interface enables domain experts to interact with the system and provide their knowledge and preferences. The system then utilizes the expert's input to update the Bayesian network and infer the most likely cause of tool wear. We demonstrate the effectiveness of our approach through a case study in the field of sheet metal forming. The results show that our system can provide valuable insights into tool wear, leading to improved manufacturing processes and product quality."
    },
    {
      "id": "8321edd3-bee5-435f-8a2d-77e77c07c90d",
      "adv_source_id": "8321edd3-bee5-435f-8a2d-77e77c07c90d",
      "source_id": "cffac8c8-68a7-4d71-8489-a40318541075",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation\".",
      "generation": "Abstract:\nWe present a novel approach for cardiac segmentation named Dual-Teacher that innovatively integrates intra-domain and inter-domain teachers to improve segmentation performance. Our model leverages the power of both teachers to provide robust and effective guidance to the student model, leading to significant improvements in segmentation accuracy. The intra-domain teacher is trained on a large dataset of cardiac images, while the inter-domain teacher is trained on a diverse set of medical images. By combining the strengths of both teachers, our model is able to benefit from the rich context provided by the intra-domain teacher and the generalization capabilities offered by the inter-domain teacher. This approach significantly improves segmentation performance, demonstrating the effectiveness of our Dual-Teacher framework for cardiac segmentation."
    },
    {
      "id": "77dd8724-fb02-4b2b-97a8-04e0720461d5",
      "adv_source_id": "77dd8724-fb02-4b2b-97a8-04e0720461d5",
      "source_id": "cffac8c8-68a7-4d71-8489-a40318541075",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Dual-Teacher: Integrating Intra-domain and Inter-domain Teachers for Annotation-efficient Cardiac Segmentation\".",
      "generation": "Abstract:\nWe present a novel approach for cardiac segmentation named Dual-Teacher that integrates intra-domain and inter-domain teachers to improve segmentation performance. The intra-domain teacher is a convolutional neural network (CNN) trained on a large dataset of cardiac images, while the inter-domain teacher is a CNN trained on a diverse set of anatomical images. The proposed method leverages the power of both teachers to improve the segmentation performance on cardiac images. We evaluated Dual-Teacher on the IEEE ISBI 2017 Cardiac Segmentation Challenge and achieved the best performance in the oversegmentation track. Our method achieved a dice similarity coefficient of 95.80% on the validation set, which was significantly higher than the second-best score of 92.10%. Our approach achieved comparable performance with state-of-the-art methods while being more annotation-efficient. The source code and models have been made publicly available at https://github.com/ Wonderland-Park/Dual-Teacher."
    },
    {
      "id": "0fa05c20-3a07-4a68-aa38-d522574150e7",
      "adv_source_id": "0fa05c20-3a07-4a68-aa38-d522574150e7",
      "source_id": "a3ac3bc2-73ce-4a07-90a0-8eb60b5215f1",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Hyperbolic Convolutional Neural Networks",
      "prompt": "Write the abstract for the academic paper titled \"Fully Hyperbolic Convolutional Neural Networks\".",
      "generation": "An important problem in deep learning is to understand the limitations and trade-offs of model capacity, and in particular, to understand whether existing theoretical results for feed-forward neural networks extend to convolutional neural networks (CNNs). In this paper, we answer this question in the affirmative by proving that, unlike feed-forward neural networks, all CNNs with linear activations are inherently infinitely wide in the sense of Khot et al. (2018). Furthermore, we show that even an arbitrary CNN with ReLU activations can be made fully hyperbolic by adding a single hidden layer."
    },
    {
      "id": "3c267c4f-9390-4b22-8b10-1a8c5bc383a9",
      "adv_source_id": "3c267c4f-9390-4b22-8b10-1a8c5bc383a9",
      "source_id": "a3ac3bc2-73ce-4a07-90a0-8eb60b5215f1",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Hyperbolic Convolutional Neural Networks",
      "prompt": "Write the abstract for the academic paper titled \"Fully Hyperbolic Convolutional Neural Networks\".",
      "generation": "An important problem in understanding neural networks is how to reconcile the pervasive vanishing gradient problem with the commonly observed success of gradient-based optimization. We show that convolutional neural networks (CNNs) with purely hyperbolic activation functions, which we call fully hyperbolic CNNs (FH-CNNs), are (globally) equivariant to the exponential mapping, i.e., the gradient flow initiated at any point remains at equilibrium. This implies that every local minimum of the loss function is also a global minimum. Consequently, FH-CNNs do not suffer from the vanishing gradient problem and can be trained effectively using stochastic gradient descent (SGD) without accleration. We prove these results via a novel extension of the implicit function theorem to Banach spaces, which may be of independent interest. We also provide a constructive proof that every FH-CNN with a depth of two is globally equivariant to the exponential mapping. We show that FH-CNNs exhibit a rich spectrum of behaviors, from global equilibrium to unstable gradient flow, as a function of the choice of the activation function and the network depth."
    },
    {
      "id": "f17c8884-359d-4ac4-8371-033fd9c7d600",
      "adv_source_id": "f17c8884-359d-4ac4-8371-033fd9c7d600",
      "source_id": "e346ff27-0566-4875-93c4-2099b7a279d6",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation\".",
      "generation": "The paper aims to tackle the problem of video segmentation in bad weather by proposing a new frame work that improves the performance. A new approach for obtaining edge-aware features in a hierarchical framework is presented, in which a Rain/Snow filter is proposed to remove the adverse weather effects. The proposed method has been evaluated using the Cityscapes dataset and has achieved better results in comparison to the other state-of-the-art works."
    },
    {
      "id": "5f4cbb97-282a-4bfe-a1ab-db8bc2da548f",
      "adv_source_id": "5f4cbb97-282a-4bfe-a1ab-db8bc2da548f",
      "source_id": "e346ff27-0566-4875-93c4-2099b7a279d6",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Robust Semantic Segmentation in Adverse Weather Conditions by means of Fast Video-Sequence Segmentation\".",
      "generation": "The paper aims to tackle the problem of video segmentation in bad weather by proposing a new approach that takes into account the limitations of existing algorithms. A novel algorithm for fast video-sequence segmentation is presented, which is based on the assumption that adjacent frames in a video have similar contents, and can be segmented using a similarity measure. The proposed approach is evaluated on the UC-Merced dataset, and results show that the algorithm is able to achieve a competitive performance when compared to state-of-the-art algorithms while running at a faster speed."
    },
    {
      "id": "1a72ff7f-9e17-4d49-959e-508c2c51732e",
      "adv_source_id": "1a72ff7f-9e17-4d49-959e-508c2c51732e",
      "source_id": "7f6e9b63-40ad-4587-b03e-3492501ed835",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Region-of-interest guided Supervoxel Inpainting for Self-supervision",
      "prompt": "Write the abstract for the academic paper titled \"Region-of-interest guided Supervoxel Inpainting for Self-supervision\".",
      "generation": "In this paper, we present an approach for self-supervised learning for image segmentation. Our method utilizes a Region-of-Interest (ROI) guided inpainting approach to generate a large amount of labeled data for training a segmentation model. We begin by generating a coarse segmentation using a clustering algorithm. We then apply an inpainting algorithm to fill in the voxels outside the region of interest, which is defined by the coarse segmentation. This produces a synthetic image with a labeled region of interest. We then repeat this process on a set of images and use the produced labeled images to train a segmentation model. Our method does not require any labeled data for training and can be used to segment any image."
    },
    {
      "id": "0f659ce6-7eaa-43d6-959a-c26dc391cd67",
      "adv_source_id": "0f659ce6-7eaa-43d6-959a-c26dc391cd67",
      "source_id": "7f6e9b63-40ad-4587-b03e-3492501ed835",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Region-of-interest guided Supervoxel Inpainting for Self-supervision",
      "prompt": "Write the abstract for the academic paper titled \"Region-of-interest guided Supervoxel Inpainting for Self-supervision\".",
      "generation": "In this paper, we present an approach for self-supervised monocular depth estimation called Region-of-interest guided Supervoxel Inpainting for Self-supervision (RISE). Our method utilizes an attention module that focuses on regions of interest for inpainting and depth estimation, guided by a coarse depth estimation map. We train an MLP to produce a binary mask to classify voxels into either foreground or background, which is then upsampled using the binning method. We utilize the predicted mask for supervised depth estimation in the regions of interest and for inpainting in the background regions. We validate our method on the SceneNN and LytGrid datasets, and demonstrate that our method achieves state-of-the-art performance on both datasets compared to other self-supervised methods while having an order of magnitude faster training time."
    },
    {
      "id": "ab34c62e-87f4-4ceb-8a7c-53f44ea16694",
      "adv_source_id": "ab34c62e-87f4-4ceb-8a7c-53f44ea16694",
      "source_id": "17f6f9c7-dcc8-44e1-82b4-8233059f569b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift",
      "prompt": "Write the abstract for the academic paper titled \"Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift\".",
      "generation": "Medical image segmentation is prone to label corruption since the training and test domains differ. We propose a simple yet effective solution to address this domain shift issue via a cross-denoising network (CDN). Given a medical image, our proposed CDN estimates a clean label for each pixel and refines it through a denoising branch to reduce noise. To mitigate the domain shift issue, we propose a simple yet effective domain adaptation (DA) layer that aligns the distributions of source and target domains. Specifically, we estimate the source domain with only clean training labels and the target domain with only noisy labels. We then align the feature distributions of the source and target domains via maximum mean discrepancy (MMD) and minimize the distance between them via domain adaptation. Experiments on three public medical image segmentation datasets with noisy labels (e.g., due to annotation errors or low-quality images) demonstrate the effectiveness of the proposed approach against various types of noise. In particular, compared to the state-of-the-art methods, our approach achieves 9.92%, 6.73%, and 11.45% relative improvements in segmentation accuracy."
    },
    {
      "id": "7db142ba-83dc-48c6-ad9d-edf7bf2653fe",
      "adv_source_id": "7db142ba-83dc-48c6-ad9d-edf7bf2653fe",
      "source_id": "17f6f9c7-dcc8-44e1-82b4-8233059f569b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift",
      "prompt": "Write the abstract for the academic paper titled \"Cross-denoising Network against Corrupted Labels in Medical Image Segmentation with Domain Shift\".",
      "generation": "Medical image segmentation is prone to label corruption, which is usually addressed by label cleaning. However, it inadequately addresses domain shift across clinical sites/centers, leading to inaccurate segmentation results. We name this problem as Domain Shift in Label Corruption (DSLC). To tackle DSLC, we propose a Cross-denoising Network (CDN), which explicitly models the domain shift variability for denoising corrupted labels. Specifically, given a pair of sites with domain shift, our proposed CDN estimates the domain shift variability between the sites from only a few images without requiring any correspondence between the images across sites. Then, it effectively integrates the estimated domain shift variability into a denoising framework to mitigate DSLC. We validate the effectiveness of our approach on three public medical image segmentation benchmark datasets. Results on the PTB, IAM and ILPA datasets show our approach achieves the best segmentation performance when comparing with the state-of-the-art methods."
    },
    {
      "id": "174e3cff-06f1-4949-bdf1-99ae3d49eed0",
      "adv_source_id": "174e3cff-06f1-4949-bdf1-99ae3d49eed0",
      "source_id": "d70883cd-cb2b-4b55-be3c-861b1e796fbd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Elastic Boundary Projection for 3D Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Elastic Boundary Projection for 3D Medical Image Segmentation\".",
      "generation": "In this paper, we present a novel method for 3D medical image segmentation based on elastic boundary projection (EBP). Our approach combines the advantages of both boundary-based and region-based segmentation methods, allowing for accurate and efficient segmentation of medical images. We utilize a boundary-based approach to obtain a initial segmentation, which is then refined using a region-based approach. The resulting segmentation is then projected onto the original image using an elastic transformation, resulting in a final segmentation that accurately reflects the underlying anatomy. Our method has been evaluated on a variety of 3D medical images, including MRI and CT scans, and has been shown to outperform existing state-of-the-art methods in terms of both accuracy and efficiency."
    },
    {
      "id": "1214b9ee-a4f5-46d4-b87c-e8e7ad630387",
      "adv_source_id": "1214b9ee-a4f5-46d4-b87c-e8e7ad630387",
      "source_id": "d70883cd-cb2b-4b55-be3c-861b1e796fbd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Elastic Boundary Projection for 3D Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Elastic Boundary Projection for 3D Medical Image Segmentation\".",
      "generation": "Abstract:\nTraditional image segmentation techniques are limited when it comes to handling the challenges posed by 3D medical images. Elastic Boundary Projection (EBP) is a fast and efficient technique for 3D medical image segmentation. In this paper, we present an improved version of the EBP algorithm that provides better accuracy and efficiency. The improvements include the use of a more robust initialization method and a new optimization algorithm that is better suited for 3D images. The algorithm is evaluated on a public dataset and the results show that the improved EBP algorithm outperforms the original algorithm in terms of accuracy and efficiency.\n\nEBP is a hybrid segmentation algorithm that combines edge-based and region-based segmentation techniques. It first detects the edges of the object to be segmented using an edge-detection algorithm. It then fits a surface to these edges to obtain a initial segmentation. The algorithm then refining the segmentation by iteratively projecting the surface towards the edges of the object. The projection is done using a elastic kernel that allows the surface to deform in a elastic manner. This allows the algorithm to handle objects with complex shapes and surfaces.\n\nThe improved EBP algorithm uses a more robust initialization method that is based on the watershed algorithm. The watershed algorithm is an image processing algorithm that is used to segment images based on the location of local maxima and minima in the image. The improved algorithm then uses a new optimization algorithm that is based on the conjugate gradient method. The conjugate gradient method is an optimization algorithm that is used to find the minimum or maximum of a function. The improved algorithm then iteratively projects the surface towards the edges of the object using the elastic kernel."
    },
    {
      "id": "21d1b5c8-8eac-4571-9e08-b37c69bc0ba2",
      "adv_source_id": "21d1b5c8-8eac-4571-9e08-b37c69bc0ba2",
      "source_id": "48e0cc4d-96c8-4986-85d7-48cd599984ec",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation\".",
      "generation": "The paper presents a novel image segmentation approach called Super-BPD, which utilizes a Boundary-to-Pixel Direction (BPD) strategy. The proposed method efficiently segments images while preserving object boundaries and reducing computational complexity. The key innovation of Super-BPD is the use of a superpixel graph, where each node represents a superpixel, and the edges capture the object boundaries. By iteratively refining the superpixel graph, the algorithm achieves accurate segmentation results. The experimental results demonstrate the superiority of Super-BPD compared to state-of-the-art image segmentation methods in terms of runtime and segmentation quality. The paper concludes by discussing the potential applications of the proposed method in various computer vision tasks."
    },
    {
      "id": "41f9065c-6c76-451b-8fe4-d1e273f01552",
      "adv_source_id": "41f9065c-6c76-451b-8fe4-d1e273f01552",
      "source_id": "48e0cc4d-96c8-4986-85d7-48cd599984ec",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Super-BPD: Super Boundary-to-Pixel Direction for Fast Image Segmentation\".",
      "generation": "The paper proposes a new image segmentation approach called Super Boundary-to-Pixel Direction (Super-BPD). Unlike previous methods that rely on complex architectures and training procedures, Super-BPD is a simple, yet effective method that utilizes the power of boundary-based segmentation and pixel-based segmentation. By combining these two approaches, Super-BPD achieves state-of-the-art performance in terms of speed and accuracy. The experimental results demonstrate that Super-BPD is significantly faster and more accurate than the state-of-the-art methods on various image segmentation benchmarks. The code and models are available for download, making it easy for researchers and practitioners to reproduce the results."
    },
    {
      "id": "fced4c27-8a23-443a-9409-7cbffc21a313",
      "adv_source_id": "fced4c27-8a23-443a-9409-7cbffc21a313",
      "source_id": "6bdc1744-0c1f-4e11-82d5-0ecd0c7578c4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Traditional Method Inspired Deep Neural Network for Edge Detection",
      "prompt": "Write the abstract for the academic paper titled \"Traditional Method Inspired Deep Neural Network for Edge Detection\".",
      "generation": "The abstract for the academic paper titled \"Traditional Method Inspired Deep Neural Network for Edge Detection\" would include:\n\nThis paper proposes a deep neural network (DNN) for edge detection that is inspired by traditional methods. The DNN uses a multi-layer convolutional neural network (CNN) architecture and is trained on a large dataset of images with manual annotations for edges. The proposed method achieves state-of-the-art performance on the popular Berkeley Segmentation Dataset, outperforming traditional methods in terms of both accuracy and speed. The trained model is able to detect edges in a wide range of images, including those with complex shapes and textures. We also explore the use of transfer learning to pre-train the model on a large dataset of images with a similar edge detection task, which further improves the performance of the proposed method."
    },
    {
      "id": "8f829448-09f8-420d-952c-fc934c0e9543",
      "adv_source_id": "8f829448-09f8-420d-952c-fc934c0e9543",
      "source_id": "6bdc1744-0c1f-4e11-82d5-0ecd0c7578c4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Traditional Method Inspired Deep Neural Network for Edge Detection",
      "prompt": "Write the abstract for the academic paper titled \"Traditional Method Inspired Deep Neural Network for Edge Detection\".",
      "generation": "The paper presents a deep neural network (DNN) approach for edge detection that is inspired by traditional methods such as Gaussian filters and gradient operators. The DNN is designed to mimic the behavior of these traditional methods, which have been widely used for edge detection in images. The proposed network is composed of several convolutional layers, followed by fully connected layers. The initial layers are designed to capture low-level edge features, while the subsequent layers focus on more complex edge patterns. The network is trained on a dataset of images with manually annotated edges, and the performance is evaluated on the same dataset using metrics such as precision, recall, and F1-score. The results show that the proposed method achieves state-of-the-art performance in terms of accuracy and speed, compared to other DNN-based edge detection methods. The code and dataset used in this work are made publicly available to facilitate future research and development."
    },
    {
      "id": "e2dbb409-a034-4c51-8fbb-9dcd344c4a4e",
      "adv_source_id": "e2dbb409-a034-4c51-8fbb-9dcd344c4a4e",
      "source_id": "c7117aa4-b7c6-4d3f-93fb-d61228c887ca",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation\".",
      "generation": "The paper proposes a method for segmenting eye fundus images using a deep neural convolution network (DNCN) with a convex shape prior. The abstract for the paper would typically summarize the proposed method, the results, and the significance of the work.\n\nHere is a sample abstract for the paper:\n\nAbstract:\n\nWe propose a method for segmenting eye fundus images using a DNCN with a convex shape prior. The DNCN is trained on a dataset of eye fundus images and their corresponding segmentations. The convex shape prior is used to regularize the output of the DNCN, improving the accuracy of the segmentations. Our method is evaluated on a separate test dataset, and the results show that our method outperforms existing state-of-the-art methods for eye fundus image segmentation. The proposed method has the potential to improve the diagnosis and treatment of eye diseases, such as diabetic retinopathy and age-related macular degeneration.\n\nWould you like to know more about the paper?"
    },
    {
      "id": "954e3fe0-5422-4f78-878e-d89e5d7351a5",
      "adv_source_id": "954e3fe0-5422-4f78-878e-d89e5d7351a5",
      "source_id": "c7117aa4-b7c6-4d3f-93fb-d61228c887ca",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Convex Shape Prior for Deep Neural Convolution Network based Eye Fundus Images Segmentation\".",
      "generation": "Abstract:\nIn this paper, we present a method for the segmentation of eye fundus images based on a deep neural convolution network. We propose the use of a convex shape prior as a regularization term to improve the performance of the network. The shape prior is defined using a set of convex regions that are predetermined by anatomical knowledge. The regions are overlaid on the input images as a guidance for the network. The network is then trained to classify each pixel in the image as either belonging to one of the predefined regions or as an inter-region boundary. During testing, the network classifies each pixel in a new image as either healthy or diseased, with the shape prior acting as a regularization to improve the accuracy of the classification. The experimental results show that the incorporation of the shape prior significantly improves the performance of the network in terms of precision and recall.\n\nOCIS codes: NAIF-3225, EA74-M3C9, EA82-1000, EA83-1200, EA85-1600, EA-87-2200"
    },
    {
      "id": "11476b56-a5e2-4ee9-bd05-97a96768ab9d",
      "adv_source_id": "11476b56-a5e2-4ee9-bd05-97a96768ab9d",
      "source_id": "996a50cb-2911-4900-ad48-7ec8df8c4eaa",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Meta-Learning Initializations for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Meta-Learning Initializations for Image Segmentation\".",
      "generation": "Abstract:\nThe paper discusses the use of meta-learning initializations for image segmentation. Meta-learning is a technique where a learner begins with a pre-trained model, rather than starting from scratch. This pre-trained model is used as a starting point, and the learner then fine-tunes the model for the specific task at hand. The paper examines the impact of using different types of pre-trained models on the performance of image segmentation tasks. The results show that using a pre-trained model as a starting point improves the performance of the segmentation task. The paper also explores the use of transfer learning for image segmentation, where a pre-trained model trained on a different but related task is used as a starting point. The results show that transfer learning can also improve the performance of the segmentation task. \n\nOverall, the paper highlights the potential of meta-learning and transfer learning for image segmentation, and suggests that using pre-trained models as initializations can improve the performance of the task."
    },
    {
      "id": "94bb6a97-d3c3-4861-9e56-a353c043ced5",
      "adv_source_id": "94bb6a97-d3c3-4861-9e56-a353c043ced5",
      "source_id": "996a50cb-2911-4900-ad48-7ec8df8c4eaa",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Meta-Learning Initializations for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Meta-Learning Initializations for Image Segmentation\".",
      "generation": "Abstract:\nThe paper examines the use of meta-learning initializations for image segmentation, a technique that uses machine learning to classify each pixel in an image into one of several classes. The paper discusses the challenges of training a deep neural network for image segmentation, including the large amount of labeled data required and the complexity of the network. Meta-learning initializations use a pre-trained network to initialize the weights of a new network, which can then be fine-tuned on a smaller set of labeled data. The paper explores the use of several different meta-learning initializations and evaluates their performance on the CamVid and Cityscapes segmentation datasets. The results show that meta-learning initializations can significantly improve the performance of image segmentation models, reducing the amount of labeled data required and the time required to train the model.\n\nOverall, the paper makes a significant contribution to the field of image segmentation by introducing the use of meta-learning initializations, which have the potential to make the training process more efficient and reduce the resources required."
    },
    {
      "id": "7e8e4f41-e7d4-4d62-b19e-4e8885462304",
      "adv_source_id": "7e8e4f41-e7d4-4d62-b19e-4e8885462304",
      "source_id": "20d41ea9-f3cc-4c0c-b3bf-4d421e3c1b38",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Importance Driven Continual Learning for Segmentation Across Domains",
      "prompt": "Write the abstract for the academic paper titled \"Importance Driven Continual Learning for Segmentation Across Domains\".",
      "generation": "The paper presents a novel approach to continual learning for image segmentation across domains. The proposed method, called Importance Driven Continual Learning for Segmentation (ID-CLS), seeks to address the challenge of domain shift in continual learning. The key idea is to learn a common feature representation across domains, while maintaining the ability to adapt to new domains through a domain-specific segmentation component. The proposed method is evaluated on the challenging task of semantic segmentation across five different domains, including indoor, outdoor, daytime, and nighttime scenes, and shows significant improvement over state-of-the-art approaches."
    },
    {
      "id": "285bf190-6e78-4e38-8ae6-ddb573932da4",
      "adv_source_id": "285bf190-6e78-4e38-8ae6-ddb573932da4",
      "source_id": "20d41ea9-f3cc-4c0c-b3bf-4d421e3c1b38",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Importance Driven Continual Learning for Segmentation Across Domains",
      "prompt": "Write the abstract for the academic paper titled \"Importance Driven Continual Learning for Segmentation Across Domains\".",
      "generation": "We present a method for segmentation of medical images using a collection of domain-specific models pre-trained on non-medical data. Our method is designed to address the challenges of domain adaptation, which arise when applying a model trained on data from one domain to data from a different domain. We propose a novel framework that combines domain adaptation with continual learning, allowing the model to learn from a stream of new, domain-specific data while retaining knowledge from previous domains. Our experimental results show that our method can improve the performance of domain adaptation for medical image segmentation."
    },
    {
      "id": "4eac7f7f-254e-4721-8216-c36f1b4dbb19",
      "adv_source_id": "4eac7f7f-254e-4721-8216-c36f1b4dbb19",
      "source_id": "f71ade37-4be2-40be-ab66-ede2b049ba02",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means",
      "prompt": "Write the abstract for the academic paper titled \"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means\".",
      "generation": "This paper proposes a color image segmentation method using adaptive particle swarm optimization and fuzzy C-means (FCM). The proposed method combines the advantages of both algorithms to achieve accurate and robust segmentation. The adaptive particle swarm optimization is used to optimize the initial parameters of the FCM algorithm, such as the initial centroid positions and the initial variance. The optimized parameters provide a better initial guess for the FCM algorithm, which improves its performance. The proposed method is evaluated on a dataset of color images, and the results show that it achieves a higher segmentation accuracy compared to the original FCM algorithm. The proposed method is also compared to other state-of-the-art segmentation algorithms, and the results show that it performs competitively in terms of accuracy and efficiency."
    },
    {
      "id": "10533a25-a126-4d8c-9149-cf6625c03f4b",
      "adv_source_id": "10533a25-a126-4d8c-9149-cf6625c03f4b",
      "source_id": "f71ade37-4be2-40be-ab66-ede2b049ba02",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means",
      "prompt": "Write the abstract for the academic paper titled \"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means\".",
      "generation": "Here is an abstract for the academic paper titled \"Color Image Segmentation using Adaptive Particle Swarm Optimization and Fuzzy C-means\":\n\nAbstract:\nIn this paper, we present a color image segmentation algorithm that combines the benefits of adaptive particle swarm optimization (APSO) and fuzzy C-means (FCM). The proposed algorithm utilizes the APSO algorithm to optimize the initial parameters of the FCM algorithm, which improves the segmentation results. The proposed algorithm was tested on various color images, and the results showed that it outperformed existing algorithms in terms of segmentation quality and running time. The experimental results demonstrate the effectiveness of the proposed algorithm in segmenting color images, and show the potential for future research in this area."
    },
    {
      "id": "964ffb0b-b8aa-4e49-aee9-670f4bd1be50",
      "adv_source_id": "964ffb0b-b8aa-4e49-aee9-670f4bd1be50",
      "source_id": "a5251cdf-e973-4a19-8c07-3bd0ebdbc0d6",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised few-shot learning for medical image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised few-shot learning for medical image segmentation\".",
      "generation": "The paper presents a semi-supervised few-shot learning approach for medical image segmentation. The proposed method utilizes a sparse-coding based few-shot learner, where a deep neural network is trained using a set of few labeled images and a large set of unlabeled images. The network is trained to learn a shared feature representation for the labeled and unlabeled images, which is then used to segment new images. The proposed method was evaluated on a public dataset of abdominal computed tomography (CT) scans and achieved a mean dice similarity coefficient of 95.7%, outperforming the state-of-the-art fully-supervised segmentation methods."
    },
    {
      "id": "968fdde8-9299-4e95-91fd-d76f4401cc85",
      "adv_source_id": "968fdde8-9299-4e95-91fd-d76f4401cc85",
      "source_id": "a5251cdf-e973-4a19-8c07-3bd0ebdbc0d6",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semi-supervised few-shot learning for medical image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Semi-supervised few-shot learning for medical image segmentation\".",
      "generation": "The paper presents a semi-supervised few-shot learning approach for medical image segmentation. The proposed method utilizes a sparse-coding based few-shot learner and a semi-supervised segmentation framework. The sparse-coding based few-shot learner enables the model to effectively adapt to new classes with only a few labeled examples. The semi-supervised segmentation framework utilizes unlabeled data to improve the model's performance on both the few-shot and baseline classes. The experimental results demonstrate that the proposed method significantly improves the segmentation performance compared to the baseline method. The code and dataset used in this study are available at https://github.com/Razi-Khorasanizadeh/fewshot-medical-image-segmentation."
    },
    {
      "id": "f498660b-cd4b-444d-bb91-bd18d9feaa6f",
      "adv_source_id": "f498660b-cd4b-444d-bb91-bd18d9feaa6f",
      "source_id": "32640571-5d21-4e21-8ec3-365726cc1d77",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation\".",
      "generation": "The paper presents a fast fully octave convolutional neural network (FCN) for document image segmentation. The proposed method combines the advantages of both the fully convolutional network (FCN) and the octave convolutional network (OCN) to improve the efficiency and accuracy of document image segmentation.\n\nThe FCN is based on the VGG16 architecture and is modified to use octave convolutions instead of regular convolutions. The use of octave convolutions allows the network to capture a wider range of features and improve the performance of the segmentation task.\n\nThe proposed method was evaluated on the publicly available ICDAR 2015 document image segmentation benchmark. The experimental results show that the proposed method achieves state-of-the-art performance in terms of both speed and accuracy.\n\nThe proposed method is fast, accurate, and general, and can be used for document image segmentation in a variety of applications."
    },
    {
      "id": "ba6d0dd5-7f54-45b9-a8a4-2596f5607474",
      "adv_source_id": "ba6d0dd5-7f54-45b9-a8a4-2596f5607474",
      "source_id": "32640571-5d21-4e21-8ec3-365726cc1d77",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Fast Fully Octave Convolutional Neural Network for Document Image Segmentation\".",
      "generation": "The paper presents a novel approach to document image segmentation based on a fully octave convolutional neural network (FCN). The proposed method achieves fast performance by utilizing a lightweight network architecture and an efficient training strategy. The network utilizes octave convolutional layers, which reduces the number of parameters and computations required for the segmentation process. The training strategy involves an iterative process of training and fine-tuning the network on a small dataset, which is then applied to a larger dataset to optimize performance.\n\nThe proposed method was evaluated on the publicly available IIIT-5dataset, which contains document images with complex backgrounds and varying resolutions. The experimental results show that the proposed method achieves state-of-the-art performance in terms of speed and accuracy. The method is able to process images at a rate of 0.6 seconds per image, while achieving an accuracy of 96.3%.\n\nThe paper concludes by discussing the potential applications of the proposed method in various document image processing tasks, such as document classification, OCR, and document understanding. The proposed method is a promising approach for fast and accurate document image segmentation, which can be used to improve the performance of these tasks."
    },
    {
      "id": "e1ddf8bb-3442-4359-9e6b-228b95cd2a61",
      "adv_source_id": "e1ddf8bb-3442-4359-9e6b-228b95cd2a61",
      "source_id": "e68fce69-dbdc-43a7-a9b6-a799d85dda13",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Grouping Model for Unified Perceptual Parsing",
      "prompt": "Write the abstract for the academic paper titled \"Deep Grouping Model for Unified Perceptual Parsing\".",
      "generation": "In this paper, we present a deep grouping model for unified perceptual parsing. This model utilizes a novel grouping mechanism to combine information from multiple modalities and achieve state-of-the-art performance in perceptual parsing. The proposed model is trained on a large dataset and evaluated on a standard benchmark, demonstrating significant improvements over the current state of the art. The source code and trained models are available at https://github.com/AI4Finance-Foundation/Deep-Grouping-Model-for-Unified-Perceptual-Parsing."
    },
    {
      "id": "141651b7-4344-4b5b-b2a1-36997e1bc523",
      "adv_source_id": "141651b7-4344-4b5b-b2a1-36997e1bc523",
      "source_id": "e68fce69-dbdc-43a7-a9b6-a799d85dda13",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Grouping Model for Unified Perceptual Parsing",
      "prompt": "Write the abstract for the academic paper titled \"Deep Grouping Model for Unified Perceptual Parsing\".",
      "generation": "The abstract for the academic paper titled \"Deep Grouping Model for Unified Perceptual Parsing\" would be as follows:\n\nThis paper introduces a deep grouping model for unified perceptual parsing, a new approach to object recognition that combines convolutional neural networks and graphical models. We propose a novel architecture that integrates both global and local information in a hierarchical manner, allowing the model to capture the underlying structure of the data and improve performance on a variety of object recognition tasks. Our approach is trained end-to-end and is able to learn from a single training dataset, making it suitable for real-world applications. We evaluate our model on the ImageNet dataset and show that it outperforms state-of-the-art approaches in terms of accuracy and speed. We also demonstrate the effectiveness of our model on a variety of benchmark datasets, including CIFAR-10, CIFAR-100, and SVHN.\n\nWould you like me to generate a full-length academic paper based on this abstract?"
    },
    {
      "id": "cc7a6973-1c7e-457e-8c29-b11619d1a403",
      "adv_source_id": "cc7a6973-1c7e-457e-8c29-b11619d1a403",
      "source_id": "4be64437-d36f-4dd1-a995-92ea99cc97e4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours",
      "prompt": "Write the abstract for the academic paper titled \"Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours\".",
      "generation": "Abstract:\nThis paper proposes a method for the automatic tracking of the tongue surface in real-time ultrasound videos using deep learning. A landmark-based approach is used instead of a contour-based approach, which requires the detection and tracking of the tongue contour in each frame. The proposed method uses a deep learning model to detect and track the landmarks on the tongue surface, which are then used to estimate the motion of the tongue. The deep learning model is trained on a large dataset of ultrasound videos and their corresponding ground truth landmarks. The proposed method was evaluated on a separate test dataset and compared to a state-of-the-art contour-based method. The results show that the proposed method is comparable in accuracy to the state-of-the-art method while being more efficient in terms of computation time.\n\nThe rest of the paper is organized as follows. Section 2 introduces the related work on automatic tongue tracking in ultrasound videos. Section 3 describes the landmark-based approach used in this paper. Section 4 presents the deep learning model used for landmark detection and tracking. Section 5 discusses the training and evaluation of the proposed method. Section 6 presents the results and compares them to the state-of-the-art. Finally, Section 7 concludes the paper and suggests future work."
    },
    {
      "id": "c89a7256-33b5-42ad-9bc4-2073d2d9b681",
      "adv_source_id": "c89a7256-33b5-42ad-9bc4-2073d2d9b681",
      "source_id": "4be64437-d36f-4dd1-a995-92ea99cc97e4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours",
      "prompt": "Write the abstract for the academic paper titled \"Deep Learning for Automatic Tracking of Tongue Surface in Real-time Ultrasound Videos, Landmarks instead of Contours\".",
      "generation": "Abstract:\nPurpose: To investigate whether deep learning could be used for automatic tracking of the tongue surface in real-time ultrasound videos, instead of using the current method of contours.\nMethod: A deep-learning-based algorithm was developed to track the tongue surface in real-time ultrasound videos. The algorithm was trained on a large dataset of ultrasound videos and was able to learn the patterns and structures of the tongue surface. The algorithm was then tested on a separate dataset of ultrasound videos and was compared to the traditional method of tracking contours.\nResults: The deep-learning-based algorithm was able to track the tongue surface in real-time ultrasound videos with high accuracy and speed. The algorithm was able to learn the complex structures and movements of the tongue surface, and was able to track the tongue even when it was moving rapidly or when the image was noisy. The algorithm was also able to handle occlusions and background clutter better than the traditional method of tracking contours.\nConclusions: The deep-learning-based algorithm was able to outperform the traditional method of tracking contours for tracking the tongue surface in real-time ultrasound videos. The algorithm has the potential to improve the accuracy and efficiency of tongue tracking in real-time ultrasound videos, and could be used to develop new applications for speech and language processing."
    },
    {
      "id": "ec14e748-507d-469f-bf2f-7b6657e3eff0",
      "adv_source_id": "ec14e748-507d-469f-bf2f-7b6657e3eff0",
      "source_id": "6f053b54-071e-40b2-855d-37fd4885addf",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Improving Training on Noisy Stuctured Labels",
      "prompt": "Write the abstract for the academic paper titled \"Improving Training on Noisy Stuctured Labels\".",
      "generation": "Abstract:\nTraining effective machine learning models requires large amounts of accurately labeled data. However, obtaining such data can be challenging, particularly in domains where obtaining clean labels is difficult or expensive. Noisy labels, which are labels that are predicted by a model rather than verified by humans, can be used to improve the training process in such situations. However, standard training procedures do not perform well on noisy labels, resulting in a need for improved training methods. In this paper, we present a new training procedure that is specifically designed to handle noisy labels. Our method, which we call \"Noisy Student,\" uses a combination of clean and noisy labels to improve model training. We evaluate our method on a variety of datasets and show that it significantly improves model performance compared to standard training procedures. Our results suggest that Noisy Student is a powerful tool for training machine learning models with limited access to clean labels."
    },
    {
      "id": "1c174fd5-2014-48e8-b965-4a41d9654891",
      "adv_source_id": "1c174fd5-2014-48e8-b965-4a41d9654891",
      "source_id": "6f053b54-071e-40b2-855d-37fd4885addf",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Improving Training on Noisy Stuctured Labels",
      "prompt": "Write the abstract for the academic paper titled \"Improving Training on Noisy Stuctured Labels\".",
      "generation": "Abstract:\nTraining effective machine learning models requires large amounts of accurately labeled data. However, obtaining such data can be challenging, especially in domains where obtaining clean labels is difficult or expensive. Noisy labels, which are labels that are predicted by a model rather than verified by humans, can be used to improve the training process in such cases. However, standard training procedures do not work well with noisy labels, as they tend to degrade performance. This paper proposes a new training procedure that is specifically designed to optimize the use of noisy labels. Our approach, which we call Noisy Structured Label (NSL) training, uses a combination of supervised and self-supervised learning to optimize the use of noisy labels. Specifically, we use a conditional generative model to convert the noisy labels into a clean label space, and then use a standard supervised learning algorithm to train a model on the clean labels. We show that this approach can improve the performance of machine learning models on a variety of tasks, including image classification and natural language processing."
    },
    {
      "id": "6f81540f-a8ab-4fc1-bece-3e3c1818b022",
      "adv_source_id": "6f81540f-a8ab-4fc1-bece-3e3c1818b022",
      "source_id": "9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization",
      "prompt": "Write the abstract for the academic paper titled \"A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization\".",
      "generation": "Abstract:\nA fully automatic variational image segmentation model is presented, based on the normalized cut criterion with adaptive similarity and spatial regularization terms. In contrast to previous work on normalized cut, the proposed method explicitly adapts the weighting of the similarity between pixel labels and the smoothness of the segmentation mask, to the local characteristics of the image. In particular, the influence of the similarity term can be spatially modulated, leading to a powerful and intuitive model. The parameters of the model are learned automatically from the data using an efficient stochastic gradient descent scheme. The experimental results show that the proposed method is able to produce accurate segmentations of images from different datasets, often outperforming the state-of-the-art."
    },
    {
      "id": "35f068d3-ea88-40ed-a04a-902e7c55ce1d",
      "adv_source_id": "35f068d3-ea88-40ed-a04a-902e7c55ce1d",
      "source_id": "9ad3ff6b-b309-4a44-9a1f-ecd14cd04e10",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization",
      "prompt": "Write the abstract for the academic paper titled \"A Variational Image Segmentation Model based on Normalized Cut with Adaptive Similarity and Spatial Regularization\".",
      "generation": "This paper presents a novel variational image segmentation model based on normalized cut with adaptive similarity and spatial regularization. The model combines the benefits of normalized cut, which optimizes a smooth energy function, with adaptive similarity and spatial regularization, which optimize a non-smooth energy function. The resulting model is able to segment images with accuracy and efficiency, and is competitive with state-of-the-art segmentation models. The paper includes a comprehensive experimental evaluation of the model on a range of image segmentation benchmarks, and demonstrates the effectiveness of the proposed approach."
    },
    {
      "id": "a01fd39a-fc22-4d25-869a-11d16c37a2e3",
      "adv_source_id": "a01fd39a-fc22-4d25-869a-11d16c37a2e3",
      "source_id": "9f504e80-00f5-491e-a43d-003687f64ea4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Neural arbitrary style transfer for portrait images using the attention mechanism",
      "prompt": "Write the abstract for the academic paper titled \"Neural arbitrary style transfer for portrait images using the attention mechanism\".",
      "generation": "Arbitrary style transfer has attracted increasing attention from the computer vision community in recent years. In this paper, we present a neural network with the attention mechanism for arbitrary style transfer on portrait images. We propose a novel architecture that combines the advantages of the style-based and feature-based methods to achieve fast and effective style transfer. Our network performs style transfer in parallel on a feature map extracted from the input image, achieving a speed of about 5 seconds per image with acceptable quality. Furthermore, we introduce the attention mechanism to our architecture to improve the quality of the output image by dynamically adjusting the strength of the style transfer based on local features. We evaluate our method on the recently introduced FFHQ dataset, and our results demonstrate that our method achieves the state-of-the-art performance in terms of both speed and quality."
    },
    {
      "id": "ba120082-4e20-4073-9b96-cdee9a7d2149",
      "adv_source_id": "ba120082-4e20-4073-9b96-cdee9a7d2149",
      "source_id": "9f504e80-00f5-491e-a43d-003687f64ea4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Neural arbitrary style transfer for portrait images using the attention mechanism",
      "prompt": "Write the abstract for the academic paper titled \"Neural arbitrary style transfer for portrait images using the attention mechanism\".",
      "generation": "Arbitrary style transfer has attracted increasing attention from the computer vision and deep learning communities, where the goal is to transform an input image into the style of an arbitrary reference image. In this paper, we focus on portrait images and propose a neural network with the attention mechanism to achieve arbitrary style transfer for portraits. We argue that the attention mechanism is crucial for adapting the style of the reference image to the content of the input image, and our proposed method demonstrates notable superiority over the state-of-the-art style transfer methods. We showcase the applications of our method in various portrait editing tasks."
    },
    {
      "id": "5d367790-d435-4b91-8080-f2aa6812476f",
      "adv_source_id": "5d367790-d435-4b91-8080-f2aa6812476f",
      "source_id": "34071d29-fbbf-4230-9ee4-55f1c54b2b5e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-scale self-guided attention for medical image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Multi-scale self-guided attention for medical image segmentation\".",
      "generation": "Here is an abstract for the academic paper titled \"Multi-scale self-guided attention for medical image segmentation\":\n\nAbstract: We present a multi-scale self-guided attention model for the segmentation of medical images. Our model is based on the self-attention mechanism, which allows the model to focus on relevant features and context within the image. We propose to apply the self-attention mechanism at multiple scales, enabling the model to capture both global and local information. We further introduce a novel multi-scale training scheme, which adaptively adjusts the attention weights based on the image scale. Our model is evaluated on the ISBI 2017 Challenge dataset for neuron segmentation, where it achieves state-of-the-art performance.\n\nWould you like me to generate the introduction section of the academic paper?"
    },
    {
      "id": "e7452e50-057f-4a35-a1b6-4cdce0217d31",
      "adv_source_id": "e7452e50-057f-4a35-a1b6-4cdce0217d31",
      "source_id": "34071d29-fbbf-4230-9ee4-55f1c54b2b5e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-scale self-guided attention for medical image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Multi-scale self-guided attention for medical image segmentation\".",
      "generation": "Abstract:\nTo meet the requirements of medical image segmentation, we propose a novel approach that incorporates multi-scale self-guided attention. This method enables the network to effectively capture both global and local information within the image. The self-guided attention is learned by minimizing the relative position bias between the attended and ignored regions. Our approach demonstrates state-of-the-art performance on three public datasets, enhancing the segmentation of medical images while preserving the spatial information."
    },
    {
      "id": "ce4401e1-bfb4-4909-b4b9-5fe626aaa3be",
      "adv_source_id": "ce4401e1-bfb4-4909-b4b9-5fe626aaa3be",
      "source_id": "01b52818-c3d8-49e2-b33c-100d6262b860",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation\".",
      "generation": "The paper proposes a novel approach for image segmentation based on deep convolutional neural networks (CNNs) with spatial regularization, volume, and star-shape priors. The method utilizes a U-Net architecture with additional skip connections for improved feature extraction and spatial regularization. The volume prior is incorporated using a 3D convolutional layer, while the star-shape prior is implemented using a novel star-shape convolutional layer. The proposed method demonstrates improved performance in terms of segmentation accuracy and efficiency compared to existing state-of-the-art approaches. The paper discusses the experimental results and provides a comprehensive comparison with other methods."
    },
    {
      "id": "17a6228f-4319-484b-a6ce-9b49f8b76703",
      "adv_source_id": "17a6228f-4319-484b-a6ce-9b49f8b76703",
      "source_id": "01b52818-c3d8-49e2-b33c-100d6262b860",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Deep Convolutional Neural Networks with Spatial Regularization, Volume and Star-shape Priori for Image Segmentation\".",
      "generation": "The paper proposes a novel approach for image segmentation based on deep convolutional neural networks (DCNNs) with spatial regularization, volume, and star-shape priors. The method combines the benefits of both supervised and unsupervised learning to achieve state-of-the-art performance. The star-shape prior is used to impose a prior on the predicted segmentation map, while the volume prior is used to regularize the intensity of the predicted segmentation map. The spatial regularization term is used to impose a prior on the spatial arrangement of the predicted segmentation map. The method was evaluated on the Cityscapes and CamVid datasets, and achieved state-of-the-art performance."
    },
    {
      "id": "887a4ae3-412c-4b3e-9082-c9f3a8397728",
      "adv_source_id": "887a4ae3-412c-4b3e-9082-c9f3a8397728",
      "source_id": "26407846-eb66-4f03-a951-ac6ab785adf2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual Convolutional LSTM Network for Referring Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Dual Convolutional LSTM Network for Referring Image Segmentation\".",
      "generation": "The paper presents a novel approach for referring image segmentation, which combines the strengths of convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. The proposed method utilizes a dual convolutional LSTM network, where the CNN and LSTM layers are alternatively stacked and connected. The CNN layers capture the spatial information and local dependencies in the image data, while the LSTM layers capture the temporal dependencies and context information. The dual convolutional LSTM network was evaluated on the ReferItSeg dataset and achieved state-of-the-art performance, with an F1-score of 95.6% on the test set. The code and models are available at https://github.com/AlexiaJ/Dual-ConvLSTM-ReferItSeg-PyTorch."
    },
    {
      "id": "45109667-504e-4bd6-ab6e-1bbf77219d82",
      "adv_source_id": "45109667-504e-4bd6-ab6e-1bbf77219d82",
      "source_id": "26407846-eb66-4f03-a951-ac6ab785adf2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dual Convolutional LSTM Network for Referring Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Dual Convolutional LSTM Network for Referring Image Segmentation\".",
      "generation": "The paper presents a novel approach for referring image segmentation, which combines the strengths of convolutional neural networks (CNNs) and long short-term memory (LSTM) networks. The proposed method utilizes a dual convolutional LSTM network, in which the CNN and LSTM layers are alternatively stacked. The CNN layers capture the low-level visual features, while the LSTM layers capture the high-level contextual information. The experimental results show that the proposed method significantly improves the performance of referring image segmentation, outperforming existing state-of-the-art methods."
    },
    {
      "id": "a753101b-5d77-4aba-9e73-c2a5ad24b05b",
      "adv_source_id": "a753101b-5d77-4aba-9e73-c2a5ad24b05b",
      "source_id": "dc3ec32e-2230-4be2-9c40-8268932279f5",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning",
      "prompt": "Write the abstract for the academic paper titled \"Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning\".",
      "generation": "Entropic regularization optimal transport (EROT) is a recently developed approach for volume-preserving image segmentation. In this paper, we apply EROT to segment medical images and demonstrate its effectiveness compared to state-of-the-art methods. We show that EROT can improve the segmentation accuracy of deep learning models by incorporating prior knowledge of the volume preservation property. In addition, we present a novel approach to segmenting multi-channel images using EROT. Our method segments each channel independently while enforcing volume preservation across channels. We demonstrate the efficacy of our approach on the task of segmenting multi-channel medical images. Finally, we explore the use of EROT for semi-supervised segmentation, where only a small fraction of the images are labeled. Our results show that EROT can improve the segmentation accuracy in semi-supervised learning by leveraging the unlabeled data to learn a better volume-preserving segmentation model."
    },
    {
      "id": "50fd7566-88e3-48f3-98e0-9ce6bc28dc97",
      "adv_source_id": "50fd7566-88e3-48f3-98e0-9ce6bc28dc97",
      "source_id": "dc3ec32e-2230-4be2-9c40-8268932279f5",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning",
      "prompt": "Write the abstract for the academic paper titled \"Volume Preserving Image Segmentation with Entropic Regularization Optimal Transport and Its Applications in Deep Learning\".",
      "generation": "Entropic regularization optimal transport (EROT) is a recently developed approach for volume-preserving image segmentation. In this paper, we apply EROT to segment images in a way that maintains the volume of the segments, and we demonstrate the effectiveness of the method on a variety of deep learning tasks.\n\nWe show that by incorporating entropic regularization into the optimal transport framework, we can achieve volume-preserving segmentation that is robust to noise and outliers. The entropic regularization term encourages the solution to be close to the original image, which helps to preserve the volume of the segments.\n\nWe demonstrate the effectiveness of the EROT method on three deep-learning tasks: image segmentation, object detection, and image generation. On image segmentation, we show that EROT can segment images in a way that maintains the volume of the segments, even in the presence of noise and outliers. On object detection, we show that EROT can improve the performance of the popular Faster R-CNN algorithm by providing better regional features. On image generation, we show that EROT can be used to generate images that are more coherent and have better texture than images generated using traditional methods.\n\nOur results show that EROT is a powerful tool for volume-preserving image segmentation and can be effectively applied to a variety of deep-learning tasks. We believe that the method will find wide applications in computer vision and beyond."
    },
    {
      "id": "0adb942d-946c-4d9f-86de-eb8f596032e8",
      "adv_source_id": "0adb942d-946c-4d9f-86de-eb8f596032e8",
      "source_id": "32d76482-0724-4631-9784-6be3ab41cd44",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification",
      "prompt": "Write the abstract for the academic paper titled \"Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification\".",
      "generation": "Abstract:\nWe present a self-learning AI framework for skin lesion image segmentation and classification. The framework utilizes a convolutional neural network (CNN) with a residual learning structure and a long short-term memory (LSTM) module to extract features from skin lesion images. The CNN is trained on a large dataset of skin lesion images, while the LSTM is pre-trained on a dataset of medical literature. The extracted features are then used to train a support vector machine (SVM) for segmentation and classification. The framework is designed to be self-learning, allowing for continuous improvement and adaptation to new data. We evaluated the framework on a dataset of 6,000 skin lesion images and achieved an accuracy of 95% for segmentation and 94% for classification. Our results show that the self-learning AI framework is effective for skin lesion image segmentation and classification, and can continuously improve its performance with new data."
    },
    {
      "id": "f8feb784-34e9-47fb-ba92-224f6fb1904b",
      "adv_source_id": "f8feb784-34e9-47fb-ba92-224f6fb1904b",
      "source_id": "32d76482-0724-4631-9784-6be3ab41cd44",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification",
      "prompt": "Write the abstract for the academic paper titled \"Self-Learning AI Framework for Skin Lesion Image Segmentation and Classification\".",
      "generation": "Skin lesion segmentation and classification are essential for accurate diagnosis and treatment of dermatological conditions. We propose a self-learning AI framework that utilizes a convolutional neural network (CNN) for skin lesion image segmentation and a recursive neural tensor network (RNTN) for classification. The framework utilizes a database of skin lesion images and their corresponding ground truth segmentations and classifications to train the CNN and RNTN. The CNN learns to segment skin lesion images into foreground and background regions, while the RNTN learns to classify skin lesions based on their characteristics. The framework is designed to be self-learning, requiring minimal human intervention, and can be used for both research and clinical purposes. We evaluate the performance of the framework on a public skin lesion image dataset and compare it to state-of-the-art methods. Our results show that the proposed framework achieves competitive performance in terms of segmentation and classification accuracy."
    },
    {
      "id": "8dbeaa46-66d2-4739-86e7-a378471d5e94",
      "adv_source_id": "8dbeaa46-66d2-4739-86e7-a378471d5e94",
      "source_id": "ef6fa6f5-e136-4ba4-b009-053b5dc61476",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\".",
      "generation": "Here is an abstract for the academic paper \"Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\":\n\n\"Deepfakes\" refer to the phenomenon of malicious actors creating forged or fabricated contents that are very difficult to distinguish from genuine images or videos. In this paper, we present a robust and effective method for detecting deepfakes based on a separable convolutional neural network (CNN) and image segmentation. Our approach utilizes the power of CNNs to extract relevant features from the forged content, and the segmentation process helps in localizing the areas of interest for further analysis. We evaluate our method on a large dataset of real and forged images and achieve an accuracy of 98%, significantly outperforming the state-of-the-art methods. Our approach is not only effective but also computationally efficient, making it suitable for real-time applications. We hope that our work will contribute to the ongoing efforts in combating the spread of deepfakes and promoting authenticity in online content.\""
    },
    {
      "id": "b680fd92-ef85-4071-9efc-4be26bf78a46",
      "adv_source_id": "b680fd92-ef85-4071-9efc-4be26bf78a46",
      "source_id": "ef6fa6f5-e136-4ba4-b009-053b5dc61476",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\".",
      "generation": "Here is the abstract for the academic paper titled \"Detecting Deepfake-Forged Contents with Separable Convolutional Neural Network and Image Segmentation\":\n\n\"Deepfakes\" refer to the phenomenon of malicious actors creating and disseminating fake contents, particularly images and videos containing forged or altered information. Detecting deepfakes has become an increasingly important task, given its potential to manipulate public opinion and cause harm. In this paper, we propose a novel approach to detect deepfake-forged contents using a separable convolutional neural network (SCNN) and image segmentation. Our method first segments the input image into three regions (foreground, background, and texture) using a fully convolutional network. The segmented regions are then fed into three separate streams of the SCNN, which extract features from each region independently. The features from each stream are then combined to produce a final prediction. Our method achieves state-of-the-art performance on three standard deepfake detection benchmarks, demonstrating the effectiveness of our approach.\""
    },
    {
      "id": "f38355e8-c3c6-48a0-9409-c0e0f8ab0397",
      "adv_source_id": "f38355e8-c3c6-48a0-9409-c0e0f8ab0397",
      "source_id": "38d4f731-4259-4770-9669-255b61bf61b2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computing Valid p-values for Image Segmentation by Selective Inference",
      "prompt": "Write the abstract for the academic paper titled \"Computing Valid p-values for Image Segmentation by Selective Inference\".",
      "generation": "Below is an abstract for the academic paper titled, \"Computing Valid p-values for Image Segmentation by Selective Inference\":\n\nAbstract\nWe propose a new method for computing valid p-values for image segmentation. Our method is based on the idea of selective inference, which is a technique for making inferences about a subset of the data while controlling the false discovery rate. We first divide the image into a set of regions, and then we use a permutation test to evaluate the statistical significance of each region. Our method provides a valid p-value for each region, which can be used to determine whether the region is statistically significant or not. We demonstrate the effectiveness of our method on a variety of image segmentation tasks, and we show that it can be used to improve the accuracy and efficiency of image segmentation."
    },
    {
      "id": "a0273cc8-4ee4-484b-a324-0ad0dae2a6b6",
      "adv_source_id": "a0273cc8-4ee4-484b-a324-0ad0dae2a6b6",
      "source_id": "38d4f731-4259-4770-9669-255b61bf61b2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computing Valid p-values for Image Segmentation by Selective Inference",
      "prompt": "Write the abstract for the academic paper titled \"Computing Valid p-values for Image Segmentation by Selective Inference\".",
      "generation": "Below is an abstract for the academic paper titled, \"Computing Valid p-values for Image Segmentation by Selective Inference\":\n\nBackground: Parameter estimation for stochastic image models, as typically measured by the Mahalanobis distance, often encounters the assumption of normally distributed sampling errors, which is known to be invalid in the presence of spatial dependence. \n\nResults: We derive confidence intervals for the Mahalanobis distance that are valid under both normality and long-range dependence. The validity of our approach is assessed through numerical studies and an application to pixel-level image segmentation. We compare our method to the standard bootstrap and propose a selective inference strategy that chooses between the two methods. \n\nConclusions: The proposed method induces bias in the estimate of the Mahalanobis distance, but produces more accurate p-values, as it accounts for the typically violated assumption of normality. The method is implemented in the widely used Brain Extraction Algorithm and in the software library SLICER."
    },
    {
      "id": "d74771f3-ba40-48de-8d4d-d7a747296460",
      "adv_source_id": "d74771f3-ba40-48de-8d4d-d7a747296460",
      "source_id": "36f82111-ad27-4407-9c52-b884f3302815",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages",
      "prompt": "Write the abstract for the academic paper titled \"Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages\".",
      "generation": "Image collage generation is a creative visual design process, driven by user-provided or auto-generated layout and style specifications. In \u201cTransform a Set\u201d, we follow a data-driven approach to jointly reason about the generation of image collages and their layout. We tackle the problem as an image retrieval and transformation task, where image collages are generated from a given set of images by applying specified transformations. Our approach performs attentive image pasting that respects the exogenous layout specifications, and integrates a memory mechanism to retrieve and transform relevant image regions, guided by a textual caption when available. We demonstrate the efficacy of our approach by automatically generating image collages of unprecedented, varied, and creative quality as compared to the state of the art."
    },
    {
      "id": "83fc091e-7b6c-41f5-9399-4e0f63284fa6",
      "adv_source_id": "83fc091e-7b6c-41f5-9399-4e0f63284fa6",
      "source_id": "36f82111-ad27-4407-9c52-b884f3302815",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages",
      "prompt": "Write the abstract for the academic paper titled \"Transform the Set: Memory Attentive Generation of Guided and Unguided Image Collages\".",
      "generation": "Image collage generation provides an avenue for creating new images by combining parts of existing images. A generative adversarial network (GAN) has been previously used to transform a given input image into a collage, given a set of example collages. However, existing methods tend to generate collages that are deficient in diversity, and the transformations applied are arbitrary, failing to leverage the innate structure of collages. In this paper, we propose a novel framework that transforms a given set of example collages into a new collage while preserving the memorability of the individual components. We propose a Memory Attention network that generates guided and unguided image collages by transforming a given set of example collages while preserving the memorability of individual components. Our framework enables the generation of diverse collages by maintaining the memory of the original components. We evaluate our method on the Image Collage Sets dataset and the results demonstrate the effectiveness of our approach in generating diverse collages."
    },
    {
      "id": "12f3b2ef-b6c6-495f-a888-73a7f63541be",
      "adv_source_id": "12f3b2ef-b6c6-495f-a888-73a7f63541be",
      "source_id": "6524540e-d24e-47e1-8956-86b603c57120",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Identify the cells' nuclei based on the deep learning neural network",
      "prompt": "Write the abstract for the academic paper titled \"Identify the cells' nuclei based on the deep learning neural network\".",
      "generation": "Abstract:\nTraditional cell nuclei segmentation methods are based on heuristic rules or model-based methods, which require a large amount of training data and manual parameter adjustment. With the development of deep learning, cell nuclei segmentation has been applied to convolutional neural network (CNN) and fully convolutional network (FCN) for cell image analysis. However, the existing methods need a large number of cell image datasets with ground truth for training. In this paper, we propose a new approach for cell nuclei segmentation based on the deep learning neural network. Our method utilizes transfer learning to overcome the problem of lacking cell image datasets with ground truth. We extract multi-scale features from the pre-trained VGG16 and ResNet50 models, and then use a fully connected CRF (Conditional Random Field) layer for segmentation. Our approach achieves state-of-the-art performance on the cell nuclei segmentation task, and can be effectively used in the analysis of cell images."
    },
    {
      "id": "6fd4d67c-24cc-454a-9a45-0edf55d21e69",
      "adv_source_id": "6fd4d67c-24cc-454a-9a45-0edf55d21e69",
      "source_id": "6524540e-d24e-47e1-8956-86b603c57120",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Identify the cells' nuclei based on the deep learning neural network",
      "prompt": "Write the abstract for the academic paper titled \"Identify the cells' nuclei based on the deep learning neural network\".",
      "generation": "Abstract:\nTraditional cell nuclei segmentation methods rely on hand-crafted features and prior knowledge, which may not perform well when the image quality is poor or there is a lack of training data. We address this problem by proposing a deep learning-based approach to nuclei segmentation. Our method uses a convolutional neural network (CNN) to learn a features representation directly from the image data. The CNN is trained on a large dataset of cell images with ground truth nuclei segmentation. Our method achieves state-of-the-art performance on the NIH Cell Identification Challenge dataset, outperforming traditional approaches by a large margin. We also analyze the performance of different network architectures and training strategies. Our results show that deep learning can be effectively applied to cellular image analysis problems."
    },
    {
      "id": "3535cdd9-8a0b-4ae4-8024-b477efb78dda",
      "adv_source_id": "3535cdd9-8a0b-4ae4-8024-b477efb78dda",
      "source_id": "be0133e4-a4b2-4611-a616-e941d7fa8fe2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning",
      "prompt": "Write the abstract for the academic paper titled \"Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning\".",
      "generation": "The abstract for the academic paper titled \"Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning\" would be:\n\nThis paper proposes a method for the segmentation of medical images for radiotherapy treatment planning using clinically available delineations. The proposed method uses a convolutional neural network (CNN) to extract features from the medical images, and a deep learning model to map these features to clinically available delineations. The model is trained on a large dataset of medical images and their corresponding delineations, and can be used to segment medical images for radiotherapy treatment planning. The proposed method is compared to existing methods for medical image segmentation, and is shown to be more accurate and efficient. The results show that the proposed method can effectively utilize the rich information encapsulated in clinically available delineations for CNN-based segmentation, resulting in improved performance compared to traditional approaches. Furthermore, a comprehensive analysis of the various components of our framework reveals valuable insights into the contribution of each component to the overall performance, aiding in the identification of potential areas for future improvements."
    },
    {
      "id": "f7760e80-c029-45af-b5a9-ff6cfe9eea52",
      "adv_source_id": "f7760e80-c029-45af-b5a9-ff6cfe9eea52",
      "source_id": "be0133e4-a4b2-4611-a616-e941d7fa8fe2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning",
      "prompt": "Write the abstract for the academic paper titled \"Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning\".",
      "generation": "The abstract for the academic paper titled \"Exploiting Clinically Available Delineations for CNN-based Segmentation in Radiotherapy Treatment Planning\" would be:\n\nThis paper proposes a method for the segmentation of medical images for radiotherapy treatment planning using clinically available delineations. The proposed method uses a convolutional neural network (CNN) to classify tissues in medical images. The CNN is trained using a small amount of manually annotated images and a large number of clinically available delineations. The clinically available delineations are used as additional training data to improve the performance of the CNN. The proposed method was evaluated on a dataset of manually annotated images and showed an improvement in performance compared to a CNN trained only on manually annotated images. The results show that the proposed method is a promising approach for the segmentation of medical images for radiotherapy treatment planning.\n\nWould you like me to generate the introduction from this paper?"
    },
    {
      "id": "f1a778cc-4940-40c9-8cff-916c89ba2b29",
      "adv_source_id": "f1a778cc-4940-40c9-8cff-916c89ba2b29",
      "source_id": "25bf5762-79a2-4034-ae85-e1035e44f71d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice",
      "prompt": "Write the abstract for the academic paper titled \"Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice\".",
      "generation": "Abstract:\nThe Dice score and Jaccard index are widely used evaluation metrics for medical image segmentation. However, they have some limitations, such as being sensitive to noise and outliers, and not taking the overlap between classes into account. In this paper, we propose a new evaluation metric that addresses these limitations. Our metric is based on a combination of the Dice score and Jaccard index, and uses a weighted average of the two to optimize for both class specificity and overlap. We demonstrate the effectiveness of our metric on a variety of medical image segmentation tasks, and show that it outperforms the Dice score and Jaccard index in terms of accuracy and robustness. We also propose a new algorithm for calculating the Dice score and Jaccard index that is more efficient and accurate than the current state-of-the-art."
    },
    {
      "id": "d97ccf78-a81e-4b4e-ad38-790643f68b04",
      "adv_source_id": "d97ccf78-a81e-4b4e-ad38-790643f68b04",
      "source_id": "25bf5762-79a2-4034-ae85-e1035e44f71d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice",
      "prompt": "Write the abstract for the academic paper titled \"Optimizing the Dice Score and Jaccard Index for Medical Image Segmentation: Theory & Practice\".",
      "generation": "Abstract:\nThe Dice score and Jaccard index are popular evaluation metrics for binary and multi-class segmentation tasks, respectively. Despite their widespread use, these metrics have some undesirable properties for medical image segmentation, including sensitivity to class imbalance, and lack of discrimination power for challenging segmentations. In this paper, we investigate the modifications of the Dice score and Jaccard index to improve their performance for medical image segmentation. We propose a new variant of the Jaccard index that adapts to the class imbalance and performs better than the original Jaccard index and other popular evaluation metrics. We also analyze the properties of the Dice score and propose a method to improve its performance. Finally, we discuss the theoretical and practical considerations of optimizing the Dice score and Jaccard index for medical image segmentation."
    },
    {
      "id": "de8bb835-2b97-40ce-b4a1-4a0f7227ba5a",
      "adv_source_id": "de8bb835-2b97-40ce-b4a1-4a0f7227ba5a",
      "source_id": "624d70db-276c-46c4-a8a1-b636b9308d04",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging",
      "prompt": "Write the abstract for the academic paper titled \"U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging\".",
      "generation": "Abstract:\nTime series data are increasingly common in many fields, such as finance, energy, and healthcare. U-Time is a fully convolutional network for time series segmentation, applicable to various domains. In this paper, we demonstrate the effectiveness of U-Time on a specific application: sleep staging using ambulatory electroencephalography (EEG) data. Compared with prior methods, our approach requires less manual effort, is more adaptable to different time series, and achieves higher accuracy. We contribute a new dataset, UTK-EEP, comprising 24-hour, preprocessed EEG recordings from 19 participants, each annotated with 5-second intervals for sleep staging. Our code and dataset are available at https://github.com/UT-CIS/u-time."
    },
    {
      "id": "8d77b8c3-0c2c-48e4-9833-be15e4116e6f",
      "adv_source_id": "8d77b8c3-0c2c-48e4-9833-be15e4116e6f",
      "source_id": "624d70db-276c-46c4-a8a1-b636b9308d04",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging",
      "prompt": "Write the abstract for the academic paper titled \"U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging\".",
      "generation": "Abstract:\nTime series data are increasingly common in many fields, such as finance, sensor data, and healthcare. U-Time is a fully convolutional network for time series segmentation, applicable to many domains. In this paper, we demonstrate the effectiveness of U-Time on a particularly challenging time series segmentation task: sleep staging from polysomnographic recordings. Polysomnography (PSG) records brain activity, heart rate, and breathing, and other physiological signals, providing a rich dataset for sleep staging. However, state-of-the-art deep learning models for sleep staging either require lengthy manual labeling or brittle heuristic feature engineering, limiting their applicability in practice. In contrast, U-Time learns representations from raw signals and produces a segmentation without any engineered features, with performance close to models utilizing hand-engineered features on a held-out validation set."
    },
    {
      "id": "e36452cd-daed-4cc5-9be5-c653a1583d4f",
      "adv_source_id": "e36452cd-daed-4cc5-9be5-c653a1583d4f",
      "source_id": "3a90d69b-7e0f-4553-86f6-ac29ffea9b61",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Multiphase Level Set for Scene Parsing",
      "prompt": "Write the abstract for the academic paper titled \"Deep Multiphase Level Set for Scene Parsing\".",
      "generation": "Deep Multiphase Level Set for Scene Parsing Abstract. The goal of scene parsing is to simultaneously classify each pixel in an input image into one of several pre-defined classes (or categories) of interest and subsequently reconstruct the image regions corresponding to each class. In this paper, we propose a novel approach to scene parsing based on the multiphase level set method, a powerful technique for modeling interacting substances in computer graphics. Our method combines a deep neural network with the multiphase level set framework to effectively parse images while capturing complex object interactions. We introduce a new architecture, the Multiphase Convolutional Neural Network (MCNN), which learns to predict the evolution of the level set interface and the labels of each pixel. The proposed method is able to handle scene parsing tasks with object interactions, such as overlapping objects and object occlusions. We evaluate our approach on the challenging Cityscapes dataset and show that it achieves state-of-the-art performance in terms of both accuracy and speed."
    },
    {
      "id": "22dbe3bc-50e5-4e53-a131-4d8f139a1947",
      "adv_source_id": "22dbe3bc-50e5-4e53-a131-4d8f139a1947",
      "source_id": "3a90d69b-7e0f-4553-86f6-ac29ffea9b61",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Multiphase Level Set for Scene Parsing",
      "prompt": "Write the abstract for the academic paper titled \"Deep Multiphase Level Set for Scene Parsing\".",
      "generation": "Deep Multiphase Level Set for Scene Parsing Abstract. The goal of scene parsing is to assign semantic labels to each pixel in an input image, a process that is challenging due to the presence of similar textures and objects in the world. In this paper, we propose a novel approach to scene parsing that combines a deep convolutional neural network with a multiphase level set method. Our network learns to predict a label map for the input image, with each pixel labeled with a semantic class. We then use the label map to train a multiphase level set method, which segments the image based on the predicted labels. Our approach is able to handle highly textured images and scenes with complex object arrangements, and we demonstrate its effectiveness on the Pascal VOC 2012 dataset."
    },
    {
      "id": "349cf259-2606-4a8e-8aa8-61c2b652e523",
      "adv_source_id": "349cf259-2606-4a8e-8aa8-61c2b652e523",
      "source_id": "4762b0a8-cbbc-4e95-bbbc-f6f7333946d8",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NeurReg: Neural Registration and Its Application to Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"NeurReg: Neural Registration and Its Application to Image Segmentation\".",
      "generation": "The paper presents a novel approach for neural registration, a process that aligns neural networks to a target domain. Our approach utilizes a loss function that measures the similarity between the output of the neural network and the target domain. This loss function is optimized using a gradient-based optimization algorithm to find the parameters that minimize the difference between the output and the target domain.\n\nWe demonstrate the effectiveness of our approach on the task of image segmentation. We show that by registering a pre-trained segmentation network to a target domain, we can improve the performance of the network on the target domain. Our results show that the proposed method can improve the segmentation accuracy by up to 2% on the Cityscapes dataset.\n\nThe paper concludes by discussing the potential applications of neural registration in image segmentation and other domains. We believe that the proposed approach can be applied to any task where aligning a neural network to a target domain is beneficial."
    },
    {
      "id": "0d9f17e8-acfb-40b4-8c59-8bcbe199f9c5",
      "adv_source_id": "0d9f17e8-acfb-40b4-8c59-8bcbe199f9c5",
      "source_id": "4762b0a8-cbbc-4e95-bbbc-f6f7333946d8",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NeurReg: Neural Registration and Its Application to Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"NeurReg: Neural Registration and Its Application to Image Segmentation\".",
      "generation": "An important step in advanced image processing applications, such as image segmentation, is the accurate registration of multi-channel images, especially in the presence of intensity differences and large deformations. In this paper, we propose a neural network approach to the registration problem, which we refer to as NeurReg. Given a pair of images, our method seeks to learn an invertible transformation that maps one image onto the other. A direct application of deep learning to this problem, without any constraints, fails due to the ambiguity in the solutions. To overcome this challenge, we propose to incorporate a physical model of the image formation process into the learning process. This is achieved by enforcing a sparse-perspective deformation representation on the parameters of the neural network. Our experiments demonstrate the efficacy of our approach on a variety of publicly available image data sets."
    },
    {
      "id": "5d802b1a-0844-405b-97c0-fbc59e271154",
      "adv_source_id": "5d802b1a-0844-405b-97c0-fbc59e271154",
      "source_id": "c29c60c6-dfc8-4a33-8ef9-d76b4ce39608",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth",
      "prompt": "Write the abstract for the academic paper titled \"SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth\".",
      "generation": "Abstract:\nTraining segmentation models often requires a significant amount of labeled target modality ground truth (e.g., pixel-level object annotations). However, obtaining such ground truth is expensive, time-consuming, and often requires expert knowledge. In this paper, we present SynSeg-Net, a novel approach for synthetic segmentation that addresses this issue by leveraging synthetic data without the need for target modality ground truth. We propose a self-supervised framework that uses a generative adversarial network (GAN) to produce synthetic images with ground truth segmentation labels. These labels are then used to train a segmentation model. Our approach is evaluated on the Cityscapes and CamVid datasets, and we demonstrate that our method achieves competitive results compared to state-of-the-art self-supervised segmentation approaches while requiring less data and no target modality ground truth."
    },
    {
      "id": "73d8ed4b-7cab-4b35-8e23-7338b493367e",
      "adv_source_id": "73d8ed4b-7cab-4b35-8e23-7338b493367e",
      "source_id": "c29c60c6-dfc8-4a33-8ef9-d76b4ce39608",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth",
      "prompt": "Write the abstract for the academic paper titled \"SynSeg-Net: Synthetic Segmentation Without Target Modality Ground Truth\".",
      "generation": "Abstract:\nTraining segmentation models on real-world datasets is challenging due to the significant amount of labeled target modality ground truth required, especially in the case of fine-grained segmentation. We circumvent this problem by learning a segmentation model with synthetic data without target modality ground truth. We train our model on a synthetic dataset generated by an analyst using a semi-automated tool we developed, which uses a domain-specific language to produce labeled image data with pixel-level granularity. Our method enables the use of an inexpensive, self-hosted synthetic data pipeline, as opposed to acquiring expensive labeled real-world data. We evaluate our approach on three segmentation benchmarks, and our model achieves 77.4% mean accuracy on semantic segmentation of real-world imagery, comparable to state-of-the-art performance. Additionally, our synthetic data pipeline trained models faster and more reliably than models trained on real-world data."
    },
    {
      "id": "21318e14-3b04-4394-88cb-dcc9a323d6b7",
      "adv_source_id": "21318e14-3b04-4394-88cb-dcc9a323d6b7",
      "source_id": "ca486756-3abb-4c00-bdf8-a000bef913a4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation\".",
      "generation": "We present a simple and effective approach for unsupervised image-to-image translation, utilizing an encoder with linear layers masked by rotation and scaling. Our approach is based on the intuition that an image encoder with random linear layers is capable of learning to translate images, if the translation is allowed to depend on the input image. We show that a standard image encoder can be trained with a random initialization of the linear layers to perform translation of images while keeping their appearance, including textures and colors, intact. We demonstrate the effectiveness of our approach on the task of semantic segmentation, where we use unsupervised image-to-image translation to convert images taken under one set of conditions to images taken under a different set of conditions. We show that our approach is able to produce high quality translations, and that our method outperforms the state of the art in unsupervised image translation."
    },
    {
      "id": "eb18891c-4f4c-4563-bbb0-e1fa36ba0267",
      "adv_source_id": "eb18891c-4f4c-4563-bbb0-e1fa36ba0267",
      "source_id": "ca486756-3abb-4c00-bdf8-a000bef913a4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Extremely Weak Supervised Image-to-Image Translation for Semantic Segmentation\".",
      "generation": "We present a novel approach for training semantic segmentation models using only the image-level labels and a weakly-supervised image-to-image translation loss. We observe that for semantic segmentation, the spatial distribution of pixels in the image plays a crucial role in predicting the pixel-level labels. Our approach leverages this insight and learns a mapping such that the translated image preserves the spatial distribution of pixels in the source image. We demonstrate the efficacy of our approach on the CityScapes dataset, where we achieve a pixel-level accuracy of 82.7% on the test set, significantly outperforming the previous state-of-the-art. Our approach is computationally efficient and can be easily extended to other datasets and segmentation tasks."
    },
    {
      "id": "35431867-6c57-4291-a75a-8dbb59a22573",
      "adv_source_id": "35431867-6c57-4291-a75a-8dbb59a22573",
      "source_id": "6fd64468-6ce6-4c70-a32f-70d8f9095853",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mumford-Shah Loss Functional for Image Segmentation with Deep Learning",
      "prompt": "Write the abstract for the academic paper titled \"Mumford-Shah Loss Functional for Image Segmentation with Deep Learning\".",
      "generation": "The paper aims to present a novel loss functional for image segmentation with deep learning. It proposes to use the Mumford-Shah functional as a loss function for training deep neural networks for image segmentation. The Mumford-Shah functional is famous for its ability to segment images with sharp boundaries. The proposed method segments images with deep neural networks, which are trained with the Mumford-Shah loss functional. The experimental results show that the proposed method achieves state-of-the-art performance in image segmentation. The paper concludes by discussing the implications of the proposed method for future research in image segmentation with deep learning."
    },
    {
      "id": "6c169592-848d-4cbb-9a7b-f5e3b32a5b35",
      "adv_source_id": "6c169592-848d-4cbb-9a7b-f5e3b32a5b35",
      "source_id": "6fd64468-6ce6-4c70-a32f-70d8f9095853",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mumford-Shah Loss Functional for Image Segmentation with Deep Learning",
      "prompt": "Write the abstract for the academic paper titled \"Mumford-Shah Loss Functional for Image Segmentation with Deep Learning\".",
      "generation": "The paper aims to present a novel loss functional for image segmentation with deep learning. It proposes to use the Mumford-Shah functional as a loss function to train a deep neural network for image segmentation. The network is designed to output a binary image, where each pixel is either 1 (foreground) or 0 (background). The Mumford-Shah loss functional is a widely used measure for image segmentation, and it captures the similarity between the predicted binary image and the ground truth image, as well as the smoothness of the predicted image. The proposed method is evaluated on the CamVid dataset, and the results show that it outperforms the conventional cross-entropy loss functional in terms of segmentation accuracy."
    },
    {
      "id": "e5df7378-8802-4395-9268-1f1b9781ffb7",
      "adv_source_id": "e5df7378-8802-4395-9268-1f1b9781ffb7",
      "source_id": "9dc9ce24-bb8d-4c22-a2e9-a9f6708a7aad",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Temporality for Semi-Supervised Video Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Exploiting Temporality for Semi-Supervised Video Segmentation\".",
      "generation": "Abstract:\nWe introduce a novel framework for semi-supervised video segmentation that exploits temporal coherence to learn a mask predictor. Our approach leverages the inherent temporal continuity of video data to propagate labels across frames, facilitating the segmentation of sparsely annotated videos. We introduce a temporal embedding network that captures the temporal relationships between frames, and utilize it to enhance the semi-supervised learning process. Furthermore, we propose a temporal loss function that enforces coherent predictions across frames, effectively preventing excessive drifting of the model's predictions. Our approach achieves state-of-the-art performance on the challenging Vimeo-90K dataset, significantly improving upon the previous state-of-the-art both in terms of precision and recall."
    },
    {
      "id": "54940b29-7e78-4f50-8c85-cba36a90e6ed",
      "adv_source_id": "54940b29-7e78-4f50-8c85-cba36a90e6ed",
      "source_id": "9dc9ce24-bb8d-4c22-a2e9-a9f6708a7aad",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exploiting Temporality for Semi-Supervised Video Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Exploiting Temporality for Semi-Supervised Video Segmentation\".",
      "generation": "Abstract:\nWe introduce a new temporal modeling module for semi-supervised video segmentation that leverages the temporal coherence of video frames to propagate labels across frames. This module exploits the temporal dependencies between frames to propagate labels even in the absence of a fully annotated video. We demonstrate the effectiveness of our approach on the Vimeo-90K and Youtube-8M datasets, achieving state-of-the-art performance on both datasets with a single model. Our code and models are available at https://github.com/facebookresearch/Exploiting-Temporality-for-Semi-Supervised-Video-Segmentation."
    },
    {
      "id": "e71b53ba-6f85-4abd-b5c1-ca05a2c9de35",
      "adv_source_id": "e71b53ba-6f85-4abd-b5c1-ca05a2c9de35",
      "source_id": "3728d50f-31c2-42a0-ac3d-e3a4df58cdb3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths",
      "prompt": "Write the abstract for the academic paper titled \"ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths\".",
      "generation": "Augmented Contracting and Expanding (ACE) paths are a variant of the popular Chan-Vese (CV) energy functional for image segmentation. The ACE variant, introduced in this paper, mitigates the issue of the CV energy functional, which tends to oversmooth segmented regions, affecting its performance in the presence of noise and/or weak/fuzzy object boundaries. We show that the ACE variant can be optimized by the well-known Level Set Method (LSM) and derive the conditions for convergence. We implement the proposed method in 2D and 3D and evaluate its performance on several benchmarks, including the well-known INRIA and DTI-MRI datasets, showing that the ACE variant outperforms the standard CV method in terms of precision and recall, especially in the presence of noise."
    },
    {
      "id": "dc358eb9-a842-4241-9ea7-dbefb2b91613",
      "adv_source_id": "dc358eb9-a842-4241-9ea7-dbefb2b91613",
      "source_id": "3728d50f-31c2-42a0-ac3d-e3a4df58cdb3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths",
      "prompt": "Write the abstract for the academic paper titled \"ACE-Net: Biomedical Image Segmentation with Augmented Contracting and Expansive Paths\".",
      "generation": "Abstract:\nTraditional image segmentation techniques rely on hand-designed features and heuristics to classify pixels in images into different segments. These methods are often sensitive to noise, outliers, and local minima. Recently, a new technique called Augmented Contracting and Expanding (ACE) has shown promise in segmenting biomedical images more accurately and efficiently. ACE-Net is a novel deep learning architecture that utilizes the power of ACE to segment images. It extends the ACE technique by using a contracting path to capture local details and an expansive path to capture global context. The contracting path consists of a series of convolutional layers with increasing spatial resolutions, while the expansive path consists of a series of convolutional layers with decreasing spatial resolutions. This allows ACE-Net to capture both local and global features in the image, improving segmentation accuracy. We evaluate ACE-Net on a public dataset of biomedical images and show that it achieves state-of-the-art performance in terms of segmentation accuracy and efficiency."
    },
    {
      "id": "1d63a416-bdc0-43f6-983a-31df3e0e606e",
      "adv_source_id": "1d63a416-bdc0-43f6-983a-31df3e0e606e",
      "source_id": "3045ff11-c626-427f-a9d9-7e9b98c5055e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization",
      "prompt": "Write the abstract for the academic paper titled \"Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization\".",
      "generation": "Abstract\nWe present a novel approach for self-supervised fine-grained segmentation of images for improved long-term visual localization. Our method utilizes a convolutional neural network (CNN) to extract features from images and a segmentation network to assign a label to each pixel. The segmentation network is trained using a self-supervised learning strategy, where the only training data is the set of images themselves. Our approach demonstrates superior performance compared to other state-of-the-art self-supervised segmentation methods, such as Mask R-CNN and DeepLab, in terms of both accuracy and efficiency. We show that our method can improve the performance of long-term visual localization systems, resulting in more accurate and robust localization.\n\nOur approach has two main components: a convolutional neural network (CNN) for feature extraction and a segmentation network for pixel-level labeling. The CNN extracts a set of features from the input image, and the segmentation network uses these features to assign a label to each pixel in the image. The segmentation network is trained using a self-supervised learning strategy, where the only training data is the set of images themselves. This allows us to learn a robust and general segmentation model that can be used for improved long-term visual localization.\n\nWe evaluate our method on the Cityscapes dataset, which contains a diverse set of images of urban scenes, and compare it to other state-of-the-art self-supervised segmentation methods, such as Mask R-CNN and DeepLab. Our approach achieves superior performance in terms of both accuracy and efficiency. We also show that our method can improve the performance of long-term visual localization systems, resulting in more accurate and robust localization.\n\nOur work demonstrates the advantages of using a self-supervised segmentation network for improved long-term visual localization. We believe that our method has the potential to improve the performance of other computer vision tasks that require accurate and robust localization, such as object detection and recognition."
    },
    {
      "id": "414ee6af-1d45-4c76-8fef-cb6eb234af10",
      "adv_source_id": "414ee6af-1d45-4c76-8fef-cb6eb234af10",
      "source_id": "3045ff11-c626-427f-a9d9-7e9b98c5055e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization",
      "prompt": "Write the abstract for the academic paper titled \"Fine-Grained Segmentation Networks: Self-Supervised Segmentation for Improved Long-Term Visual Localization\".",
      "generation": "Abstract\n\nLong-term visual localization is crucial for robots to autonomously navigate through unseen environments. Current techniques rely on accurate segmentation of objects in the scene, followed by matching with stored images in a pre-built map. However, object segmentation networks are prone to misclassification especially in environments with significant occlusion and clutter. In this work, we propose a novel approach to fine-grained segmentation for long-term visual localization. Our approach leverages self-supervised segmentation, which simultaneously learns a segmentation network and a localization network. The segmentation network helps to refine object regions, while the localization network uses the segmented regions to improve mapping and object recognition. We demonstrate the efficacy of our approach on the popular UCI benchmark dataset for long-term visual localization. Our method achieves state-of-the-art performance with an improvement of 9.4% in the average precision for object recognition.\n\n vou.mv/FGSL"
    },
    {
      "id": "c903b1f1-e4e6-428e-b8ab-348112b5da47",
      "adv_source_id": "c903b1f1-e4e6-428e-b8ab-348112b5da47",
      "source_id": "d9e15ea1-7544-4aee-bf98-cfdae94eacdc",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation\".",
      "generation": "Abstract:\nTraditional medical image segmentation methods require extensive amounts of manual effort to annotate training data and tune hyperparameters. In this paper we introduce Conv-MCD, a fully convolutional neural network for multi-class segmentation of medical images. Our network is designed to be modular, with a top layer that can be easily swapped out to adapt to different medical image segmentation tasks. We demonstrate the effectiveness of our method on three different segmentation tasks: hippocampus segmentation in T1-weighted magnetic resonance images (MRIs), whole tumor segmentation in T2-weighted MRIs, and nucleus segmentation in transmission electron microscopy images. Our results show that Conv-MCD achieves state-of-the-art performance on the hippocampus segmentation task, and competitive performance on the other two tasks, while requiring minimal hyperparameter tuning."
    },
    {
      "id": "6fdb8a02-e55c-4e57-910a-c746c3cfbf0f",
      "adv_source_id": "6fdb8a02-e55c-4e57-910a-c746c3cfbf0f",
      "source_id": "d9e15ea1-7544-4aee-bf98-cfdae94eacdc",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Conv-MCD: A Plug-and-Play Multi-task Module for Medical Image Segmentation\".",
      "generation": "Conv-MCD is a plug-and-play multi-task module for medical image segmentation. It utilizes a novel hierarchical architecture to learn a shared representation of the input image and can be easily integrated into existing segmentation frameworks. The module is designed to adapt to different tasks and imaging modalities, and can segment multiple structures in the same image. In this paper, we evaluate the performance of Conv-MCD on four different segmentation tasks: tumor segmentation in MRI, lung segmentation in CT, bone segmentation in CT, and organ segmentation in MRI. We compare its performance to a fully-trained network and show that Conv-MCD achieves similar performance with a significant reduction in training time."
    },
    {
      "id": "13ba8cfb-ab70-441a-ba16-d439651e2538",
      "adv_source_id": "13ba8cfb-ab70-441a-ba16-d439651e2538",
      "source_id": "6c1e5ba0-032d-417c-8ebc-c5f45992f6a8",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks",
      "prompt": "Write the abstract for the academic paper titled \"AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks\".",
      "generation": "AttentionBoost is a novel approach to computer vision that combines the strengths of two powerful techniques: fully convolutional networks (FCNs) and attention mechanisms. By adding a boosting layer to an FCN, we enable the network to learn what to attend to in an image, improving performance on a variety of tasks. Our approach is fully convolutional, meaning it can be applied to arbitrary input sizes and is efficient in terms of computational resources. We demonstrate the effectiveness of our method on the task of object detection, achieving state-of-the-art performance on the COCO dataset. Our approach is also competitive on the VOC dataset, despite using a much smaller model. We believe that AttentionBoost is a promising step towards the development of more effective and efficient computer vision algorithms."
    },
    {
      "id": "c3605667-d282-4692-a177-f16ab4d3162f",
      "adv_source_id": "c3605667-d282-4692-a177-f16ab4d3162f",
      "source_id": "6c1e5ba0-032d-417c-8ebc-c5f45992f6a8",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks",
      "prompt": "Write the abstract for the academic paper titled \"AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks\".",
      "generation": "AttentionBoost is a technique used to improve the performance of fully convolutional networks. It involves training a network to attend to relevant information in an image, and to ignore irrelevant information. The network is trained using a series of attention maps, which are learned from the input image. The attention maps are used to guide the network's attention, and to improve its performance.\n\nThe abstract for the academic paper titled \"AttentionBoost: Learning What to Attend by Boosting Fully Convolutional Networks\" should include:\n\nIntroduction: AttentionBoost is a technique used to improve the performance of fully convolutional networks. \nBackground: The use of attention maps in image processing is not a new idea, but it has not been widely used in fully convolutional networks. \nMethod: The network is trained using a series of attention maps, which are learned from the input image. \nResults: The use of AttentionBoost significantly improved the performance of the fully convolutional network. \nConclusion: AttentionBoost is a powerful technique that can be used to improve the performance of fully convolutional networks. It is a novel approach that uses attention maps to guide the network's attention and improve its performance. \n\nIs there anything specific you would like me to cover?"
    },
    {
      "id": "ca624ea5-c74e-456a-ab1a-c40ca7296667",
      "adv_source_id": "ca624ea5-c74e-456a-ab1a-c40ca7296667",
      "source_id": "00e000e6-0bf4-4e8a-8484-f377088eb55a",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation\".",
      "generation": "In this paper, we introduce a new approach for automatic quality evaluation of noisy-labeled image segmentation datasets. Our approach, which we refer to as Pick-and-Learn, involves using a pre-trained image segmentation model to select the most confident and accurate labels for each pixel in an image, and then learning a quality evaluation model based on the selected labels. We show that our approach achieves state-of-the-art performance on the Cityscapes and KITTI datasets, and that it is effective at selecting confident and accurate labels for training deep learning models. We also demonstrate that our approach is general and can be used to select confident and accurate labels for other types of datasets, such as medical imaging and satellite imagery. Finally, we discuss the limitations of our approach and potential future work."
    },
    {
      "id": "5bc8f209-2d38-412a-86a8-a35eb5e9f439",
      "adv_source_id": "5bc8f209-2d38-412a-86a8-a35eb5e9f439",
      "source_id": "00e000e6-0bf4-4e8a-8484-f377088eb55a",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Pick-and-Learn: Automatic Quality Evaluation for Noisy-Labeled Image Segmentation\".",
      "generation": "Automated quality evaluation (QE) of noisy labels is essential for the success of modern segmentation benchmarks, which nearly always contain noisy labels. However, QE is inherently challenging because it is prone to overfitting on the evaluation set, and it is difficult to train QE models on noisy data without the need for clean labels. In this paper, we exploit a connection between the QE and segmentation tasks by casting QE as a pick-and-learn problem. Specifically, given a segmentation model and its noisy predictions, the goal is to pick the label for each pixel that maximizes the expected downstream performance. We term this picked label as the ideal label. We then train a QE model to predict the ideal labels, which can be treated as clean labels for training the QE model. With extensive experiments on three public benchmarks, we show the effectiveness of our approach in comparison to the state-of-the-art methods for QE."
    },
    {
      "id": "cd2f5324-819d-4f9d-9ef5-f7a973073237",
      "adv_source_id": "cd2f5324-819d-4f9d-9ef5-f7a973073237",
      "source_id": "91c38d06-f963-4f7c-b22a-6942b2fe2917",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation\".",
      "generation": "Edge-attention guidance networks (EAGNs) have demonstrated promising results for medical image segmentation. However, most existing EAGNs are designed for specific tasks or organs and cannot be generalized to handle different segmentation tasks. In this paper, we present ET-Net, a generic edge-attention guidance network that is capable of handling various segmentation tasks with minimal network modifications. ET-Net employs a transformer-based encoder-decoder architecture, with attention modules carefully integrated into both the encoder and decoder. A novel edge-attention module is introduced to effectively utilize multi-scale contextual information around the current prediction. Furthermore, we propose a channel-wise attention module to capture the global dependencies within the feature channels of medical images. The proposed method is evaluated on three different segmentation tasks: abdominal aortic aneurysm segmentation from CT, abdominal organ segmentation from CT, and brain tumor segmentation from MRI. Results show that our approach achieves state-of-the-art performance on all three datasets without any dataset-specific modifications. The code and models are available at https://github.com/AI4Finance-Foundation/ET-Net."
    },
    {
      "id": "442034d3-e7dc-44e5-a668-1e925cd4fe95",
      "adv_source_id": "442034d3-e7dc-44e5-a668-1e925cd4fe95",
      "source_id": "91c38d06-f963-4f7c-b22a-6942b2fe2917",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation\".",
      "generation": "Edge-attention guidance networks (EAGNs) have demonstrated promising performance for medical image segmentation. However, most existing methods treat edge information as a simple vector, limiting their ability to capture complex structural information present in medical images. We address this limitation by proposing a novel edge-attention guidance network (ET-Net) that encodes edge information using a 1D convolutional neural network (CNN). This enables ET-Net to capture complex edge information, resulting in improved segmentation performance. We evaluated ET-Net on three publicly available abdominal computed tomography (CT) image datasets (81 images in total) and compared it to three state-of-the-art EAGN-based methods. Results showed that ET-Net improved the mean dice similarity coefficient (DSC) and 95th percentile DSC compared to the next-best method, especially for images with low contrast and noise. Furthermore, our implementation of ET-Net has an execution time of approximately 1.43 seconds per case on a Titan RTX GPU, making it suitable for real-time applications."
    },
    {
      "id": "537d4787-84b8-4045-b8f8-3a2770e5e3a6",
      "adv_source_id": "537d4787-84b8-4045-b8f8-3a2770e5e3a6",
      "source_id": "68f52eba-10ab-4832-a715-1a83b7304752",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient 3D CNN for Action/Object Segmentation in Video",
      "prompt": "Write the abstract for the academic paper titled \"An Efficient 3D CNN for Action/Object Segmentation in Video\".",
      "generation": "3D convolutional neural networks (3D CNNs) have become a powerful tool for action/object segmentation in video sequences. However, existing 3D CNN architectures are often computationally expensive and memory-intensive, making them difficult to deploy on resource-constrained devices. In this paper, we present a novel 3D CNN architecture that is both efficient and effective for action/object segmentation in video. Our network is based on a lightweight 3D CNN architecture, which is designed to reduce computational and memory requirements. We then introduce a new module, called the Interleaved Convolutional Module, which enhances the performance of the network by allowing it to capture more complex and abstract features. Our network is evaluated on the popular Kinetics-400 and Cityscapes datasets, and achieves state-of-the-art performance. We further demonstrate the efficiency of our network on a resource-constrained device, and show that it can achieve real-time performance."
    },
    {
      "id": "17a8257c-e661-4bf3-b94b-1112d95bc6f5",
      "adv_source_id": "17a8257c-e661-4bf3-b94b-1112d95bc6f5",
      "source_id": "68f52eba-10ab-4832-a715-1a83b7304752",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient 3D CNN for Action/Object Segmentation in Video",
      "prompt": "Write the abstract for the academic paper titled \"An Efficient 3D CNN for Action/Object Segmentation in Video\".",
      "generation": "Abstract:\nWe present a 3D convolutional neural network (3D CNN) for efficient action/object segmentation in video. Our approach leverages 3D convolutions to effectively capture spatial and temporal dependencies in video data. We propose a novel 3D CNN architecture that incorporates spatio-temporal convolutions and residual connections to capture complex patterns and improve network representation learning. Additionally, we introduce a conditional random field (CRF) post-processing step to refine the segmentation results and improve accuracy. Our method achieves state-of-the-art performance on the popular action/object segmentation dataset JHMDB, with a mean accuracy of 86.3% on the test set. We demonstrate the efficiency of our approach by achieving real-time segmentation with a frame rate of over 30 FPS on a single GPU."
    },
    {
      "id": "2960210e-0015-4056-8906-1d577fc5e7a6",
      "adv_source_id": "2960210e-0015-4056-8906-1d577fc5e7a6",
      "source_id": "8a130ebe-e41d-4815-9616-a12868315538",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis",
      "prompt": "Write the abstract for the academic paper titled \"Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis\".",
      "generation": "In this paper, we present a novel approach for stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis. Our method utilizes a stereo camera system to capture the terrain surface and extract the corresponding point cloud. The point cloud is then preprocessed to estimate the surface normals, which are used to perform segmentation based on the gradient magnitude. The resulting segments are analyzed using superpixels, which provide a hierarchical representation of the terrain surface. By evaluating the superpixel properties, such as size, shape, and height, we can determine the traversability of the terrain. The proposed approach has been evaluated on a dataset of indoor and outdoor terrains, and the results show that it can effectively analyze the traversability of different types of terrain surfaces."
    },
    {
      "id": "de544ddb-445b-4b07-b64d-3368f1aa933e",
      "adv_source_id": "de544ddb-445b-4b07-b64d-3368f1aa933e",
      "source_id": "8a130ebe-e41d-4815-9616-a12868315538",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis",
      "prompt": "Write the abstract for the academic paper titled \"Stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis\".",
      "generation": "In this paper, we present a novel approach for stereo-based terrain traversability analysis using normal-based segmentation and superpixel surface analysis. Our method utilizes a stereo camera system to capture the terrain surface and then performs normal-based segmentation to extract the terrain details. The segmentation is followed by a superpixel surface analysis, which identifies and analyzes the superpixels on the terrain surface. By combining the segmentation and superpixel analysis, our approach is able to provide a robust and accurate traversability analysis for various types of terrain. The experimental results demonstrate the effectiveness of our approach in terms of accuracy and efficiency."
    },
    {
      "id": "7c3168c4-e58c-4c2f-8f68-1b388fb7a4ad",
      "adv_source_id": "7c3168c4-e58c-4c2f-8f68-1b388fb7a4ad",
      "source_id": "a3dcfb77-b080-42ec-93cb-f00d1ed75ef4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Understanding Deep Learning Techniques for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Understanding Deep Learning Techniques for Image Segmentation\".",
      "generation": "Abstract:\nImage segmentation is a fundamental task in computer vision with numerous applications in medical imaging, satellite imaging, and robotics. In this paper, we provide an overview of deep learning techniques commonly used for image segmentation. We discuss the advantages and limitations of these techniques and review recent advances in the field. Furthermore, we examine the impact of deep learning on image segmentation and explore potential research directions. This paper aims to provide a comprehensive understanding of the current state of deep learning for image segmentation and its future prospects."
    },
    {
      "id": "8670fd30-6fac-4be6-a76a-add47a1f4ab2",
      "adv_source_id": "8670fd30-6fac-4be6-a76a-add47a1f4ab2",
      "source_id": "a3dcfb77-b080-42ec-93cb-f00d1ed75ef4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Understanding Deep Learning Techniques for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Understanding Deep Learning Techniques for Image Segmentation\".",
      "generation": "Abstract:\nImage segmentation is an important task in computer vision with a wide range of applications. It refers to the process of dividing an image into multiple segments, which facilitates a better understanding and analysis of the image. In recent years, deep learning techniques have achieved significant advancements in image segmentation. This paper provides an overview of the state-of-the-art deep learning techniques for image segmentation, including popular algorithms such as U-Net, Mask R-CNN, and DeepLab. The paper discusses the advantages and limitations of each technique, and provides a comprehensive comparison of their performance. Additionally, the paper explores the application of these techniques in various fields such as medical imaging, autonomous driving, and multimedia analysis. The aim of this paper is to provide a comprehensive understanding of the current state of deep learning techniques for image segmentation and their potential applications."
    },
    {
      "id": "6228c32c-3a82-4fc1-b403-56485551bbde",
      "adv_source_id": "6228c32c-3a82-4fc1-b403-56485551bbde",
      "source_id": "9d1b4e7c-7d7c-40fb-b26b-ccaa1c8242a8",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A General Framework for Complex Network-Based Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A General Framework for Complex Network-Based Image Segmentation\".",
      "generation": "Abstract:\nComplex network-based image segmentation is a powerful technique for image processing and analysis. In this paper, we present a general framework for complex network-based image segmentation that combines the strengths of both supervised and unsupervised learning. Our framework utilizes a complex network to capture the underlying structure and patterns in the image data, and uses a supervised learning approach to train a classifier for segmentation. We demonstrate the effectiveness of our approach on a variety of image segmentation tasks, including semantic segmentation and instance-based segmentation. Our framework is flexible and can be applied to a wide range of images and network structures. We believe that our work will contribute to the ongoing development of complex network-based image segmentation techniques."
    },
    {
      "id": "93de5db2-3bfa-4bb9-bf82-ffb2270aa1d5",
      "adv_source_id": "93de5db2-3bfa-4bb9-bf82-ffb2270aa1d5",
      "source_id": "9d1b4e7c-7d7c-40fb-b26b-ccaa1c8242a8",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A General Framework for Complex Network-Based Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A General Framework for Complex Network-Based Image Segmentation\".",
      "generation": "This paper presents a general framework for complex network-based image segmentation. Our approach leverages the relationship between image pixels, which we represent as a complex network, to capture global structure information. We propose a novel layer-wise optimization strategy that combines the strengths of both directed and undirected networks, enabling efficient and effective segmentation. The proposed framework is evaluated on a public dataset, and results show that our approach outperforms state-of-the-art algorithms in terms of segmentation quality. We also demonstrate the generalization ability of our method on a variety of images, highlighting its applicability to real-world complex network-based image segmentation problems."
    },
    {
      "id": "c62a178e-6781-42fb-88bf-2d5209aa58a4",
      "adv_source_id": "c62a178e-6781-42fb-88bf-2d5209aa58a4",
      "source_id": "f2d40d9e-c6ac-4f4a-bae6-a5cd0cbd355b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning of Image Dehazing Models for Segmentation Tasks",
      "prompt": "Write the abstract for the academic paper titled \"Learning of Image Dehazing Models for Segmentation Tasks\".",
      "generation": "Abstract:\nHaze is a common factor in degrading the quality and visibility of images. The removal of haze from images is essential to many computer vision applications. In this paper, we present a method for learning a dehazing model from a single image without any haze information. Our approach utilizes a fully convolutional neural network (FCN) to learn a mapping from hazy images to clear images. We then use a segmentation model pre-trained on clean images to produce a segmentation mask from the dehazed image. We evaluate our approach on the Cityscapes dataset and show that our method produces a more accurate segmentation map than other state-of-the-art methods. Our code and models are available at this https URL."
    },
    {
      "id": "d1372219-7c25-492a-9012-04b66766e021",
      "adv_source_id": "d1372219-7c25-492a-9012-04b66766e021",
      "source_id": "f2d40d9e-c6ac-4f4a-bae6-a5cd0cbd355b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning of Image Dehazing Models for Segmentation Tasks",
      "prompt": "Write the abstract for the academic paper titled \"Learning of Image Dehazing Models for Segmentation Tasks\".",
      "generation": "This paper presents a novel approach to image dehazing for segmentation tasks. Our proposed method utilizes a deep learning framework to learn a mapping from hazy images to clear images. The learned model is then used to enhance the segmentation accuracy of hazy images. We demonstrate the effectiveness of our approach through extensive experiments on a variety of datasets, and show that our method outperforms the state-of-the-art in terms of segmentation accuracy.\n\nOur approach has several advantages over existing methods. First, it is a fully automated process that does not require any manual intervention. Second, it is a learnable model that can be adapted to different types of images and haze conditions. Third, it is a segmentation-aware approach that takes into account the specific needs of segmentation tasks.\n\nWe believe that our approach has the potential to significantly improve the performance of segmentation algorithms on hazy images. It can be used in a wide range of applications including autonomous driving, medical imaging, and remote sensing."
    },
    {
      "id": "89524b50-deed-4527-b7cd-c54358f48e05",
      "adv_source_id": "89524b50-deed-4527-b7cd-c54358f48e05",
      "source_id": "43f9c3dd-2f60-4f4f-bdc1-03a60d49c4c4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images",
      "prompt": "Write the abstract for the academic paper titled \"From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images\".",
      "generation": "Abstract:\nTraditional image segmentation methods based on sliding windows or region-based approaches are inefficient and inaccurate for retinal image segmentation tasks. Recent advancements in deep learning have proven to be successful in various computer vision applications. In this paper, we propose a method for retinal image segmentation using a fully convolutional network (FCN). Our approach is based on the assumption that the retinal image can be expressed as a collection of patches. We propose a fully convolutional network that takes a patch as input and produces a segmentation mask as output. To train the network, we use a large dataset of retinal images and a patch-based augmentation technique to generate a diverse set of training samples. Our approach achieves state-of-the-art performance on the ISBI challenge 2016 retinal image segmentation benchmark."
    },
    {
      "id": "3210d974-80a0-440a-bae8-bc22f6965218",
      "adv_source_id": "3210d974-80a0-440a-bae8-bc22f6965218",
      "source_id": "43f9c3dd-2f60-4f4f-bdc1-03a60d49c4c4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images",
      "prompt": "Write the abstract for the academic paper titled \"From Patch to Image Segmentation using Fully Convolutional Networks -- Application to Retinal Images\".",
      "generation": "Abstract:\nThis paper presents a method for image segmentation of retinal images based on a fully convolutional network. The network is trained using a patch-based approach, where small patches of the image are used as the training labels. This allows the network to learn patterns and structures at multiple scales, resulting in accurate segmentation of the entire image. The fully convolutional network is capable of processing the entire image in one forward pass, making it efficient for large images such as retinal images. Quantitative and qualitative results show that the proposed method produces accurate and robust segmentations. This approach can be used for automatic screening of retinal images for various diseases, and can assist clinicians in their diagnosis."
    },
    {
      "id": "d508a8ca-d588-41a3-bc8f-5318a1745c7f",
      "adv_source_id": "d508a8ca-d588-41a3-bc8f-5318a1745c7f",
      "source_id": "bec92e93-d17a-4252-86da-9e53c55211b4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation\".",
      "generation": "Abstract:\nAutomated skin lesion segmentation is a crucial task in clinical imaging applications, but it is challenging due to the variation in lesion appearance and the complexity of the skin tissue structure. This paper addresses the problem of inter-annotator agreement in skin lesion segmentation, which is essential for reliable and accurate results. We propose a novel approach that combines deep learning with a graphical model to improve inter-annotator agreement. Our method utilizes a U-Net deep learning architecture to extract features from the image and a graphical model to refine the segmentation results based on agreement among multiple annotators. The experimental results show that our method significantly improves inter-annotator agreement and outperforms the state-of-the-art in skin lesion segmentation.\n\nHighlights:\n- We address the problem of inter-annotator agreement in skin lesion segmentation, which is crucial for reliable and accurate results.\n- We propose a novel approach that combines deep learning with a graphical model to improve inter-annotator agreement.\n- Our method utilizes a U-Net deep learning architecture to extract features from the image and a graphical model to refine the segmentation results based on agreement among multiple annotators.\n- The experimental results show that our method significantly improves inter-annotator agreement and outperforms the state-of-the-art in skin lesion segmentation.\n\nConclusion:\nThe proposed method improves inter-annotator agreement in skin lesion segmentation, which is crucial for reliable and accurate results. By combining deep learning with a graphical model, our approach utilizes the strengths of both approaches to refine the segmentation results based on agreement among multiple annotators. The experimental results show that our method significantly outperforms the state-of-the-art in skin lesion segmentation, demonstrating the effectiveness of our approach."
    },
    {
      "id": "3607b211-4f1f-45bb-9672-65b4c1f10d77",
      "adv_source_id": "3607b211-4f1f-45bb-9672-65b4c1f10d77",
      "source_id": "bec92e93-d17a-4252-86da-9e53c55211b4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Handling Inter-Annotator Agreement for Automated Skin Lesion Segmentation\".",
      "generation": "Automated skin lesion segmentation is a technique that utilizes computer vision to identify and classify skin lesions in medical images. This paper focuses on the challenges of improving inter-annotator agreement (IAA) in skin lesion segmentation. The aim is to develop a robust and reliable segmentation algorithm that accurately identifies and classifies skin lesions. The proposed approach utilizes a deep learning-based segmentation model, which is trained on a large dataset of medical images. A key component of the proposed approach is the utilization of a novel loss function that incorporates both the segmentation accuracy and the IAA. The experimental results demonstrate that the proposed approach significantly improves IAA, resulting in more accurate and reliable skin lesion segmentation."
    },
    {
      "id": "7f478478-d533-40da-b2a0-6fc96c7b3657",
      "adv_source_id": "7f478478-d533-40da-b2a0-6fc96c7b3657",
      "source_id": "b4048678-ad25-460f-b1e3-c104fbf529dc",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary Loss for Remote Sensing Imagery Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Boundary Loss for Remote Sensing Imagery Semantic Segmentation\".",
      "generation": "Semantic segmentation of remote sensing imagery has been widely used in urban development analysis, environmental monitoring, and disaster management. However, the boundary loss of semantic segmentation remains to be effectively addressed, especially when dealing with complex scenarios and large-scale datasets. In this paper, we propose a novel boundary loss function for remote sensing imagery semantic segmentation based on the structural similarity (SSIM) index. Our approach effectively captures the boundary information of the semantic segmentation map and utilizes the SSIM index to measure the similarity between the predicted and ground truth boundaries. We evaluate our approach on two public remote sensing imagery datasets, and the experimental results demonstrate the effectiveness of our proposed method in reducing boundary loss and improving segmentation performance."
    },
    {
      "id": "a834b1b2-bced-4876-97c3-d99bf28344bd",
      "adv_source_id": "a834b1b2-bced-4876-97c3-d99bf28344bd",
      "source_id": "b4048678-ad25-460f-b1e3-c104fbf529dc",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Boundary Loss for Remote Sensing Imagery Semantic Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Boundary Loss for Remote Sensing Imagery Semantic Segmentation\".",
      "generation": "Semantic segmentation of remote sensing imagery has demonstrated its superiority over traditional pixel-based classification approaches in extracting object-centric information. However, to achieve accurate and pixel-wise labeling, most existing semantic segmentation methods treat image segmentation as a supervised learning problem and require a large amount of accurately annotated training samples. However, acquiring detailed and accurate semantic annotations for remote sensing images is time-consuming and expensive. To address this issue, in this paper, we propose a novel boundary loss (BoL) for remote sensing imagery semantic segmentation. Specifically, given a pair of images with only sparse and noisy annotations, our approach first estimates the semantic boundaries of the objects in the images using a boundary detection network. Next, we propagate the estimated boundaries to obtain initial semantic segmentations. Finally, we minimize the boundary loss between the initial predictions and the ground truth annotations to refine the semantic segmentations. Experiments on two public remote sensing datasets, i.e., the Stanford Background Segmentation Dataset (BSDS) and the RedEdge-encoded Semantic Segmentation Dataset (RSDS-RED), demonstrate the superiority of the proposed BoL in comparison to the state-of-the-art methods."
    },
    {
      "id": "93989bae-568d-4794-a9b0-64ed1df02424",
      "adv_source_id": "93989bae-568d-4794-a9b0-64ed1df02424",
      "source_id": "d3e5887c-59e2-4cf4-ad32-d15bf49e3608",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Diversity in Machine Learning",
      "prompt": "Write the abstract for the academic paper titled \"Diversity in Machine Learning\".",
      "generation": "Here is an abstract for an academic paper titled \"Diversity in Machine Learning\":\n\nDiversity in Machine Learning refers to the idea that machine learning algorithms and systems should be designed and developed to be inclusive and representative of the diverse populations and contexts in which they are used. This paper explores the importance of diversity in machine learning, including the ethical and social implications of biased algorithms, and the potential for machine learning to promote diversity and inclusion. The paper also discusses the challenges and opportunities for incorporating diversity into machine learning, including the need for diverse and representative data sets, the development of fair and unbiased algorithms, and the importance of collaboration and communication between stakeholders. Overall, the paper argues that diversity in machine learning is essential for ensuring the ethical and social responsibility of the technology, and for promoting fairness and equity in society."
    },
    {
      "id": "dafb3783-ccf0-45c3-be55-f4fe9133df51",
      "adv_source_id": "dafb3783-ccf0-45c3-be55-f4fe9133df51",
      "source_id": "d3e5887c-59e2-4cf4-ad32-d15bf49e3608",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Diversity in Machine Learning",
      "prompt": "Write the abstract for the academic paper titled \"Diversity in Machine Learning\".",
      "generation": "Machine learning (ML) algorithms have become an integral part of many technological advancements, from facial recognition and natural language processing to self-driving cars. However, there is growing concern about the lack of diversity in the field of ML, both in terms of the data used to train these algorithms and the people who develop and deploy them. This paper examines the current state of diversity in ML and explores how a more diverse approach to data collection and algorithm development can lead to more robust and accurate results. We discuss the potential benefits of diverse data sets and algorithms, including improved performance and better generalization to real-world data. We also examine the challenges associated with increasing diversity in ML, including concerns about fairness and bias, and explore potential solutions. Ultimately, we argue that diversity is critical to the development of effective and responsible ML algorithms, and that increased efforts to promote diversity in the field are necessary for the future of AI."
    },
    {
      "id": "f0c48429-a04f-4b7b-bef3-8316c5b14acf",
      "adv_source_id": "f0c48429-a04f-4b7b-bef3-8316c5b14acf",
      "source_id": "39bcc341-6760-4abf-a953-64499f3f3b73",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation of Video Sequences with Convolutional LSTMs",
      "prompt": "Write the abstract for the academic paper titled \"Semantic Segmentation of Video Sequences with Convolutional LSTMs\".",
      "generation": "Semantic segmentation of video sequences is a fundamental computer vision problem that has gained significant attention in recent years. In this paper, we propose a novel approach for semantic segmentation of video sequences using convolutional long short-term memory (ConvLSTM) networks.\n\nOur approach utilizes a ConvLSTM network to capture the temporal dependencies in video sequences and generate high-level representations for each frame. These representations are then fed into a fully connected network for segmentation. We train the ConvLSTM network using a combination of supervised and unsupervised learning techniques. The supervised learning component is used to learn the mapping from video frames to high-level representations, while the unsupervised learning component is used to learn the temporal dependencies between the frames.\n\nWe evaluate our approach on the Cityscapes dataset, which contains a large collection of video sequences with pixel-level annotations for semantic segmentation. Our results show that our approach achieves state-of-the-art performance on the Cityscapes dataset, with an improvement of over 5% in terms of pixel accuracy compared to the previous state-of-the-art.\n\nOur approach has several practical applications in computer vision, including autonomous driving, medical imaging, and video surveillance. By accurately segmenting the objects in video sequences, our approach can help improve the performance of various computer vision tasks such as object detection, tracking, and recognition."
    },
    {
      "id": "93246114-fed1-46df-b6db-bf46667909c7",
      "adv_source_id": "93246114-fed1-46df-b6db-bf46667909c7",
      "source_id": "39bcc341-6760-4abf-a953-64499f3f3b73",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation of Video Sequences with Convolutional LSTMs",
      "prompt": "Write the abstract for the academic paper titled \"Semantic Segmentation of Video Sequences with Convolutional LSTMs\".",
      "generation": "Semantic segmentation of video sequences is a fundamental task in computer vision with numerous applications in numerous fields. In this paper, we present a novel approach for semantic segmentation of video sequences using convolutional long short-term memory (ConvLSTM) networks. Our approach combines the strengths of convolutional neural networks (CNNs) and LSTMs to capture both spatial and temporal dependencies in video data. We introduce a novel architecture that integrates CNNs and LSTMs in a stacked manner, allowing for efficient computation and optimization. Furthermore, we propose a novel attention mechanism that weighs the importance of different frames in the video sequence, facilitating the learning of robust representations. Our approach demonstrates state-of-the-art performance on the Cityscapes dataset, achieving competitive results with fewer parameters. We believe that our work establishes a strong foundation for future research on video semantic segmentation."
    },
    {
      "id": "d12dfe78-b0c9-4b1c-b02e-ce3cdf520c11",
      "adv_source_id": "d12dfe78-b0c9-4b1c-b02e-ce3cdf520c11",
      "source_id": "ba7ac9f3-6578-4aa6-a574-b8a7d4442d53",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation\".",
      "generation": "Abstract:\nThis paper proposes a distance map regularized CNN for cardiac cine MR image segmentation. The proposed method combines the use of a CNN with a distance map regularization to improve the segmentation accuracy. The CNN is trained on a large dataset of cardiac cine MR images and their corresponding segmentations. The distance map regularization is used to prevent overfitting and to improve the generalization performance of the CNN. The proposed method is evaluated on a separate test dataset and compared to a state-of-the-art segmentation method. The results show that the proposed method achieves a higher segmentation accuracy than the state-of-the-art method. The proposed method is also shown to be more robust to noise and artifacts in the input images."
    },
    {
      "id": "632363be-b54a-4bfe-9d3e-c0164111303d",
      "adv_source_id": "632363be-b54a-4bfe-9d3e-c0164111303d",
      "source_id": "ba7ac9f3-6578-4aa6-a574-b8a7d4442d53",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Distance Map Regularized CNN for Cardiac Cine MR Image Segmentation\".",
      "generation": "This paper proposes a novel approach for cardiac cine MR image segmentation based on a distance map regularized convolutional neural network (DRM-CNN). The proposed method combines the strengths of both supervised and unsupervised learning to overcome the limitations of existing techniques. The DRM-CNN is trained on a small set of manually labeled images and then used to segment a larger set of unlabeled images. The distance map regularization ensures that the predicted segmentations are smooth and continuous, which is important for the interpretation of the results. The proposed method is evaluated on a large dataset of cardiac cine MR images and compared to state-of-the-art supervised and unsupervised segmentation methods. The results show that the proposed method achieves competitive performance while requiring less labeled data and computational resources."
    },
    {
      "id": "f4eb6688-6fae-4162-ae6c-0ee8ecba05cc",
      "adv_source_id": "f4eb6688-6fae-4162-ae6c-0ee8ecba05cc",
      "source_id": "14eace65-a9c8-4af7-bd8f-058a26858920",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology",
      "prompt": "Write the abstract for the academic paper titled \"Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology\".",
      "generation": "The paper presents an adaptive weighting multi-field-of-view convolutional neural network (MW-MFOV-CNN) for semantic segmentation of whole slide images (WSIs) in digital pathology. The proposed method incorporates both local and global information within each FOV to overcome the limitations of existing approaches. A novel adaptive weighting scheme is designed to balance the contributions of local and global features, allowing the network to focus on relevant information and adapt to varying local and global contexts. The MW-MFOV-CNN achieves state-of-the-art performance on two public WSIs datasets, exceeding the F1-score of the previous best method by 2.7% and 3.1% on the Camelyon17 and Catheter Segmentation datasets, respectively. The code and additional results are available at https://github.com/AI4Pathology/Adaptive-Weighting-MFOV."
    },
    {
      "id": "f08bd054-ad7b-44f7-8748-3aae22fd313f",
      "adv_source_id": "f08bd054-ad7b-44f7-8748-3aae22fd313f",
      "source_id": "14eace65-a9c8-4af7-bd8f-058a26858920",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology",
      "prompt": "Write the abstract for the academic paper titled \"Adaptive Weighting Multi-Field-of-View CNN for Semantic Segmentation in Pathology\".",
      "generation": "The paper presents an adaptive weighting mechanism for multi-field-of-view (MFOV) convolutional neural networks (CNNs) that improves the performance of semantic segmentation in pathology images. The proposed method enables the network to focus on the most relevant information by adaptively weighting the contributions of different fields of view. \n\nThe paper discusses the challenges of semantic segmentation in pathology images, including the presence of low-contrast regions, complex textures, and large variations in size and shape. It then introduces the concept of MFOV CNNs, which combine multiple fields of view to capture a wider range of information. The adaptive weighting mechanism is then proposed to adjust the importance of different fields of view based on the context of the image. \n\nThe paper demonstrates the effectiveness of the proposed method through extensive experiments on a public dataset, showing significant improvements in segmentation performance compared to a conventional MFOV CNN. The results show that the adaptive weighting mechanism is particularly useful for improving the accuracy of segmentation in low-contrast regions and complex textures. \n\nThe paper concludes by discussing the potential applications of the proposed method in clinical practice, including the detection and diagnosis of pathological conditions. It also highlights the importance of data augmentation and transfer learning in improving the performance of semantic segmentation in pathology images."
    },
    {
      "id": "ce1210a5-d512-41f5-9fcd-cc2759803471",
      "adv_source_id": "ce1210a5-d512-41f5-9fcd-cc2759803471",
      "source_id": "cb534d5c-f0fd-48d3-9601-a353d1c2fe81",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds",
      "prompt": "Write the abstract for the academic paper titled \"Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds\".",
      "generation": "In this paper, we present a novel approach to 3D medical image compression that combines machine vision and 3D graphics technologies. Our method utilizes a machine vision-guided 3D graphics engine to compress medical images, which can be transmitted more efficiently to the cloud for accurate segmentation. We demonstrate that our method can significantly reduce the file size of 3D medical images while maintaining high levels of segmentation accuracy. Furthermore, our method is fully automated and does not require any manual intervention, making it suitable for clinical use. We believe that our method can help to improve the efficiency and accuracy of 3D medical image segmentation in the cloud, and we hope that it will be widely adopted in the medical imaging community."
    },
    {
      "id": "59f98ff4-073e-41eb-8b56-e8b6ffdaad28",
      "adv_source_id": "59f98ff4-073e-41eb-8b56-e8b6ffdaad28",
      "source_id": "cb534d5c-f0fd-48d3-9601-a353d1c2fe81",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds",
      "prompt": "Write the abstract for the academic paper titled \"Machine Vision Guided 3D Medical Image Compression for Efficient Transmission and Accurate Segmentation in the Clouds\".",
      "generation": "Abstract:\nMedical image segmentation in the clouds can potentially alleviate the computational burden on local resources. However, transmitting volumetric image data to the cloud consumes a considerable amount of time due to the large size of the data. In this paper, we investigate the compression of 3D medical images using a machine vision guided approach for efficient transmission and segmentation in the clouds. Our method utilizes a convolutional neural network (CNN) to extract the most relevant features from the medical images. The extracted features are then compressed using a 3D wavelet transform and transmitted to the cloud. We demonstrate that our method achieves a higher compression ratio compared to state-of-the-art image compression techniques while preserving the segmentation accuracy. Furthermore, our method reduces the transmission time by a factor of 5 compared to the original image size. We validate the efficacy of our method on a public dataset and show that our method can achieve accurate segmentation in the clouds while reducing the computational time."
    },
    {
      "id": "6c5c14b9-a76b-4052-aa36-013afa71cceb",
      "adv_source_id": "6c5c14b9-a76b-4052-aa36-013afa71cceb",
      "source_id": "1204734a-a062-4ab7-95ad-52664d55d8a9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\".",
      "generation": "Auto-DeepLab is a novel approach to semantic image segmentation, a process that involves assigning a label to each pixel in an image. This approach utilizes a hierarchical neural architecture search to identify an optimal network architecture for the task. This is done by first searching for an optimal network architecture at a low level of abstraction, and then using this architecture as a starting point for searching for an optimal architecture at a higher level of abstraction. The process is repeated until an optimal network architecture is found at the highest level of abstraction. \n\nThe paper discusses the advantages of this approach over existing methods, and presents results that show the effectiveness of the proposed method. The results show that the proposed method is able to identify an optimal network architecture for semantic image segmentation, and that it is able to improve the performance of the network in terms of both accuracy and efficiency. \n\nThe paper concludes by discussing the potential applications of the proposed method, and suggests that it may be useful for improving the performance of semantic image segmentation in a variety of domains."
    },
    {
      "id": "497bf024-da6b-459f-bed2-fa0243e7920a",
      "adv_source_id": "497bf024-da6b-459f-bed2-fa0243e7920a",
      "source_id": "1204734a-a062-4ab7-95ad-52664d55d8a9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\".",
      "generation": "Abstract:\nTraditional hand-designed architectures for semantic image segmentation often require laborious engineering efforts. Recently, neural architecture search (NAS) has been introduced to automate the design process. However, existing NAS methods usually search for a single architecture across all levels of the segmentation hierarchy, which limits their ability to explore complex and varied structures that may exist across different levels. In this work, we propose Auto-DeepLab, a novel NAS method designed for semantic image segmentation. Instead of searching for a single architecture, Auto-DeepLab simultaneously searches for separate architectures to reuse at each level of the segmentation hierarchy. Furthermore, to effectively search for suitable architectures, we propose an innovative hierarchical encoding method that explicitly encodes the structural dependencies between different levels. Our approach leads to significant improvements in segmentation performance with comparisons to popular hand-designed and state-of-the-art NAS methods on the PASCAL VOC 2012 and Cityscapes datasets."
    },
    {
      "id": "41d975c1-20ab-4620-bc53-7dda9f8704a6",
      "adv_source_id": "41d975c1-20ab-4620-bc53-7dda9f8704a6",
      "source_id": "dc899aff-901a-47ea-a194-33ac1c5dc818",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Vehicle Image Generation Going Well with The Surroundings",
      "prompt": "Write the abstract for the academic paper titled \"Vehicle Image Generation Going Well with The Surroundings\".",
      "generation": "Abstract:\nWith the development of computer vision technology, image generation has become an increasingly important topic. Vehicle image generation is a specific type of image generation that focuses on generating images of vehicles, such as cars, trucks, and motorcycles. These images are often used for training and testing computer vision algorithms. The surroundings in which a vehicle is located can significantly impact the image generation process. This paper explores how the surroundings can be incorporated into the vehicle image generation process to improve the quality and realism of the generated images. We discuss different methods for incorporating the surroundings, including using contextual information and incorporating background knowledge. We also present experimental results that demonstrate the effectiveness of our approach. Finally, we discuss the implications of our work for future research in vehicle image generation."
    },
    {
      "id": "3d2c3099-4e4b-47f9-a8cf-a43020367db1",
      "adv_source_id": "3d2c3099-4e4b-47f9-a8cf-a43020367db1",
      "source_id": "dc899aff-901a-47ea-a194-33ac1c5dc818",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Vehicle Image Generation Going Well with The Surroundings",
      "prompt": "Write the abstract for the academic paper titled \"Vehicle Image Generation Going Well with The Surroundings\".",
      "generation": "Abstract:\nWith the recent developments in the field of computer graphics and computer vision, the generation of images of vehicles that are realistic and accurate has become possible. This paper discusses the process of generating images of vehicles, from the initial idea to the final product. The research focuses on the surroundings of the vehicles as well, as they are an important aspect of the image and can greatly affect the overall result. Several techniques for generating images of vehicles and their surroundings are proposed and evaluated, with the best practices and recommendations presented. The paper concludes by discussing the potential applications of these techniques and how they can be used to create more realistic and accurate images of vehicles in the future."
    },
    {
      "id": "8d4a4706-6e87-4654-bf12-107729c29c4d",
      "adv_source_id": "8d4a4706-6e87-4654-bf12-107729c29c4d",
      "source_id": "52afd030-fc45-4f6e-8f11-d86c68aae4ac",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy",
      "prompt": "Write the abstract for the academic paper titled \"CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy\".",
      "generation": "Below is an abstract for the academic paper titled, \"CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy\":\n\nAbstract\nA common problem in the analysis of DSC perfusion MR head scans with abnormal brain anatomy is the accurate segmentation of brain tissue, in particular the segmentation of grey matter (GM) and white matter (WM). The CUSUM filter is a data-driven approach for image segmentation that is independent of a priori information and assumptions about the image and the segmentation. It provides a robust and reliable segmentation of GM and WM that is suitable for clinical use. This paper describes the CUSUM filter and its application to the segmentation of DSC perfusion MR head scans with abnormal brain anatomy. The results show that the CUSUM filter is a promising approach for brain segmentation in this context.\n\nWould you like assistance with another paper?"
    },
    {
      "id": "d6d4027e-196b-4aca-87a2-ee0f166ff225",
      "adv_source_id": "d6d4027e-196b-4aca-87a2-ee0f166ff225",
      "source_id": "52afd030-fc45-4f6e-8f11-d86c68aae4ac",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy",
      "prompt": "Write the abstract for the academic paper titled \"CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy\".",
      "generation": "The abstract for the academic paper titled \"CUSUM Filter for Brain Segmentation on DSC Perfusion MR Head Scans with Abnormal Brain Anatomy\" should include the following sections:\n\n1. Introduction: This section should provide a brief overview of the research question and the significance of the study. It should also include a description of the study population and the data used in the research.\n\n2. Method: This section should describe the research methods used in the study, including the data collection, preprocessing, and analysis techniques. It should also include a description of the CUSUM filter and its application to the brain segmentation task.\n\n3. Results: This section should present the findings of the study, including the performance of the CUSUM filter on the brain segmentation task. It should include metrics such as accuracy, precision, recall, and F1-score.\n\n4. Conclusion: This section should summarize the main findings of the study and their implications for future research. It should also include a discussion of the limitations of the study and potential directions for future research.\n\nHere is a sample abstract for the paper:\n\nAbstract:\n\nBrain segmentation on DSC perfusion MR head scans with abnormal brain anatomy is a challenging task. The current state-of-the-art techniques have limitations in handling such scans. In this paper, we propose a CUSUM filter for brain segmentation on DSC perfusion MR head scans with abnormal brain anatomy. Our proposed method utilizes the cumulative sum of the image intensity values to robustly segment the brain. We evaluated our proposed method on a public dataset and compared it with the current state-of-the-art techniques. The results show that our proposed method achieved higher accuracy and F1-score compared to the current state-of-the-art techniques. Our proposed method is able to handle DSC perfusion MR head scans with abnormal brain anatomy and is suitable for clinical use."
    },
    {
      "id": "3313831d-fd8e-4996-b9c7-e3fef42e4560",
      "adv_source_id": "3313831d-fd8e-4996-b9c7-e3fef42e4560",
      "source_id": "3d2553f0-ad50-4f5e-a675-a550982c3f7a",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive segmentation of medical images through fully convolutional neural networks",
      "prompt": "Write the abstract for the academic paper titled \"Interactive segmentation of medical images through fully convolutional neural networks\".",
      "generation": "The paper discusses an interactive segmentation method for medical images using fully convolutional neural networks. The method enables users to provide feedback on the segmentation results in an efficient and intuitive manner. The feedback is used to refine the segmentation results, improving the accuracy and efficiency of the process. The paper also explores the use of a fully convolutional neural network for the task, and discusses the benefits of this approach compared to traditional region-based methods.\n\nThe abstract should cover the following:\n\nIntroduction: The increasing complexity of medical images, such as MRI and CT scans, has resulted in a growing need for accurate and efficient image segmentation techniques.\n\nProblem: Existing interactive segmentation methods often require extensive user interaction and feedback, which can be time-consuming and subjective.\n\nApproach: The proposed method utilizes a fully convolutional neural network (FCNN) for image segmentation, which is trained on a large dataset of medical images.\n\nResults: The method is evaluated on a variety of medical images, and compared to traditional region-based methods. The results show that the FCNN-based method is significantly more accurate and efficient, requiring less user interaction and feedback.\n\nConclusions: The paper concludes by discussing the potential of the proposed method for improving the accuracy and efficiency of medical image segmentation, and suggests further research into the use of FCNNs for interactive segmentation."
    },
    {
      "id": "9da50faa-1145-4241-8158-f6960de8cbb3",
      "adv_source_id": "9da50faa-1145-4241-8158-f6960de8cbb3",
      "source_id": "3d2553f0-ad50-4f5e-a675-a550982c3f7a",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive segmentation of medical images through fully convolutional neural networks",
      "prompt": "Write the abstract for the academic paper titled \"Interactive segmentation of medical images through fully convolutional neural networks\".",
      "generation": "The paper presents an interactive segmentation method for medical images using a fully convolutional neural network. The network is trained on a large dataset of images and their corresponding segmentations and uses a novel loss function that encourages the network to produce segmentations similar to the ground truth. During the testing phase, the user can interactively adjust the segmentation by providing additional information through a graphical user interface. The network then refines its output based on the user's input, producing a more accurate segmentation. The proposed method has been evaluated on a variety of medical images, including MRI and CT scans, and has been shown to produce accurate and reliable segmentations. It has the potential to improve the accuracy and efficiency of medical image segmentation, which is crucial for diagnosis and treatment planning."
    },
    {
      "id": "f547540c-9487-4664-a95a-e442e3f49451",
      "adv_source_id": "f547540c-9487-4664-a95a-e442e3f49451",
      "source_id": "b69792fc-988e-41fd-b78d-56f1d76676f1",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative",
      "prompt": "Write the abstract for the academic paper titled \"Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative\".",
      "generation": "In this work, we propose a novel, learning-based approach for the segmentation of 3D and 4D multi-surface multi-object knee MRI data. Our method utilizes a cost function that is learned from a large dataset of manually segmented images. The cost function is designed to be invariant to patient-specific variations in anatomy and scanner settings, and is trained to segment multiple objects, including bone, cartilage, and meniscus, in 3D and 4D space. We evaluated our method on a large dataset of images from the Osteoarthritis Initiative and compared it to a state-of-the-art, manual segmentation method. Our results show that our method is comparable to the manual segmentation method in terms of accuracy, while providing significant time savings. Furthermore, our method is able to generalize to images that were not included in the training dataset, demonstrating its ability to handle unseen data. We believe that our method has the potential to transform the field of knee MRI segmentation by providing a fast, accurate, and generalizable solution."
    },
    {
      "id": "e0ae4e62-2d5c-49a7-b844-7be2ea2bc568",
      "adv_source_id": "e0ae4e62-2d5c-49a7-b844-7be2ea2bc568",
      "source_id": "b69792fc-988e-41fd-b78d-56f1d76676f1",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative",
      "prompt": "Write the abstract for the academic paper titled \"Learning-Based Cost Functions for 3D and 4D Multi-Surface Multi-Object Segmentation of Knee MRI: Data from the Osteoarthritis Initiative\".",
      "generation": "In this work, we develop an algorithm that utilizes a learning-based cost function for 3D and 4D multi-surface multi-object segmentation of knee MRI. The algorithm is trained on data from the Osteoarthritis Initiative, a longitudinal study that captures imaging and clinical data from a diverse population of participants with knee osteoarthritis. The cost function is designed to minimize the difference between the predicted and ground truth segmentations, and is trained using a supervised learning approach. The algorithm is evaluated on a separate test set, and results show that it achieves a high degree of accuracy and precision in segmenting the knee MRI data. The algorithm is able to handle 3D and 4D multi-surface segmentation of the knee, and is able to segment multiple objects in the knee, including the femur, tibia, and cartilage. The algorithm is also able to handle images with different contrast and noise levels, and is able to segment the data in a way that is consistent with the ground truth segmentations."
    },
    {
      "id": "0aef721d-79d6-41b6-9e44-ba9d04676dab",
      "adv_source_id": "0aef721d-79d6-41b6-9e44-ba9d04676dab",
      "source_id": "1b31bd11-bd00-4350-a26e-1a9bb6b8ee50",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation\".",
      "generation": "The segmentation of the left atrium (LA) in cardiac magnetic resonance images (CMRIs) is a fundamental step in the analysis of atrial fibrillation (AF). We propose a new method for LA segmentation in CMRI based on a mixture model (MM) global shape prior and an autoencoder (AE) local intensity prior. The MM is used to model the global shape of the LA, while the AE is used to learn a low-dimensional representation of the local intensity patterns within the LA. The MM and AE are trained on a large dataset of annotated CMRI scans and are used to segment new, unseen CMRI scans. We evaluate our method on a separate test dataset and compare it to state-of-the-art segmentation methods. Our results show that our method achieves competitive performance compared to other methods while requiring less training data. Furthermore, our method is able to generalize well to unseen data, demonstrating the effectiveness of the MM and AE in capturing global and local intensity patterns, respectively."
    },
    {
      "id": "3077c023-bedc-47b5-9769-af20dca3350b",
      "adv_source_id": "3077c023-bedc-47b5-9769-af20dca3350b",
      "source_id": "1b31bd11-bd00-4350-a26e-1a9bb6b8ee50",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Mixture Modeling of Global Shape Priors and Autoencoding Local Intensity Priors for Left Atrium Segmentation\".",
      "generation": "The segmentation of the left atrium in cardiac MR images is a fundamental step in the analysis of atrial fibrillation (AF). We propose a new method for left atrium segmentation based on a hybrid model of global shape priors and autoencoded local intensity priors. The global shape prior is modeled using a Gaussian mixture model (GMM) while the local intensity prior is learned using an autoencoder. The GMM provides a prior on the shape of the left atrium while the autoencoder learns the local intensity patterns in cardiac MR images. The segmentation is performed by maximizing the likelihood of the joint distribution of the GMM and the autoencoder. We evaluated our method on 100 cardiac MR images and compared it to three state-of-the-art methods. Our method achieved a mean Dice similarity coefficient of 0.88, which is significantly better than the other methods. Furthermore, our method is able to handle a large range of atrial shapes and sizes, and is able to segment poorly segmented regions."
    },
    {
      "id": "fe4697f7-8447-43e5-a6a8-ad2b6b3a710e",
      "adv_source_id": "fe4697f7-8447-43e5-a6a8-ad2b6b3a710e",
      "source_id": "92e2b851-29c4-414c-a5c3-21d8702e74a2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation\".",
      "generation": "Abstract:\nTraditional convolutional neural networks (CNNs) have a sparse connectivity pattern, where each layer is connected to every other layer in a feed-forward fashion. This paper presents HyperDense-Net, a hyper-densely connected CNN for multi-modal image segmentation. HyperDense-Net introduces a new layer type, the hyper-dense layer, which connects each node in a layer to every other node in all subsequent layers. This dense connectivity pattern enables the network to capture complex relationships between modalities and improves the flow of information throughout the network. We evaluate HyperDense-Net on the multi-modal image segmentation task and show that it outperforms state-of-the-art models in terms of segmentation accuracy and efficiency.\n\nKeywords:\nMulti-modal image segmentation, Hyper-dense connectivity, Convolutional neural networks"
    },
    {
      "id": "c5e5bc28-8a14-4446-be27-ff2668d7ebb4",
      "adv_source_id": "c5e5bc28-8a14-4446-be27-ff2668d7ebb4",
      "source_id": "92e2b851-29c4-414c-a5c3-21d8702e74a2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"HyperDense-Net: A hyper-densely connected CNN for multi-modal image segmentation\".",
      "generation": "Abstract:\nTraditional convolutional neural networks (CNNs) predominantly rely on local connections between adjacent layers, which limits the network's expressiveness in capturing complex relationships among features. This limitation poses challenges for tasks like multi-modal image segmentation, where the underlying relationships among multiple modalities are intricate and arduous to capture. In this paper, we propose HyperDense-Net, a novel CNN architecture that introduces hyper-dense connections between every pair of layers within a network. These additional connections enable the network to learn powerful and intricate relationships among the features, thereby enhancing the representation capacity. We evaluate HyperDense-Net on the multi-modal image segmentation task, where it achieves state-of-the-art performance on two standard benchmarks, i.e., the MIT-CBCL and Stanford-40 cell image segmentation datasets. Our code and models are publicly available at https://github.com/AI4Finance-Foundation/HyperDense-Net."
    },
    {
      "id": "b4f05c25-31d4-4b61-83a0-afddfb58503f",
      "adv_source_id": "b4f05c25-31d4-4b61-83a0-afddfb58503f",
      "source_id": "7b04c7b1-2830-452a-9f54-77a2c8f612cd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dense 3D Visual Mapping via Semantic Simplification",
      "prompt": "Write the abstract for the academic paper titled \"Dense 3D Visual Mapping via Semantic Simplification\".",
      "generation": "An indirect multi-view geometry technique for dense 3D mapping utilising single-view image semantics is presented in this paper. Given a number of images from several views of a scene, the goal is to produce a dense 3D map of the scene that is consistent with the image semantics. In contrast to direct methods, which optimise the 3D map directly, indirect techniques first construct an initial map without considering image data and then refine it so that it is consistent with the images. One significant advantage of indirect methods is that they can use parallelised processing to speed up computation. In this paper, we create a dense 3D map by simplifying the scene using the semantics in a single view. The 3D map is then refined to be consistent with the rest of the images. We demonstrate the effectiveness of our approach on both synthetic and real datasets. Compared to previous work, our method is efficient, scalable, and able to deal with scenes that contain multiple objects."
    },
    {
      "id": "cfbe4d7e-ed4e-4348-8b3f-1aa3333c1909",
      "adv_source_id": "cfbe4d7e-ed4e-4348-8b3f-1aa3333c1909",
      "source_id": "7b04c7b1-2830-452a-9f54-77a2c8f612cd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Dense 3D Visual Mapping via Semantic Simplification",
      "prompt": "Write the abstract for the academic paper titled \"Dense 3D Visual Mapping via Semantic Simplification\".",
      "generation": "An immersive augmented reality experience requires live and consistent 3D mapping of the real-world environment. However, 3D mapping systems are typically hampered by the ambiguity of scene layout, resulting in either incomplete or inaccurate spatial understanding. In this paper, we tackle the problem of dense 3D visual mapping via semantic simplification, with a focus on indoor scenes. We exploit complementary properties of RGB and depth data streams, using each for what it does best. Our approach first builds a coarse but complete 3D map from RGB data, using a state-of-the-art convolutional neural network encoder-decoder architecture. We then densify the resulting sparse map via learned propagation of confidence measures in a coarse-to-fine fashion. To exploit additional information from the depth data stream, we propose a depth-condition-based approach that refines the 3D maps where depth data is available. We further show that a simplified representation, obtained by recursive elimination of kinematically infeasible loops in the graph structure of the map, improves the robustness of mapping in challenging scenarios, including scenes with many reflective surfaces. We validate the proposed approach through comprehensive experiments, and demonstrate efficient online mapping on a mobile platform."
    },
    {
      "id": "dd881986-274c-4169-a4f1-29a1aca9680c",
      "adv_source_id": "dd881986-274c-4169-a4f1-29a1aca9680c",
      "source_id": "87aa367e-068d-49a2-92d3-a9f380cc6fff",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain MRI Segmentation using Rule-Based Hybrid Approach",
      "prompt": "Write the abstract for the academic paper titled \"Brain MRI Segmentation using Rule-Based Hybrid Approach\".",
      "generation": "Abstract:\nThis paper presents a rule-based hybrid approach for brain MRI segmentation. The proposed method combines both thresholding and clustering techniques to achieve accurate and automated segmentation. In the proposed method, the MRI scan is first thresholded to isolate the brain region from the background. The thresholding step is based on the Otsu's method, which maximizes the variance between the brain and background regions. The isolated brain region is then subjected to clustering analysis using the K-means algorithm. The resulting clusters are merged using a rule-based approach to obtain the final segmentation. The proposed method was evaluated on a public dataset and compared to existing state-of-the-art methods. The results show that the proposed method achieves a higher segmentation accuracy compared to the existing methods. The proposed method has been successfully applied to segment the brain MRI scans of patients with multiple sclerosis and healthy controls. Furthermore, the proposed method has been compared to manual segmentation by an expert radiologist, and the results show a significant agreement between the automated segmentation and the manual segmentation."
    },
    {
      "id": "13f68882-346f-44a7-b054-d3fbc99bc493",
      "adv_source_id": "13f68882-346f-44a7-b054-d3fbc99bc493",
      "source_id": "87aa367e-068d-49a2-92d3-a9f380cc6fff",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain MRI Segmentation using Rule-Based Hybrid Approach",
      "prompt": "Write the abstract for the academic paper titled \"Brain MRI Segmentation using Rule-Based Hybrid Approach\".",
      "generation": "Abstract:\nThis paper presents a rule-based hybrid approach for brain MRI segmentation. The proposed method combines both thresholding and clustering techniques to achieve accurate and robust segmentation. The thresholding approach is used to segment the MRI data into two regions: the foreground (brain) and the background (the rest of the image). The clustering approach is then used to segment the foreground into multiple regions representing different brain structures. The proposed method has been evaluated on a large dataset of MRI scans and compared to state-of-the-art segmentation methods. The results show that the proposed method achieves competitive performance in terms of accuracy and speed. The proposed method has been implemented as a plugin for the open-source MRI analysis software FSL."
    },
    {
      "id": "358efbe8-a94a-4b44-940f-001c1e615db9",
      "adv_source_id": "358efbe8-a94a-4b44-940f-001c1e615db9",
      "source_id": "5ed7b754-5bb0-4aff-a803-d376e9e26edd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images",
      "prompt": "Write the abstract for the academic paper titled \"Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images\".",
      "generation": "The abstract for the academic paper titled \"Learning Metric Graphs for Neuron Segmentation in Electron Microscopy Images\" could be:\n\nThe lack of available large-scale training data is a major challenge in training deep learning models for neuron segmentation in electron microscopy images. In this paper, we propose a new approach for training deep learning models for this task by learning metric graphs. These graphs provide a way to represent the spatial relationships between pixels in an image, and can be used to improve the performance of deep learning models. We show that our approach can achieve better results than existing methods, even when the training data is limited. We evaluate our approach on a large dataset of electron microscopy images and compare it to state-of-the-art algorithms. Our results show that our approach can achieve a competitive performance, even with limited training data. We believe that our approach can be useful for training deep learning models in other medical imaging tasks where the training data is limited."
    },
    {
      "id": "7d189b85-f130-40d7-9e39-ca1f41e472a4",
      "adv_source_id": "7d189b85-f130-40d7-9e39-ca1f41e472a4",
      "source_id": "5ed7b754-5bb0-4aff-a803-d376e9e26edd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images",
      "prompt": "Write the abstract for the academic paper titled \"Learning Metric Graphs for Neuron Segmentation In Electron Microscopy Images\".",
      "generation": "The recent development of neuron segmentation algorithms for electron microscopy (EM) images has led to an increased need for quantitative evaluations. However, existing evaluation metrics do not fully capture the complexity of neuron segmentation in EM images, leading to inaccuracies in the evaluation. In this paper, we present a new learning-based approach for metric graph construction that evaluates the quality of neuron segmentations in EM images. Our approach leverages a deep learning architecture to learn a metric graph from a small set of ground truth annotations. The proposed method captures intricate structural details of neurons and provides a more accurate evaluation of segmentation quality. We demonstrate the effectiveness of our approach on a public EM dataset, achieving a significant improvement in segmentation quality compared to the state-of-the-art. We believe that our method will facilitate the development of more accurate neuron segmentation algorithms and promote the advancement of EM image analysis."
    },
    {
      "id": "88e70311-309b-4d31-8328-c229ae3c94ca",
      "adv_source_id": "88e70311-309b-4d31-8328-c229ae3c94ca",
      "source_id": "3832df81-81ca-4296-8ce0-fbe676a4a967",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Joint shape learning and segmentation for medical images using a minimalistic deep network",
      "prompt": "Write the abstract for the academic paper titled \"Joint shape learning and segmentation for medical images using a minimalistic deep network\".",
      "generation": "The paper presents a joint shape learning and segmentation framework for medical images using a minimalistic deep network. The network is designed to jointly learn a low-dimensional shape representation and perform segmentation of medical images. The framework is based on a deep fully convolutional network (FCN) with a shared encoder-decoder architecture. The encoder is responsible for learning a low-dimensional shape representation, while the decoder is used for segmentation. The network is trained using a sparse-coding loss function, which ensures that the shape representation is robust to noise and artifacts in the medical images. The segmentation is performed by predicting a binary mask for each class, followed by a thresholding step to obtain the final segmentation. The framework is evaluated on a public dataset of abdominal computed tomography (CT) scans and compared to a state-of-the-art method. The results show that the proposed method achieves comparable performance to the state-of-the-art method while being more efficient in terms of computational cost and training time."
    },
    {
      "id": "f8872628-17d8-42fa-a1c4-43b74f4e0412",
      "adv_source_id": "f8872628-17d8-42fa-a1c4-43b74f4e0412",
      "source_id": "3832df81-81ca-4296-8ce0-fbe676a4a967",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Joint shape learning and segmentation for medical images using a minimalistic deep network",
      "prompt": "Write the abstract for the academic paper titled \"Joint shape learning and segmentation for medical images using a minimalistic deep network\".",
      "generation": "Here is the abstract for the academic paper titled \"Joint shape learning and segmentation for medical images using a minimalistic deep network\":\n\nAbstract: We present a deep learning framework for joint shape learning and segmentation of medical images. Our framework utilizes a minimalistic deep network architecture, comprised of a convolutional layer, followed by a recurrent layer, and finally a fully connected layer. The convolutional layer extracts features from the input image, while the recurrent layer captures the temporal dependencies between the features. The fully connected layer is utilized for classification and segmentation tasks. Our framework is trained using a large dataset of medical images and achieves state-of-the-art performance on multiple medical image segmentation benchmarks. We show that our minimalistic network architecture is able to learn complex shapes and structures within medical images, and can generalize well to unseen data."
    },
    {
      "id": "76edee90-fda7-4f0e-b10d-90b76e53373e",
      "adv_source_id": "76edee90-fda7-4f0e-b10d-90b76e53373e",
      "source_id": "e58de623-8ccb-4452-bad0-85a2456277c2",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation\".",
      "generation": "Sure, here is an abstract for the academic paper titled \u201cCascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation\u201d:\n\nDecoding is a fundamental step in most segmentation algorithms, especially for biomedical image segmentation. It converts a label map into a binary mask, which can be used for various applications such as object detection, image registration, and image analysis. However, the performance of the decoding step is often limited by the quality of the input label map, which may be noisy, incomplete, or inaccurate. In this paper, we propose a new decoding method called Cascade Decoder, which is designed to handle such noisy and incomplete label maps. Our method first applies a series of cascaded decoders to refine the input label map, and then uses a final decoder to produce the final binary mask. Each decoder in the cascade consists of a convolutional neural network (CNN) and a conditional random field (CRF) model. The CNN extracts features from the input label map, and the CRF model refines the label map based on these features. Our method is able to handle various types of biomedical images, including MRI, CT, and ultrasound images. We evaluate our method on the ISBI 2017 Challenge dataset, and show that it significantly improves the performance of the segmentation algorithm compared to the baseline method."
    },
    {
      "id": "be819f97-6f99-405a-8ab1-8b65e04a1a3c",
      "adv_source_id": "be819f97-6f99-405a-8ab1-8b65e04a1a3c",
      "source_id": "e58de623-8ccb-4452-bad0-85a2456277c2",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Cascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation\".",
      "generation": "Sure! Here is the abstract for the academic paper titled \u201cCascade Decoder: A Universal Decoding Method for Biomedical Image Segmentation\u201d:\n\n\"Biomedical image segmentation is a fundamental task in medical imaging analysis, and it plays a crucial role in the computer-aided diagnosis and treatment of various diseases. However, accurate segmentation of biomedical images remains a challenging task due to the complexity and variability of the images. In this paper, we propose a novel segmentation framework called Cascade Decoder, which is designed to address this challenge. The proposed method combines a set of weak learners, including morphological filters and machine learning-based classifiers, to form a strong segmentation model. The weak learners are trained on a subset of the training data and are combined in a hierarchical manner to improve the segmentation performance. The proposed method was evaluated on three different datasets: MRI brain images, CT lung images, and histopathology images. The experimental results show that the proposed method achieved competitive performance compared to the state-of-the-art methods in terms of accuracy and speed. The Cascade Decoder is a universal decoding method that can be applied to various biomedical image segmentation tasks with promising results.\""
    },
    {
      "id": "ac867822-ac75-4cb5-9c36-d0902cdc49d9",
      "adv_source_id": "ac867822-ac75-4cb5-9c36-d0902cdc49d9",
      "source_id": "d7b9b9fb-356c-4126-9f09-e853fa89f5fd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks",
      "prompt": "Write the abstract for the academic paper titled \"Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks\".",
      "generation": "Abstract:\nIris recognition is a well-known biometric authentication method. In this study, we aimed to implement an effective iris recognition system using image segmentation through a retrained deep neural network. Our approach involved using pre-trained deep neural networks for image segmentation, followed by extraction of iris features and classification. We retrained the segmentation network using our dataset, which improved the segmentation accuracy. The extracted iris features were then used to recognize users in the verification process. Our proposed method achieved a recognition rate of 99.35% and a retrieval time of 2.58 seconds for a database of 100 users. Our approach demonstrates the potential for achieving both high accuracy and efficiency in iris recognition systems through retraining off-the-shelf deep neural networks."
    },
    {
      "id": "9f50a7e7-3d13-4f2d-be42-5c48a3b2539c",
      "adv_source_id": "9f50a7e7-3d13-4f2d-be42-5c48a3b2539c",
      "source_id": "d7b9b9fb-356c-4126-9f09-e853fa89f5fd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks",
      "prompt": "Write the abstract for the academic paper titled \"Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks\".",
      "generation": "Here is the potential abstract for the academic paper titled \"Iris Recognition with Image Segmentation Employing Retrained Off-the-Shelf Deep Neural Networks\":\n\nAbstract: Iris recognition is a well-known method of biometric identification. In this work, we explore the use of retrained off-the-shelf deep neural networks for iris recognition. We segment the iris region from the image and then use a retrained network to classify the iris images. Our approach achieves a high accuracy, outperforming other state-of-the-art methods. The retrained network is able to generalize well to unseen data, demonstrating the effectiveness of our approach. We also perform a comprehensive analysis of the performance of our method in different scenarios. Our work shows that retrained off-the-shelf deep neural networks can be effectively used for iris recognition."
    },
    {
      "id": "e0e3c0cb-13d0-4a22-8ff0-a92791ca38ce",
      "adv_source_id": "e0e3c0cb-13d0-4a22-8ff0-a92791ca38ce",
      "source_id": "45731520-dba4-4450-b64b-9dde0330cbdb",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images",
      "prompt": "Write the abstract for the academic paper titled \"Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images\".",
      "generation": "The abstract for the academic paper titled \"Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images\" could be:\n\nRecent advances in medical imaging have enabled the development of three-dimensional (3D) magnetic resonance (MR) images for improved diagnostic and clinical analysis. However, the segmentation of 3D MR images remains a challenging task due to the complexity and variability of the images. In this paper, we propose a novel approach for semantic segmentation of 3D MR images based on a holistic decomposition convolution (HDC) network. The HDC network is designed to effectively capture the complex and high-dimensional nature of 3D MR images, enabling accurate and efficient segmentation. The network is composed of multiple convolutional layers, each of which is designed to capture a specific aspect of the image, such as intensity, texture, and shape. The outputs of the convolutional layers are then combined using a fusion layer to produce a final segmentation output. We evaluated the performance of the HDC network on a challenging dataset of 3D MR images and compared it to state-of-the-art approaches. The results show that the HDC network significantly improves the segmentation accuracy and efficiency, making it a promising approach for the semantic segmentation of 3D MR images."
    },
    {
      "id": "de849348-8ee1-42f3-a6d9-81c8ee22034f",
      "adv_source_id": "de849348-8ee1-42f3-a6d9-81c8ee22034f",
      "source_id": "45731520-dba4-4450-b64b-9dde0330cbdb",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images",
      "prompt": "Write the abstract for the academic paper titled \"Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images\".",
      "generation": "The abstract for the academic paper titled \"Holistic Decomposition Convolution for Effective Semantic Segmentation of 3D MR Images\" could be:\n\nAbstract: Holistic decomposition convolution (HDC) is a novel approach for effective semantic segmentation of 3D MR images. HDC combines locally-decimated Convolutional Neural Networks (CNNs) with a 3D MR image segmentation algorithm to achieve accurate and efficient segmentation. The proposed method utilizes a sparse convolutional layer to capture global context and a dense convolutional layer to capture local details. The sparse layer utilizes a novel holistically-decomposed convolutional filter that enables the network to efficiently capture the global context of the input image. The dense layer utilizes a standard convolutional filter to capture local details. The proposed method has been evaluated on the MRI segmentation challenge 2017 dataset and achieved competitive results compared to other state-of-the-art methods."
    },
    {
      "id": "9d03506c-50ce-4813-a969-eb7ba378177a",
      "adv_source_id": "9d03506c-50ce-4813-a969-eb7ba378177a",
      "source_id": "a3e91b6c-ef13-40cb-b0bd-7c551b74dc7c",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SwipeCut: Interactive Segmentation with Diversified Seed Proposals",
      "prompt": "Write the abstract for the academic paper titled \"SwipeCut: Interactive Segmentation with Diversified Seed Proposals\".",
      "generation": "Here is the abstract for the academic paper titled \"SwipeCut: Interactive Segmentation with Diversified Seed Proposals\":\n\nAbstract: Traditional interactive segmentation methods rely on expensive, post-processing refinement steps to improve initial, predominantly rectangular, segment proposals. In this work, we propose to diversify the set of initial segment proposals by augmenting the predictions of a segmentation model. Our approach, SwipeCut, actively involves the user in the segmentation process by allowing them to quickly swipe to cut out interesting regions-of-interest (RoIs) from the augmented proposal set. We demonstrate that this simple interface leads to more accurate and preferred segmentations compared to existing methods, even without any post-processing refinement. We perform an extensive analysis to understand the effectiveness of SwipeCut and show that our method consistently produces more accurate segmentations than the state-of-the-art across a variety of images and users."
    },
    {
      "id": "929e17fa-6b3e-4e79-a5fb-82e4a555c75e",
      "adv_source_id": "929e17fa-6b3e-4e79-a5fb-82e4a555c75e",
      "source_id": "a3e91b6c-ef13-40cb-b0bd-7c551b74dc7c",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SwipeCut: Interactive Segmentation with Diversified Seed Proposals",
      "prompt": "Write the abstract for the academic paper titled \"SwipeCut: Interactive Segmentation with Diversified Seed Proposals\".",
      "generation": "Here is the abstract for the academic paper titled \"SwipeCut: Interactive Segmentation with Diversified Seed Proposals\":\n\nAbstract: Traditional interactive segmentation methods rely on expensive iterative optimization processes, which hinders their applicability to large images. In this paper, we propose SwipeCut, a fast and efficient interactive segmentation method that leverages diversified seed proposals to achieve accurate results. Our approach combines the speed of greedy algorithms with the benefits of interactive segmentation, allowing users to quickly and easily segment objects in large images. We evaluate our method on the Cityscapes dataset and demonstrate that it achieves state-of-the-art performance in terms of both speed and accuracy. SwipeCut is a versatile and user-friendly tool that has the potential to revolutionize the field of interactive segmentation."
    },
    {
      "id": "af0ea82e-83f8-463c-a27a-2b016d5d9f5c",
      "adv_source_id": "af0ea82e-83f8-463c-a27a-2b016d5d9f5c",
      "source_id": "0cf92d6f-211c-4c83-8c42-c22cf86edde4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks",
      "prompt": "Write the abstract for the academic paper titled \"NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks\".",
      "generation": "The paper presents a novel approach for computing the graph Laplacian of fully connected networks using Non-Uniform FFT (NFFT) techniques in combination with Krylov methods. The graph Laplacian is a fundamental matrix in graph theory and appears in many applications such as network analysis, image processing, and machine learning. The proposed method exploits the structure of the fully connected network to achieve significant speedups compared to existing methods. In particular, the paper discusses how to compute the matrix-vector products involving the graph Laplacian efficiently using NFFT and Krylov methods. The paper also includes experimental results that demonstrate the performance of the proposed method on a variety of networks."
    },
    {
      "id": "d423923f-374e-41d3-ab47-d2c70615a9b0",
      "adv_source_id": "d423923f-374e-41d3-ab47-d2c70615a9b0",
      "source_id": "0cf92d6f-211c-4c83-8c42-c22cf86edde4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks",
      "prompt": "Write the abstract for the academic paper titled \"NFFT meets Krylov methods: Fast matrix-vector products for the graph Laplacian of fully connected networks\".",
      "generation": "The graph Laplacian of a fully connected network has a nice structure which, up to local linear transformations, depends only on the connectivity of the network: the so-called blocking structure. This paper proposes a fast matrix-vector product (MVP) algorithm for the graph Laplacian of fully connected networks that exploits this structure. The algorithm reduces the complexity of the MVP from O(n^3) to O(n), where n is the number of nodes of the network. This result is obtained by combining non-equispaced Fast Fourier Transforms (NFFT) with the Arnoldi method, a variant of the Lanczos method which is a Krylov space method. The NFFT is used to transform the Laplacian into a Hankel matrix whose Hankel structure is well suited for the application of the Arnoldi method. The proposed algorithm is compared to state-of-the-art algorithms for sparse matrices, and numerical results show that the proposed algorithm is faster in many cases."
    },
    {
      "id": "a838e290-b6fe-4279-a8aa-351627e14406",
      "adv_source_id": "a838e290-b6fe-4279-a8aa-351627e14406",
      "source_id": "9a574030-0f87-43e1-9dec-5b367ec4bf89",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation\".",
      "generation": "The abstract for the academic paper titled \"Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation\" would be:\n\nExclusive Independent Probability (EIP) estimation is a technique commonly used to solve difficult binary segmentation problems, such as the IsoIntense Infant Brain MRI segmentation challenge. We apply a 3D fully convolutional densenet (FCD) to estimate EIPs from MR images. Our method directly estimates the EIPs on small patches without involving any histogram estimation, thus avoiding the labeling of foreground and background regions, as required by previous methods. We evaluate our method on the IsoIntense Infant Brain MRI segmentation challenge and achieve a competitive performance. Our code and models are publicly available at https://github.com/FCDNet/FCDNet."
    },
    {
      "id": "5eabc4c1-50b3-4d3e-95dd-10180bbf7c84",
      "adv_source_id": "5eabc4c1-50b3-4d3e-95dd-10180bbf7c84",
      "source_id": "9a574030-0f87-43e1-9dec-5b367ec4bf89",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation\".",
      "generation": "The abstract for the academic paper titled \"Exclusive Independent Probability Estimation using Deep 3D Fully Convolutional DenseNets: Application to IsoIntense Infant Brain MRI Segmentation\" would be:\n\nExclusive Independent Probability (EIP) estimation is a technique to address the spatial coherence of histogram-based intensity normalization. In this paper, we propose an approach to estimate EIPs from 3D volumes using deep learning. Our method leverages 3D fully convolutional dense neural networks (FC-Dnets) to predict EIPs from MRI volumes. We evaluate our method on the task of IsoIntense Infant Brain MRI segmentation, where EIPs are used to normalize the intensity of MRI volumes by removing the global intensity trend across brain scans. Our method achieves state-of-the-art performance on this task, outperforming previous methods in terms of segmentation accuracy and computational efficiency. We show that our method can estimate EIPs from a single 3D MRI volume, and can generalize well to different MRI scanner types and scan protocols."
    },
    {
      "id": "6f01c760-c339-4bf0-8ae1-9601d35b4681",
      "adv_source_id": "6f01c760-c339-4bf0-8ae1-9601d35b4681",
      "source_id": "dce41517-e86c-4d65-a674-43352e023a02",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features",
      "prompt": "Write the abstract for the academic paper titled \"Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features\".",
      "generation": "Here is the abstract for the academic paper titled \"Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features\":\n\nAbstract: Brain tumor segmentation and survival prediction are essential tasks in clinical imaging. In this paper, we propose a novel ensemble of 3D U-Nets for brain tumor segmentation and radiomic features for overall survival prediction. The ensemble of 3D U-Nets is designed to effectively segment brain tumors in 3D MRI images, while the radiomic features are extracted from these segmented images to predict overall survival. The proposed method was evaluated on the BraTS 2020 dataset and achieved a dice similarity coefficient of 0.83 and 0.81 for the tumor and tumor core segmentation, respectively. The radiomic features were able to predict overall survival with an AUC-ROC of 0.79. The proposed method shows promise for clinical use, and further evaluation is required."
    },
    {
      "id": "8ecb41e7-d2bd-4858-9309-e6761beed055",
      "adv_source_id": "8ecb41e7-d2bd-4858-9309-e6761beed055",
      "source_id": "dce41517-e86c-4d65-a674-43352e023a02",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features",
      "prompt": "Write the abstract for the academic paper titled \"Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features\".",
      "generation": "Here is the abstract for the academic paper titled \"Brain Tumor Segmentation using an Ensemble of 3D U-Nets and Overall Survival Prediction using Radiomic Features\":\n\nAbstract: Brain tumor segmentation and survival prediction is a challenging task in clinical practice. In this paper, we present a 3D U-Net based ensemble approach for brain tumor segmentation and overall survival prediction using radiomic features. Our ensemble consists of multiple 3D U-Nets, each trained on a different subset of the training data and combining their predictions to produce the final segmentation. We evaluate our approach on the multimodal brain tumor image dataset and compare it to the state-of-the-art methods. Our results show that our ensemble approach improves the segmentation performance and overall survival prediction compared to the single 3D U-Net. We also investigate the use of radiomic features for overall survival prediction and observe that the proposed features can improve the prediction performance."
    },
    {
      "id": "ba680a22-a255-4687-95b9-907f14bb743b",
      "adv_source_id": "ba680a22-a255-4687-95b9-907f14bb743b",
      "source_id": "a40cf626-40ec-439a-8944-5eb955709f88",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Foreground Clustering for Joint Segmentation and Localization in Videos and Images",
      "prompt": "Write the abstract for the academic paper titled \"Foreground Clustering for Joint Segmentation and Localization in Videos and Images\".",
      "generation": "An important problem in computer vision is to understand the 3D layout of the world from a video or a collection of images. In recent years, there has been significant progress in monocular depth estimation from a single image. However, most algorithms require a non-null baseline, which is harder to estimate in videos, or additional calibration, which is hard to obtain in the wild. In this paper, we propose a simple yet effective method to perform joint segmentation and localization in videos and images without needing any baseline or calibration. Our method Foreground Clustering is based on a simple observation: if we can cluster pixels that belong to the same object across frames, then we can use the cluster centroids as 3D localization points. We show how this idea can be made effective by using a simple, deterministic clustering algorithm, and discuss the challenges of doing so. We demonstrate the accuracy of our method on the KITTI odometry dataset, and show that our method outperforms the state of the art in both accuracy and speed."
    },
    {
      "id": "29c0f6e7-984f-4bea-9a54-7ac90b4602d7",
      "adv_source_id": "29c0f6e7-984f-4bea-9a54-7ac90b4602d7",
      "source_id": "a40cf626-40ec-439a-8944-5eb955709f88",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Foreground Clustering for Joint Segmentation and Localization in Videos and Images",
      "prompt": "Write the abstract for the academic paper titled \"Foreground Clustering for Joint Segmentation and Localization in Videos and Images\".",
      "generation": "An important problem in computer vision is to understand the 3D layout of the world from a video or a collection of images. In recent years, there has been significant progress in monocular depth estimation from a single image. However, most approaches focus on depth estimation and do not provide a full 3D reconstruction as an output. On the other hand, 3D object detection methods provide a 3D box around each object instance in a scene, but they do not explicitly reconstruct the 3D geometry of the scene or the surfaces of the objects. In this paper, we attempt to bridge this gap by proposing a new method for joint segmentation and localization in videos and images. Our method combines foreground clustering with a new 3D localization algorithm that uses a set of point-based features to localize objects in a scene. We demonstrate the efficacy of our approach on the Cityscapes dataset and show that our method can accurately segment and localize objects in a scene, resulting in a full 3D reconstruction of the scene."
    },
    {
      "id": "34f2df31-a1b3-47ac-9288-6a96c68f58cb",
      "adv_source_id": "34f2df31-a1b3-47ac-9288-6a96c68f58cb",
      "source_id": "dd063bd6-2998-43c7-9dbd-011fc7990abe",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans",
      "prompt": "Write the abstract for the academic paper titled \"RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans\".",
      "generation": "The abstract for the academic paper titled \"RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans\" could be:\n\nThis paper presents a hybrid deep attention-aware network (RA-UNet) for the extraction of liver and tumor in CT scans. The network is designed to address the challenge of extracting complex anatomical structures from medical images, which are often noisy and contain various artifacts. The RA-UNet combines a U-Net architecture with a Recurrent Attention (RA) module, which learns to focus on relevant regions of the input image while suppressing noise and artifacts.\n\nThe RA-UNet is trained on a large dataset of CT scans and evaluated on a separate test dataset. The results show that the network is able to extract liver and tumor with high accuracy and precision, outperforming existing state-of-the-art methods. The network is also able to generalize well to unseen data, demonstrating its ability to extract anatomical structures from a wide range of CT scans.\n\nThe paper concludes by discussing the potential applications of the RA-UNet in medical image analysis and the possibility of integrating the network into clinical workflows. The RA-UNet has the potential to improve the accuracy and efficiency of medical image analysis, which could lead to better patient outcomes and reduced costs."
    },
    {
      "id": "ba640a5c-1da0-4a32-8d6a-dfa1109070fa",
      "adv_source_id": "ba640a5c-1da0-4a32-8d6a-dfa1109070fa",
      "source_id": "dd063bd6-2998-43c7-9dbd-011fc7990abe",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans",
      "prompt": "Write the abstract for the academic paper titled \"RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans\".",
      "generation": "The abstract for the academic paper titled \"RA-UNet: A hybrid deep attention-aware network to extract liver and tumor in CT scans\" could be:\n\nThis paper proposes a hybrid deep attention-aware network (RA-UNet) for the extraction of liver and tumor in CT scans. The network combines the strengths of a UNet-based architecture and a attention-based mechanism to improve the accuracy and efficiency of the extraction process. The RA-UNet utilizes a series of convolutional layers and attention mechanisms to capture the spatial and contextual information in the CT scans. The proposed network was evaluated on a public dataset and achieved state-of-the-art performance in terms of accuracy and efficiency. The results demonstrate the effectiveness of the proposed approach in extracting liver and tumor in CT scans."
    },
    {
      "id": "38b10470-8676-4ee7-b59a-7d18adc1edf7",
      "adv_source_id": "38b10470-8676-4ee7-b59a-7d18adc1edf7",
      "source_id": "c5e404c8-c6dd-494b-994a-42e253ff8d8c",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images",
      "prompt": "Write the abstract for the academic paper titled \"Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images\".",
      "generation": "An algorithm for dense deformable registration of volume images based on graph cuts is presented. It optimizes a cost function of the similarity between voxels, using a fast graph cut algorithm that efficiently converges to the global minimizer. A multiphase approach is used to incorporate prior knowledge about the initialization, and a smoothness constraint is imposed using total variation as a regularization term. The method is applied to synthetic and real volume images, and the results are evaluated using metrics that measure the similarity between the deformed and fixed images, as well as the speed of the registration process. The experimental results show that the method is effective in registering volume images quickly and accurately."
    },
    {
      "id": "edcb2073-67fc-46a6-a9a5-3cb2b166e5d4",
      "adv_source_id": "edcb2073-67fc-46a6-a9a5-3cb2b166e5d4",
      "source_id": "c5e404c8-c6dd-494b-994a-42e253ff8d8c",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images",
      "prompt": "Write the abstract for the academic paper titled \"Fast Graph-Cut Based Optimization for Practical Dense Deformable Registration of Volume Images\".",
      "generation": "An algorithm for dense deformable registration of volume images based on graph cuts optimization is presented in this paper. The algorithm is designed to achieve fast and accurate registration by optimizing a cost function that captures the intensity similarities between the volume images and the spatial relationships between their corresponding pixels. The contributions of this work are threefold. First, a fast graph-cut based optimization algorithm is developed to efficiently solve the non-convex, non-smooth, and highly nonlinear registration problem. Unlike most previous methods, the proposed algorithm does not require any parameter tuning or initialization. Second, a novel dense deformation vector field estimation approach is proposed, which is capable of accurately estimating the non-linear deformation between the volume images. Third, the proposed algorithm is evaluated on a public dataset, and the results show that it achieves accurate registration while being significantly faster than state-of-the-art algorithms."
    },
    {
      "id": "7ea6aadb-28d0-40f9-95d0-dfda64e77bee",
      "adv_source_id": "7ea6aadb-28d0-40f9-95d0-dfda64e77bee",
      "source_id": "38aab8a9-49b3-4d95-8775-7c898d09a3da",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Domain Adaptation Framework for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Novel Domain Adaptation Framework for Medical Image Segmentation\".",
      "generation": "Abstract:\nDomain adaptation is a crucial problem in medical image segmentation, as the availability of labeled data in the target domain is often limited. In this paper, we propose a novel domain adaptation framework that leverages the use of labeled data in the source domain and unlabeled data in the target domain. Our framework consists of a domain adaptation module and a segmentation module. The domain adaptation module utilizes a convolutional neural network (CNN) to learn the domain-invariant features from the source domain, and then adapts the CNN to the target domain using unlabeled data. The segmentation module takes the adapted CNN and applies it to segment the medical images in the target domain. We evaluate our framework on the task of brain tumor segmentation in magnetic resonance (MR) images, and demonstrate that it outperforms the state-of-the-art approaches."
    },
    {
      "id": "c9ca6c82-e79d-484b-989d-02787c003b6e",
      "adv_source_id": "c9ca6c82-e79d-484b-989d-02787c003b6e",
      "source_id": "38aab8a9-49b3-4d95-8775-7c898d09a3da",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Domain Adaptation Framework for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Novel Domain Adaptation Framework for Medical Image Segmentation\".",
      "generation": "Abstract:\nDomain adaptation is a crucial challenge in medical image segmentation, as the availability of labeled data from the target domain is often limited. In this paper, we propose a novel domain adaptation framework that leverages the power of self-supervised learning and conditional generation to reduce the gap between source and target domains. Our approach consists of two main components: a self-supervised learning module that learns a shared representation across domains, and a conditional generation module that generates target domain samples based on the learned representation. By combining these components, we achieve state-of-the-art performance on multiple medical image segmentation benchmarks, despite limited target domain data. We demonstrate the effectiveness of our approach on segmentation tasks for lung, brain, and liver CT images."
    },
    {
      "id": "5743dfee-47f6-49f0-93ea-c08e25902e28",
      "adv_source_id": "5743dfee-47f6-49f0-93ea-c08e25902e28",
      "source_id": "0d25935a-141c-45ee-b6b0-7c84c0fb6365",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation\".",
      "generation": "Interactive image segmentation is a fundamental task in computer vision with a wide range of applications in the field of medical imaging, autonomous driving, and robotics. In this paper, we propose a fully convolutional two-stream fusion network (FC-TSFN) that combines the strengths of two-stream networks and fully convolutional networks for interactive image segmentation. Our network is composed of a low-level feature stream and a high-level feature stream, which are processed separately and then fused to produce the final segmentation output. The low-level feature stream captures fine-grained details and edge information, while the high-level feature stream captures global context and object boundaries. Our proposed network is fully convolutional, allowing for efficient inference and real-time segmentation. We evaluate our network on the Cityscapes and CamVid datasets, and our results show that our network outperforms the state-of-the-art in terms of segmentation accuracy and speed."
    },
    {
      "id": "c83f4987-b98e-4c8b-9777-ab5cd50de3fe",
      "adv_source_id": "c83f4987-b98e-4c8b-9777-ab5cd50de3fe",
      "source_id": "0d25935a-141c-45ee-b6b0-7c84c0fb6365",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Fully Convolutional Two-Stream Fusion Network for Interactive Image Segmentation\".",
      "generation": "Interactive image segmentation is an essential step for medical image analysis, which aims to accurately and efficiently segment different tissues from the medical images to aid doctors in diagnosis and disease treatment. We propose a fully convolutional two-stream fusion network (FC-TSFN) that combines the strengths of both supervised and unsupervised learning for interactive image segmentation. Our network utilizes a two-stream architecture, where one stream is trained with supervised learning and the other with unsupervised learning. The two streams are then combined using a fusion layer that merges the features learned by each stream. Our proposed network was validated on the ISBI challenge 2016 segmentation dataset and achieved excellent performance in terms of both accuracy and speed."
    },
    {
      "id": "3d2c04e4-5c12-43a3-88e6-667ab653000c",
      "adv_source_id": "3d2c04e4-5c12-43a3-88e6-667ab653000c",
      "source_id": "6614f2e5-0751-45d0-9633-265eff0097aa",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS",
      "prompt": "Write the abstract for the academic paper titled \"Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS\".",
      "generation": "The paper presents a method for effective cloud detection and segmentation in satellite imagery using a gradient-based algorithm. The proposed method utilizes the gradient information of the image to detect and segment clouds, resulting in a more accurate and efficient cloud detection process. The method has been applied to improve the performance of the PERSIANN-CCS cloud-detection algorithm. The experimental results show that the proposed method significantly improves the accuracy and efficiency of cloud detection, especially in challenging cases such as high cloud cover and complex terrain.\n\nThe paper discusses the advantages of the proposed method over existing techniques and highlights its potential applications in various fields such as remote sensing, image processing, and environmental monitoring."
    },
    {
      "id": "849e5869-d1ae-45e9-b514-204fc33ad742",
      "adv_source_id": "849e5869-d1ae-45e9-b514-204fc33ad742",
      "source_id": "6614f2e5-0751-45d0-9633-265eff0097aa",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS",
      "prompt": "Write the abstract for the academic paper titled \"Effective Cloud Detection and Segmentation using a Gradient-Based Algorithm for Satellite Imagery; Application to improve PERSIANN-CCS\".",
      "generation": "The paper presents a method for effective cloud detection and segmentation in satellite imagery using a gradient-based algorithm. The algorithm is applied to improve the Performance of the PERSIANN-CCS (Conservation of Conditional Symmetry) method, which is commonly used for cloud detection in satellite imagery. The proposed method utilizes the gradient information of the image to detect and segment clouds, and utilizes a multi-scale approach to improve the performance of the PERSIANN-CCS method. The proposed method achieves a high level of accuracy in cloud detection and segmentation, and can be used to improve the performance of various applications that utilize satellite imagery, such as climate monitoring and weather forecasting. \n\nThe paper focuses on the application of the proposed method to improve the performance of the PERSIANN-CCS method, and includes a comprehensive evaluation of the proposed method using various satellite imagery datasets. The evaluation results demonstrate that the proposed method achieves a high level of accuracy in cloud detection and segmentation, and can effectively improve the performance of the PERSIANN-CCS method.\n\nThe proposed method has several practical applications in various domains, such as climate monitoring, weather forecasting, and environmental monitoring. For example, accurate cloud detection and segmentation can improve the performance of climate models by providing better input data for the models. Furthermore, the proposed method can be used to improve the performance of satellite-based weather forecasting systems by providing more accurate information about the location and movement of clouds. \n\nThe proposed method is a simple and efficient approach that can be used to improve the performance of various applications that utilize satellite imagery. The method utilizes the gradient information of the image to detect and segment clouds, and utilizes a multi-scale approach to improve the performance of the PERSIANN-CCS method. The proposed method is easy to implement and does not require any additional training data or parameters."
    },
    {
      "id": "c511e969-c0fb-417d-892b-5bda0e1a67ce",
      "adv_source_id": "c511e969-c0fb-417d-892b-5bda0e1a67ce",
      "source_id": "4f72dde4-9d71-4c2c-a9e9-19c831d3d2b4",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes",
      "prompt": "Write the abstract for the academic paper titled \"3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes\".",
      "generation": "3D segmentation is an important task in robotics and has many applications including object recognition, and autonomous navigation. In this paper, we propose a novel 3D segmentation approach using an exponential logarithmic loss (ELO) objective function. Our proposed method is designed to handle highly unbalanced object sizes in the 3D point cloud. We evaluate our method on the popular Stanford 3D Segmentation benchmark and show that it achieves state-of-the-art performance. We also show that our method is efficient and runs in real-time on a modern GPU."
    },
    {
      "id": "006ed017-629e-4410-9830-daff51acccf0",
      "adv_source_id": "006ed017-629e-4410-9830-daff51acccf0",
      "source_id": "4f72dde4-9d71-4c2c-a9e9-19c831d3d2b4",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes",
      "prompt": "Write the abstract for the academic paper titled \"3D Segmentation with Exponential Logarithmic Loss for Highly Unbalanced Object Sizes\".",
      "generation": "Abstract:\nThe problem of 3D segmentation of objects with highly unbalanced object sizes is addressed in this paper. The proposed method utilizes an exponential logarithmic loss function that adapts to the size of the objects, allowing for more accurate segmentation of small and thin objects. The network is trained on a large dataset of 3D shapes and is evaluated on the ShapeNet part segmentation benchmark, where it achieves state-of-the-art performance. The proposed method is able to effectively handle objects with varying sizes and shapes, making it suitable for a wide range of applications including medical imaging, autonomous driving, and virtual reality. \n\nThe paper makes the following contributions: (1) A new loss function is proposed that adaptively weights the loss based on the size of the objects, allowing for more accurate segmentation of small and thin objects. (2) The network is trained on a large dataset of 3D shapes and is evaluated on the ShapeNet part segmentation benchmark, where it achieves state-of-the-art performance. (3) The ability to handle objects with varying sizes and shapes is demonstrated, making the method suitable for a wide range of applications."
    },
    {
      "id": "620b6e1d-a3e3-4d75-98d6-3dceb540b077",
      "adv_source_id": "620b6e1d-a3e3-4d75-98d6-3dceb540b077",
      "source_id": "a92f9691-f56a-409b-b928-42b7b872a42e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors",
      "prompt": "Write the abstract for the academic paper titled \"A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors\".",
      "generation": "Below is an abstract for the academic paper titled, \"A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors\":\n\nAbstract\nWe present a time series graph cut image segmentation scheme for liver tumors in ultrasound images. We propose a novel method to extract tumor boundary points (TBPs) from ultrasound frames using a graph cut algorithm. The algorithm utilizes a confidence map, which is generated by a fully convolutional neural network, to guide the segmentation process. The confidence map provides an initial estimate of the tumor region, which is then refined by the graph cut algorithm to produce a final segmentation. The proposed method was evaluated on a dataset of ultrasound images with ground truth annotations. The results show that the proposed method can accurately segment liver tumors, with an average precision of 92.3% and recall of 95.2%. The proposed method has the potential to be used for early detection and monitoring of liver tumors, which can significantly improve patient outcomes."
    },
    {
      "id": "2568ba15-ab60-41c4-9a13-d6d2b7304e95",
      "adv_source_id": "2568ba15-ab60-41c4-9a13-d6d2b7304e95",
      "source_id": "a92f9691-f56a-409b-b928-42b7b872a42e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors",
      "prompt": "Write the abstract for the academic paper titled \"A Time Series Graph Cut Image Segmentation Scheme for Liver Tumors\".",
      "generation": "Abstract.\nMost segmentation schemes rely on either of the two extreme assumptions: (a) the images are homogeneous and (b) the objects of interest have well-defined boundaries, In this paper, we relax both these assumptions in the context of liver tumor segmentation from contrast-enhanced CT (CECT) images. We propose a time series graph cut (TSGC) scheme that exploits the temporal information available from CECT studies. Our scheme is based on a posteriori probability distribution of the tumor label at each voxel given the previous frames, and an a priori probabilistic model learned from the training data. Since the proposed method fully exploits the temporal information, it is suitable for automated segmentation of liver tumors from CECT studies. In the experimental section, we demonstrate the superiority of the proposed method over the state-of-the-art techniques."
    },
    {
      "id": "ec1a2d07-d3bd-45d6-a6db-0e0d12ff4361",
      "adv_source_id": "ec1a2d07-d3bd-45d6-a6db-0e0d12ff4361",
      "source_id": "8ab94b1f-f372-4747-8c52-ec4f4f279997",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark",
      "prompt": "Write the abstract for the academic paper titled \"YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark\".",
      "generation": "Abstract:\nWe introduce YouTube-VOS, a large-scale video object segmentation benchmark comprising over 100,000 videos and 1.5 million annotated objects. The dataset is collected from YouTube and contains a wide variety of scenes, objects, and occlusions. We provide baseline results for the dataset using state-of-the-art object detection and segmentation models, and show that current models perform poorly on YouTube-VOS due to the challenging nature of the dataset. YouTube-VOS is an important step towards enabling research on large-scale video object segmentation."
    },
    {
      "id": "8ed3b82e-f1ca-4558-9fc3-16b7a778c7be",
      "adv_source_id": "8ed3b82e-f1ca-4558-9fc3-16b7a778c7be",
      "source_id": "8ab94b1f-f372-4747-8c52-ec4f4f279997",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark",
      "prompt": "Write the abstract for the academic paper titled \"YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark\".",
      "generation": "Abstract:\nWe introduce YouTube-VOS, a large-scale video object segmentation benchmark comprising 3,746 YouTube videos and 49,588 annotated object segments. To collect this dataset, we augmented existing video object segmentation datasets as well as synthesized new data using computer graphics. We propose an extension of the foreground/background assumption for videos using a temporal decision boundary that exploits the continuity of object segments over time. This allows us to directly apply existing image object segmentation models to video without the need for any additional temporal modeling. We evaluate the state-of-the-art in video object segmentation on our new benchmark, and provide an in-depth analysis of the models and dataset."
    },
    {
      "id": "3a257447-8567-4979-9fe3-83dcf2ab31ce",
      "adv_source_id": "3a257447-8567-4979-9fe3-83dcf2ab31ce",
      "source_id": "b5146fec-a44a-41a4-bd1b-f4e87e0812cf",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera",
      "prompt": "Write the abstract for the academic paper titled \"Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera\".",
      "generation": "Abstract:\nIris recognition is a well-known biometric identification method. In this paper, we present a system for iris recognition that uses a database of iris images obtained in visible light with a smartphone camera. The system consists of a feature extraction module and a classification module. The feature extraction module uses a combination of Gaussian smoothing and the Local Binary Pattern (LBP) operator to extract features from the iris images. The classification module uses a support vector machine (SVM) classifier to match the extracted features against the database of iris images. The system was tested on a database of 100 iris images and achieved a recognition rate of 92%. Our approach shows that iris recognition is feasible with a database of iris images obtained in visible light using a smartphone camera."
    },
    {
      "id": "a3db81b0-2af5-453f-8f77-d85ce79f0c1f",
      "adv_source_id": "a3db81b0-2af5-453f-8f77-d85ce79f0c1f",
      "source_id": "b5146fec-a44a-41a4-bd1b-f4e87e0812cf",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera",
      "prompt": "Write the abstract for the academic paper titled \"Iris Recognition with a Database of Iris Images Obtained in Visible Light Using Smartphone Camera\".",
      "generation": "The paper presents a practical approach towards implementing an iris recognition system using a database of iris images obtained in visible light with a smartphone camera as the capture device. The study aimed to investigate the feasibility of developing an automated biometric recognition system for the iris, and to evaluate its performance compared to other biometric traits such as fingerprints and face recognition. In this work, a total of 132 iris images from 65 subjects were collected using a smartphone camera in a controlled environment. The collected images were then preprocessed, and an automated segmentation algorithm was applied to extract the iris region from the image. The extracted iris images were then compared to the enrolled images in the database using a matching algorithm based on the Euclidean distance metric. \n\nThe experimental results showed that the developed system achieved an overall accuracy of 95.12%, which is comparable to other iris recognition systems. In addition, the results showed that the system was able to correctly recognize the iris of the enrolled subjects with a high degree of accuracy, and that the recognition rate was not significantly affected by factors such as the distance between the smartphone camera and the subject's eye, or the lighting conditions in which the images were captured. These results demonstrate the feasibility of developing an automated iris recognition system using a database of iris images obtained in visible light with a smartphone camera.\n\nFurthermore, our proposed approach showcases the potential of using deep learning techniques to perform iris recognition using images captured with readily available devices such as smartphones. This work can be further extended to improve the robustness of the system in terms of lighting conditions, eye position, and other factors that may affect the quality of the captured iris images."
    },
    {
      "id": "4381770f-a304-456c-98cc-aa9338d1a0aa",
      "adv_source_id": "4381770f-a304-456c-98cc-aa9338d1a0aa",
      "source_id": "f6cb3441-e401-4b16-a8bc-9e6b55f7717d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation of Microscopy Data for finding Nuclei in Divergent Images",
      "prompt": "Write the abstract for the academic paper titled \"Segmentation of Microscopy Data for finding Nuclei in Divergent Images\".",
      "generation": "An unsupervised machine learning approach has been followed to segment microscopy data of cell images obtained from the Cancer Cell Line Encyclopedia (CCLE) database. To achieve this, an automated approach based on a combination of Gaussian mixture models (GMMs) and Expectation-Maximization (EM) algorithm has been implemented to segment nuclei regions in a pre-processing step. In order to verify the efficiency of the proposed approach, the segmented output was compared with ground truth images. The performance evaluation was conducted in terms of four measures that are Recall, Precision, F-measure and Area Under the Receiver Operating Characteristic (AUC-ROC). The implementation shows a high AUC-ROC score of 0.94."
    },
    {
      "id": "9e00b377-d334-4517-9b25-9517ec970d93",
      "adv_source_id": "9e00b377-d334-4517-9b25-9517ec970d93",
      "source_id": "f6cb3441-e401-4b16-a8bc-9e6b55f7717d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Segmentation of Microscopy Data for finding Nuclei in Divergent Images",
      "prompt": "Write the abstract for the academic paper titled \"Segmentation of Microscopy Data for finding Nuclei in Divergent Images\".",
      "generation": "Abstract: \nThe process of segmentation involves the identification of nuclei regions in images. However, the nuclei in microscopy images often appear in diverse shapes, sizes, and intensities, which makes it difficult to accurately identify and extract them. We propose a method of segmentation that combines both supervised and unsupervised techniques to tackle this challenge. Our supervised approach uses a convolutional neural network (CNN) trained on a large dataset of images, while our unsupervised approach relies on a density-based clustering algorithm to identify nuclei regions based on their density. The combined approach achieves a high level of accuracy in identifying nuclei regions, outperforming existing methods in terms of both precision and recall. Furthermore, our approach is able to handle a large variety of images and is robust to variations in the size, shape, and intensity of the nuclei. We believe that our method will be a valuable tool for biological image analysis and other applications requiring precise segmentation of microscopic images."
    },
    {
      "id": "7155d8b3-d32c-42c7-8d53-47dd1fcc129a",
      "adv_source_id": "7155d8b3-d32c-42c7-8d53-47dd1fcc129a",
      "source_id": "f7da78e0-ee78-459a-bc61-93d7d103b66c",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concept Mask: Large-Scale Segmentation from Semantic Concepts",
      "prompt": "Write the abstract for the academic paper titled \"Concept Mask: Large-Scale Segmentation from Semantic Concepts\".",
      "generation": "An important prerequisite for many higher-level computer vision tasks is the segmentation of objects in the image. We introduce Concept Mask, a new approach for semantic segmentation that bypasses the need to design pixel-level annotations and instead leverages a small number of bounding box annotations and a semantic concept dictionary. Our key observation is that the semantic concept labels of objects can be used to create a coarse mask for the entire object, which can then be refined by a segmentation model. We show how to optimize the refinement process using a new loss function that encourages the predicted mask to be sharp and accurate. We demonstrate the effectiveness of our approach on the Cityscapes dataset, achieving state-of-the-art performance with only a fraction of the labeled data typically required. Our approach is particularly useful for scenarios where the collection of pixel-level annotations is costly and time-consuming."
    },
    {
      "id": "753186ac-36f0-4c08-b821-4cc17fd10b0b",
      "adv_source_id": "753186ac-36f0-4c08-b821-4cc17fd10b0b",
      "source_id": "f7da78e0-ee78-459a-bc61-93d7d103b66c",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concept Mask: Large-Scale Segmentation from Semantic Concepts",
      "prompt": "Write the abstract for the academic paper titled \"Concept Mask: Large-Scale Segmentation from Semantic Concepts\".",
      "generation": "Abstract:\nTraditional image segmentation methods are built upon strong assumptions about the images, such as pixel similarity within segments or appearance similarity between objects and their surroundings. These methods perform poorly when these assumptions are violated, which is often the case in real-world applications. We propose Concept Mask, a segmentation method that makes no assumptions about the images other than the presence of a semantic concept in the image. Concept Mask uses a large number of annotated images from the Conceptual Images Collection (Flickr), which contains images labeled with high-level semantic concepts. We train a convolutional neural network to classify whether a given image contains a specific concept. We then use the neural network as a component in a graph cut optimization framework to refine the segmentation based on the concept label. Our system is able to segment images of objects that have never been seen before, and is competitive with state-of-the-art segmentation methods on standard benchmark datasets."
    },
    {
      "id": "f6f06482-faf5-4b92-95a5-85bd69778625",
      "adv_source_id": "f6f06482-faf5-4b92-95a5-85bd69778625",
      "source_id": "124e3778-cc90-4e40-bae4-1f7b655fa8bd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation\".",
      "generation": "The abstract for the academic paper titled \"A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation\" would be:\n\nMedical image segmentation is crucial for the diagnosis and treatment of various diseases. We propose a 3D coarse-to-fine framework for volumetric medical image segmentation that incorporates a fully convolutional network (FCN) and a conditional random field (CRF). The FCN is trained on a sparse set of labeled voxels, which are obtained by applying a threshold to the intensity values of the medical image. The CRF is then used to refine the segmentation results obtained by the FCN. The proposed framework is evaluated on a dataset of abdominal computed tomography (CT) scans and compared to a state-of-the-art method. The results show that the proposed framework improves the segmentation accuracy and is able to handle large variations in the intensity values of the medical images."
    },
    {
      "id": "56ee32b5-17a4-464d-a462-5e6a57e60fc7",
      "adv_source_id": "56ee32b5-17a4-464d-a462-5e6a57e60fc7",
      "source_id": "124e3778-cc90-4e40-bae4-1f7b655fa8bd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation\".",
      "generation": "Below is an abstract for the academic paper titled \"A 3D Coarse-to-Fine Framework for Volumetric Medical Image Segmentation\":\n\nMedical image segmentation is crucial in clinical practice as it enables the detection and accurate representation of anatomical structures. This paper presents a 3D coarse-to-fine framework for volumetric medical image segmentation. The proposed method combines an encoder-decoder architecture with a fully connected conditional random field (CRF). We utilize a coarse-to-fine strategy, where the encoder-decoder architecture segments the image in a coarse-to-fine manner, and the CRF refines the segmentation results. The proposed method has been trained and evaluated on the ISLES 2017 Challenge dataset, which contains 320 kV computed tomography (CT) scans of lungs. The experimental results show that the proposed method achieves a dice similarity coefficient (DSC) of 0.9283 and a Hausdorff distance (HD) of 6.58 mm on the validation dataset. The code and dataset used in this study are available at https://github.com/ Execution-Legend/3D-Segmentation-CVPR-2023."
    },
    {
      "id": "5a2f61ad-87f8-4496-9afd-3f89ba8bbc49",
      "adv_source_id": "5a2f61ad-87f8-4496-9afd-3f89ba8bbc49",
      "source_id": "92208120-efed-4ea8-8828-a80528a442d8",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A multi-contrast MRI approach to thalamus segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A multi-contrast MRI approach to thalamus segmentation\".",
      "generation": "An automated segmentation of the thalamus is a fundamental step towards building a normative database for thalamus morphology and ultimately enabling accurate diagnosis and treatment of thalamically mediated diseases. We propose a method for fast and robust segmentation of the thalamus in multi-contrast magnetic resonance images (MRI). The method is based on a multi-stage approach, using a combination of automated segmentation and expert annotation. It has been validated on a large dataset of MRI scans from 100 patients and has achieved high accuracy and reproducibility. The proposed method has the potential to be used as a tool for automated segmentation of the thalamus in clinical practice, helping to improve the diagnosis and treatment of thalamically mediated diseases."
    },
    {
      "id": "031c6836-7142-470d-a3a9-b56f18b6e5a0",
      "adv_source_id": "031c6836-7142-470d-a3a9-b56f18b6e5a0",
      "source_id": "92208120-efed-4ea8-8828-a80528a442d8",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A multi-contrast MRI approach to thalamus segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A multi-contrast MRI approach to thalamus segmentation\".",
      "generation": "Abstract:\nThis paper presents a multi-contrast magnetic resonance imaging (MRI) approach for segmenting the thalamus in neuroimaging datasets. The proposed method combines the contrast enhancements of T1-weighted, T2-weighted, and diffusion-weighted MRI to improve the accuracy and robustness of thalamus segmentation. The T1-weighted image provides structural information, the T2-weighted image highlights anatomical details, and the diffusion-weighted image captures white matter connectivity. A fully automated segmentation algorithm based on a deep learning architecture is developed to exploit the complementary information from these contrasts. The algorithm utilizes a U-net architecture with an encoder-decoder structure and incorporates residual connections for improved performance. The multi-contrast MRI data are preprocessed to enhance the discriminatory power of the features used for segmentation. The proposed method is evaluated on a public dataset and compared to existing state-of-the-art methods. The results demonstrate the superiority of the proposed approach in terms of segmentation accuracy and robustness. This multi-contrast MRI-based method can serve as a valuable tool for clinical research and disease diagnosis, enabling reliable and reproducible thalamus segmentation."
    },
    {
      "id": "834776ac-4d08-4851-857d-3c1ad1c4a40a",
      "adv_source_id": "834776ac-4d08-4851-857d-3c1ad1c4a40a",
      "source_id": "9e0eccf7-89ba-4900-9c48-e03ac204d395",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"UNet++: A Nested U-Net Architecture for Medical Image Segmentation\".",
      "generation": "In this paper, we present \"UNet++,\" an enhanced version of the popular U-Net architecture designed for medical image segmentation. U-Net has shown state-of-the-art performance in various segmentation tasks but has limitations in capturing complex structures and handling large variations in image intensity. To address these limitations, we introduce a nested U-Net architecture that incorporates a series of skip connections between the encoder and decoder blocks. These skip connections enable the network to capture more global information and improve the segmentation performance, particularly in challenging areas of the image. We evaluate UNet++ on three different medical image segmentation datasets, and our results show significant improvements over the original U-Net architecture in terms of segmentation accuracy and robustness. Our proposed method achieves a mean dice similarity coefficient of 0.9143, 0.9189, and 0.9146 on the MNIST-digits, 2D-chest-radiology, and 3D-kidney datasets, respectively."
    },
    {
      "id": "77779979-8bf3-4d37-ab73-0c15e2aa19b4",
      "adv_source_id": "77779979-8bf3-4d37-ab73-0c15e2aa19b4",
      "source_id": "9e0eccf7-89ba-4900-9c48-e03ac204d395",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "UNet++: A Nested U-Net Architecture for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"UNet++: A Nested U-Net Architecture for Medical Image Segmentation\".",
      "generation": "In this paper, we present \"UNet++,\" an improved version of the popular U-Net architecture for medical image segmentation. U-Net is a convolutional neural network (CNN) that has shown promising results in various medical image segmentation tasks. However, it can be challenging to train U-Net on certain datasets due to its fully connected structure, which can lead to overfitting and poor generalization performance. To address these issues, we propose a nested U-Net architecture that incorporates a series of dilated convolutions and residual connections to improve the model's performance. Our proposed architecture is easy to train and generalize, and we demonstrate its effectiveness on three different medical image segmentation datasets, achieving state-of-the-art performance in two of them. We believe that our proposed architecture will serve as a strong foundation for future research in medical image segmentation."
    },
    {
      "id": "7179ce12-2ce5-442d-8080-df69ddc1b507",
      "adv_source_id": "7179ce12-2ce5-442d-8080-df69ddc1b507",
      "source_id": "625ed860-7a58-4842-8055-b1da093316d0",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Data-Driven Segmentation of Post-mortem Iris Images",
      "prompt": "Write the abstract for the academic paper titled \"Data-Driven Segmentation of Post-mortem Iris Images\".",
      "generation": "The accurate segmentation of post-mortem iris images is a fundamental step towards the development of a robust automated system for iris recognition of deceased individuals. In this paper, we present a data-driven approach to the segmentation of post-mortem iris images. Our approach is based on a deep learning architecture that has been specifically designed for the task of iris segmentation. We utilize a combination of supervised and unsupervised learning techniques to train our model. Our supervised learning approach employs a dataset of pre-mortem iris images, while our unsupervised learning approach utilizes a dataset of post-mortem iris images. The proposed method has been evaluated on a dataset of post-mortem iris images, and the results show that it significantly outperforms the state-of-the-art in terms of accuracy. We believe that the proposed method has the potential to become a valuable tool for the identification of deceased individuals in forensic settings."
    },
    {
      "id": "f00715fe-723a-4f7a-8d7e-bb6ae13520bb",
      "adv_source_id": "f00715fe-723a-4f7a-8d7e-bb6ae13520bb",
      "source_id": "625ed860-7a58-4842-8055-b1da093316d0",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Data-Driven Segmentation of Post-mortem Iris Images",
      "prompt": "Write the abstract for the academic paper titled \"Data-Driven Segmentation of Post-mortem Iris Images\".",
      "generation": "The rising application of biometric technology has resulted in a greater need for automated systems that can enhance the post-mortem iris image quality and segment the iris region from the acquired images for further processing. In this paper, we propose a data-driven approach that leverages a fully connected neural network to achieve both image quality enhancement and region segmentation of post-mortem iris images. The proposed approach has been evaluated on a public dataset containing images acquired from both living and post-mortem eyes, and the results demonstrate its effectiveness in improving the image quality and segmenting the iris region.\n\nFurthermore, our proposed approach achieves a high segmentation performance, surpassing the state-of-the-art methods in the literature. The proposed network leverages a multi-scale strategy to capture the underlying hierarchical structure of the iris, allowing it to outperform other segmentation methods that rely on heuristic rules or tedious parameter tuning."
    },
    {
      "id": "0686bea4-bd03-4098-a111-3266468e4b99",
      "adv_source_id": "0686bea4-bd03-4098-a111-3266468e4b99",
      "source_id": "921fbb2b-0995-4de5-adc1-87409d109abd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Function Norms and Regularization in Deep Networks",
      "prompt": "Write the abstract for the academic paper titled \"Function Norms and Regularization in Deep Networks\".",
      "generation": "Abstract:\nFunction norms play a crucial role in regularization of deep networks, allowing for the prevention of overfitting and improved generalization performance. In this paper, we examine the effects of function norms on the optimization of deep networks, focusing on two specific regularization techniques: weight decay and dropout. We analyze the properties of function norms and their impact on the optimization process, highlighting the trade-offs between different norms. Our results show that function norms can have a significant effect on the generalization performance of deep networks, and that choosing the appropriate norm can lead to improved results. Furthermore, we demonstrate that a combination of function norms can enhance regularization and optimize deep networks more effectively. This work provides valuable insights into the role of function norms in the regularization of deep networks, contributing to the advancement of deep learning research."
    },
    {
      "id": "ca311bd0-edc8-4c92-ae47-44d98c19b14b",
      "adv_source_id": "ca311bd0-edc8-4c92-ae47-44d98c19b14b",
      "source_id": "921fbb2b-0995-4de5-adc1-87409d109abd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Function Norms and Regularization in Deep Networks",
      "prompt": "Write the abstract for the academic paper titled \"Function Norms and Regularization in Deep Networks\".",
      "generation": "**Function Norms and Regularization in Deep Networks**\n\nThis paper examines the application of function norms to deep networks, demonstrating their effectiveness as a regularization technique. We introduce a novel method of applying function norms to deep networks, which we call \"function regularization.\" This method involves optimizing the function norms of the weights and biases in the network, in addition to the usual optimization of the network's loss function.\n\nWe show that function regularization can improve the generalization performance of deep networks, leading to better performance on a variety of tasks. We also show that function regularization can help reduce overfitting and improve the stability of the training process.\n\nThis paper discusses the theoretical properties of function norms and their application to deep networks. We also present experimental results that demonstrate the effectiveness of function regularization on a variety of deep learning tasks, including image classification, object detection, and natural language processing.\n\nOverall, this paper presents a novel and effective method of regularizing deep networks, which has the potential to improve the performance and robustness of deep learning systems."
    },
    {
      "id": "76f56f67-159a-45ef-9bc2-2c1d175e3f43",
      "adv_source_id": "76f56f67-159a-45ef-9bc2-2c1d175e3f43",
      "source_id": "71eaf1a6-f920-4a5c-b280-55c857c01a9f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model",
      "prompt": "Write the abstract for the academic paper titled \"Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model\".",
      "generation": "Here is the abstract for the academic paper titled \"Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model\":\n\nAbstract: We present a fully-automatic method for the segmentation of the left ventricular myocardium in contrast echocardiography sequences (ces). The method is based on random forests (rf) guided by a shape model. The shape model is built from manually segmented ces frames and describes the left ventricle geometry as a deformable model. The random forests classify the pixels of each frame into myocardial and background voxels. The segmentation of each frame is obtained by deforming the shape model to fit the rf classification. The method was evaluated on a database of 25 ces frames (125 slices). For comparison, we also evaluated a method based on a classic active contour without shape model. The results show that the proposed method gives more accurate and robust segmentation."
    },
    {
      "id": "67ba03b9-6f51-47a4-80ea-b38205cc0f15",
      "adv_source_id": "67ba03b9-6f51-47a4-80ea-b38205cc0f15",
      "source_id": "71eaf1a6-f920-4a5c-b280-55c857c01a9f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model",
      "prompt": "Write the abstract for the academic paper titled \"Fully Automatic Myocardial Segmentation of Contrast Echocardiography Sequence Using Random Forests Guided by Shape Model\".",
      "generation": "The paper presents a fully-automatic method for the segmentation of the left ventricle in contrast echocardiography sequences. The proposed method is based on the combination of a shape model and random forests. The shape model is built from manually segmented images and describes the geometric structure of the left ventricle. The random forests are used to classify the pixels in each frame of the sequence. The abstract for the academic paper would include:\n\nOutcomes: The random forests were trained using features extracted from the shape model and from the intensity of the contrast agent in the images. The shape model was built from manually segmented images and described the geometric structure of the left ventricle. The abstract would also include the proposed method's performance compared to state-of-the-art methods, as well as its potential applications in clinical practice. It may also include a description of the dataset used to test the proposed method and the criteria used to evaluate its performance."
    },
    {
      "id": "cd2cb655-69a9-43a0-9455-797de6a85623",
      "adv_source_id": "cd2cb655-69a9-43a0-9455-797de6a85623",
      "source_id": "0a3de390-010c-4cad-be4f-a663ae4f4311",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Real-time Prediction of Segmentation Quality",
      "prompt": "Write the abstract for the academic paper titled \"Real-time Prediction of Segmentation Quality\".",
      "generation": "Abstract:\nIn this paper, we present a real-time method for predicting the quality of image segmentation results. Our method takes advantage of the parallelism offered by modern graphics processing units (GPUs) to simultaneously train a deep neural network and evaluate its performance on a set of ground truth segmentations. We demonstrate that our method can predict the quality of segmentations with high accuracy, and that it is suitable for use in interactive applications such as video editing and medical imaging.\n\nOur method consists of two main components: a deep neural network trained using a supervised learning approach, and a set of ground truth segmentations used to evaluate the performance of the network. The neural network is trained on a large dataset of images and their corresponding segmentations, and learns to predict the quality of segmentations based on the features of the input images. The ground truth segmentations are created by manually labeling a set of images, and are used to evaluate the performance of the network during training.\n\nWe demonstrate the effectiveness of our method by evaluating its performance on a set of images with known segmentations. Our results show that our method can predict the quality of segmentations with high accuracy, and that it is suitable for use in real-time applications. We also show that our method is able to generalize to images that were not included in the training dataset, demonstrating its ability to predict the quality of segmentations for a wide range of images."
    },
    {
      "id": "67fc7868-9e65-4799-82ac-e5003975c61d",
      "adv_source_id": "67fc7868-9e65-4799-82ac-e5003975c61d",
      "source_id": "0a3de390-010c-4cad-be4f-a663ae4f4311",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Real-time Prediction of Segmentation Quality",
      "prompt": "Write the abstract for the academic paper titled \"Real-time Prediction of Segmentation Quality\".",
      "generation": "Abstract:\nIn this paper, we present a real-time method for predicting the quality of segmentation in a medical imaging context. Our approach utilizes a deep learning model trained on a large dataset of images and segmentations, and is capable of making predictions in real time. The model takes into account various factors such as image quality, contrast, and noise, as well as the complexity of the segmentation task. We evaluated our method on a large dataset of images and found that it significantly outperformed existing methods in terms of accuracy and speed. Our approach has the potential to improve the efficiency and accuracy of medical image segmentation, and could be used to assist radiologists in their clinical decision-making."
    },
    {
      "id": "9f6c8806-77a2-48c2-aad2-c71213bac158",
      "adv_source_id": "9f6c8806-77a2-48c2-aad2-c71213bac158",
      "source_id": "fd27ce13-36b8-43c6-9083-350afddf9680",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks",
      "prompt": "Write the abstract for the academic paper titled \"Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\".",
      "generation": "Here is the abstract for the academic paper titled \"Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\":\n\nThis paper proposes a novel approach to improve the performance of fully convolutional networks for image classification tasks. The proposed method combines the spatial squeeze & excitation (SSE) and channel squeeze & excitation (CSE) operations in a concurrent manner, allowing for efficient feature extraction and channel-wise attention. The concurrent SSE-CSE module is designed to capture both local and global information within the feature maps, resulting in improved classification performance. The proposed method is evaluated on the CIFAR-10 and CIFAR-100 datasets, and compared to other state-of-the-art fully convolutional networks. The results show that the proposed method achieves superior performance in terms of both accuracy and efficiency.\n\nThis abstract provides a brief overview of the proposed method and its evaluation results. It highlights the key advantages of the concurrent SSE-CSE module in capturing both local and global information within the feature maps, resulting in improved classification performance. The abstract also emphasizes the efficiency of the proposed method in terms of both accuracy and computational cost."
    },
    {
      "id": "d5992128-04a1-41f6-8fa2-8a26e682a060",
      "adv_source_id": "d5992128-04a1-41f6-8fa2-8a26e682a060",
      "source_id": "fd27ce13-36b8-43c6-9083-350afddf9680",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks",
      "prompt": "Write the abstract for the academic paper titled \"Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\".",
      "generation": "Here is the abstract for the academic paper \"Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\":\n\nWe propose Concurrent Spatial and Channel Squeeze & Excitation (CSCSE), a building block that enhances the representation power of fully convolutional networks (FCNs). Our proposed module is composed of two distinct operations: (1) A spatial squeeze & excitation (SSE) block that applies channel-wise spatial-wise convolutional operations; (2) A channel squeeze & excitation (CSE) block that applies channel-wise point-wise convolutional operations. This hybrid block allows the network to capture both intra-channel and inter-channel dependencies, resulting in improved performance in natural image datasets. We demonstrate the effectiveness of our approach on three benchmark datasets: CIFAR-10, CIFAR-100, and SVHN. Our model achieves state-of-the-art performance on CIFAR-10, improving the top validation accuracy from 92.73% to 93.89%, and on CIFAR-100, improving the top validation accuracy from 74.63% to 76.43%.\n\nWould you like me to generate the rest of the paper for you as well?"
    },
    {
      "id": "83e2044d-5298-4822-b5c6-7c3254e24fdc",
      "adv_source_id": "83e2044d-5298-4822-b5c6-7c3254e24fdc",
      "source_id": "22391927-6f6e-4432-96f7-a86db2bc425d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CFCM: Segmentation via Coarse to Fine Context Memory",
      "prompt": "Write the abstract for the academic paper titled \"CFCM: Segmentation via Coarse to Fine Context Memory\".",
      "generation": "Abstract:\nTraditional image segmentation methods are usually based on hand-designed features and heuristics, making them hard to extend to new domains or adapt to new tasks, and they fail to leverage the power of massive amount of image data available. This work presents a learnable approach to image segmentation, which is based on a memory-augmented fully convolutional network (FCN). The key component of our approach is a stacked hourglass network (SHN), which learns to iteratively refine the context representation of the input image, and generates the final segmentation mask. In particular, the network is able to remember the global context information of the image via a stack of hourglass networks, which can be used to refine the segmentation at different scales. Our approach achieves state-of-the-art performance on the Cityscapes segmentation benchmark, and runs in real-time."
    },
    {
      "id": "67cbcf1c-91a6-41be-a15e-60521aa26ee5",
      "adv_source_id": "67cbcf1c-91a6-41be-a15e-60521aa26ee5",
      "source_id": "22391927-6f6e-4432-96f7-a86db2bc425d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "CFCM: Segmentation via Coarse to Fine Context Memory",
      "prompt": "Write the abstract for the academic paper titled \"CFCM: Segmentation via Coarse to Fine Context Memory\".",
      "generation": "Abstract:\nTraditional image segmentation methods rely on hand-designed features and heuristics, making them fragile to significant variation in the data distribution. This paper presents a new approach to image segmentation based on a coarse-to-fine segmentation framework inspired by the recent success of coarse-to-fine attention. We introduce a Context Memory (CM) module that learns contextual information on-the-fly for effective localization of difficult pixels. Our key insight is that the contextual information in the CM can be effectively used to guide the segmentation process from the coarse level to the fine level, leading to a significant improvement in segmentation quality. We validate the effectiveness of our approach on the Cityscapes and CamVid datasets, and show that our method outperforms the state-of-the-art approaches."
    },
    {
      "id": "c72fc501-3fd2-48d0-863c-d7fbe4358c44",
      "adv_source_id": "c72fc501-3fd2-48d0-863c-d7fbe4358c44",
      "source_id": "f5d9f666-2531-4417-bb70-be7f381b36b9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Shape Segmentation via Shape Fully Convolutional Networks",
      "prompt": "Write the abstract for the academic paper titled \"3D Shape Segmentation via Shape Fully Convolutional Networks\".",
      "generation": "The abstract for the academic paper titled \"3D Shape Segmentation via Shape Fully Convolutional Networks\" would include:\n\nWe present a fully convolutional neural network architecture for 3D shape segmentation, specifically for segmenting 3D shapes in a point cloud. Our network is designed to take a point cloud as input and output a segmentation of the shape, where each point in the point cloud is assigned a label indicating the object it belongs to.\n\nOur network is based on a fully convolutional architecture, which allows us to process the input point cloud in a continuous manner, without any explicit shape decomposition or explicit 3D spatial pooling. This enables our network to handle complex and irregular shapes, as well as to capture fine-grained details in the input data.\n\nWe utilize a combination of supervised and semi-supervised learning techniques to train our network. Our experimental results show that our network is able to achieve state-of-the-art performance on several public 3D shape segmentation benchmarks."
    },
    {
      "id": "29452ee6-e9fa-4769-b4e4-3db62c346e39",
      "adv_source_id": "29452ee6-e9fa-4769-b4e4-3db62c346e39",
      "source_id": "f5d9f666-2531-4417-bb70-be7f381b36b9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Shape Segmentation via Shape Fully Convolutional Networks",
      "prompt": "Write the abstract for the academic paper titled \"3D Shape Segmentation via Shape Fully Convolutional Networks\".",
      "generation": "The abstract for the academic paper titled \"3D Shape Segmentation via Shape Fully Convolutional Networks\" would include:\n\nWe present a novel approach for 3D shape segmentation using fully convolutional neural networks (FCNs). Our method directly operates on 3D point clouds, without the need for voxelization or surface reconstruction. We utilize a shape-aware loss function that enables the network to learn robust representations for shape segmentation tasks. Furthermore, we introduce a shape-fully convolutional network (SFCN) architecture that performs end-to-end training while maintaining spatial information. Our approach achieves state-of-the-art performance on the ShapeNet part segmentation benchmark, outperforming previous methods in both segmentation accuracy and speed. We believe that our approach lays a foundation for future research on 3D shape segmentation using deep learning techniques."
    },
    {
      "id": "509620a5-b98b-4db9-afc3-3e49911ab3b1",
      "adv_source_id": "509620a5-b98b-4db9-afc3-3e49911ab3b1",
      "source_id": "b77f7de9-a9bb-40e6-9179-4d303a00b494",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention U-Net: Learning Where to Look for the Pancreas",
      "prompt": "Write the abstract for the academic paper titled \"Attention U-Net: Learning Where to Look for the Pancreas\".",
      "generation": "Here is the abstract for the academic paper titled \"Attention U-Net: Learning Where to Look for the Pancreas\":\n\nAbstract:\nWe present Attention U-Net, a novel attention-based architecture for pancreatic tumor segmentation in CT scans. Our approach leverages attention mechanisms to weigh the importance of different spatial locations within the image, enabling the network to focus on regions with strong pixel-level relevance to the task. We introduce a multi-scale attention module that computes attention weights for each pixel across multiple resolutions, capturing both global and local context. Furthermore, we propose a novel global attention pooling layer that aggregates attention weights from all image scales, providing a global attention map that highlights the most important regions for segmentation. Our extensive experiments on the publicly available Pancreas Tumor Segmentation Challenge (PTSC) 2020 dataset show that our method achieves state-of-the-art performance in both the overall and the hard-tumor segmentation categories.\n\nThis abstract provides a concise overview of the paper's main contributions: the proposed Attention U-Net architecture, the multi-scale attention module, and the global attention pooling layer. The results on the PTSC 2020 dataset demonstrate the effectiveness of the proposed method in pancreatic tumor segmentation."
    },
    {
      "id": "5da62211-2052-471b-a7aa-a436bd95a663",
      "adv_source_id": "5da62211-2052-471b-a7aa-a436bd95a663",
      "source_id": "b77f7de9-a9bb-40e6-9179-4d303a00b494",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Attention U-Net: Learning Where to Look for the Pancreas",
      "prompt": "Write the abstract for the academic paper titled \"Attention U-Net: Learning Where to Look for the Pancreas\".",
      "generation": "The abstract for the academic paper titled \"Attention U-Net: Learning Where to Look for the Pancreas\" could be:\nThis paper presents a novel attention mechanism for U-Net-based architectures to learn the selective attention mechanism to focus on the pancreas while ignoring other structures in CT volumes for pancreas segmentation. We propose to use an attention module that is trained end-to-end with the U-Net. The attention module consists of a series of convolutional layers with the last layer followed by a softmax function to produce a attention map. We introduce a new loss function that combines the segmentation loss with an attention consistency loss. This model significantly improves the localization of the pancreas and the surrounding organs while maintaining a similar performance in terms of the F1-score on the pancreas vs. all other organs (P vs. O). We achieve a 2% increase in the F1-score on the pancreas vs. other organs (P vs. O) and 37% increase in the localization metric while maintaining similar performance in terms of the F1-score."
    },
    {
      "id": "355861d1-9a32-4f8c-afa2-8e703a21982e",
      "adv_source_id": "355861d1-9a32-4f8c-afa2-8e703a21982e",
      "source_id": "412b173e-6cfb-446f-9818-48c8094e7caf",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning",
      "prompt": "Write the abstract for the academic paper titled \"Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning\".",
      "generation": "The paper discusses an interactive medical image segmentation technique that combines point-based interaction and sequential patch learning. The proposed method enables users to efficiently and effectively segment medical images by interactively specifying boundary points and training a patch-based segmentation model on the provided training data. The paper discusses the implementation of the proposed method on MRI brain images and demonstrates its effectiveness compared to existing state-of-the-art techniques. The results show that the proposed method can accurately segment medical images while reducing the amount of user interaction required.\n\nThe abstract for the academic paper titled \"Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning\" should include the following:\n\nIntroduction: The paper discusses an interactive medical image segmentation technique that combines point-based interaction and sequential patch learning.\nMethod: The proposed method enables users to efficiently and effectively segment medical images by interactively specifying boundary points and training a patch-based segmentation model on the provided training data.\nResults: The paper discusses the implementation of the proposed method on MRI brain images and demonstrates its effectiveness compared to existing state-of-the-art techniques. The results show that the proposed method can accurately segment medical images while reducing the amount of user interaction required.\nConclusion: The paper concludes by discussing the potential applications of the proposed method in medical image segmentation and highlighting the advantages of using a point-based interaction and sequential patch learning approach."
    },
    {
      "id": "77d29911-df71-439a-bcd6-98b288331dd8",
      "adv_source_id": "77d29911-df71-439a-bcd6-98b288331dd8",
      "source_id": "412b173e-6cfb-446f-9818-48c8094e7caf",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning",
      "prompt": "Write the abstract for the academic paper titled \"Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning\".",
      "generation": "The paper presents a novel approach for interactive medical image segmentation that combines point-based interaction and sequential patch learning. The proposed method enables users to quickly and efficiently segment structures of interest in medical images by interactively selecting a few points on the image. The algorithm then utilizes a patch-based learning approach to sequentially refine the segmentation result. The experimental results show that the proposed method is effective in segmenting medical images and significantly reduces the user interaction time compared to traditional segmentation methods.\n\nThe abstract for the academic paper titled \"Interactive Medical Image Segmentation via Point-Based Interaction and Sequential Patch Learning\" should now be fully written. Is there any other information that can be useful for you regarding paper writing?"
    },
    {
      "id": "c8e7fa26-f53a-497d-8320-15b30b900d58",
      "adv_source_id": "c8e7fa26-f53a-497d-8320-15b30b900d58",
      "source_id": "6e8be1dc-7f31-4511-9832-b8e324403bfb",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation",
      "prompt": "Write the abstract for the academic paper titled \"Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation\".",
      "generation": "This paper presents a Matlab implementation of a machine vision algorithm for evaluating ballast degradation in railway tracks. The algorithm utilizes image processing techniques to analyze the images of the ballast and identify the level of degradation. The implementation was conducted in two main stages: image preprocessing and feature extraction. For the preprocessing stage, the image was first filtered using a Gaussian filter to reduce noise and then thresholded to separate the ballast from the surrounding area. The feature extraction stage involved the use of morphological operations to extract the features of the ballast. These features were then used to evaluate the degradation level of the ballast. The implementation was tested on a database of images and the results were compared to the manually evaluated degradation levels. The results showed that the Matlab implementation of the machine vision algorithm can accurately evaluate the ballast degradation levels and can be used for real-time evaluation of ballast degradation in railway tracks."
    },
    {
      "id": "f5dbedc6-9cae-4cef-ad72-6a620b9089a8",
      "adv_source_id": "f5dbedc6-9cae-4cef-ad72-6a620b9089a8",
      "source_id": "6e8be1dc-7f31-4511-9832-b8e324403bfb",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation",
      "prompt": "Write the abstract for the academic paper titled \"Matlab Implementation of Machine Vision Algorithm on Ballast Degradation Evaluation\".",
      "generation": "Ballast degradation evaluation using image processing technique has gained attention in the recent past. This paper presents the Matlab implementation of a machine vision algorithm to conduct ballast degradation evaluation. The algorithm utilizes the Color Channel Combination (CCC) method to enhance the degradation level of ballast particles. The system was tested on a database of ballast images and the results were compared with the conventional manual method of degradation evaluation. The results showed that the machine vision algorithm can identify the degradation level of ballast particles with an accuracy of 92%. The algorithm can be used as a tool to assist railway engineers in conducting ballast degradation evaluation and optimizing ballast maintenance strategies.\n\nThe Matlab implementation of the machine vision algorithm for ballast degradation evaluation can be useful for railway engineers to assess the condition of ballast and optimize maintenance strategies. The algorithm can accurately identify the degree of degradation in ballast particles, providing valuable information for making informed decisions on ballast management. This technology can be applied in the field to analyze ballast images and assess the condition of the ballast in real-time, helping to maintain the integrity and performance of the railway system."
    },
    {
      "id": "f5c13e54-c7f5-40a4-ad5d-8079cf65df10",
      "adv_source_id": "f5c13e54-c7f5-40a4-ad5d-8079cf65df10",
      "source_id": "e5025a46-510c-4eab-9c95-3bfd0684ef89",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means",
      "prompt": "Write the abstract for the academic paper titled \"Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means\".",
      "generation": "In this paper, we present a new approach for unsupervised pathology image segmentation using representation learning with spherical k-means. We propose to use a deep learning architecture called Convolutional Neural Network (CNN) as a feature extractor, and a spherical k-means clustering algorithm to segment the images. Our method is unsupervised, meaning that it does not require any labeled data for training. We show that our approach can achieve state-of-the-art performance on the ISBI challenge 2016 segmentation benchmark, and can also be used for automatic segmentation of pathological images.\n\nOur approach has two main components. The first component is a deep learning architecture called Convolutional Neural Network (CNN), which is used as a feature extractor. The CNN is trained on a large dataset of images, and learns to extract a variety of features from the images. The second component is a spherical k-means clustering algorithm, which is used to segment the images. The spherical k-means algorithm is trained on the features extracted by the CNN, and learns to segment the images into a plurality of clusters.\n\nWe evaluate our approach on the ISBI challenge 2016 segmentation benchmark, and show that it can achieve state-of-the-art performance. We also show that our approach can be used for automatic segmentation of pathological images, and can achieve better performance than other unsupervised segmentation methods."
    },
    {
      "id": "0fe71412-4461-4da8-a29e-c596dcc02f56",
      "adv_source_id": "0fe71412-4461-4da8-a29e-c596dcc02f56",
      "source_id": "e5025a46-510c-4eab-9c95-3bfd0684ef89",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means",
      "prompt": "Write the abstract for the academic paper titled \"Unsupervised Pathology Image Segmentation Using Representation Learning with Spherical K-means\".",
      "generation": "Abstract:\nPathology image segmentation is a fundamental step in digital pathology analysis. We present a novel unsupervised approach for pixel-level pathology image segmentation using representation learning with spherical k-means. We leverage the representational power of a pre-trained convolutional neural network (CNN) and the clustering ability of spherical k-means to segment whole slide images (WSIs) into meaningful regions without requiring any labeled data. Our approach is fully unsupervised, meaning it learns from scratch in an unsupervised manner and does not rely on any annotated WSIs. We demonstrate the effectiveness of our approach on a challenging dataset of breast cancer WSIs, showing that our method can accurately segment different tissue types and structures, including tumor regions, stroma, and necrosis. We further show that our method generalizes well to different tissue types and can be used to segment other types of cancer, such as prostate cancer. We believe that our approach has the potential to transform digital pathology analysis by enabling unsupervised segmentation of WSIs and improving the accuracy and efficiency of pathology diagnosis and research."
    },
    {
      "id": "290297ab-5265-4617-8bb5-8ee8b5a6870c",
      "adv_source_id": "290297ab-5265-4617-8bb5-8ee8b5a6870c",
      "source_id": "641f6cbf-e9ce-462f-95f1-37c1755bc265",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound",
      "prompt": "Write the abstract for the academic paper titled \"Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound\".",
      "generation": "The paper proposes a sample selection strategy based on the relaxed upper confident bound (RUCB) for training multi-organ segmentation networks. The RUCB sample selection strategy is a generalization of the upper confident bound (UCB) sample selection strategy, which is a commonly used strategy for training deep learning models. The paper shows that the RUCB sample selection strategy improves the training speed and performance of multi-organ segmentation networks compared to the UCB strategy. The paper also shows that the RUCB strategy is effective for training deep learning models for a wide range of datasets and architectures.\n\nThe paper concludes by discussing the implications of the RUCB sample selection strategy for training deep learning models and suggests future research directions."
    },
    {
      "id": "2039c799-9d1c-4220-a8c5-de36f699c69f",
      "adv_source_id": "2039c799-9d1c-4220-a8c5-de36f699c69f",
      "source_id": "641f6cbf-e9ce-462f-95f1-37c1755bc265",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound",
      "prompt": "Write the abstract for the academic paper titled \"Training Multi-organ Segmentation Networks with Sample Selection by Relaxed Upper Confident Bound\".",
      "generation": "The paper presents a novel approach for training deep learning models for multi-organ segmentation, a task in medical imaging that involves the identification and segmentation of multiple organs in a given image. The proposed method utilizes a sample selection technique based on a relaxed upper confident bound (RUCB) to improve the training data efficiency. The RUCB sampling strategy is used to select a subset of images from a larger dataset, with the goal of reducing the amount of training data required while still achieving high performance. The proposed method is evaluated on the publicly available dataset of the MICCAI Multi-Organ Segmentation Challenge 2017 (MOS 2017), where it achieves state-of-the-art performance. The results show that the proposed method can significantly improve the training data efficiency while maintaining high segmentation performance."
    },
    {
      "id": "23fe33c4-c266-4236-a10e-9a4bf98b520d",
      "adv_source_id": "23fe33c4-c266-4236-a10e-9a4bf98b520d",
      "source_id": "40ad0431-c84c-4f1c-b797-48a0c19c6ffd",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Compassionately Conservative Balanced Cuts for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Compassionately Conservative Balanced Cuts for Image Segmentation\".",
      "generation": "Abstract:\nThe paper presents a novel method for image segmentation that combines the strengths of both conservative and aggressive cutting techniques. The proposed method, called Compassionately Conservative Balanced Cuts (CCBC), aims to improve the efficiency and accuracy of image segmentation by finding a balance between conservative and aggressive cutting strategies. The CCBC method utilizes a multi-stage approach, starting with a conservative initial cut and gradually refining it using aggressive cutting techniques. The goal is to achieve a balance between accurate segmentation and computational efficiency. The paper evaluates the proposed method on a variety of image datasets, and the results show that CCBC significantly outperforms the state-of-the-art in terms of both accuracy and efficiency. \n\nFurthermore, the paper discusses the practical implications of the proposed method for various applications, including medical imaging, satellite imagery, and video processing. The results demonstrate that CCBC is a powerful and versatile image segmentation technique that is particularly well-suited for resource-constrained and time-sensitive applications. \n\nOverall, this paper makes a significant contribution to the field of image processing by presenting a new method that combines the advantages of conservative and aggressive cutting techniques in a balanced and compassionate manner."
    },
    {
      "id": "157e1064-de33-40de-ae6c-9edfd58561d3",
      "adv_source_id": "157e1064-de33-40de-ae6c-9edfd58561d3",
      "source_id": "40ad0431-c84c-4f1c-b797-48a0c19c6ffd",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Compassionately Conservative Balanced Cuts for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Compassionately Conservative Balanced Cuts for Image Segmentation\".",
      "generation": "A novel algorithm for image segmentation is proposed that selectively smoothes an image while preserving important details. The algorithm iteratively compresses an image using a predictor-corrector scheme. A key feature of the algorithm is that the compression phase is designed to preserve edges, unlike previous work in the area. In the predictor stage, the algorithm utilizes a predictor that is specialized for images. The corrector in the algorithm is an adaptive controller that selectively updates the pixels in the image to reduce the error between the compressed image and the predictor's output. Since the controller only updates pixels in an area of uncertainty, the algorithm is very fast. In addition, the uncertainty map can be computed very quickly using the predictor. Since the edges in the image are preserved, the segmented image is very similar to the original image. Finally, the algorithm is scale-invariant, and can segment images at a variety of resolutions."
    },
    {
      "id": "8d17c2c3-641f-4203-9830-19b98f879abc",
      "adv_source_id": "8d17c2c3-641f-4203-9830-19b98f879abc",
      "source_id": "92e24d44-2ce3-4aa8-97be-7571af82aa59",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive strategy for superpixel-based region-growing image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Adaptive strategy for superpixel-based region-growing image segmentation\".",
      "generation": "Adaptive Strategy for Superpixel-Based Region-Growing Image Segmentation\n\nIn this paper, we present an adaptive strategy for superpixel-based region-growing image segmentation. Our approach incorporates an adaptive energy function that considers the local characteristics of the image, as well as a novel region-growing algorithm that efficiently explores the segmentation space. We evaluate our method on a variety of images, and demonstrate that it achieves state-of-the-art performance in terms of both objective measures and subjective evaluations. Our approach is efficient and scalable, and can be easily extended to handle more complex images and segmentation tasks."
    },
    {
      "id": "b06d7770-f0ab-490a-9494-003c073584fb",
      "adv_source_id": "b06d7770-f0ab-490a-9494-003c073584fb",
      "source_id": "92e24d44-2ce3-4aa8-97be-7571af82aa59",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Adaptive strategy for superpixel-based region-growing image segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Adaptive strategy for superpixel-based region-growing image segmentation\".",
      "generation": "An adaptive strategy for the superpixel-based region-growing image segmentation algorithm is proposed in this paper to improve the accuracy of the algorithm. The proposed strategy incorporates an adaptive parameter into the region-growing algorithm, which is based on the local characteristics of the image. The adaptive parameter is used to control the growth of the regions, and it is calculated using the mean and standard deviation of the pixel intensities in each superpixel. The proposed strategy was evaluated on a database of mammographic images, and the results show that it improves the accuracy of the algorithm."
    },
    {
      "id": "c6ada8c8-db7d-4139-91e0-3b601f6661a6",
      "adv_source_id": "c6ada8c8-db7d-4139-91e0-3b601f6661a6",
      "source_id": "f5db7b0d-ca4e-4b11-8b07-41006bc8a59e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions",
      "prompt": "Write the abstract for the academic paper titled \"IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions\".",
      "generation": "The abstract for the academic paper titled \"IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions\" could be:\n\nAbstract: Image segmentation plays a crucial role in medical imaging and has been extensively studied for over a decade. While many approaches exist, few effectively address the challenge of segmenting images with inhomogeneities. This paper presents a novel active contour model, IEOPF (Inhomogeneity-Estimated Orthogonal Primary Functions), for image segmentation that estimates inhomogeneities in the image using orthogonal primary functions. The proposed method captures local variations in intensity and adapts to the image data, resulting in accurate segmentation performance. A comprehensive evaluation of the method using a publicly available dataset demonstrates its effectiveness compared to state-of-the-art active contour models. The results show that the proposed method significantly improves segmentation accuracy, particularly in challenging cases involving inhomogeneities. Furthermore, the IEOPF model is computationally efficient and offers a robust solution even in the presence of noise. This work presents a significant advancement in active contour-based image segmentation and has potential applications in various medical imaging tasks."
    },
    {
      "id": "baf03fea-a448-4774-b0e9-de417d0d7797",
      "adv_source_id": "baf03fea-a448-4774-b0e9-de417d0d7797",
      "source_id": "f5db7b0d-ca4e-4b11-8b07-41006bc8a59e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions",
      "prompt": "Write the abstract for the academic paper titled \"IEOPF: An Active Contour Model for Image Segmentation with Inhomogeneities Estimated by Orthogonal Primary Functions\".",
      "generation": "The paper proposes an active contour model for image segmentation with inhomogeneities estimated by orthogonal primary functions. The proposed method utilizes the orthogonal primary function (OPF) to estimate the inhomogeneities in an image, enabling the active contour model to accurately segment the image. The OPF is a vector function that can approximate any scalar function. The proposed method has been tested on a variety of images with inhomogeneities, and the results demonstrate its effectiveness in segmenting images with complex shapes and textures. The proposed method is computationally efficient and can be used for real-time image segmentation.\n\nThe paper discusses the advantages of the proposed method over existing methods for image segmentation with inhomogeneities. The existing methods rely on parameter estimation, which is a slow and computationally expensive process. The proposed method does not require parameter estimation, as the OPF can estimate the inhomogeneities directly. The proposed method is therefore faster and more efficient than the existing methods. The paper also discusses the robustness of the proposed method to noise and texture. The proposed method is able to segment images with complex shapes and textures, despite the presence of noise and texture. The paper concludes by discussing the potential applications of the proposed method for image segmentation with inhomogeneities. The proposed method could be used for medical imaging, satellite imaging, and image processing in general."
    },
    {
      "id": "3652bf04-6953-4191-a5c5-20b0f9c9ec9d",
      "adv_source_id": "3652bf04-6953-4191-a5c5-20b0f9c9ec9d",
      "source_id": "1a062307-d3f9-4d2f-921e-e4ab41966337",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey",
      "prompt": "Write the abstract for the academic paper titled \"Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey\".",
      "generation": "Here is the abstract for the academic paper titled \"Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey\":\n\nKnee joint magnetic resonance imaging (MRI) is an important tool for the diagnosis and treatment of knee joint diseases, such as osteoarthritis and rheumatoid arthritis. Computer-aided knee joint MRI image segmentation is a process of automatically identifying and segmenting the different structures in the knee joint MRI image, such as the bone, cartilage, and soft tissue. This survey paper reviews the current state of the art in computer-aided knee joint MRI image segmentation, including the different methods and techniques that have been proposed, the challenges that have been encountered, and the future direction of research. The goal of this survey paper is to provide a comprehensive overview of the current state of the art in computer-aided knee joint MRI image segmentation and to identify the challenges and opportunities for future research."
    },
    {
      "id": "6d6b57e1-2339-4471-b144-5db226dd7b7d",
      "adv_source_id": "6d6b57e1-2339-4471-b144-5db226dd7b7d",
      "source_id": "1a062307-d3f9-4d2f-921e-e4ab41966337",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey",
      "prompt": "Write the abstract for the academic paper titled \"Computer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey\".",
      "generation": "Sure! Here is the abstract for the academic paper titled \u201cComputer-Aided Knee Joint Magnetic Resonance Image Segmentation - A Survey\u201d:\n\n knee joint magnetic resonance imaging (MRI) is a non-invasive medical imaging technique that is used to diagnose and monitor the progress of knee joint diseases, such as osteoarthritis and rheumatoid arthritis. The segmentation of MRI images is a crucial step in the analysis of these images, as it involves the identification and delineation of the different anatomical structures in the knee joint, such as the bone, cartilage, and ligaments. This paper provides a survey of the current state of the art in computer-aided knee joint MRI image segmentation. We discuss the various techniques that have been proposed for segmenting knee joint MRI images, including thresholding, clustering, and machine learning-based methods. We also review the performance of these techniques on public datasets and highlight the challenges and limitations of the current state of the art. Finally, we provide a brief overview of the future direction of research in this field."
    },
    {
      "id": "f0d4512f-3299-4cd5-8f52-6af8d78e1420",
      "adv_source_id": "f0d4512f-3299-4cd5-8f52-6af8d78e1420",
      "source_id": "ce96623b-70a5-4e98-a000-88f14a9b1d6b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Iterative Spanning Forest Framework for Superpixel Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"An Iterative Spanning Forest Framework for Superpixel Segmentation\".",
      "generation": "An Iterative Spanning Forest (ISF) framework is proposed for superpixel segmentation, which iteratively constructs a spanning forest with a novel edge-weighting scheme. The ISF framework initializes superpixels based on a greedy algorithm and then refines the initial segmentation by iteratively merging and splitting superpixels. The proposed framework is evaluated on the Berkeley Segmentation Dataset and compares favorably with the state-of-the-art. In particular, the proposed method achieves an F-measure of 0.829, outperforming the state-of-the-art method by 2%."
    },
    {
      "id": "20e1b66b-d0cf-4286-97f6-99ba0ce5d1d7",
      "adv_source_id": "20e1b66b-d0cf-4286-97f6-99ba0ce5d1d7",
      "source_id": "ce96623b-70a5-4e98-a000-88f14a9b1d6b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Iterative Spanning Forest Framework for Superpixel Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"An Iterative Spanning Forest Framework for Superpixel Segmentation\".",
      "generation": "The increase in the number of pixels with the growing size of convolutional neural networks (CNNs) results in the doubling of the computational demands and the prediction time in every iteration of training. One way to overcome this issue is to reduce the input size of the images. However, reducing the input size results in a loss of details, which affects the performance of the CNN. An alternative solution is to segment the images into groups of pixels called superpixels, and then classify each superpixel separately. In this paper, we propose a framework for superpixel segmentation based on the spanning forest algorithm. Our proposed method iteratively partitions the pixels into superpixels based on their similarities in colour and location. We evaluate our method on the UC-Merced dataset, and the results show that our method achieves a higher segmentation quality compared to the state-of-the-art."
    },
    {
      "id": "e72958a9-44c0-4489-b4f9-b16c5f0e1f7d",
      "adv_source_id": "e72958a9-44c0-4489-b4f9-b16c5f0e1f7d",
      "source_id": "88e5c1cd-2edc-4c4d-b72a-63ed078c3c5e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation\".",
      "generation": "In this paper, we present TernausNet, an image segmentation model based on the U-Net architecture pre-trained using the VGG11 encoder on the ImageNet dataset. Our proposed model utilizes the pre-trained encoder to extract features from the input image and then applies a series of upsampling and downsampling operations to generate a final segmentation output. We evaluate our model on the Cityscapes and VOC2012 segmentation datasets, and achieve competitive results compared to other state-of-the-art models. Our code and model weights are made publicly available for future research and development."
    },
    {
      "id": "f3d97340-08c5-41c6-9fe0-93119f0954b4",
      "adv_source_id": "f3d97340-08c5-41c6-9fe0-93119f0954b4",
      "source_id": "88e5c1cd-2edc-4c4d-b72a-63ed078c3c5e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation\".",
      "generation": "Here is the abstract for the academic paper titled \"TernausNet: U-Net with VGG11 Encoder Pre-Trained on ImageNet for Image Segmentation\":\n\nThis paper introduces TernausNet, a novel approach to image segmentation that combines the strengths of U-Net and VGG11 Encoder. The U-Net architecture, which has been successfully applied to various image processing tasks, is used as the backbone of the network. The VGG11 Encoder, pre-trained on the ImageNet dataset, is utilized as a feature extractor to improve the performance of the network. The proposed method is evaluated on the Cityscapes and VOC 2012 segmentation datasets, and the results show that it outperforms the state-of-the-art in terms of both accuracy and efficiency."
    },
    {
      "id": "1e3a176d-a9d4-4185-a2d1-17312c7588c7",
      "adv_source_id": "1e3a176d-a9d4-4185-a2d1-17312c7588c7",
      "source_id": "00664552-af87-4295-a1bc-9149639bbe92",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions",
      "prompt": "Write the abstract for the academic paper titled \"Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions\".",
      "generation": "Abstract:\nIn this paper, we present a novel approach for semantic segmentation based on a highly fused convolutional network (HFCN) with multiple soft cost functions. Our method combines the strengths of both supervised and unsupervised learning to achieve state-of-the-art performance. We propose a new architecture that integrates multiple soft cost functions, such as edge detection and gradient magnitude, to enhance the segmentation accuracy. Our experimental results on the Cityscapes and CamVid datasets demonstrate the effectiveness of our approach, achieving competitive performance compared to the state-of-the-art methods. We show that our method can effectively handle challenges such as object occlusion and texture similarity, common in urban scene segmentation."
    },
    {
      "id": "ec993419-0d00-40f7-b9b5-baa20075974b",
      "adv_source_id": "ec993419-0d00-40f7-b9b5-baa20075974b",
      "source_id": "00664552-af87-4295-a1bc-9149639bbe92",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions",
      "prompt": "Write the abstract for the academic paper titled \"Semantic Segmentation via Highly Fused Convolutional Network with Multiple Soft Cost Functions\".",
      "generation": "Abstract:\nIn this paper, we present a novel approach for semantic segmentation based on a highly fused convolutional network (HFCN) with multiple soft cost functions. Our method combines the strengths of two promising techniques: deep learning and semantic segmentation. By utilizing a deep learning framework, we are able to learn complex features from large amounts of data. Additionally, by incorporating multiple soft cost functions, we are able to improve the accuracy and robustness of our segmentation model.\n\nOur HFCN consists of two main components: a convolutional neural network (CNN) and a fusion layer. The CNN is pre-trained on a large dataset of images and subsequently fine-tuned on a smaller dataset specific to the task of semantic segmentation. The fusion layer combines the outputs of the CNN with additional contextual information, such as image edges and textures, to produce a final segmentation map.\n\nOur experimental results show that our method achieves state-of-the-art performance on several benchmark datasets, including the PASCAL VOC 2012 and Cityscapes datasets. We also perform a comprehensive analysis of the effects of different design choices, such as the number of soft cost functions and the architecture of the CNN, on the performance of our method."
    },
    {
      "id": "8a875f0e-8df1-4ecb-a42c-0e4b2f5ee717",
      "adv_source_id": "8a875f0e-8df1-4ecb-a42c-0e4b2f5ee717",
      "source_id": "7980f1b3-1665-4905-a105-2a48c4204a67",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection",
      "prompt": "Write the abstract for the academic paper titled \"Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection\".",
      "generation": "Semantic image segmentation has achieved remarkable performance on a variety of benchmarks, driven by the success of deep learning. However, recent work still mainly focuses on pixel-level prediction, treating object boundaries as an afterthought. In this paper, we argue that a direct treatment of boundaries in the segmentation architecture is crucial yet overlooked, as it can improve both accuracy and efficiency. We propose a simple yet effective approach to incorporate boundary information within a semantic segmentation framework, by predicting boundaries as part of the segmentation process. We show that this approach not only improves accuracy by reducing errors at object boundaries, but also enables the use of a smaller model size while maintaining performance, resulting in faster inference. We demonstrate the effectiveness of our approach on the Cityscapes and CamVid datasets, achieving state-of-the-art results."
    },
    {
      "id": "845d2ae5-ffe8-4b8f-9523-fb13e0c42e03",
      "adv_source_id": "845d2ae5-ffe8-4b8f-9523-fb13e0c42e03",
      "source_id": "7980f1b3-1665-4905-a105-2a48c4204a67",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection",
      "prompt": "Write the abstract for the academic paper titled \"Classification With an Edge: Improving Semantic Image Segmentation with Boundary Detection\".",
      "generation": "Semantic image segmentation has gained significant attention in recent years due to its fundamental role in many computer vision applications. While many approaches have been proposed, most of them overlook the importance of boundary detection in improving segmentation accuracy. In this paper, we argue that boundary information can significantly enhance the performance of semantic image segmentation. To this end, we propose a novel approach that integrates boundary detection within a deep learning framework. Our approach, called Classification with an Edge (CWE), leverages the ability of deep neural networks to learn representations from data and combines it with boundary detection to improve segmentation accuracy. We demonstrate the effectiveness of our approach on the challenging semantic image segmentation task of the Cityscapes dataset, where we achieve state-of-the-art performance. Our results show that integrating boundary detection within deep learning frameworks can lead to significant improvements in segmentation accuracy, especially in challenging scenarios."
    },
    {
      "id": "d24f1fe8-30b8-4cc6-b06c-8c63509f5f88",
      "adv_source_id": "d24f1fe8-30b8-4cc6-b06c-8c63509f5f88",
      "source_id": "a766a7e6-c1bd-4449-8d5a-58a596462371",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient Evolutionary Based Method For Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"An Efficient Evolutionary Based Method For Image Segmentation\".",
      "generation": "Abstract:\nImage segmentation is a technique used to partition an image into multiple segments. The evolutionary-based method uses an optimization process to find the best partitioning of the image. This method is efficient because it uses a genetic algorithm to search for the best solution. The genetic algorithm uses a population of individuals to search for the best partitioning of the image. Each individual represents a potential solution to the problem. The individuals are evaluated using a fitness function. The fitness function measures the quality of the partitioning of the image. The evolutionary-based method is efficient because it only uses a small number of individuals to search for the best solution. This makes the method faster and more efficient than other, more exhaustive, search methods."
    },
    {
      "id": "5eda1898-85cf-481f-a7c6-96bf3504c685",
      "adv_source_id": "5eda1898-85cf-481f-a7c6-96bf3504c685",
      "source_id": "a766a7e6-c1bd-4449-8d5a-58a596462371",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An Efficient Evolutionary Based Method For Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"An Efficient Evolutionary Based Method For Image Segmentation\".",
      "generation": "Abstract:\nImage segmentation is a technique used to partition an image into multiple segments, to simplify the representation of an image and to reduce the amount of data, to make it more meaningful and easier to analyze. In this paper, we present an efficient evolutionary based method for image segmentation. Our approach combines the advantages of both evolutionary algorithms and image processing techniques. We use a genetic algorithm to optimize the segmentation process, and a multi-layer perceptron (MLP) to extract the features from the image. The proposed method has been tested on a variety of images, and the results show that it is effective in segmenting images and extracting features. The proposed method achieves a mean segmentation error of 0.172, which is lower than the mean error of the state-of-the-art method.\n\n plagiarized"
    },
    {
      "id": "96fb6c56-e889-4872-bcdc-03be086e5dbc",
      "adv_source_id": "96fb6c56-e889-4872-bcdc-03be086e5dbc",
      "source_id": "9596d503-ffc7-4359-ad4a-48aad4a13cdb",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks",
      "prompt": "Write the abstract for the academic paper titled \"Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks\".",
      "generation": "Abstract:\nWe propose a method for the segmentation of splenomegaly in ultrasound images using global convolutional kernels and conditional generative adversarial networks. The global convolutional kernels are used to extract features from the ultrasound images, and the conditional generative adversarial networks are used to segment the splenomegaly. The proposed method has been trained on a large dataset of ultrasound images and has been tested on a separate test dataset, with promising results. The proposed method achieves a mean accuracy of 94.5% and a mean precision of 92.1%."
    },
    {
      "id": "e547d403-7dc1-424c-9ff7-5ac899e0435e",
      "adv_source_id": "e547d403-7dc1-424c-9ff7-5ac899e0435e",
      "source_id": "9596d503-ffc7-4359-ad4a-48aad4a13cdb",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks",
      "prompt": "Write the abstract for the academic paper titled \"Splenomegaly Segmentation using Global Convolutional Kernels and Conditional Generative Adversarial Networks\".",
      "generation": "Abstract\n\nSplenomegaly is a medical condition in which the spleen becomes enlarged. The enlarged spleen can cause abdominal pain, anemia, and fatigue. This paper proposes a method for the segmentation of splenomegaly in ultrasound images using global convolutional kernels and conditional generative adversarial networks. The global convolutional kernels are used to extract features from the ultrasound images, and the conditional generative adversarial networks are used to generate synthetic ultrasound images that match the features extracted from the real images. The synthetic images are then used to train a convolutional neural network for splenomegaly segmentation. The proposed method was tested on a dataset of ultrasound images and achieved a dice coefficient of 0.79.\n\nTODO: Add authors"
    },
    {
      "id": "4a6d5b8d-ed6d-4a73-9b7d-11f30e6fd85a",
      "adv_source_id": "4a6d5b8d-ed6d-4a73-9b7d-11f30e6fd85a",
      "source_id": "60b73c57-6333-44c1-8fcf-eb3f65ffa16d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method",
      "prompt": "Write the abstract for the academic paper titled \"Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method\".",
      "generation": "The paper presents a novel automatic color image segmentation method based on a square elemental region-based seeded region growing and merging algorithm. The proposed method utilizes a square element as the basic unit of segmentation, which enables the algorithm to capture complex and irregular object boundaries in a more accurate and efficient manner. The algorithm consists of two stages: region growing and region merging. In the region growing stage, the algorithm starts with a set of seed points and iteratively grows the regions by expanding the boundaries of adjacent squares until a specified stopping criterion is met. In the region merging stage, the algorithm combines adjacent regions that share a common boundary to form a larger region. The proposed method has been evaluated on a standard dataset, and the experimental results demonstrate its effectiveness and efficiency compared to existing algorithms."
    },
    {
      "id": "07645bb6-dfd0-452d-a594-6e3f7c5be48b",
      "adv_source_id": "07645bb6-dfd0-452d-a594-6e3f7c5be48b",
      "source_id": "60b73c57-6333-44c1-8fcf-eb3f65ffa16d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method",
      "prompt": "Write the abstract for the academic paper titled \"Automatic Color Image Segmentation Using a Square Elemental Region-Based Seeded Region Growing and Merging Method\".",
      "generation": "Automated color picture segmentation is a crucial method in image processing since it extracts significant information from images. In this paper, we present a new automated color image segmentation technique based on a square elemental region-based seeded region growing and merging approach. Our method employs a square element decomposition of the picture, which simplifies the segmentation process and enables for more precise region growing and merging. Furthermore, our suggested approach incorporates a seed selection technique that selects the most relevant regions for growing, resulting in more precise and coherent segmentations. We evaluate our method on a variety of pictures, and the results demonstrate that it performs well in terms of speed and accuracy when compared to other state-of-the-art techniques."
    },
    {
      "id": "ae95f26b-fcfc-4d9d-b2aa-2169b007abd1",
      "adv_source_id": "ae95f26b-fcfc-4d9d-b2aa-2169b007abd1",
      "source_id": "c568b816-b63e-4969-9a57-99d4c072e0b6",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "WAYLA - Generating Images from Eye Movements",
      "prompt": "Write the abstract for the academic paper titled \"WAYLA - Generating Images from Eye Movements\".",
      "generation": "In this paper, we present an approach for generating images based on eye movements. Our system, WAYLA, utilizes eye-tracking data to determine the regions of an image that are of interest to the viewer. By combining this information with a deep generative model, we are able to create new images that are tailored to the user's gaze. Our approach provides a new way to interact with image generation systems, allowing users to focus on specific aspects of the image and generate new images based on their visual attention. We demonstrate the effectiveness of our approach through a user study, showing that our system can generate images that are more aligned with the user's interests.\n\nOur system has a wide range of applications including image editing, visualization, and accessibility. Ultimately, WAYLA offers an intuitive and creative approach for the generation of images, while considering the importance of visual attention."
    },
    {
      "id": "1f506fda-4abb-4eee-9132-7c3331581c73",
      "adv_source_id": "1f506fda-4abb-4eee-9132-7c3331581c73",
      "source_id": "c568b816-b63e-4969-9a57-99d4c072e0b6",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "WAYLA - Generating Images from Eye Movements",
      "prompt": "Write the abstract for the academic paper titled \"WAYLA - Generating Images from Eye Movements\".",
      "generation": "In this paper, we present an innovative approach for generating images using eye movements. Our proposed system, named WAYLA, utilizes a neural network to interpret eye movements and generate images based on the user's gaze. The system is designed to be intuitive and user-friendly, allowing individuals with limited artistic skills to create personalized images by simply looking at a screen.\n\nWAYLA is trained on a large dataset of eye movements and corresponding images, enabling it to learn the relationship between gaze patterns and visual content. The system can generate images in a variety of styles, including abstract art, portraits, and landscapes.\n\nThe evaluation results show that WAYLA is capable of generating high-quality images that are visually appealing and align with the user's intended vision. The system can be applied in various domains, including art, design, and entertainment.\n\nWAYLA - Generating Images from Eye Movements presents a novel approach to image generation that is intuitive, user-friendly, and capable of producing high-quality images. The system has the potential to revolutionize the way images are created and provide a new avenue for artistic expression."
    },
    {
      "id": "b57eabec-8670-4c80-964f-d2cb557d30b3",
      "adv_source_id": "b57eabec-8670-4c80-964f-d2cb557d30b3",
      "source_id": "5394c7c4-535b-4f88-9fdb-c89387b8b5db",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes",
      "prompt": "Write the abstract for the academic paper titled \"Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes\".",
      "generation": "Here is an abstract for the academic paper* titled \"Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes\":\n\nAbstract: Medical image segmentation plays a crucial role in the diagnosis and treatment of various medical conditions. In this paper, we propose a novel approach for medical image segmentation based on a multi-modal convolutional neural network (CNN). We explore the use of multiple imaging modalities, such as MRI and CT, to enhance the segmentation accuracy. Our approach combines the strengths of different imaging modalities by fusing the corresponding feature maps from the CNN. We evaluate the performance of our approach on a public dataset and compare it with state-of-the-art methods. The results show that our approach significantly improves the segmentation accuracy and is robust to noise and artifacts. We also perform a comprehensive study on different image fusion schemes and discuss the impact of each scheme on the segmentation performance. Our work contributes to the development of more accurate and robust medical image segmentation methods for improved clinical decision-making.\n\n*Note: The abstract is written in a general context and should be replaced with the specific details of the paper."
    },
    {
      "id": "58f56b71-a4e5-4b21-ad17-58aaee4ac1be",
      "adv_source_id": "58f56b71-a4e5-4b21-ad17-58aaee4ac1be",
      "source_id": "5394c7c4-535b-4f88-9fdb-c89387b8b5db",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes",
      "prompt": "Write the abstract for the academic paper titled \"Medical Image Segmentation Based on Multi-Modal Convolutional Neural Network: Study on Image Fusion Schemes\".",
      "generation": "Abstract:\nMedical image segmentation plays a crucial role in the computer-aided diagnosis and treatment of medical conditions. In this paper, we present a multi-modal convolutional neural network (MCNN) for medical image segmentation based on the fusion of two different imaging modalities: magnetic resonance imaging (MRI) and computed tomography (CT) scans. Our MCNN is designed to take advantage of the complementary information contained in both MRI and CT scans, and it is trained on a large dataset of medical images. We evaluate the performance of our MCNN on a variety of medical image segmentation tasks, and compare it to state-of-the-art single-modal segmentation networks. Our results show that our MCNN significantly improves segmentation accuracy and outperforms existing approaches in terms of both precision and recall. We also perform a comprehensive study on the impact of different image fusion schemes on the performance of our MCNN, and discuss the practical implications of our approach in the context of medical image analysis."
    },
    {
      "id": "4a0ba10a-1c11-480a-b5fb-40374de4c832",
      "adv_source_id": "4a0ba10a-1c11-480a-b5fb-40374de4c832",
      "source_id": "8c0e30f7-a6f7-4360-908b-b484f52814ec",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks",
      "prompt": "Write the abstract for the academic paper titled \"Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks\".",
      "generation": "Abstract:\nIn this paper, we present an approach for historical document image segmentation with LDA-initialized deep neural networks. Our method utilizes the Latent Dirichlet Allocation (LDA) algorithm to extract the underlying topic structure from a set of document images. The resulting topic representations are then used to initialize the deep neural network, which is trained to segment the document images into text and non-text regions. Our approach is particularly useful for historical document images, which often suffer from poor image quality, uneven illumination, and complex backgrounds. The experimental results show that our method outperforms the state-of-the-art in terms of segmentation accuracy and running time."
    },
    {
      "id": "4289b671-8ca2-4fac-83eb-b5da08188de2",
      "adv_source_id": "4289b671-8ca2-4fac-83eb-b5da08188de2",
      "source_id": "8c0e30f7-a6f7-4360-908b-b484f52814ec",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks",
      "prompt": "Write the abstract for the academic paper titled \"Historical Document Image Segmentation with LDA-Initialized Deep Neural Networks\".",
      "generation": "Abstract:\nIn this paper, we present an approach for historical document image segmentation with LDA-initialized deep neural networks. Our method utilizes the popular convolutional neural network (CNN) architecture and leverages the benefits of topic modeling with Latent Dirichlet Allocation (LDA). We demonstrate that by initializing the CNN with LDA-derived topic representations, our approach can effectively segment historical document images into text and non-text regions. To evaluate the performance of our approach, we conducted extensive experiments on a benchmark dataset and compared the results with state-of-the-art methods. Our results show that our LDA-initialized CNN outperforms the compared methods in terms of segmentation accuracy, demonstrating the effectiveness of our approach for historical document image segmentation."
    },
    {
      "id": "c322c557-a217-46be-8b6a-13da954dde73",
      "adv_source_id": "c322c557-a217-46be-8b6a-13da954dde73",
      "source_id": "b92ae04c-342d-4221-8ffc-cf81dae44f5b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm",
      "prompt": "Write the abstract for the academic paper titled \"FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm\".",
      "generation": "The paper presents a re-configurable hardware based parallelized graph-based image segmentation algorithm. The algorithm has been implemented on a Field Programmable Gate Array (FPGA) platform to achieve real-time performance for segmenting images into meaningful regions. In this work, we used a parallelized implementation of the algorithm to achieve a high throughput. The implementation has been done in VHDL, and the synthesis results show that our design can achieve a throughput of more than 10 frames per second for images of size 1024x768 pixels. The design has been tested on a Xilinx Virtex-6 FPGA."
    },
    {
      "id": "03dec135-2826-4899-8d18-ca1533e484f6",
      "adv_source_id": "03dec135-2826-4899-8d18-ca1533e484f6",
      "source_id": "b92ae04c-342d-4221-8ffc-cf81dae44f5b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm",
      "prompt": "Write the abstract for the academic paper titled \"FPGA based Parallelized Architecture of Efficient Graph based Image Segmentation Algorithm\".",
      "generation": "The paper presents a reconfigurable hardware-based parallelized architecture of an efficient graph-based image segmentation algorithm. The algorithm is designed to segment images quickly and efficiently, using a graph-based approach that allows for parallel processing. The FPGA-based implementation is designed to take advantage of the parallel processing capabilities of FPGAs, allowing for faster processing times compared to traditional CPU-based implementations. The architecture is composed of a CPU interface, a memory controller, and a parallelized graph-based segmentation engine. The engine is designed to process multiple pixels simultaneously, utilizing the parallel processing capabilities of the FPGA. The memory controller is designed to handle the transfer of data between the CPU and the FPGA, allowing for efficient communication between the two. The CPU interface is designed to provide a simple and straightforward interface for the CPU to control the segmentation engine. The paper will discuss the architecture of the implemented system, as well as the results of testing and evaluation. The results show that the FPGA-based implementation is significantly faster than the CPU-based implementation, with improved performance and reduced processing time.\n\nThe parallelized architecture is designed to take advantage of recent advances in FPGA technology, which have made them more powerful and easier to program. The architecture is composed of a CPU interface, a memory controller, and a parallelized graph-based segmentation engine. The paper will discuss the implementation details of each component and show how they work together to achieve efficient image segmentation.\n\nThe results of testing and evaluation show that the FPGA-based implementation is significantly faster than the CPU-based implementation, with improved performance and reduced processing time. The paper will discuss the performance improvements and show how they can be attributed to the parallelized architecture and the efficient utilization of FPGA resources."
    },
    {
      "id": "bc74da9a-771c-4302-a4e7-bc2ebdcf48c7",
      "adv_source_id": "bc74da9a-771c-4302-a4e7-bc2ebdcf48c7",
      "source_id": "823822b8-9b81-4fd1-bdd1-0246093e110f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation\".",
      "generation": "The paper presents an approach for medical image segmentation based on a deep learning framework called DeepIGeoS. The framework utilizes a geodesic distance map to encode the spatial relationships between pixels, and a deep learning model to learn the segmentation criteria from the input images. The model is trained on a large dataset of medical images and their corresponding ground truth segmentations. The experimental results show that the proposed approach achieves state-of-the-art performance on multiple medical image segmentation benchmarks. The code and trained models are made publicly available to facilitate further research and development."
    },
    {
      "id": "f99c6e30-5e26-420e-994f-6da39073137e",
      "adv_source_id": "f99c6e30-5e26-420e-994f-6da39073137e",
      "source_id": "823822b8-9b81-4fd1-bdd1-0246093e110f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation\".",
      "generation": "The abstract for the academic paper titled \"DeepIGeoS: A Deep Interactive Geodesic Framework for Medical Image Segmentation\" would be:\n\nThis paper presents DeepIGeoS, a deep learning framework for interactive medical image segmentation. The proposed method utilizes a geodesic distance map, computed from a pre-trained convolutional neural network (CNN), to enable efficient and effective segmentation of medical images. The geodesic distance map guides the segmentation process by providing a measure of similarity between pixels, allowing for intelligent pixel propagation and improved segmentation results. The framework is designed to be interactive, allowing users to iteratively refine the segmentation by providing feedback and guidance to the model. The proposed method has been evaluated on a public dataset and compared to state-of-the-art segmentation methods, demonstrating its effectiveness and efficiency.\n\nWould you like me to generate the whole paper for you?"
    },
    {
      "id": "3bb53334-a435-4a76-a6d7-6be7d696d938",
      "adv_source_id": "3bb53334-a435-4a76-a6d7-6be7d696d938",
      "source_id": "c0009c67-d594-4da3-af81-a06984e70ab5",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "4D Multi-atlas Label Fusion using Longitudinal Images",
      "prompt": "Write the abstract for the academic paper titled \"4D Multi-atlas Label Fusion using Longitudinal Images\".",
      "generation": "The paper discusses the challenges of labeling longitudinal images for 4D multi-atlas label fusion. The aim is to address the issue of missing labels in longitudinal image sequences and improve the accuracy of the fused labels. The proposed method combines the strengths of 3D and 4D label fusion techniques. 3D label fusion is performed first to obtain a preliminary fused label. Then, a 4D label fusion is performed using the longitudinal image sequences to refine the fused label. To handle the missing labels, an imputation method based on the fused label is proposed. The experimental results show that the proposed method improves the accuracy of the fused labels and is suitable for 4D multi-atlas label fusion using longitudinal images."
    },
    {
      "id": "7d2646d3-3e87-4c4f-8032-6d96fafa2c44",
      "adv_source_id": "7d2646d3-3e87-4c4f-8032-6d96fafa2c44",
      "source_id": "c0009c67-d594-4da3-af81-a06984e70ab5",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "4D Multi-atlas Label Fusion using Longitudinal Images",
      "prompt": "Write the abstract for the academic paper titled \"4D Multi-atlas Label Fusion using Longitudinal Images\".",
      "generation": "The abstract for the academic paper titled \"4D Multi-atlas Label Fusion using Longitudinal Images\" could be:\n\nThis paper presents a novel approach for 4D multi-atlas label fusion using longitudinal images. We propose a new algorithm that combines the labels of multiple atlases to generate a fused label for each subject in a longitudinal study. The algorithm uses a cost function to measure the similarity between the labels of the atlases and the longitudinal images, and a optimization process to find the best combination of labels for each subject. The proposed method has been evaluated on a dataset of longitudinal images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) and has been shown to improve the accuracy of the fused labels compared to existing methods. The results show that the proposed method is a promising approach for 4D multi-atlas label fusion using longitudinal images."
    },
    {
      "id": "751de46a-8c2f-45c5-817c-76dcb0a4215b",
      "adv_source_id": "751de46a-8c2f-45c5-817c-76dcb0a4215b",
      "source_id": "7184b7e7-e328-4ad1-acd3-593389fce265",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation by Iterative Inference from Conditional Score Estimation",
      "prompt": "Write the abstract for the academic paper titled \"Image Segmentation by Iterative Inference from Conditional Score Estimation\".",
      "generation": "In this paper, we present an image segmentation algorithm using a conditional score estimation approach. Our algorithm iteratively infers the optimal segmentation by maximizing the conditional score of the image given the segmentation. We estimate the conditional score by a deep neural network, which takes the image and the segmentation as input and outputs a score map. We then use a graph cuts algorithm to optimize the segmentation by maximizing the conditional score. Our approach is fully automatic and does not require any manual initialization or post-processing steps. We evaluate our algorithm on the Cityscapes and CamVid datasets, and show that it achieves state-of-the-art performance in terms of segmentation quality. We also perform a comprehensive analysis of the influence of the proposed method on various segmentation aspects, and discuss the implications for future research."
    },
    {
      "id": "7f453654-e252-4163-ad67-490c03820b48",
      "adv_source_id": "7f453654-e252-4163-ad67-490c03820b48",
      "source_id": "7184b7e7-e328-4ad1-acd3-593389fce265",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation by Iterative Inference from Conditional Score Estimation",
      "prompt": "Write the abstract for the academic paper titled \"Image Segmentation by Iterative Inference from Conditional Score Estimation\".",
      "generation": "The paper presents an image segmentation algorithm that iteratively infers the optimal segmentation by scoring candidate segmentations. The scoring process is based on a conditional score estimation framework, where the score of a candidate segmentation is a function of the image and the candidate segmentation. The algorithm is efficient and effective, with a performance that is comparable to state-of-the-art image segmentation algorithms.\n\nThe paper focuses on the development of the conditional score estimation framework and the efficient iterative inference algorithm. The framework is based on a supervised learning approach, where a scoring function is learned from a set of labeled training images. The scoring function is designed to estimate the conditional score of a candidate segmentation given an image and the candidate segmentation. The iterative inference algorithm is based on a graph-cut optimization algorithm, which iteratively improves the segmentation by selecting the candidate segmentation with the highest score.\n\nThe paper evaluates the performance of the algorithm on a set of image segmentation benchmark datasets. The results show that the algorithm is competitive with state-of-the-art image segmentation algorithms in terms of accuracy and efficiency. The paper also discusses the potential applications of the algorithm in various fields, including medical imaging, remote sensing, and computer vision."
    },
    {
      "id": "ffe731b4-649f-45b3-9bae-4d9962d008dd",
      "adv_source_id": "ffe731b4-649f-45b3-9bae-4d9962d008dd",
      "source_id": "8a74b221-8794-4a05-b598-b856f58d6f5e",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection",
      "prompt": "Write the abstract for the academic paper titled \"Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection\".",
      "generation": "Abstract:\nModel-based dataset generation is a crucial technique for training accurate machine learning models. In this paper, we present a method for automatically generating datasets for crop and weeds detection based on pre-existing models. Our approach utilizes a pre-trained model to generate synthetic data that closely resembles the real world data distribution. We demonstrate that our method can generate high quality datasets with minimal effort and time, and can improve the accuracy of crop and weeds detection models. Furthermore, we show that our method is general and can be applied to other similar detection tasks.\n\n```"
    },
    {
      "id": "64f017c7-d354-4379-a87d-e1f153f05bf1",
      "adv_source_id": "64f017c7-d354-4379-a87d-e1f153f05bf1",
      "source_id": "8a74b221-8794-4a05-b598-b856f58d6f5e",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection",
      "prompt": "Write the abstract for the academic paper titled \"Automatic Model Based Dataset Generation for Fast and Accurate Crop and Weeds Detection\".",
      "generation": "Abstract:\nModel-based dataset generation is a crucial process in machine learning. It involves the use of a reference model to create a dataset that can be used to train a machine learning model. The quality of the dataset generated is critical for the performance of the machine learning model. In this paper, we present a new approach for automatic model-based dataset generation for crop and weeds detection. Our approach utilizes a reference model constructed using a small set of labeled examples. The reference model is then used to generate a larger dataset with a variety of crop and weed types. The generated dataset is then used to train a machine learning model for crop and weeds detection. Our approach is fast and accurate, and it produces a high-quality dataset that improves the performance of the machine learning model.\n\n upheaval detection remains a challenging task in agriculture. It is time-consuming and tedious, as farmers or agricultural experts need to inspect every crop row in the field. Model-based approaches offer a promising solution, but they require extensive datasets for training. Our proposed method aims to generate such datasets automatically using a reference model, thus eliminating the need for manual labor and improving the accuracy of crop and weed detection systems."
    },
    {
      "id": "8d0a4f91-625f-44e1-8a97-d53d0dcc0cc3",
      "adv_source_id": "8d0a4f91-625f-44e1-8a97-d53d0dcc0cc3",
      "source_id": "48cae5f6-0109-460d-9e7f-1ca679052e18",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Locally Adapting Technique for Boundary Detection using Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Locally Adapting Technique for Boundary Detection using Image Segmentation\".",
      "generation": "The paper presents a technique for boundary detection using image segmentation. It adapts to local image characteristics and provides accurate detection of object boundaries. Initially, the image is pre-processed using Gaussian filtering to reduce noise. Then, the image is segmented using a modified watershed segmentation algorithm. Subsequently, the boundaries of the objects in the image are extracted. The proposed technique is evaluated using a database of images. The experimental results demonstrate the effectiveness of the proposed technique in terms of accuracy and efficiency. It achieves a high degree of accuracy in boundary detection and outperforms existing techniques in terms of efficiency."
    },
    {
      "id": "83dec1bd-6b9f-45c6-8995-c2122236d881",
      "adv_source_id": "83dec1bd-6b9f-45c6-8995-c2122236d881",
      "source_id": "48cae5f6-0109-460d-9e7f-1ca679052e18",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Locally Adapting Technique for Boundary Detection using Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Locally Adapting Technique for Boundary Detection using Image Segmentation\".",
      "generation": "The paper presents a novel and effective technique for boundary detection using image segmentation. It addresses the problem of detecting boundaries in images that have complex shapes and textures. The proposed method combines edge detection and region segmentation to extract meaningful boundaries from the image. It first applies an edge detection algorithm to identify regions of interest and then uses a region segmentation algorithm to divide the regions into smaller segments. The segments are then classified into different categories based on their features, such as size, shape, and texture. The resulting boundaries are refined using a morphological operation to produce accurate and smooth boundaries. The experimental results show that the proposed method produces high-quality boundaries and outperforms existing techniques in terms of accuracy and speed."
    },
    {
      "id": "2a8851af-efdb-41e2-9a5d-7b0ce2ce3078",
      "adv_source_id": "2a8851af-efdb-41e2-9a5d-7b0ce2ce3078",
      "source_id": "71653c06-9860-47b9-9528-c636e6e14767",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Aerial Image Segmentation from Online Maps",
      "prompt": "Write the abstract for the academic paper titled \"Learning Aerial Image Segmentation from Online Maps\".",
      "generation": "Here is an abstract for the academic paper* titled \"Learning Aerial Image Segmentation from Online Maps\":\n\nAbstract:\nWe present a new approach to training an aerial image segmentation model using online maps. Our method uses a convolutional neural network (CNN) to segment aerial images into different land cover classes. We train our model using a large dataset of aerial images collected from online maps, and our results show that our model can accurately segment aerial images into different land cover classes. Our method is simple, efficient, and cost-effective, and it can be used to segment large areas of land quickly and accurately.\n\n*Note: This abstract is written informatively and succinctly, and provides a clear overview of the paper. However, it is not the original abstract of the paper \"Learning Aerial Image Segmentation from Online Maps\"."
    },
    {
      "id": "509bf9bd-0d28-4325-968e-8810c143f7cb",
      "adv_source_id": "509bf9bd-0d28-4325-968e-8810c143f7cb",
      "source_id": "71653c06-9860-47b9-9528-c636e6e14767",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Aerial Image Segmentation from Online Maps",
      "prompt": "Write the abstract for the academic paper titled \"Learning Aerial Image Segmentation from Online Maps\".",
      "generation": "Here is the beginning of the abstract for the academic paper titled, \"Learning Aerial Image Segmentation from Online Maps\":\n\nAutomatic segmentation of aerial images obtained from online maps is a challenging task due to the inherent complexity and variability of the urban scene. In this paper, we propose a novel approach to learn a segmentation model directly from online maps, which are a rich and diverse source of annotated data. We train a convolutional neural network using a large dataset of maps and aerial images, and show that our model can accurately segment aerial images. Our approach is highly general and can be used to segment aerial images in a wide range of urban environments. We evaluate our approach on a large dataset of aerial images and show that it outperforms state-of-the-art segmentation methods. We also show that our approach is highly efficient and can be used to segment large datasets of aerial images in a reasonable amount of time."
    },
    {
      "id": "7184b7d4-2ddc-455e-ace9-52de81360712",
      "adv_source_id": "7184b7d4-2ddc-455e-ace9-52de81360712",
      "source_id": "2180a38c-3528-420b-b712-f2cb27a3d6fa",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task",
      "prompt": "Write the abstract for the academic paper titled \"On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task\".",
      "generation": "We introduce a new architecture for 3D convolutional neural networks (CNNs) that aims to improve the representation power and efficiency of 3D CNNs for medical image analysis tasks. Our new architecture, called 3D ResNet, is based on the ResNet architecture and incorporates 3D convolutional layers and residual connections. We propose a new brain parcellation pretext task that takes advantage of the architecture's ability to extract complex features from medical images. Our pretext task is designed to improve the representation power of the network by encouraging it to learn a hierarchy of features that are relevant to the task. We show that our new architecture outperforms the state-of-the-art in terms of representation power and efficiency on a variety of medical image analysis tasks, including brain parcellation, and one task in the domain of chest radiography."
    },
    {
      "id": "d4166786-a2bf-4532-b165-1f454da9aa70",
      "adv_source_id": "d4166786-a2bf-4532-b165-1f454da9aa70",
      "source_id": "2180a38c-3528-420b-b712-f2cb27a3d6fa",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task",
      "prompt": "Write the abstract for the academic paper titled \"On the Compactness, Efficiency, and Representation of 3D Convolutional Networks: Brain Parcellation as a Pretext Task\".",
      "generation": "We introduce a new architecture for learning on 3D objects, designed to improve the computational performance, representation power, and training data efficiency of previous models. Our network is constructed using 3D convolutional layers with small filters, which gives it a large field of view and a non-linear, hierarchical architecture. This enables the network to learn complex, multi-scale features within a single end-to-end training process and without the need for manual feature engineering. We demonstrate the effectiveness of our approach on the problem of annotating the cortical sheet of the human brain with a parcellation, and show results competitive with previous state-of-the-art methods, while requiring fewer training examples."
    },
    {
      "id": "213b3bf2-d400-4635-a7e7-909711e82d61",
      "adv_source_id": "213b3bf2-d400-4635-a7e7-909711e82d61",
      "source_id": "f780a932-62e7-469a-97e8-c3f2d1ef3e95",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Tversky loss function for image segmentation using 3D fully convolutional deep networks",
      "prompt": "Write the abstract for the academic paper titled \"Tversky loss function for image segmentation using 3D fully convolutional deep networks\".",
      "generation": "Abstract:\nThe Tversky loss function is a generalization of the popular cross-entropy loss function that is used for image segmentation using 3D fully convolutional deep networks. The Tversky loss function incorporates the concept of uncertainty and is more robust to noise and outliers in the data. We show that the Tversky loss function improves the performance of image segmentation models, particularly in challenging scenarios such as low contrast and noisy images. We evaluate our approach on the Cityscapes and KITTI datasets, and show that our method achieves state-of-the-art performance.\n\nBibtex:\n@article{tversky-loss-function-for-image-segmentation-using-3d-fully-convolutional-deep-networks,\n\ttitle = {Tversky loss function for image segmentation using 3D fully convolutional deep networks},\n\tauthor = {First Author and Second Author},\n\tyear = {2023},\n\tjournal = {International Journal of Computer Vision},\n}"
    },
    {
      "id": "5ec40fa3-97b7-4395-ba51-28f7c0543cf3",
      "adv_source_id": "5ec40fa3-97b7-4395-ba51-28f7c0543cf3",
      "source_id": "f780a932-62e7-469a-97e8-c3f2d1ef3e95",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Tversky loss function for image segmentation using 3D fully convolutional deep networks",
      "prompt": "Write the abstract for the academic paper titled \"Tversky loss function for image segmentation using 3D fully convolutional deep networks\".",
      "generation": "Abstract:\nThe Tversky loss function is a generalization of the popular cross-entropy loss function that is used for image segmentation using 3D fully convolutional deep networks. The Tversky loss function incorporates the uncertainty of the model in addition to the cross-entropy loss function, which improves the model's performance in segmenting images with noisy or ambiguous boundaries. We evaluate the Tversky loss function on the Cityscapes and KITTI datasets, and show that it improves the model's performance on both datasets compared to the cross-entropy loss function. We also show that the Tversky loss function is more effective at segmenting images with noisy or ambiguous boundaries than the cross-entropy loss function.\n\nMgcommons.com:\nA novel image segmentation approach using 3D fully convolutional deep networks and the Tversky loss function is proposed. In order to improve the performance of the model in segmenting images with noisy or ambiguous boundaries, the Tversky loss function is used to incorporate the uncertainty of the model. The proposed method showed improved performance over the cross-entropy loss function on the Cityscapes and KITTI datasets. The Tversky loss function proved to be more effective in segmenting images with noisy or ambiguous boundaries in comparison to the cross-entropy loss function. The experimental results show that the proposed method can effectively segment images and improve the model's performance in noisy or ambiguous boundary conditions."
    },
    {
      "id": "4285d35d-0d1d-4ac2-b5da-fe0e4cf809b8",
      "adv_source_id": "4285d35d-0d1d-4ac2-b5da-fe0e4cf809b8",
      "source_id": "1fbea034-ec8e-4bf2-81b5-3b627f0a9b4f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images",
      "prompt": "Write the abstract for the academic paper titled \"A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images\".",
      "generation": "Below is an abstract for the academic paper titled, \"A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images:\n\nRecent progress in ultrasound (US) image segmentation has achieved remarkable performance in various applications. However, most existing methods are dedicated to specific organs and lack the generalization ability to other organs or pathologies. In this paper, we propose a dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in US images. Our method incorporates multi-scale convolutional neural network (CNN) feature maps and anatomical knowledge into the graph-cuts framework. The multi-scale CNN feature maps capture the global and local structures of the US images, while the anatomical knowledge improves the accuracy of the segmentation by enforcing the consistency of the predicted contours with the expected kidney shapes. We evaluate our method on a public US image dataset and compare it with state-of-the-art segmentation methods. The experimental results demonstrate the effectiveness and superiority of our method in terms of segmentation accuracy and efficiency."
    },
    {
      "id": "17735ba9-c8e4-405a-8596-0f98880398ee",
      "adv_source_id": "17735ba9-c8e4-405a-8596-0f98880398ee",
      "source_id": "1fbea034-ec8e-4bf2-81b5-3b627f0a9b4f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images",
      "prompt": "Write the abstract for the academic paper titled \"A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images\".",
      "generation": "Below is an abstract for the academic paper titled, \"A dynamic graph-cuts method with integrated multiple feature maps for segmenting kidneys in ultrasound images:\n\nRecent progress in ultrasound (US) image analysis has led to many segmentation algorithms. However, most of these algorithms are dedicated to binary segmentation and hence, lack the capability to segment the kidney with renal pelves and renal contours both included. In this paper, we propose a new graph-cuts-based semi-automatic segmentation approach that can reliably extract the whole kidney region including both the renal pelvis and renal contour from US images. To achieve this, we integrate multiple feature maps, including shape, texture, and context, into a dynamic graph-cuts framework in a mutually beneficial way. The shape feature map highlights the region of interest, whereas the texture feature map refines the segmentation of the kidney. To alleviate the burden of manually selecting the regions to be included in the graph-cuts framework, we propose a context feature map that automatically selects the appropriate regions. The proposed method was evaluated on 100 kidney US images and was compared with manual segmentation by three experienced radiologists. The results show that the proposed method achieves an accuracy of 96.43%, a sensitivity of 95.83%, and a specificity of 96.94%."
    },
    {
      "id": "deb666cb-b8db-45d5-aa3d-34d99bdd0067",
      "adv_source_id": "deb666cb-b8db-45d5-aa3d-34d99bdd0067",
      "source_id": "43bf4bad-9105-4bdf-bd78-d1ccdfa6dbfc",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
      "prompt": "Write the abstract for the academic paper titled \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\".",
      "generation": "Abstract:\nDeepLab is a state-of-the-art semantic image segmentation model that combines deep convolutional nets, atrous convolution, and fully connected conditional random fields (CRFs). Atrous convolution allows the model to utilize larger filters than the input size, which is particularly useful for accurately localizing objects in images. The use of CRFs further improves the model's performance by considering spatial dependencies between pixels. We evaluate DeepLab on the PASCAL VOC 2012 dataset and achieve a mIOU score of 79.7%, which is a significant improvement over existing models. Our approach demonstrates the power of deep learning in semantic image segmentation and sets a new benchmark for the task."
    },
    {
      "id": "08a92ba6-1481-4029-9be7-9d5a20c74d50",
      "adv_source_id": "08a92ba6-1481-4029-9be7-9d5a20c74d50",
      "source_id": "43bf4bad-9105-4bdf-bd78-d1ccdfa6dbfc",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
      "prompt": "Write the abstract for the academic paper titled \"DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs\".",
      "generation": "DeepLab is a state-of-the-art algorithm for semantic image segmentation, which aims to assign a semantic label to each pixel in an input image. The algorithm is based on a deep convolutional neural network (DCNN) and a fully connected conditional random field (CRF).\n\nAtrous convolution, also known as dilated convolution, is a technique used to increase the field of view of the convolutional filters, allowing the network to capture more context information. This is particularly useful for tasks such as image segmentation, where the spatial layout of the features in an image is important.\n\nIn this paper, we present a new DCNN architecture called DeepLab, which incorporates atrous convolution and a fully connected CRF. We show that this architecture achieves state-of-the-art performance on the semantic image segmentation task, improving over previous methods by a large margin. We also present a comprehensive evaluation of the algorithm on a variety of datasets, including the PASCAL VOC 2012 dataset, and show that it is effective in a wide range of scenarios."
    },
    {
      "id": "901ace3d-1ff0-4745-945a-4a39b7b60c50",
      "adv_source_id": "901ace3d-1ff0-4745-945a-4a39b7b60c50",
      "source_id": "771f8e8c-a41a-410b-bb61-2b0e70467c95",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Object Discovery via Cohesion Measurement",
      "prompt": "Write the abstract for the academic paper titled \"Object Discovery via Cohesion Measurement\".",
      "generation": "An important problem in object discovery is to identify objects of interest in a given image. This paper approaches the problem by proposing a novel framework that measures the cohesion of objects in images. Our framework comprises a convolutional neural network (CNN) based encoder and a reinforcement learning based decoder. The encoder extracts multi-scale features from the input image, while the decoder learns to discover objects by iteratively refining object proposals. To measure the cohesion of objects, we introduce a novel loss function that combines the Chamfer distance and the Euclidean distance between the predicted and ground truth object boundaries. Our experimental results show that our approach achieves state-of-the-art performance on the VOC 2012 and VOC 2017 object discovery datasets."
    },
    {
      "id": "ccb129bb-6334-4edf-b94d-8285d4d0c70c",
      "adv_source_id": "ccb129bb-6334-4edf-b94d-8285d4d0c70c",
      "source_id": "771f8e8c-a41a-410b-bb61-2b0e70467c95",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Object Discovery via Cohesion Measurement",
      "prompt": "Write the abstract for the academic paper titled \"Object Discovery via Cohesion Measurement\".",
      "generation": "An important problem in object discovery is to identify objects of interest in a given image. This paper approaches the problem by measuring object cohesion, which captures how much an object belongs to a given image. We propose a simple yet effective approach to measuring object cohesion, which is based on the idea of comparing image patches with a set of object patches. Our approach is fully automated, efficient, and easy to use, and it can discover objects in a wide variety of images. We demonstrate the effectiveness of our approach on a large dataset of images, and show that our approach can discover objects that are difficult to find using other methods."
    },
    {
      "id": "a416bdce-9363-4b7d-b100-5397f5e06f96",
      "adv_source_id": "a416bdce-9363-4b7d-b100-5397f5e06f96",
      "source_id": "1afb7c7d-24f7-48d5-9f6a-aa755c67bca9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities",
      "prompt": "Write the abstract for the academic paper titled \"Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities\".",
      "generation": "The abstract for the academic paper titled \"Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities\" could be:\n\nMedical image segmentation is a fundamental task in radiology with deep learning techniques achieving promising results in recent years. However, the majority of the state-of-the-art approaches focus on single-modality images with limited research on multi-modality images. This paper aims to address the gap in research by proposing a deep learning-based approach for multi-task medical image segmentation in multiple modalities. A comprehensive review of the current state of the art in multi-modality medical image segmentation is provided, followed by a discussion on the advantages and limitations of the existing approaches. Our proposed approach combines a U-Net-based architecture with a multi-task learning strategy, where a shared encoder-decoder network is trained on a dataset of multi-modality images. The network is designed to segment multiple anatomical structures in a single forward pass, with separate output layers for each structure. The training process is performed using a cross-entropy loss function with an additional regularization term to encourage collaborative learning among the tasks. The proposed approach is evaluated on a publicly available multi-modality medical image segmentation dataset, and the results show that our method achieves state-of-the-art performance in terms of both segmentation accuracy and computational efficiency."
    },
    {
      "id": "11d3afeb-f3e5-4ae3-a0c8-2279f62d352a",
      "adv_source_id": "11d3afeb-f3e5-4ae3-a0c8-2279f62d352a",
      "source_id": "1afb7c7d-24f7-48d5-9f6a-aa755c67bca9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities",
      "prompt": "Write the abstract for the academic paper titled \"Deep Learning for Multi-Task Medical Image Segmentation in Multiple Modalities\".",
      "generation": "Deep learning has emerged as a powerful tool for medical image segmentation in multiple modalities. In this paper, we present a comprehensive review of the current state of the art in multi-task medical image segmentation, with a focus on deep learning techniques. We discuss the advantages and challenges of training deep learning models for multi-task segmentation, and review the most commonly used architectures, including fully convolutional networks (FCNs), U-nets, and encoder-decoder networks. We also review the current state of the art in multi-modal medical image segmentation, including methods that combine multiple modalities such as MRI, CT, and ultrasound. Finally, we discuss the potential applications of multi-task and multi-modal segmentation in clinical practice, and review the most commonly used evaluation metrics for these methods. This review aims to provide a comprehensive overview of the current state of the art in deep learning for multi-task and multi-modal medical image segmentation."
    },
    {
      "id": "df955419-fc46-4450-8a9f-d8a75543268f",
      "adv_source_id": "df955419-fc46-4450-8a9f-d8a75543268f",
      "source_id": "26d1f313-f244-4df4-9929-c66886bb29b3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Structured Learning of Tree Potentials in CRF for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Structured Learning of Tree Potentials in CRF for Image Segmentation\".",
      "generation": "We present a method for learning tree potentials in conditional random fields (CRFs) for image segmentation. Our approach leverages the structural information available in the CRF and learns a tree potential that is tailored to the specific problem. We introduce a novel loss function that combines an adversarial loss with a reconstruction loss, which allows us to effectively learn the tree potential. Our approach is highly efficient, and we demonstrate state-of-the-art performance on the challenging task of image segmentation."
    },
    {
      "id": "2012b51b-cdbe-45ba-ac22-34e42e9fdf94",
      "adv_source_id": "2012b51b-cdbe-45ba-ac22-34e42e9fdf94",
      "source_id": "26d1f313-f244-4df4-9929-c66886bb29b3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Structured Learning of Tree Potentials in CRF for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Structured Learning of Tree Potentials in CRF for Image Segmentation\".",
      "generation": "We propose a novel approach for learning tree potentials in conditional random fields (CRFs) for image segmentation. Our method is based on structured output learning (SOL), which allows us to train a CRF with tree-structured outputs using a supervised learning approach. We introduce a novel architecture called the tree-CRF, which combines the advantages of both the CRF and the tree-structured output model. Our method enables the use of a linear-time algorithm for inference, which is crucial for image segmentation tasks with large datasets. We demonstrate the effectiveness of our approach on the Berkeley segmentation dataset, where we achieve state-of-the-art performance."
    },
    {
      "id": "685f7629-4122-49da-845e-14ba0f668477",
      "adv_source_id": "685f7629-4122-49da-845e-14ba0f668477",
      "source_id": "722f13c4-cf25-4279-9191-a023b49e53ea",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge",
      "prompt": "Write the abstract for the academic paper titled \"k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge\".",
      "generation": "The abstract for the academic paper titled \"k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge\":\n\nWe present a segmentation algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge, based on a combination of k-means clustering and an ensemble of regressions. The proposed algorithm is designed to segment skin lesions in dermoscopic images, and it achieves competitive results compared to other state-of-the-art algorithms. The key idea is to use a set of regression trees to learn a mapping from the image pixels to lesion segmentation labels, and then to combine the predictions of these trees using an ensemble method. The ensemble method is based on k-means clustering, which allows us to combine the predictions of multiple regression trees into a single segmentation result. We evaluate the proposed algorithm on the ISIC 2017 Skin Lesion Segmentation Challenge dataset, and compare its performance to other state-of-the-art algorithms. Our results show that the proposed algorithm is competitive in terms of segmentation accuracy and efficiency."
    },
    {
      "id": "8a2b6d40-3b55-493d-aadb-600467df7398",
      "adv_source_id": "8a2b6d40-3b55-493d-aadb-600467df7398",
      "source_id": "722f13c4-cf25-4279-9191-a023b49e53ea",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge",
      "prompt": "Write the abstract for the academic paper titled \"k-Means Clustering and Ensemble of Regressions: An Algorithm for the ISIC 2017 Skin Lesion Segmentation Challenge\".",
      "generation": "Skin lesion segmentation from dermoscopic images plays a significant role in the early detection of skin cancer. Due to the diversity of lesions and complex background information, this task is extremely challenging. Many methods based on machine learning have been proposed, but they still struggle with accuracy. In this paper, we propose a new approach to skin lesion segmentation based on k-means clustering and an ensemble of regressions. We begin by using a convolutional neural network to extract multi-scale features from the images. The proposed ensemble of regressions is then applied to the extracted features for segmentation. In order to handle the class imbalance problem, we use synthetic samples to augment the dataset. We evaluate our method on the ISIC 2017 Skin Lesion Segmentation Challenge dataset and achieve a high classification accuracy."
    },
    {
      "id": "92f37bf5-38ef-4b57-bcad-d2a2e5bf0a0c",
      "adv_source_id": "92f37bf5-38ef-4b57-bcad-d2a2e5bf0a0c",
      "source_id": "ce2077bf-e2d9-4ae6-b49b-25b586e4edf5",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation\".",
      "generation": "The paper presents a novel approach for learning normalized inputs for iterative estimation in medical image segmentation. The proposed method aims to address the challenge of lack of training data by leveraging normalized inputs to enable the use of a larger set of training data. The experimental results demonstrate the effectiveness of the proposed method in improving the performance of the segmentation model. Further, the paper discusses the impact of the choice of normalization function on the performance of the model and provides recommendations for selecting an appropriate normalization function."
    },
    {
      "id": "46e55fdc-efab-4b58-b1e3-d963926f0a6a",
      "adv_source_id": "46e55fdc-efab-4b58-b1e3-d963926f0a6a",
      "source_id": "ce2077bf-e2d9-4ae6-b49b-25b586e4edf5",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Learning Normalized Inputs for Iterative Estimation in Medical Image Segmentation\".",
      "generation": "The paper discusses a new approach for medical image segmentation that involves learning normalized inputs for iterative estimation. The goal is to improve the accuracy and efficiency of the segmentation process by normalizing the input images to a standard format. The paper describes a method for learning the normalized inputs using a deep neural network and demonstrates its effectiveness on a variety of medical image datasets. The paper also discusses the advantages of the proposed method over existing techniques and proposes a new evaluation metric for medical image segmentation. Finally, the paper concludes by discussing the potential applications of the proposed method in the field of medical image analysis."
    },
    {
      "id": "1b8d3ac1-6c60-4c00-b7e9-998220308adc",
      "adv_source_id": "1b8d3ac1-6c60-4c00-b7e9-998220308adc",
      "source_id": "679eac63-52d1-4c28-82bb-ce60718ab36b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation Using Overlapping Group Sparsity",
      "prompt": "Write the abstract for the academic paper titled \"Image Segmentation Using Overlapping Group Sparsity\".",
      "generation": "Abstract:\nOverlapping group sparsity (OGS) has demonstrated promising results for image segmentation tasks. However, existing OGS algorithms mainly focus on binary segmentation and often require a significant amount of user interaction. In this paper, we propose a novel approach to overcome these limitations. We introduce a new loss function that enables the OGS algorithm to handle multi-class segmentation problems directly. Furthermore, our method is designed to be fully automatic, eliminating the need for user interaction. We demonstrate the effectiveness of our approach on three public datasets, achieving state-of-the-art performance in terms of both precision and recall. Our method can be applied to a wide range of image segmentation tasks with various numbers of classes, making it a general and effective solution."
    },
    {
      "id": "772eba16-ae7b-4b9b-9543-9ba2171e4c42",
      "adv_source_id": "772eba16-ae7b-4b9b-9543-9ba2171e4c42",
      "source_id": "679eac63-52d1-4c28-82bb-ce60718ab36b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Image Segmentation Using Overlapping Group Sparsity",
      "prompt": "Write the abstract for the academic paper titled \"Image Segmentation Using Overlapping Group Sparsity\".",
      "generation": "Abstract:\nImage segmentation is a technique used to partition an image into multiple segments, each corresponding to a different object or region. In this paper, we present an approach to image segmentation using overlapping group sparsity. Our method leverages the power of sparsity-based optimization algorithms to achieve accurate and efficient segmentation. We propose a novel cost function that incorporates both intensity and spatial smoothness terms, enabling the algorithm to capture complex object boundaries and relationships. Furthermore, we introduce a group-sparsity regularization term that encourages overlapping segmentations, allowing for improved segmentation performance. Our experimental results demonstrate the effectiveness of the proposed method in comparison to state-of-the-art image segmentation techniques. The code and dataset used in this research are made publicly available to facilitate future research and development."
    },
    {
      "id": "86c84d6a-d5a8-4e12-be49-b37ef59bfe4f",
      "adv_source_id": "86c84d6a-d5a8-4e12-be49-b37ef59bfe4f",
      "source_id": "405fe838-cab2-4f7e-9833-4c045c1bbdf3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images",
      "prompt": "Write the abstract for the academic paper titled \"Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images\".",
      "generation": "Below is an abstract for the academic paper titled \"Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images\":\n\nUltrasound (US) images are widely used for prostate cancer detection due to their convenience and safety. However, the manual segmentation of the prostate in US images is time-consuming and tedious, resulting in a significant bottleneck in the analysis of prostate cancer. In this paper, we propose a novel approach for automatic prostate segmentation in ultrasound images using fine-grained recurrent neural networks (RNNs). Our approach combines the use of a convolutional neural network (CNN) and an RNN, which are trained together to learn a hierarchical representation of the input ultrasound images. The CNN extracts low-level features from the images, while the RNN learns to predict the prostate boundary based on these features. We evaluate our approach on a large dataset of ultrasound images and compare it to state-of-the-art methods. Our results show that our approach achieves high accuracy and efficiency in prostate segmentation, significantly improving over the previous state of the art."
    },
    {
      "id": "d1e61d31-41f0-4dd8-b4ed-7900ebe54028",
      "adv_source_id": "d1e61d31-41f0-4dd8-b4ed-7900ebe54028",
      "source_id": "405fe838-cab2-4f7e-9833-4c045c1bbdf3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images",
      "prompt": "Write the abstract for the academic paper titled \"Fine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images\".",
      "generation": "Abstract\nProstate segmentation in ultrasound (US) images is a fundamental task in computer-aided detection (CAD) of prostate cancer. In this paper, we propose a novel approach for automatic prostate segmentation based on fine-grained recurrent neural networks (RNNs). Our method utilizes a combination of convolutional and recurrent layers to capture both local and global features within the US images. We introduce a multi-scale architecture that enables the network to identify objects at various levels of detail. Furthermore, we employ an attention mechanism to weigh the importance of different spatial locations within the input sequence. Our method has been trained and evaluated on a large dataset of US images, and the results demonstrate its effectiveness in terms of accuracy and efficiency.\n\nFine-grained Recurrent Neural Networks for Automatic Prostate Segmentation in Ultrasound Images\n\nIntroduction\nProstate cancer is one of the most common types of cancer among men, and early detection is crucial for successful treatment. Ultrasound (US) imaging is commonly used for prostate cancer detection due to its portability, real-time capability, and safety. However, the manual analysis of US images is time-consuming and requires extensive expertise. This calls for the development of automated methods for various tasks including image segmentation.\n\nDespite the progress in the field of medical image segmentation, automatic segmentation of the prostate in US images remains a challenging task. This is due to the low contrast, speckle noise, and structural similarities between the prostate and surrounding tissues. Existing methods based on conventional machine learning algorithms have shown limited performance in accurately segmenting the prostate.\n\nDeep learning approaches have demonstrated remarkable performance in various computer vision tasks. In particular, recurrent neural networks (RNNs) have shown promise in sequential data processing. However, conventional RNNs suffer from the vanishing gradient problem, limiting their effectiveness in capturing long-range dependencies in medical images.\n\nIn this work, we propose a novel approach to automatic prostate segmentation based on fine-grained RNNs. Our method combines convolutional and recurrent layers to capture both local and global features within the US images. We introduce a multi-scale architecture that enables the network to identify objects at various levels of detail. Furthermore, we employ an attention mechanism to weigh the importance of different spatial locations within the input sequence. Our method has been trained and evaluated on a large dataset of US images, and the results demonstrate its effectiveness and efficiency in prostate segmentation.\n\nMethod\nThe proposed method is based on a fine-grained RNN architecture, which combines convolution"
    },
    {
      "id": "ff7ffc44-07d8-4c40-8042-516c30b2dbae",
      "adv_source_id": "ff7ffc44-07d8-4c40-8042-516c30b2dbae",
      "source_id": "891c294c-58d9-4ba3-b085-b992f1ef2818",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors",
      "prompt": "Write the abstract for the academic paper titled \"MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\".",
      "generation": "We present a shape-based image segmentation approach that utilizes a variant of the popular Monte Carlo MCMC (Markov Chain Monte Carlo) algorithm for sampling from a joint distribution of pixels and shapes. Our approach incorporates nonparametric shape priors into the segmentation process, allowing us to recover object structures without relying on any parameterized shape models. We define a shape latent representation as a collection of region-shaped bins that are modeled as nonparametric distributions. This enables the algorithm to discover the underlying shape structures in an unsupervised manner. We further propose an efficient MCMC sampling strategy that explores the shape and pixel variables in a coordinated manner, significantly improving the convergence speed compared to existing MCMC-based segmentation methods. The proposed approach is evaluated on synthetic and real-world images, and the results demonstrate its ability to accurately segment objects while effectively exploiting the shape information."
    },
    {
      "id": "e7dacd60-3633-4ab7-b61f-032b05a32984",
      "adv_source_id": "e7dacd60-3633-4ab7-b61f-032b05a32984",
      "source_id": "891c294c-58d9-4ba3-b085-b992f1ef2818",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors",
      "prompt": "Write the abstract for the academic paper titled \"MCMC Shape Sampling for Image Segmentation with Nonparametric Shape Priors\".",
      "generation": "MCMC shape sampling is a new approach to image segmentation based on the assumption that image pixels belonging to the same object are similar in shape. A nonparametric shape prior is used to model the shape distribution of pixels in an object. The shape prior is a collection of binary patterns representing object shapes. In the approach, particles, representing possible object boundaries, are evolved towards regions with similar shape characteristics to the pattern in their current state. The evolution of the particles is guided by a shape-matching energy function that measures the similarity between the binary pattern associated with a particle and the binary patterns in the shape prior. The approach is able to segment objects in a variety of images. Results show that the method is able to produce accurate segmentations."
    },
    {
      "id": "880f0512-a1f9-4454-ad89-6869dd7b2605",
      "adv_source_id": "880f0512-a1f9-4454-ad89-6869dd7b2605",
      "source_id": "ade3fd87-30b8-4664-91fa-61b2bb99c132",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Theoretical Analysis of Active Contours on Graphs",
      "prompt": "Write the abstract for the academic paper titled \"Theoretical Analysis of Active Contours on Graphs\".",
      "generation": "The paper presents a theoretical analysis of active contours on graphs, a method for image segmentation based on the propagation of curves on a graph that represents the image. The analysis focuses on the convergence of the active contour algorithm and the impact of various parameters on the segmentation results. The paper discusses the use of active contours for image segmentation and provides a comprehensive review of the existing literature on the subject. The main contributions of the paper are the theoretical results that analyze the convergence of the active contour algorithm and the development of a new algorithm for image segmentation based on active contours."
    },
    {
      "id": "466f8d13-7a7f-45a8-9fb7-81c813d4fbf6",
      "adv_source_id": "466f8d13-7a7f-45a8-9fb7-81c813d4fbf6",
      "source_id": "ade3fd87-30b8-4664-91fa-61b2bb99c132",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Theoretical Analysis of Active Contours on Graphs",
      "prompt": "Write the abstract for the academic paper titled \"Theoretical Analysis of Active Contours on Graphs\".",
      "generation": "The abstract for the academic paper titled \"Theoretical Analysis of Active Contours on Graphs\" would be as follows:\n\nThis paper presents a theoretical analysis of active contours on graphs, a method for image segmentation based on the propagation of curves through a graph. The analysis focuses on the convergence and stability of the active contour method, and the impact of various parameters on its performance. The results of the analysis show that the active contour method is convergent and stable under certain conditions, and that the choice of parameters can significantly affect the accuracy and efficiency of the segmentation. The paper concludes by discussing the practical implications of the analysis and suggesting future research directions."
    },
    {
      "id": "c456d9c8-0c36-4f80-a6aa-7a524c853451",
      "adv_source_id": "c456d9c8-0c36-4f80-a6aa-7a524c853451",
      "source_id": "8bf02f83-d4a0-43dd-b7e5-e35d82337d9a",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combinatorial Energy Learning for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Combinatorial Energy Learning for Image Segmentation\".",
      "generation": "Combinatorial energy learning (CELL) is a new learning paradigm for fully-automatic segmentation of anatomical structures in medical images. This paper introduces the approach and shows that it is a well-founded and effective method for addressing the difficult segmentation problem. The CELL algorithm minimizes an energy function consisting of a data term and a smoothness term, expressed as a combinatorial optimization problem. An efficient algorithm, incremental clustering, is presented to solve the combinatorial optimization problem. The CELL algorithm is applied to segmentation of lung CT images. The performance of CELL is compared to a state-of-the-art algorithm, SN, which uses a different segmentation approach. Comparative experiments show that the CELL algorithm is very effective in segmenting lung CT images and is much faster than SN."
    },
    {
      "id": "42159c91-199e-4f47-a83c-e448799ff497",
      "adv_source_id": "42159c91-199e-4f47-a83c-e448799ff497",
      "source_id": "8bf02f83-d4a0-43dd-b7e5-e35d82337d9a",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Combinatorial Energy Learning for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Combinatorial Energy Learning for Image Segmentation\".",
      "generation": "Combinatorial energy learning is a new approach to image segmentation that combines the benefits of both combinatorial and energy-based methods. It uses a combinatorial algorithm to optimize an energy function that is defined on a graph representing the image. The energy function combines both pixel-level and region-level features, allowing the algorithm to capture both local and global characteristics of the image. The method is efficient and effective, producing accurate and coherent segmentations of images with complex structures and textures."
    },
    {
      "id": "c2ea1885-e2af-4ce5-b885-78889525e89e",
      "adv_source_id": "c2ea1885-e2af-4ce5-b885-78889525e89e",
      "source_id": "3ce9a084-3b0a-4e7c-aeab-acb4b02105b3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 4D Light-Field Dataset and CNN Architectures for Material Recognition",
      "prompt": "Write the abstract for the academic paper titled \"A 4D Light-Field Dataset and CNN Architectures for Material Recognition\".",
      "generation": "Here is the abstract for the academic paper titled \"A 4D Light-Field Dataset and CNN Architectures for Material Recognition\":\n\nAbstract:\nWe present a new dataset of 4D light-field images of materials, along with a comprehensive set of CNN architectures for material recognition. Our dataset consists of over 10,000 images of 100 different materials, captured using a 4D light-field camera. We then train and evaluate a series of CNN architectures for material recognition, including both 2D and 3D networks. Our results show that 3D CNNs perform the best, with an accuracy of over 90%. We also find that the use of a 4D light-field dataset improves the performance of material recognition, compared to using a 2D image dataset.\n\nThis paper provides a comprehensive overview of the state-of-the-art in material recognition using CNNs, and offers a new dataset and architectures for future research."
    },
    {
      "id": "0efcead1-639d-4789-809d-827b481875e7",
      "adv_source_id": "0efcead1-639d-4789-809d-827b481875e7",
      "source_id": "3ce9a084-3b0a-4e7c-aeab-acb4b02105b3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A 4D Light-Field Dataset and CNN Architectures for Material Recognition",
      "prompt": "Write the abstract for the academic paper titled \"A 4D Light-Field Dataset and CNN Architectures for Material Recognition\".",
      "generation": "An abstract is a summary of a academic paper.\nAbstract:\nTo render realistic images, computer graphics algorithms must be able to distinguish and render a variety of materials. Although deep learning has achieved remarkable results in image-based material recognition, current datasets have limited size and diversity, containing only a small number of materials and lacking fine-grained annotations. We overcome these limitations by introducing a large-scale 4D light-field material recognition dataset with 1,826 fine-grained material labels and 25,326 images captured under diverse viewpoints, lighting conditions, and occlusions. We design a new deep neural network (DNN) architecture, FedNet, that directly utilizes the 4D light-field data for material recognition. Given a 4D light-field input, FedNet separately predicts material labels for each microlens image and merges the predictions to generate the final output. In addition, we propose a new 3D feature pooling strategy that effectively utilizes the spatial information in the 4D light-field input. Experiments show that FedNet achieves an accuracy of 89.4% on the fine-grained material recognition task, outperforming previous methods. Our dataset and code are publicly available at https://github.com/Material-Net/LightField-Material-Recognition."
    },
    {
      "id": "f07b6521-4daa-4034-a258-cdb38cc5bac9",
      "adv_source_id": "f07b6521-4daa-4034-a258-cdb38cc5bac9",
      "source_id": "eed27bff-8deb-44ec-ad7b-3c6316bd345d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation\".",
      "generation": "The abstract for the academic paper titled \"SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation\" could be something like this:\n\nThis paper presents SSHMT, a novel semi-supervised hierarchical merge tree algorithm for the segmentation of electron microscopy images. The SSHMT algorithm combines the strengths of both supervised and unsupervised learning, using a hierarchical merge tree structure to combine labeled and unlabeled data for efficient segmentation. The proposed method utilizes a novel cost function that considers both label consistency and spatial coherence, enabling accurate segmentation of electron microscopy images while preserving important structural details.\n\nThe experimental results demonstrate the effectiveness of the proposed method in segmenting a variety of electron microscopy images, including those with complex structures and noisy backgrounds. The SSHMT algorithm is shown to outperform state-of-the-art semi-supervised segmentation methods in terms of both accuracy and efficiency.\n\nThe paper concludes by discussing the potential applications of the SSHMT algorithm in the field of electron microscopy image segmentation, and suggests further research directions in the development of more robust and efficient segmentation methods."
    },
    {
      "id": "41547359-9042-4dc4-84f2-e49f16a8e3ae",
      "adv_source_id": "41547359-9042-4dc4-84f2-e49f16a8e3ae",
      "source_id": "eed27bff-8deb-44ec-ad7b-3c6316bd345d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation\".",
      "generation": "The abstract for the academic paper titled \"SSHMT: Semi-supervised Hierarchical Merge Tree for Electron Microscopy Image Segmentation\" could be something like this:\n\nRecent advances in electron microscopy (EM) have enabled the acquisition of images with increasingly better resolution and larger sizes, presenting challenges for image segmentation. We present SSHMT, a novel approach for EM image segmentation based on a semi-supervised hierarchical merge tree. SSHMT utilizes a hierarchical clustering algorithm to group similar pixels and build a merge tree, and a semi-supervised learning strategy to refine the segmentation results. Our approach is trained using a small amount of labeled data and can handle images of varying sizes and resolutions. We demonstrate the effectiveness of SSHMT on a public EM image segmentation benchmark, achieving state-of-the-art performance.\n\nWould you like me to generate a more academically styled abstract?"
    },
    {
      "id": "9a2a40e8-37a5-4052-850c-7ccac2bdcbb5",
      "adv_source_id": "9a2a40e8-37a5-4052-850c-7ccac2bdcbb5",
      "source_id": "54e0f95e-00d1-469e-ad5f-aee23ffdcc3d",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Superpixel Convolutional Networks using Bilateral Inceptions",
      "prompt": "Write the abstract for the academic paper titled \"Superpixel Convolutional Networks using Bilateral Inceptions\".",
      "generation": "The recent development of deep learning and computer vision has revived the research on superpixel algorithms, a class of interest point detection methods that detect pixels as interest points. In this paper, we present a superpixel convolutional network using bilateral inceptions that outperform the state-of-the-art in both object detection and segmentation. Our approach uses a novel superpixel convolutional network architecture inspired by the recent success of the inception module. Our network is composed of multiple inception modules that perform a series of convolutions with different filters, followed by a max pooling layer. We introduce a new module called the bilateral inception module, which performs a series of convolutions with filters in both the x and y directions, allowing the network to capture complex structures in the input image. Our approach is evaluated on the PASCAL VOC 2007 and 2012 datasets, and achieves state-of-the-art performance on both datasets."
    },
    {
      "id": "47996b83-ff07-45f6-8a38-5e283f6e4026",
      "adv_source_id": "47996b83-ff07-45f6-8a38-5e283f6e4026",
      "source_id": "54e0f95e-00d1-469e-ad5f-aee23ffdcc3d",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Superpixel Convolutional Networks using Bilateral Inceptions",
      "prompt": "Write the abstract for the academic paper titled \"Superpixel Convolutional Networks using Bilateral Inceptions\".",
      "generation": "The abstract for the academic paper titled \"Superpixel Convolutional Networks using Bilateral Inceptions\" would be:\n\nWe present a new framework for the learnable segmentation of pixels into superpixels using a deep learning based approach. Our method, Superpixel Convolutional Networks (SCN), leverages the power of convolutional neural networks (CNNs) to jointly optimize for both the superpixel segmentation and the recovery of a higher-level representation of the input image. We introduce a novel architecture, Bilateral Inceptions (BI), that combines the use of global and local information to achieve this joint optimization. Our BI module operates at different scales, capturing both local and global context in an efficient manner. We demonstrate the effectiveness of our approach on the SDM and NYUD-2 datasets, showing that our method achieves state-of-the-art performance in terms of both superpixel quality and runtime efficiency."
    },
    {
      "id": "0dc31388-d40c-4655-8d1d-05aebababc6b",
      "adv_source_id": "0dc31388-d40c-4655-8d1d-05aebababc6b",
      "source_id": "fad4e6d3-4732-418a-9dcb-7722141430a3",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization",
      "prompt": "Write the abstract for the academic paper titled \"Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization\".",
      "generation": "Below is an abstract for the academic paper titled, \"Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization\":\n\nAbstract\nThe recent proliferation of portable devices with varying screen resolutions has resulted in challenges when displaying content on these screens. One common approach is to segment the image into a binary mask that identifies the text and graphics regions. In this paper, we propose a new segmentation algorithm based on sparse decomposition and total variation minimization. Our algorithm utilizes the properties of sparse decomposition to extract the text and graphics regions from the input image. The extracted regions are then refined using total variation minimization to improve the segmentation results. Our proposed algorithm was evaluated using the publicly available Screen Content Image Segmentation (SCIS) dataset, and the results show that it outperforms the state-of-the-art algorithms in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) metrics."
    },
    {
      "id": "52d4ac7e-b343-4ccf-8a22-a67563375c33",
      "adv_source_id": "52d4ac7e-b343-4ccf-8a22-a67563375c33",
      "source_id": "fad4e6d3-4732-418a-9dcb-7722141430a3",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization",
      "prompt": "Write the abstract for the academic paper titled \"Screen Content Image Segmentation Using Sparse Decomposition and Total Variation Minimization\".",
      "generation": "An efficient algorithm has been developed for image segmentation, which is based on a new variational formulation. The algorithm utilizes the total variation minimization and sparse decomposition. The sparse representation is employed to decompose the original image into a sparse array and a dense array. The proposed algorithm segments the image by minimizing the total variation of the dense array subject to the constraint that the sparse array and dense array have a similar solution to a sparse coding problem. An efficient algorithm, which is based on the alternating direction method of multipliers, has been developed to solve the proposed formulation. The proposed algorithm is compared with similar algorithms, and experimental results demonstrate the efficiency of the proposed algorithm."
    },
    {
      "id": "a63dc89d-1f53-4e4a-89db-3c16a16b0dd8",
      "adv_source_id": "a63dc89d-1f53-4e4a-89db-3c16a16b0dd8",
      "source_id": "32ad507a-fd83-458a-8a3d-0d4f8c6b5b47",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating prior knowledge in medical image segmentation: a survey",
      "prompt": "Write the abstract for the academic paper titled \"Incorporating prior knowledge in medical image segmentation: a survey\".",
      "generation": "In this paper, we conduct a survey on the incorporation of prior knowledge in medical image segmentation. We review the various ways in which prior knowledge has been incorporated into segmentation algorithms, including the use of anatomical knowledge, domain expertise, and machine learning. We also discuss the benefits and limitations of each approach, and provide examples of their application in medical image segmentation. Our goal is to provide a comprehensive overview of the current state of the art in this field and to identify future research directions."
    },
    {
      "id": "b5408d90-219d-41f4-86b7-8fc18ac93602",
      "adv_source_id": "b5408d90-219d-41f4-86b7-8fc18ac93602",
      "source_id": "32ad507a-fd83-458a-8a3d-0d4f8c6b5b47",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating prior knowledge in medical image segmentation: a survey",
      "prompt": "Write the abstract for the academic paper titled \"Incorporating prior knowledge in medical image segmentation: a survey\".",
      "generation": "The abstract for the academic paper titled \"Incorporating prior knowledge in medical image segmentation: a survey\" would include:\n\nMedical image segmentation is a crucial task in medical imaging analysis, and it often requires the incorporation of prior knowledge to achieve accurate and reliable results. This paper provides a comprehensive survey of the various ways in which prior knowledge has been incorporated into medical image segmentation methods.\n\nThe paper begins by introducing the concept of prior knowledge and its importance in medical image segmentation. It then reviews the different types of prior knowledge that have been used in the literature, including expert knowledge, domain knowledge, and learned knowledge. The paper then discusses the advantages and disadvantages of each type of prior knowledge, and how they have been used to improve the performance of medical image segmentation methods.\n\nThe paper then reviews the different ways in which prior knowledge has been incorporated into medical image segmentation methods. This includes the use of rule-based methods, machine learning methods, and deep learning methods. The paper also discusses the challenges associated with incorporating prior knowledge into medical image segmentation methods, and how these challenges have been addressed in the literature.\n\nThe paper concludes by discussing the future direction of medical image segmentation, and how prior knowledge will continue to play a role in improving the performance of these methods. It is hoped that the survey provided in this paper will be a useful resource for researchers and practitioners working on medical image segmentation."
    },
    {
      "id": "ce4e1b94-2440-4e62-87c9-2951034119a9",
      "adv_source_id": "ce4e1b94-2440-4e62-87c9-2951034119a9",
      "source_id": "04284051-7a7a-4104-bd93-0f44b50ed616",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient piecewise training of deep structured models for semantic segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Efficient piecewise training of deep structured models for semantic segmentation\".",
      "generation": "Abstract:\nTraining deep structured models for semantic segmentation is challenging due to the large amount of labeled training data required and the complexity of the models. In this paper, we present a piecewise training approach for deep structured models that divides the training process into multiple stages, each focusing on a specific aspect of the model. This allows us to train the model more efficiently, with a reduced amount of labeled data and improved performance. Our approach is based on the insight that different parts of the model require different amounts of training data to converge, and by training them separately, we can optimize the usage of labeled data. We demonstrate the effectiveness of our approach on the Cityscapes and CamVid datasets, and show that our method achieves state-of-the-art performance with a reduced amount of labeled training data."
    },
    {
      "id": "92540021-0000-41bb-bf73-dd0756d84bb1",
      "adv_source_id": "92540021-0000-41bb-bf73-dd0756d84bb1",
      "source_id": "04284051-7a7a-4104-bd93-0f44b50ed616",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient piecewise training of deep structured models for semantic segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Efficient piecewise training of deep structured models for semantic segmentation\".",
      "generation": "Semantic segmentation remains an challenging task in computer vision despite the recent advancements in deep learning. In this paper, we present a simple yet efficient approach for training deep structured models for semantic segmentation. Our key insight is to train the network in a piecewise manner, where each block is trained with a distinct network architecture and hyper-parameters. We show that this approach enables better generalization performance compared to training the entire network with a single architecture. We further propose a multi-scale training scheme that allows the network to learn from both coarse and fine-grained annotations. Our experimental results on the Cityscapes and CamVid datasets demonstrate the effectiveness of our approach in terms of both accuracy and efficiency."
    },
    {
      "id": "eae242a3-7212-4ce8-b2fb-7c3596467c21",
      "adv_source_id": "eae242a3-7212-4ce8-b2fb-7c3596467c21",
      "source_id": "1091bb0b-6cb0-4667-b117-58ee4136ee94",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bridging Category-level and Instance-level Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Bridging Category-level and Instance-level Semantic Image Segmentation\".",
      "generation": "Semantic image segmentation has achieved remarkable progress in the past few years, with a focus on distinguishing object instances in complex scenes. However, most existing works treat object instances and full categories as the same classification problem, ignoring the inherent structural and semantic differences between them. In this paper, we argue that instance-level and category-level segmentation should be addressed as two separate tasks, each with its own specific requirements. We propose a simple yet effective approach to bridge the gap between these two tasks. Our solution leverages the power of instance-level segmentation with the benefits of category-level segmentation, improving the performance of both tasks. We demonstrate the effectiveness of our approach on the challenging task of semantic image segmentation, achieving state-of-the-art results on the Cityscapes dataset."
    },
    {
      "id": "b16be9ec-aaf6-41a7-9e3d-122e29ac6a03",
      "adv_source_id": "b16be9ec-aaf6-41a7-9e3d-122e29ac6a03",
      "source_id": "1091bb0b-6cb0-4667-b117-58ee4136ee94",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bridging Category-level and Instance-level Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Bridging Category-level and Instance-level Semantic Image Segmentation\".",
      "generation": "Semantic image segmentation has achieved remarkable progress in the past few years, mainly driven by two lines of research: category-level and instance-level segmentation. Category-level segmentation focuses on predicting an image-level mask for each category, while instance-level segmentation aims to provide a pixel-level mask for each instance of a category. In this paper, we bridge the gap between the two lines of research and provide a unified framework to jointly tackle category-level and instance-level segmentation. We propose a novel approach that combines the strengths of both categories and instances to provide a mask for each category as well as each instance within the category. Our approach leverages the global context provided by category-level segmentation and refines it with the fine-grained details captured by instance-level segmentation. We demonstrate the effectiveness of our approach on the Cityscapes dataset and show that our method achieves state-of-the-art performance."
    },
    {
      "id": "66d0bf20-985d-4875-a12b-3e894db1838b",
      "adv_source_id": "66d0bf20-985d-4875-a12b-3e894db1838b",
      "source_id": "fe96becb-05e7-47fa-bf1f-41acbedb8636",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Semantic Image Segmentation with High Order Context and Guided Filtering",
      "prompt": "Write the abstract for the academic paper titled \"Fast Semantic Image Segmentation with High Order Context and Guided Filtering\".",
      "generation": "Semantic image segmentation has achieved remarkable progress in recent years, however, most existing methods concentrate on capturing low-order context only (e.g., spatial consistency) and fail to fully exploit the high-order context (e.g., scene coherence and object co-occurrence). To this end, we propose a Fast Semantic Image Segmentation framework (FastSi), which efficiently integrates high-order context into the semantic segmentation framework via a guided filtering process on top of a basic segmentation module. This additional processing step effectively leverages the high-order context to refine the initial predictions. Furthermore, to efficiently utilize the guidance information, we propose a novel adaptive filtering strategy that selectively applies the guided filtering to the regions that indeed require refinement. This is achieved by a learned confidence map that characterizes the reliability of the initial predictions. We demonstrate the effectiveness of the proposed approach on Cityscapes and ACDC datasets, where FastSi achieves promising results with merely a fraction of the computational cost compared to existing state-of-the-art methods."
    },
    {
      "id": "da7d6eaf-921f-405e-9038-89def2c81509",
      "adv_source_id": "da7d6eaf-921f-405e-9038-89def2c81509",
      "source_id": "fe96becb-05e7-47fa-bf1f-41acbedb8636",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Semantic Image Segmentation with High Order Context and Guided Filtering",
      "prompt": "Write the abstract for the academic paper titled \"Fast Semantic Image Segmentation with High Order Context and Guided Filtering\".",
      "generation": "Semantic image segmentation has recently benefited from the introduction of deep convolutional neural networks. However, training such networks often requires a large amount of labeled pixels, which can be a costly and tedious process. In this paper, we present a fast semantic image segmentation algorithm that combines high-order context and guided filtering to achieve accurate and efficient segmentation. Our algorithm uses a deep convolutional neural network to learn a mapping from images to a high-order context, which captures the global structure of the image. We then use a guided filtering approach to refine the segmentation based on this high-order context. Our algorithm is fast, segmenting images in less than a second, and achieves state-of-the-art performance on the semantic image segmentation benchmark PASCAL VOC 2012."
    },
    {
      "id": "42b7af82-f4a5-4498-b54c-5f57c1edb27d",
      "adv_source_id": "42b7af82-f4a5-4498-b54c-5f57c1edb27d",
      "source_id": "bd1160c4-4533-4102-b1bb-1c5b3304ff5f",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Classifier-guided Approach for Top-down Salient Object Detection",
      "prompt": "Write the abstract for the academic paper titled \"A Classifier-guided Approach for Top-down Salient Object Detection\".",
      "generation": "An important problem in computer vision is saliency detection, i.e., predicting which pixels in an input image are most salient, or foreground, and which are least salient, or background. We approach this problem in a top-down manner, first detecting objects of interest in the image, and then computing saliency within each object region. Our approach is motivated by the observation that objects in images often carry important saliency information and are of primary interest in many applications. We allow the set of objects of interest to be specified by the user, and we perform saliency detection within each object region using a classifier-guided approach. Our method first uses a deep neural network to classify the objects in the image. We then use the object class predictions to guide our saliency detection process within each object region. In particular, we condition the saliency maps on the class predictions, making the salient regions within each object more distinguishable. We evaluate our approach on the publicly available SALICON and DUT-OMRON datasets, and show that our method achieves state-of-the-art performance on both datasets compared to other top-down saliency detection methods."
    },
    {
      "id": "c964afc6-bf02-416a-b3a8-6cdd497e5abb",
      "adv_source_id": "c964afc6-bf02-416a-b3a8-6cdd497e5abb",
      "source_id": "bd1160c4-4533-4102-b1bb-1c5b3304ff5f",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Classifier-guided Approach for Top-down Salient Object Detection",
      "prompt": "Write the abstract for the academic paper titled \"A Classifier-guided Approach for Top-down Salient Object Detection\".",
      "generation": "The paper proposes a classifier-guided approach for top-down salient object detection, which is a key task in computer vision with many applications including object recognition, segmentation, and editing. The proposed approach incorporates a classifier into the detection process to improve performance. The classifier is trained on a set of images with ground truth annotations for salient objects. The proposed approach is evaluated on the seminal salient object detection benchmark, which is a standard dataset of images with ground truth annotations for salient objects. The experimental results show that the proposed approach significantly improves the state-of-the-art performance on the benchmark. The code and models for the proposed approach are publicly available at https://github.com/detection-challenge/det-saliency."
    },
    {
      "id": "47dc3005-d06f-4dcb-9e3d-eaaf1f66742a",
      "adv_source_id": "47dc3005-d06f-4dcb-9e3d-eaaf1f66742a",
      "source_id": "fce72cc8-d3d3-4ff3-973d-9371dceab939",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully-Automatic Synapse Prediction and Validation on a Large Data Set",
      "prompt": "Write the abstract for the academic paper titled \"Fully-Automatic Synapse Prediction and Validation on a Large Data Set\".",
      "generation": "An abstract is a summary of a academic paper. It should be concise and accurate, and it should convey the key points of the paper. Here is an abstract for the academic paper \"Fully-Automatic Synapse Prediction and Validation on a Large Data Set\":\n\n\"In recent years, there has been a significant increase in the number of studies on the human brain, particularly in the field of neuroscience. This has led to the development of new techniques for the study of the brain, such as magnetic resonance imaging (MRI) and electroencephalography (EEG). These techniques have allowed researchers to gain a better understanding of the human brain and its functions. However, there is still much to be learned about the brain, particularly in the area of synapse prediction and validation. This paper presents a new method for the prediction and validation of synapses on a large data set. The method is fully automatic and does not require any manual intervention. The results of the study show that the method is effective in predicting and validating synapses on a large data set.\""
    },
    {
      "id": "0d1c746b-7cd6-453b-9221-f8b4d0c7e832",
      "adv_source_id": "0d1c746b-7cd6-453b-9221-f8b4d0c7e832",
      "source_id": "fce72cc8-d3d3-4ff3-973d-9371dceab939",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fully-Automatic Synapse Prediction and Validation on a Large Data Set",
      "prompt": "Write the abstract for the academic paper titled \"Fully-Automatic Synapse Prediction and Validation on a Large Data Set\".",
      "generation": "An abstract is a summary of a academic paper. It should be concise and accurate, and it should convey the key points and findings of the paper. Here is an abstract for the academic paper \"Fully-Automatic Synapse Prediction and Validation on a Large Data Set\":\n\n\"In this paper, we present a fully-automatic method for synapse prediction and validation on a large data set. Our method uses machine learning algorithms to identify synapses in electron microscopy images of the brain. We trained a deep learning model on a large data set of images and validated the model on a separate data set of images. Our results show that our method is accurate and reliable, and it can be used to identify synapses in a variety of brain images.\""
    },
    {
      "id": "5630071f-db31-492f-a4b5-dc976cca907d",
      "adv_source_id": "5630071f-db31-492f-a4b5-dc976cca907d",
      "source_id": "8a1ace59-3d8c-483d-a8dc-7a15e898f310",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation\".",
      "generation": "Kernelized Weighted SUSAN (KWSUSAN) is a novel algorithm that combines the use of weighted SUSAN pixels and kernelization to improve the performance of fuzzy C-means (FCM) clustering for noisy image segmentation. In this paper, we propose a new approach to FCM clustering that utilizes the KWSUSAN algorithm to improve the accuracy of noisy image segmentation. The KWSUSAN algorithm is based on the idea of using a weighted SUSAN pixel approach to measure the similarity between pixels in an image. The weighted SUSAN pixel approach utilizes the structural similarity between pixels to measure the similarity between pixels in an image, which is more effective in noisy image segmentation. The KWSUSAN algorithm is then combined with the FCM clustering algorithm to improve the performance of noisy image segmentation. The proposed approach has been evaluated on a number of standard image segmentation benchmarks and has been shown to improve the accuracy of noisy image segmentation."
    },
    {
      "id": "ffcf1c8b-a6b4-486d-82f8-76516f1b9576",
      "adv_source_id": "ffcf1c8b-a6b4-486d-82f8-76516f1b9576",
      "source_id": "8a1ace59-3d8c-483d-a8dc-7a15e898f310",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Kernelized Weighted SUSAN based Fuzzy C-Means Clustering for Noisy Image Segmentation\".",
      "generation": "Kernelized Weighted SUSAN (KWS) is a new image segmentation technique that utilizes the C-Means algorithm and a novel weighted similarity measure based on the Structural Similarity (SSIM) index. The KWS algorithm is designed to handle noisy images, which are common in many real-world applications such as medical imaging and satellite imagery. The paper will discuss the motivation for the development of KWS and the challenges associated with noisy image segmentation. The proposed method is evaluated on a variety of image datasets, and the results show that KWS is able to achieve better segmentation accuracy compared to existing techniques. The paper concludes by discussing the potential applications of KWS in various fields and the future research directions in noisy image segmentation."
    },
    {
      "id": "97624504-f4dc-4e2f-9d33-bf64073d5f80",
      "adv_source_id": "97624504-f4dc-4e2f-9d33-bf64073d5f80",
      "source_id": "33990fb9-7202-4e37-8d52-b1df461a4951",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines",
      "prompt": "Write the abstract for the academic paper titled \"Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\".",
      "generation": "Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\n\nThe paper proposes a method for integrating inference and learning of neural factors in structural support vector machines (SSVMs). SSVMs are a powerful machine learning algorithm for supervised learning problems, but they require the selection of a kernel function and the tuning of its parameters. The proposed method uses a neural network to learn a kernel function directly from the data, eliminating the need for manual selection and tuning. The neural network is trained using a combination of supervised and unsupervised learning, allowing it to capture both the global structure of the data and the local structure of the decision boundary. The method is evaluated on a variety of benchmark datasets, and results show that it is competitive with state-of-the-art algorithms."
    },
    {
      "id": "1847b18f-f580-4d65-a0fc-17eb3af813e1",
      "adv_source_id": "1847b18f-f580-4d65-a0fc-17eb3af813e1",
      "source_id": "33990fb9-7202-4e37-8d52-b1df461a4951",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines",
      "prompt": "Write the abstract for the academic paper titled \"Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\".",
      "generation": "The abstract for the academic paper \"Integrated Inference and Learning of Neural Factors in Structural Support Vector Machines\" would include:\n\nIntegrated inference and learning of neural factors in structural support vector machines is a novel approach for improving the efficiency of neural networks. This approach combines the benefits of support vector machines and neural networks to create a new class of algorithms that are more efficient and effective than either of the individual algorithms.\n\nThe paper discusses the integration of support vector machines and neural networks and shows how this can improve the performance of both algorithms. The paper also discusses the use of a new class of algorithms called structural support vector machines, which are a combination of support vector machines and neural networks.\n\nThe paper concludes by showing that integrated inference and learning of neural factors in structural support vector machines is a promising approach for improving the efficiency of neural networks and can be used to create more effective algorithms for a variety of applications."
    },
    {
      "id": "d251b808-f8f7-4041-9bf3-92f0297e6a35",
      "adv_source_id": "d251b808-f8f7-4041-9bf3-92f0297e6a35",
      "source_id": "df3be76c-8027-4379-a2d3-1f92e94a7842",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Graph entropies in texture segmentation of images",
      "prompt": "Write the abstract for the academic paper titled \"Graph entropies in texture segmentation of images\".",
      "generation": "Graph entropies\u2014a new functional invariant for texture segmentation of images\u2014are introduced. These entropies are based on graph representations of images, where the nodes of the graph correspond to the pixels of the image, and each node is connected to its eight nearest neighbors. The entropies are computed using a dynamic programming algorithm that assigns labels to the nodes based on the labels of their neighbors. The resulting graph entropies are then used for texture segmentation of images.\nThe paper discusses the properties of graph entropies and shows that they are able to capture the local structure of textures in a way that is more robust to noise and distortion than other texture segmentation methods. The paper also presents a new algorithm for efficiently computing the graph entropies.\nThe paper concludes by showing results of the graph entropy texture segmentation method on a variety of images, and comparing the results to those of other texture segmentation methods. The results show that the graph entropies are able to produce more accurate and robust segmentation than other methods, especially for images with complex textures and noise."
    },
    {
      "id": "ee0604d5-85ed-4394-a247-184bb929d037",
      "adv_source_id": "ee0604d5-85ed-4394-a247-184bb929d037",
      "source_id": "df3be76c-8027-4379-a2d3-1f92e94a7842",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Graph entropies in texture segmentation of images",
      "prompt": "Write the abstract for the academic paper titled \"Graph entropies in texture segmentation of images\".",
      "generation": "The abstract for the academic paper titled \"Graph entropies in texture segmentation of images\" could be:\n\nThis paper proposes a novel approach for texture segmentation of images based on graph entropies. The proposed method utilizes the power of graph theory to capture the local structural information of an image, which is then combined with the concept of entropies to quantify the uncertainty in the segmentation process. The proposed method is evaluated on a benchmark dataset and the results show that it outperforms the state-of-the-art approaches in terms of segmentation accuracy and efficiency. The proposed method has the potential to be used in various applications such as medical imaging, satellite imaging, and image compression. \n\nWould you like me to generate more ideas for paper abstracts?"
    },
    {
      "id": "69d466f4-7538-4e9c-9d1a-222cc3f10d29",
      "adv_source_id": "69d466f4-7538-4e9c-9d1a-222cc3f10d29",
      "source_id": "630ef64e-3295-4eaf-b366-69b602196016",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation\".",
      "generation": "An active contour model based on variational level sets is proposed for image segmentation. The model is motivated by the shape of natural objects and the similarity between the interface of an image and the zero level set of a signed distance function. A regional active contour framework is developed, in which an image is iteratively divided into several regions, and an active contour is then formed within each region. A level set formulation is used to compute the evolution of the active contours. In addition, a machine learning component is introduced into the energy function of the active contours to improve their performance. The experimental results show that the proposed method is effective for image segmentation."
    },
    {
      "id": "b745f7f3-a307-4bf3-aba3-8b5375acabdd",
      "adv_source_id": "b745f7f3-a307-4bf3-aba3-8b5375acabdd",
      "source_id": "630ef64e-3295-4eaf-b366-69b602196016",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Regional Active Contours based on Variational level sets and Machine Learning for Image Segmentation\".",
      "generation": "An active contour model based on variational level sets is developed for image segmentation. The model is motivated by the fact that traditional region-based image segmentation techniques are not able to reliably detect regions in images that correspond to objects with irregular shapes. The proposed method focuses on capturing object shape information in image intensity data by an active contour model. The active contour is driven to follow the boundary of the region by minimizing a cost function that consists of two parts: data term and smoothness term. A novel regional active contour model is devised to overcome the limitation of existing methods by accommodating multiple, unspecified regions in the image via an ensemble of active contours. Each individual active contour in the ensemble is referred to as a regional active contour, which is driven to locate a particular object region in the image. The regional active contour model leverages the power of machine learning for efficient training and utilization. In the learning stage, a number of training active contours are initialized on the image regions of interest (objects). A stochastic gradient descent method is employed to update the contour models so as to fit the image data. The learned models are subsequently employed for segmentation of test images. The proposed method is evaluated on a database of images. The experimental results demonstrate the effectiveness of the proposed method for image segmentation."
    },
    {
      "id": "47603361-0b59-49c3-b016-78017a30aaa3",
      "adv_source_id": "47603361-0b59-49c3-b016-78017a30aaa3",
      "source_id": "56166659-ec7a-41d0-9396-fd6abfdf7861",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deeply Learning the Messages in Message Passing Inference",
      "prompt": "Write the abstract for the academic paper titled \"Deeply Learning the Messages in Message Passing Inference\".",
      "generation": "The paper explores the application of deep learning techniques to the field of message passing inference, a method of inference using a graphical model. The abstract should provide a brief overview of the paper, including the problem being addressed, the techniques used, and the results.\n\nHere is a possible abstract:\n\n\"Message passing inference is a powerful technique for reasoning under uncertainty, but it can be computationally expensive, especially for large graphical models. In this paper, we explore the application of deep learning techniques to the problem of message passing inference, with the goal of improving the efficiency of the process.\n\nWe propose a novel architecture called the Message Passing Network (MPN), which is a deep neural network designed to approximate the message passing algorithm. The MPN takes as input the variables of the graphical model and outputs the messages between the variables. We then use the MPN to perform message passing inference on the graphical model, with the goal of improving the efficiency of the process.\n\nOur results show that the MPN is able to learn the messages in the graphical model with high accuracy, and that it can be used to perform message passing inference with improved efficiency. We also show that the MPN is able to generalize to graphical models that it has not been trained on, suggesting that it may be able to learn a general message passing algorithm.\n\nOverall, our results suggest that deep learning techniques can be applied to the field of message passing inference to improve the efficiency of the process.\""
    },
    {
      "id": "1e276e97-9b7c-437d-b1d7-ebbb21aa7772",
      "adv_source_id": "1e276e97-9b7c-437d-b1d7-ebbb21aa7772",
      "source_id": "56166659-ec7a-41d0-9396-fd6abfdf7861",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deeply Learning the Messages in Message Passing Inference",
      "prompt": "Write the abstract for the academic paper titled \"Deeply Learning the Messages in Message Passing Inference\".",
      "generation": "The paper examines the issue of message passing inference, which is a process for determining the posterior distribution of a Bayesian network. The paper aims to highlight how the messages, which are the main components of the process, can be learned deeply using a neural network.\n\nThe paper discusses the challenges of learning the messages, which are complex and high-dimensional functions of the network structure and parameters. The paper proposes a deep learning approach that combines neural networks and message passing inference to address these challenges.\n\nThe paper discusses the experimental results, which show that the proposed approach outperforms existing methods for learning the messages. The paper concludes by discussing the implications of the results and suggesting directions for future research."
    },
    {
      "id": "9ee0da35-f4f0-4577-a79b-a6ff507282bd",
      "adv_source_id": "9ee0da35-f4f0-4577-a79b-a6ff507282bd",
      "source_id": "36db5def-75bc-48cc-9d66-1e673e563773",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Approach Towards Clustering Based Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Novel Approach Towards Clustering Based Image Segmentation\".",
      "generation": "Abstract:\nTraditional image segmentation techniques rely on pixel-level labeling, requiring a significant amount of manual labor. Recent advances in clustering based image segmentation have shown promising results towards automatic image segmentation. This paper presents a novel approach towards clustering based image segmentation that leverages the power of deep learning and semantic segmentation. Our approach utilizes a fully convolutional neural network (FCN) to extract features from the image, and a clustering algorithm to segment the image based on these features. We evaluate our approach on the Cityscapes dataset and achieve state-of-the-art results, with an Intersection over Union (IoU) of 0.83. Our approach is able to segment images with high accuracy and efficiency, and can be applied to a wide range of images.\n\nNovelty:\nOur approach is novel in the sense that it combines the power of deep learning and semantic segmentation for image segmentation. Furthermore, our approach is the first to evaluate the use of an FCN for image segmentation on the Cityscapes dataset.\n\nAdvantages:\nOur approach has several advantages over traditional image segmentation techniques. First, it is able to segment images with high accuracy and efficiency. Second, our approach is fully automatic, requiring no manual labor. Third, our approach can be applied to a wide range of images, including those with complex scenes and textures."
    },
    {
      "id": "9e298598-4eab-4547-b492-2dffe624a1d8",
      "adv_source_id": "9e298598-4eab-4547-b492-2dffe624a1d8",
      "source_id": "36db5def-75bc-48cc-9d66-1e673e563773",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Novel Approach Towards Clustering Based Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"A Novel Approach Towards Clustering Based Image Segmentation\".",
      "generation": "Abstract:\nTraditional image segmentation techniques rely on hand-crafted features and heuristics to classify pixels in images. These approaches are often specific to a particular dataset and fail to generalize to unseen data. In this paper, we explore a novel approach towards image segmentation based on clustering. We propose a method that utilizes the inherent structure of images to extract features that are robust to noise and variation in the input data. Our approach is based on the observation that pixels in similar regions of an image tend to have similar colors and intensities. We utilize this information to cluster pixels into distinct regions, which can then be used for segmentation. Our experiments show that our approach outperforms state-of-the-art segmentation algorithms on a variety of datasets, including images with high noise and complex structures."
    },
    {
      "id": "785cd499-24fe-49ed-90cb-244173b08569",
      "adv_source_id": "785cd499-24fe-49ed-90cb-244173b08569",
      "source_id": "5bd44b68-d473-4224-82a7-b1023155c1ba",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Deep Learning for Medical Image Segmentation\".",
      "generation": "The paper presents a comprehensive review of the current state of deep learning for medical image segmentation. It begins by discussing the unique challenges of medical image segmentation, including the need for high accuracy, the presence of noise and artifacts, and the limited amount of labeled training data. The paper then reviews the most common deep learning architectures used for medical image segmentation, including fully connected networks, convolutional neural networks, and recurrent neural networks. It also discusses the use of transfer learning and domain adaptation to improve the performance of deep learning models on medical image data. The paper concludes by highlighting the most promising research directions in this field, including the use of more complex architectures, the incorporation of prior knowledge, and the development of more robust evaluation metrics."
    },
    {
      "id": "a9d284bd-deb6-48e0-8923-015e760261b8",
      "adv_source_id": "a9d284bd-deb6-48e0-8923-015e760261b8",
      "source_id": "5bd44b68-d473-4224-82a7-b1023155c1ba",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Deep Learning for Medical Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Deep Learning for Medical Image Segmentation\".",
      "generation": "The paper presents a comprehensive review of recent advances in the application of deep learning for medical image segmentation. We discuss the advantages and limitations of various deep learning architectures, including fully connected networks, convolutional neural networks, and recurrent neural networks. We also explore the use of transfer learning and domain adaptation for medical image segmentation tasks. We review the performance of these methods on a variety of medical image segmentation datasets, and discuss the challenges and opportunities for future research. Finally, we highlight the potential of deep learning for accurate and automated medical image segmentation, and its impact on clinical practice."
    },
    {
      "id": "3ff77956-7765-40c8-9daf-b52f1855bb5d",
      "adv_source_id": "3ff77956-7765-40c8-9daf-b52f1855bb5d",
      "source_id": "00bc54c0-632d-49a1-afec-0b813da77a46",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Evolving Fuzzy Image Segmentation with Self-Configuration",
      "prompt": "Write the abstract for the academic paper titled \"Evolving Fuzzy Image Segmentation with Self-Configuration\".",
      "generation": "Abstract:\nFuzzy image segmentation is a technique used to partition an image into multiple segments, representing the regions of the image that contain meaningful information. In this paper, we present a novel approach to evolving fuzzy image segmentation with self-configuration, which combines fuzzy set theory with evolutionary algorithms. Our approach utilizes a fitness function that evaluates the quality of the segmentation based on the similarity between the fuzzy set and the original image. The self-configuration process is achieved through the use of a genetic algorithm, which evolves the fuzzy set to optimize the fitness function. We demonstrate the effectiveness of our approach through a series of experiments on a variety of images, and show that our method can produce accurate and meaningful segmentations."
    },
    {
      "id": "2feafd70-cc52-4449-8acd-fd2c86a1367b",
      "adv_source_id": "2feafd70-cc52-4449-8acd-fd2c86a1367b",
      "source_id": "00bc54c0-632d-49a1-afec-0b813da77a46",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Evolving Fuzzy Image Segmentation with Self-Configuration",
      "prompt": "Write the abstract for the academic paper titled \"Evolving Fuzzy Image Segmentation with Self-Configuration\".",
      "generation": "Abstract:\nFuzzy image segmentation is a technique used to partition an image into multiple segments, representing the regions of the image that correspond to different objects or boundaries. In this paper, we present a novel approach to fuzzy image segmentation that incorporates a self-configuration mechanism. Our approach utilizes a fuzzy C-means algorithm to segment the image and a self-configuration process that adjusts the number of segments based on the image content. The self-configuration process is driven by a user-defined error function that measures the quality of the segmentation. We demonstrate the effectiveness of our approach on a variety of images, and show that it produces segmentations with improved quality compared to existing techniques.\n\nFuzzy image segmentation has been an active research area for many years, with a wide range of approaches proposed for various applications. Fuzzy sets have been used to model the uncertainty and ambiguity in image segmentation tasks, allowing for more robust and flexible segmentations. Our approach extends the fuzzy C-means algorithm, a popular fuzzy image segmentation method, by incorporating a self-configuration process that dynamically adjusts the number of segments based on the image content. The self-configuration process is driven by a user-defined error function that measures the quality of the segmentation, ensuring that the resulting segmentation is optimal for the given image.\n\nWe evaluate our approach on a variety of images, and demonstrate that it produces segmentations with improved quality compared to existing techniques. Our results show that the self-configuration process is able to adapt to the image content and produce segmentations that better capture the boundaries and objects in the image.\n\nThe proposed approach has potential applications in a wide range of fields, including image processing, computer vision, and pattern recognition. It can be used to segment images for further analysis and processing, such as object detection, recognition, and tracking. It can also be used to segment images for medical imaging, remote sensing, and other applications where accurate and robust segmentation is required."
    },
    {
      "id": "842b1604-211b-48a8-8e41-b9a511010774",
      "adv_source_id": "842b1604-211b-48a8-8e41-b9a511010774",
      "source_id": "1e2a9ff9-abdb-4a71-b656-3dcefbf0faab",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bethe Learning of Conditional Random Fields via MAP Decoding",
      "prompt": "Write the abstract for the academic paper titled \"Bethe Learning of Conditional Random Fields via MAP Decoding\".",
      "generation": "The abstract for the academic paper titled \"Bethe Learning of Conditional Random Fields via MAP Decoding\" could be:\n\nThis paper presents a method for learning conditional random fields (CRFs) via MAP decoding. We introduce a novel parameterization of the CRF transition matrix using the Bethe free energy function. This parameterization allows us to apply a gradient-based optimization algorithm to learn the CRF weights from data. Our approach is demonstrated on a number of benchmark datasets, and the results show that our method can learn CRFs that perform better than those learned by traditional methods.\n\nWould you like me to generate more abstracts for this academic paper?"
    },
    {
      "id": "e3b7b38b-3ad6-4205-9b11-729407de837b",
      "adv_source_id": "e3b7b38b-3ad6-4205-9b11-729407de837b",
      "source_id": "1e2a9ff9-abdb-4a71-b656-3dcefbf0faab",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Bethe Learning of Conditional Random Fields via MAP Decoding",
      "prompt": "Write the abstract for the academic paper titled \"Bethe Learning of Conditional Random Fields via MAP Decoding\".",
      "generation": "The abstract for the academic paper titled \"Bethe Learning of Conditional Random Fields via MAP Decoding\" could be:\n\nThis paper presents a method for learning conditional random fields (CRFs) using the Bethe approximation. The Bethe approximation is a technique for approximating the partition function of a CRF by expressing it as a product of local probabilities. This paper shows how to use the Bethe approximation to learn the parameters of a CRF using a maximum a posteriori (MAP) decoding approach. The MAP decoding approach is a technique for finding the most likely state sequence for a given observation sequence, and the authors show how to use the MAP decoding approach to learn the parameters of a CRF using the Bethe approximation. The authors also show how to use the learned parameters to predict the state sequence of a new observation sequence."
    },
    {
      "id": "2235473f-0eac-4d72-b571-aacbcfdb772c",
      "adv_source_id": "2235473f-0eac-4d72-b571-aacbcfdb772c",
      "source_id": "0efbebca-d6fa-4e22-848c-d880b538be06",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Constraint Propagation for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Fast Constraint Propagation for Image Segmentation\".",
      "generation": "The abstract for the academic paper \"Fast Constraint Propagation for Image Segmentation\" would include:\n\nThis paper presents a fast constraint propagation algorithm for image segmentation. The algorithm is based on a constraint graph, where each node represents a pixel in the image and each edge represents a constraint between two pixels. The constraints are defined by a set of labels and a set of regions, where each label is assigned to a region. The algorithm starts from an initial labeling of the image and propagates the constraints to neighboring pixels to refine the segmentation. The algorithm is efficient and can be used for real-time image segmentation. The proposed method has been evaluated on several benchmark datasets and compared to other state-of-the-art image segmentation algorithms. The experimental results show that the proposed method is competitive in terms of accuracy and speed."
    },
    {
      "id": "7331e442-054e-41d0-bd46-8339a655c63e",
      "adv_source_id": "7331e442-054e-41d0-bd46-8339a655c63e",
      "source_id": "0efbebca-d6fa-4e22-848c-d880b538be06",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Constraint Propagation for Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Fast Constraint Propagation for Image Segmentation\".",
      "generation": "The paper presents a fast constraint propagation algorithm for image segmentation based on a greedy approach. The algorithm iteratively selects a constraint from a set of constraints and updates the segmentation based on the selected constraint. The process is repeated until all constraints have been considered. The proposed algorithm is compared to a state-of-the-art algorithm and is shown to have similar performance in terms of accuracy, while being significantly faster. The paper concludes by discussing the potential applications of the proposed algorithm in various image segmentation tasks."
    },
    {
      "id": "fbb9e376-a10e-403d-a895-f7bc61db94d7",
      "adv_source_id": "fbb9e376-a10e-403d-a895-f7bc61db94d7",
      "source_id": "5c337fac-1159-4849-a09e-aad39334e431",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks",
      "prompt": "Write the abstract for the academic paper titled \"Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks\".",
      "generation": "Unsupervised image segmentation is a classic problem in image processing, which is to segment images automatically without any prior knowledge of the images, target, or training. We propose an approach for unsupervised image segmentation based on optimizing a global criterion in a Bayesian network framework. Our approach combines the advantages of both a global and local criteria optimization. The global criterion is used to capture the overall structure of the image, while the local criterion is used to refine the segmentation results at a finer scale. We optimize the criteria using a dynamic programming approach. The experimental results show that our approach is effective in segmenting images and outperforms other state-of-the-art algorithms in terms of segmentation quality."
    },
    {
      "id": "a9a80ea7-ef9a-4fe2-8574-7ab7d41f3707",
      "adv_source_id": "a9a80ea7-ef9a-4fe2-8574-7ab7d41f3707",
      "source_id": "5c337fac-1159-4849-a09e-aad39334e431",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks",
      "prompt": "Write the abstract for the academic paper titled \"Unsupervised image segmentation by Global and local Criteria Optimization Based on Bayesian Networks\".",
      "generation": "Unsupervised image segmentation is a classic problem in image processing, which is to segment an image without any prior knowledge of the image\u2019s content. In this paper, we present a new algorithm for unsupervised image segmentation based on a global and local criteria optimization approach. Our algorithm combines the benefits of both global and local approaches to image segmentation, which have been shown to work well for image segmentation tasks. We use a Bayesian network to model the joint distribution of the pixel labels and the image features. We then optimize the global and local criteria for image segmentation based on the Bayesian network. Our experimental results show that our algorithm outperforms state-of-the-art unsupervised image segmentation algorithms in terms of segmentation quality."
    },
    {
      "id": "e046563b-f33d-4059-9920-f45377e379ce",
      "adv_source_id": "e046563b-f33d-4059-9920-f45377e379ce",
      "source_id": "a29d1548-e47d-4a63-b18d-706267efdf14",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification",
      "prompt": "Write the abstract for the academic paper titled \"Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification\".",
      "generation": "The abstract for the academic paper titled \"Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification\" would be:\n\nThe forward and backward propagation of convolutional neural networks (CNNs) is a computationally intensive process, particularly for pixelwise classification tasks. In this paper, we present a novel approach to optimize the propagation of CNNs for pixelwise classification. Our method involves splitting the input image into overlapping tiles and processing them in parallel, significantly improving the efficiency of forward and backward propagation. We demonstrate the effectiveness of our approach on various datasets, including CIFAR-10 and ImageNet, and show that our method achieves state-of-the-art performance in terms of accuracy and efficiency. We also provide a comprehensive analysis of the performance of our approach under different settings and compare it to existing methods. Our code and models are publicly available at https://github.com/autonomousvision/efficient-cnn-propagation."
    },
    {
      "id": "aa84c59d-00c8-42f7-a6e3-08d1c4b005ba",
      "adv_source_id": "aa84c59d-00c8-42f7-a6e3-08d1c4b005ba",
      "source_id": "a29d1548-e47d-4a63-b18d-706267efdf14",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification",
      "prompt": "Write the abstract for the academic paper titled \"Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification\".",
      "generation": "The abstract for the academic paper titled \"Highly Efficient Forward and Backward Propagation of Convolutional Neural Networks for Pixelwise Classification\" would be:\n\nAbstract:\nWe present an efficient framework for pixel-wise image classification based on convolutional neural networks. Our approach significantly reduces the computational cost and memory requirements of forward and backward propagation compared to conventional methods. We achieve this by performing convolution and activation functions on a compact representation of the input image, which we obtain by applying a parameterized mapping to the original image. Our method retains the full resolution of the original image during inference, thus preserving the quality of the classification results. We demonstrate the effectiveness of our approach on the CIFAR-10 and CIFAR-100 datasets, achieving state-of-the-art performance in terms of accuracy and efficiency.\n\nWould you like me to generate the introduction section of the academic paper based on the provided abstract?"
    },
    {
      "id": "8f8a2818-fdf7-45dc-aad4-44666f72b4fa",
      "adv_source_id": "8f8a2818-fdf7-45dc-aad4-44666f72b4fa",
      "source_id": "a99184a4-6066-4778-99ed-4f3c9d98ba83",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Edge Detection Using Structured Forests",
      "prompt": "Write the abstract for the academic paper titled \"Fast Edge Detection Using Structured Forests\".",
      "generation": "The paper presents a novel approach for fast edge detection using structured forests. Our method leverages the power of convolutional neural networks (CNNs) and the efficiency of traditional edge detection methods based on binary thresholds. We propose to use a structured forest, a special type of random forest, to learn a hierarchy of edge detection features from images. The structured forest is trained using a novel loss function that encourages the model to detect edges in an efficient way, i.e., with a single threshold. Our approach is fast, achieving real-time performance on a modern CPU, and provides state-of-the-art results on the popular Berkeley Segmentation Dataset."
    },
    {
      "id": "47b37c8f-2df8-4404-82b1-086bc0ac3547",
      "adv_source_id": "47b37c8f-2df8-4404-82b1-086bc0ac3547",
      "source_id": "a99184a4-6066-4778-99ed-4f3c9d98ba83",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Fast Edge Detection Using Structured Forests",
      "prompt": "Write the abstract for the academic paper titled \"Fast Edge Detection Using Structured Forests\".",
      "generation": "Abstract:\nTraditional edge detection algorithms are based on derivative filters, such as the first-order derivative to detect abrupt changes in intensity, or the second-order derivative to detect inflection points. These methods have limited performance in detecting edges in noisy images, and edges with different types of noise. In this paper, we propose a new edge detection algorithm based on structured forests. Our method takes advantage of the forest structure to compute the gradient of the image in a hierarchical way. This allows us to reduce noise and detect edges in a more robust way. Our method is also fast, as it uses a greedy algorithm to optimize the structure of the forest. We compare our method to state-of-the-art edge detection algorithms, and show that it performs better in terms of speed and accuracy."
    },
    {
      "id": "180bb788-7c1f-448d-904d-312894870fa3",
      "adv_source_id": "180bb788-7c1f-448d-904d-312894870fa3",
      "source_id": "7fdb0b4a-1cf4-4a4d-8ab5-c221c8e1ada9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Open-set Person Re-identification",
      "prompt": "Write the abstract for the academic paper titled \"Open-set Person Re-identification\".",
      "generation": "Abstract:\nPerson re-identification (ReID) has achieved remarkable progress in recent years with the development of deep learning techniques. However, the existing ReID algorithms are typically designed and evaluated under the closed-set scenario, where the test-time identities are known during training. In this work, we argue that open-set ReID is more appealing in real-world applications, especially for monitoring in smart cities, where the number of targets is huge and only a small fraction of them are seen during training. It poses great challenges for ReID since the identities in the test set are unknown during training and could be very different from the training identities. To address the open-set ReID problem, we propose a novel open-set aware loss consisting of an identity loss and an open-set loss. Specifically, we propose a max-margin loss for person re-identification, and a distribution matching loss to handle the open-set challenge. To evaluate our method, we create a new benchmark dataset, namely Stanford-OpenSet, which is the largest open-set person ReID dataset to the best of our knowledge with more than 100,000 images from 800 identities. Our comprehensive experiments on three datasets demonstrate the effectiveness of the proposed approach against various competing methods."
    },
    {
      "id": "205febc0-15d7-4996-a68a-14d660b3ab09",
      "adv_source_id": "205febc0-15d7-4996-a68a-14d660b3ab09",
      "source_id": "7fdb0b4a-1cf4-4a4d-8ab5-c221c8e1ada9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Open-set Person Re-identification",
      "prompt": "Write the abstract for the academic paper titled \"Open-set Person Re-identification\".",
      "generation": "Abstract:\nPerson re-identification (ReID) has achieved remarkable progress in recent years with the advances in deep learning. However, the existing ReID algorithms are typically designed under the assumption that the training and testing sets contain only a common set of persons, which is referred to as the closed-set setting. In reality, the identities in the training set may not be present in the testing set in many applications, which is the open-set setting. Open-set ReID is more challenging since the model must be able to identify unknown persons as well as known persons. In this paper, we explore the open-set person re-identification problem and propose an effective solution. Our approach is based on a novel architecture called Open-Set Feature Pyramid Network (OSFPN). OSFPN integrates a feature pyramid network with an open-set recognition module. Specifically, we design a graph-based open-set recognition module that can effectively detect and match the known persons in the feature pyramid network. OSFPN is trained in an end-to-end manner on the open-set person re-identification dataset. We evaluate OSFPN on two standard open-set person re-identification benchmarks, and experimental results demonstrate the effectiveness of our approach."
    },
    {
      "id": "8bbdff5f-3d22-4bee-9912-8cae250adf88",
      "adv_source_id": "8bbdff5f-3d22-4bee-9912-8cae250adf88",
      "source_id": "59dd570d-3eda-49b1-85aa-fa94abaabaec",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation\".",
      "generation": "Abstract:\nWe present a method for fast pathological lung segmentation that operates on lung CT scans and a single lung volume. It computes a sparse point sample from the input volume and predicts a segmentation mask for the entire volume. Our method trains a keypoint generator network to sparsely sample voxels across the entire lung volume by successively removing easy-to-segment structures. We introduce a new benchmark dataset and show that our method outperforms the state of the art both in speed and segmentation quality while running at 36 fps on a single GPU without any post-processing. Our code and dataset are available at https://github.com/RadiologyAI/Keypoint-Sampling."
    },
    {
      "id": "aac3cb82-353d-4267-89f6-08bd7ea0c6df",
      "adv_source_id": "aac3cb82-353d-4267-89f6-08bd7ea0c6df",
      "source_id": "59dd570d-3eda-49b1-85aa-fa94abaabaec",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Near-optimal Keypoint Sampling for Fast Pathological Lung Segmentation\".",
      "generation": "Abstract:\nWe present a method for fast pathological lung segmentation that operates on chest radiographs. Unlike previous approaches that rely on manually designed features, our method is based on keypoint sampling powered by an attention mechanism. We identify important regions on the image that contribute the most to the segmentation by continuously querying the model for attention. The proposed method is fast, memory-efficient, and operates in the pixel domain without requiring lung boundary annotations. We evaluate our method on 350 chest radiographs and demonstrate that it achieves state-of-the-art performance in terms of speed and segmentation quality compared to prior methods."
    },
    {
      "id": "5ce6f321-857f-425b-867f-0ccf2e3069e6",
      "adv_source_id": "5ce6f321-857f-425b-867f-0ccf2e3069e6",
      "source_id": "51d16092-4f0c-4c73-9d5e-13fc713cb941",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating Near-Infrared Information into Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Incorporating Near-Infrared Information into Semantic Image Segmentation\".",
      "generation": "Near-infrared (NIR) information is beneficial for semantic segmentation in challenging scenarios. In this paper, we present a novel approach to incorporate NIR information into semantic image segmentation. Our method combines the strengths of both NIR and visible spectrum (VIS) information to improve the accuracy of segmentation. We first extract features from the NIR and VIS images using a deep learning model. The features are then fused to generate a final segmentation mask. Our approach is evaluated on the Cityscapes dataset, and the results show that our method significantly improves the segmentation accuracy compared to the state-of-the-art. We achieve a mIOU of 85.5% on the test set, which is 1.7% higher than the previous state-of-the-art. Our approach is also efficient, with a runtime of 0.24 seconds per iteration on a single Tesla V100 GPU."
    },
    {
      "id": "db17825a-90e7-403f-a05a-8de4f3f4d572",
      "adv_source_id": "db17825a-90e7-403f-a05a-8de4f3f4d572",
      "source_id": "51d16092-4f0c-4c73-9d5e-13fc713cb941",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Incorporating Near-Infrared Information into Semantic Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Incorporating Near-Infrared Information into Semantic Image Segmentation\".",
      "generation": "Near-infrared (NIR) information is beneficial for semantic segmentation in outdoor scenes. However, NIR images are challenging to utilize effectively due to their poor contrast and spectral information. In this paper, we enhance NIR images with supervised contrast enhancement to generate high-quality NIR images. Then, we propose a novel deep learning framework that incorporates NIR information into semantic image segmentation. Our framework comprises a backbone network and two branch networks: a coarse-grained branch and a fine-grained branch. The backbone network extracts features from the visible images, while the coarse-grained branch and fine-grained branch separately process the enhanced NIR images. We utilize a novel coarse-to-fine training strategy to train our networks, which effectively leverages the NIR information for accurate segmentation. Our approach achieves state-of-the-art performance on semantic image segmentation benchmarks such as Cityscapes and PASCAL VOC 2012."
    },
    {
      "id": "91578062-7856-415a-b2c6-f10a715cd77b",
      "adv_source_id": "91578062-7856-415a-b2c6-f10a715cd77b",
      "source_id": "5c9dc6c5-d804-48a3-8309-5c671883d770",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Variational Image Segmentation Model Coupled with Image Restoration Achievements",
      "prompt": "Write the abstract for the academic paper titled \"Variational Image Segmentation Model Coupled with Image Restoration Achievements\".",
      "generation": "Abstract:\nIn the field of image processing, image segmentation is a crucial task, as it involves dividing an image into distinct regions corresponding to different objects or boundaries. However, many real-world images are degraded due to various factors such as noise, blur, or missing information. In this paper, we propose a variational image segmentation model that is coupled with image restoration techniques to overcome the challenges of degraded images. Our model utilizes a variational approach, which minimizes an energy function that enforces piecewise constant intensity and smooth spatial transitions between regions. Additionally, we incorporate image restoration techniques, such as denoising and deblurring, to enhance the quality of the input image and improve the performance of the segmentation model. Our experimental results demonstrate that the proposed model achieves state-of-the-art performance in terms of segmentation accuracy and robustness to degraded images. We believe that the combination of variational segmentation and image restoration techniques has the potential to significantly impact various applications in computer vision and image processing."
    },
    {
      "id": "4aecb35c-f78d-4cb3-89ed-1762b3706108",
      "adv_source_id": "4aecb35c-f78d-4cb3-89ed-1762b3706108",
      "source_id": "5c9dc6c5-d804-48a3-8309-5c671883d770",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Variational Image Segmentation Model Coupled with Image Restoration Achievements",
      "prompt": "Write the abstract for the academic paper titled \"Variational Image Segmentation Model Coupled with Image Restoration Achievements\".",
      "generation": "Abstract:\nIn this paper, we present a variational image segmentation model coupled with image restoration achievements. Our model combines the benefits of image segmentation and image restoration to achieve better results in both tasks. We first use a variational approach to image segmentation, which enables us to capture the underlying structure of the image and extract meaningful features. We then couple this with image restoration, which allows us to enhance the image and improve the segmentation results. Our model is trained on a large dataset of images and achieves state-of-the-art performance on various image segmentation and image restoration benchmarks. We show that our model is effective in a variety of applications, including medical imaging, remote sensing, and digital photography. \n\nVariational image segmentation aims to find a pixel-wise labeling of an image, given a predefined number of classes. This problem has received a lot of attention in the past years, due to its importance in many computer vision applications. Recent advances in deep learning have led to many successful variants of variational image segmentation models. However, most of these models rely on a clean initial estimate of the segmentation, which is not always available in practice. In this paper, we propose a new model for variational image segmentation, which is coupled with an image restoration module. Our restoration module takes an initial estimate of the segmentation as input and generates a clean image as output. This image is then used as input to the segmentation module, which produces a refined segmentation as output. We show that our model is able to produce better segmentations than existing models in the presence of noise and other artifacts."
    },
    {
      "id": "adcc6597-729a-4ed6-8edf-822ecf1c6ef9",
      "adv_source_id": "adcc6597-729a-4ed6-8edf-822ecf1c6ef9",
      "source_id": "34276620-b1c3-4c30-97d5-2f222f881076",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies",
      "prompt": "Write the abstract for the academic paper titled \"A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies\".",
      "generation": "An algorithm for making moves in the solution of a general non-submodular pairwise energy, subject to a cardinality constraint, is presented. The algorithm is based on a tiered approach, where the pairs of the solution are divided into three tiers. The algorithm makes moves by iteratively selecting a pair from each tier, and moving it to the solution, subject to the cardinality constraint. The algorithm utilizes a greedy approach in each tier, where the pair with the highest increase in the objective function is selected.\n\nThe algorithm guarantees that the objective function of the resulting solution is within a factor of two of the optimal solution. The running time of the algorithm is O(n^2), where n is the number of items.\n\nThis paper presents a novel approach to solving the problem of move-making for general non-submodular pairwise energies, and provides a theoretical guarantee for the quality of the solution."
    },
    {
      "id": "acdaefda-5278-40ab-a368-7e03cca27f73",
      "adv_source_id": "acdaefda-5278-40ab-a368-7e03cca27f73",
      "source_id": "34276620-b1c3-4c30-97d5-2f222f881076",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies",
      "prompt": "Write the abstract for the academic paper titled \"A Tiered Move-making Algorithm for General Non-submodular Pairwise Energies\".",
      "generation": "An algorithm for making moves in the solution of a general non-submodular pairwise energy, subject to a cardinality constraint, is presented. The algorithm is based on a natural generalization of the notion of tiered approaches from the submodular setting to the non-submodular case. It is shown that the algorithm outputs a point in the intersection of the convex hull of the initial placement and the set of solutions of the relaxed problem, which, in general, may not be a solution of the original problem. However, when the energy is submodular, the output point is shown to be a solution of the original problem, thus providing a generalization of the well-known greedy algorithm for submodular energies. The algorithm is tested on instances of several important energy functions that arise in practical applications."
    },
    {
      "id": "48e068e0-b582-4a3a-b9ea-7f179b801b96",
      "adv_source_id": "48e068e0-b582-4a3a-b9ea-7f179b801b96",
      "source_id": "4b1ca80e-60c9-4567-b437-90fc7c4545af",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm",
      "prompt": "Write the abstract for the academic paper titled \"Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm\".",
      "generation": "This paper introduces a new clustering algorithm called Vector Membership (VM), which is an extension of the Fuzzy C-Means (FCM) algorithm. VM uses a vector-based representation of the input data, which allows for more accurate clustering than the traditional FCM algorithm. The VM algorithm is evaluated on a number of benchmark datasets, and it is shown to be more accurate and efficient than the FCM algorithm.\n\nThe paper also explores the use of VM in a number of application areas, including image analysis, natural language processing, and speech recognition. In these applications, VM is shown to be more accurate and efficient than the FCM algorithm.\n\nOverall, this paper introduces a new and improved clustering algorithm that is more accurate and efficient than the traditional FCM algorithm. It is suitable for a wide range of applications, including image analysis, natural language processing, and speech recognition."
    },
    {
      "id": "418b9616-a26f-492a-a576-72a746e6734a",
      "adv_source_id": "418b9616-a26f-492a-a576-72a746e6734a",
      "source_id": "4b1ca80e-60c9-4567-b437-90fc7c4545af",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm",
      "prompt": "Write the abstract for the academic paper titled \"Clustering using Vector Membership: An Extension of the Fuzzy C-Means Algorithm\".",
      "generation": "This paper presents an extension of the Fuzzy C-Means (FCM) algorithm for vector-valued data, which we call Vector Membership (VM). In contrast to FCM, which works with single-valued data, VM uses a vector-valued input to represent the uncertainty or ambiguity in the data. We show that VM can capture more complex and realistic shapes in the data, and can be used to improve the performance of FCM in clustering tasks. We present a comprehensive evaluation of VM on a variety of datasets, and show that it outperforms FCM in terms of accuracy and runtime. We also discuss the advantages of VM over other clustering algorithms, and present a series of examples to demonstrate its effectiveness."
    },
    {
      "id": "f3686b3e-5904-4652-94d1-0bfea75bf36b",
      "adv_source_id": "f3686b3e-5904-4652-94d1-0bfea75bf36b",
      "source_id": "d491aa7c-3468-4ac9-acd1-d72353da723b",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient Energy Minimization for Enforcing Statistics",
      "prompt": "Write the abstract for the academic paper titled \"Efficient Energy Minimization for Enforcing Statistics\".",
      "generation": "Abstract:\nEnforcing statistics is a fundamental problem in machine learning and statistics, with many applications in data science, including hypothesis testing, confidence intervals, and clustering. It involves finding the smallest region in the feature space that contains a specified fraction of the data points while minimizing the energy of the region. We present an efficient algorithm for enforcing statistics that combines the greedy algorithm with the dynamic programming technique. Our algorithm achieves a better balance between the accuracy and the computational complexity of the solution. We evaluate the performance of our algorithm on a standard dataset, and the results show that our algorithm is significantly faster and more accurate than the existing algorithms."
    },
    {
      "id": "24dae5e6-572d-4a5d-8e11-9dea9664db3f",
      "adv_source_id": "24dae5e6-572d-4a5d-8e11-9dea9664db3f",
      "source_id": "d491aa7c-3468-4ac9-acd1-d72353da723b",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Efficient Energy Minimization for Enforcing Statistics",
      "prompt": "Write the abstract for the academic paper titled \"Efficient Energy Minimization for Enforcing Statistics\".",
      "generation": "An increasingly important aspect of statistical analysis is ensuring the accuracy and validity of the data being used. However, there are many real-world challenges associated with collecting data, especially with the rapid development of technology and communication. The paper \"Efficient Energy Minimization for Enforcing Statistics\" focuses on the innovative approach of using a cost function to minimize the difference between the expected statistical properties of data and the actual properties of the data, in order to improve the quality of the data. The cost function is designed to take into account the energy of the data, which is defined as the sum of the absolute values of the differences between the expected and actual statistical properties. The cost function is then minimized using a gradient descent algorithm, which iteratively updates the data to reduce the energy and improve the quality of the data. The paper also discusses the application of this approach to the problem of enforcing statistics in the presence of noise, and demonstrates the effectiveness of the approach through numerical examples. Overall, this paper makes a significant contribution to the field of statistics by providing a new and efficient approach to ensuring the accuracy and validity of data."
    },
    {
      "id": "0b0f1f22-c283-4718-97ba-e358e91dbcb6",
      "adv_source_id": "0b0f1f22-c283-4718-97ba-e358e91dbcb6",
      "source_id": "39b84587-7fd4-4a2b-a566-aaad6d196df1",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Live-wire 3D medical images segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Live-wire 3D medical images segmentation\".",
      "generation": "The paper presents a novel method for segmenting 3D medical images acquired through live-wire technique. The proposed method combines the advantages of 3D live-wire segmentation with the robustness of 3D watershed segmentation. In the first stage, the 3D live-wire technique is used to obtain an initial segmentation. This initial segmentation is then refined using 3D watershed segmentation. The proposed method has been evaluated on a dataset of 3D medical images acquired from real patients. The experimental results show that the proposed method achieves a significant improvement in segmentation accuracy compared to the 3D live-wire technique alone. The proposed method has been successfully applied to segment various anatomical structures in 3D medical images, including the liver, spleen, and kidneys."
    },
    {
      "id": "5e9f5757-2ab2-48ce-b154-f827fa41e5e1",
      "adv_source_id": "5e9f5757-2ab2-48ce-b154-f827fa41e5e1",
      "source_id": "39b84587-7fd4-4a2b-a566-aaad6d196df1",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Live-wire 3D medical images segmentation",
      "prompt": "Write the abstract for the academic paper titled \"Live-wire 3D medical images segmentation\".",
      "generation": "The abstract for an academic paper titled \"Live-wire 3D medical images segmentation\" would include:\n\nMedical image segmentation is often limited to 2D images, requiring humans to mentally reconstruct a 3D volume from a series of 2D slices. This process can result in misregistration and time delays in surgical procedures. This paper introduces a technique for the segmentation of 3D medical images in near real-time, with a focus on live-wire segmentation. Our technique utilizes a live-wire algorithm that is optimized for 3D images. We evaluate our technique on a series of medical images, including MRI and CT scans. Our results show that our technique is able to segment 3D medical images in near real-time, with accuracy comparable to that of manual segmentation. The implementation of our system has the potential to improve the efficiency and accuracy of medical image segmentation, leading to better surgical planning and patient care."
    },
    {
      "id": "bc80884d-0bab-483f-9cf3-eb2ba23689ec",
      "adv_source_id": "bc80884d-0bab-483f-9cf3-eb2ba23689ec",
      "source_id": "c7755a04-f926-4698-9639-a3613011e9f9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Image Segmentation in Low Depth Of Field Images",
      "prompt": "Write the abstract for the academic paper titled \"Robust Image Segmentation in Low Depth Of Field Images\".",
      "generation": "An algorithm for robust image segmentation in images with low depth of field is proposed in this paper. In images with a low depth of field, objects in different planes are not in focus, which makes it difficult to segment these objects using existing image segmentation techniques. The proposed algorithm uses a combination of edge detection and region segmentation to overcome this problem. It first detects the edges in the image using a Canny edge detector. The edges are then used to segment the image into regions using a watershed transform. The resulting regions are then merged using a region merging algorithm. The algorithm is evaluated on a dataset of images with low depth of field and compared to existing image segmentation algorithms. The experimental results show that the proposed algorithm is able to segment images with low depth of field more accurately than existing algorithms."
    },
    {
      "id": "bd57f5ed-f6e7-44e5-9a1c-ee1e07bd4577",
      "adv_source_id": "bd57f5ed-f6e7-44e5-9a1c-ee1e07bd4577",
      "source_id": "c7755a04-f926-4698-9639-a3613011e9f9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Robust Image Segmentation in Low Depth Of Field Images",
      "prompt": "Write the abstract for the academic paper titled \"Robust Image Segmentation in Low Depth Of Field Images\".",
      "generation": "An algorithm for robust image segmentation in images with low depth of field is proposed in this paper. In images with a low depth of field, objects at different distances from the camera are out of focus, which makes it difficult to segment these objects accurately using traditional image segmentation techniques. The proposed algorithm uses a combination of focus measures and clustering techniques to segment objects in low depth of field images. The focus measures are used to identify regions of the image that are in focus, and the clustering techniques are used to segment the objects in these regions. The algorithm is evaluated on a dataset of low depth of field images, and the results show that it is able to segment objects accurately even in images with a low depth of field."
    },
    {
      "id": "792e5d84-7abc-4bbb-8c16-8c7189495a01",
      "adv_source_id": "792e5d84-7abc-4bbb-8c16-8c7189495a01",
      "source_id": "5995c43a-4d3e-4323-b562-9617d4756b35",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Gray Image extraction using Fuzzy Logic",
      "prompt": "Write the abstract for the academic paper titled \"Gray Image extraction using Fuzzy Logic\".",
      "generation": "Gray image extraction is a technique used to extract the gray values of pixels in an image, which can be used for further processing or analysis. In this paper, we present a new method for gray image extraction using fuzzy logic. Our method uses a fuzzy logic algorithm to extract the gray values of pixels in an image, and the extracted values are then used to create a new image with only the gray values. The new image can be used for further processing or analysis, such as thresholding or segmentation. We evaluated our method on a variety of images and compared the results with those obtained using a traditional binary segmentation algorithm. Our results show that our method is more accurate and efficient than the traditional binary segmentation algorithm.\n\nAbstract: A new gray image extraction technique using fuzzy logic is presented in this paper. Our method uses a fuzzy logic algorithm to extract the gray values of image pixels, and the extracted values are used to generate a new image that only contains the gray values. The new image can be used for additional processing or analysis, such as thresholding or segmentation. When compared to a traditional binary segmentation algorithm, our method proved to be more precise and effective. The evaluation was conducted on a variety of images, and the results were encouraging."
    },
    {
      "id": "d8b7ffa7-30c5-4fad-bb6f-d2f6206c5326",
      "adv_source_id": "d8b7ffa7-30c5-4fad-bb6f-d2f6206c5326",
      "source_id": "5995c43a-4d3e-4323-b562-9617d4756b35",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Gray Image extraction using Fuzzy Logic",
      "prompt": "Write the abstract for the academic paper titled \"Gray Image extraction using Fuzzy Logic\".",
      "generation": "Here is the abstract for the academic paper titled \"Gray Image extraction using Fuzzy Logic\":\n\nFuzzy logic is a many-valued logic that is an extension of classical binary logic. In this paper, we discuss the application of fuzzy logic in grayscale image extraction. Our approach uses fuzzy logic to extract the intensity of each pixel in an image, and then uses these intensities to classify the image as either a grayscale image or a color image. The extraction process is based on a fuzzy rule-based system that uses a set of if-then rules to determine the intensity of each pixel. The rules are designed based on the characteristics of grayscale images, and the system is trained on a set of grayscale and color images. The results show that the proposed method is able to extract grayscale images with high accuracy.\n\nWould you like to know more about this topic?"
    },
    {
      "id": "c61c651e-006a-48e3-a1e0-0d5402470947",
      "adv_source_id": "c61c651e-006a-48e3-a1e0-0d5402470947",
      "source_id": "b78ac52e-93ca-4ed2-9b68-32f6cf1f4617",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Model Assisted Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"3D Model Assisted Image Segmentation\".",
      "generation": "3D model-assisted image segmentation is a powerful tool for extracting objects of interest from 2D images. The process involves the estimation of pixel labels based on a 3D model of the object and its projected 2D appearance. This method has been shown to improve the accuracy and efficiency of image segmentation tasks.\n\nIn this paper, we present a novel approach to 3D model-assisted image segmentation. Our method utilizes a 3D model of the object to generate a set of 2D projections that are then used to estimate pixel labels in the image. We show that our approach is able to improve the accuracy and efficiency of image segmentation tasks, particularly in the presence of noise and other challenges.\n\nWe evaluate our approach on a variety of image segmentation tasks, including the segmentation of objects in noisy and challenging environments. Our results show that our approach is able to improve the accuracy and efficiency of image segmentation tasks, even in the presence of noise and other challenges. We also show that our approach is able to generalize to new objects and environments, demonstrating its effectiveness and robustness."
    },
    {
      "id": "2dea9059-e97c-4cb6-bf83-0a0b2e856f08",
      "adv_source_id": "2dea9059-e97c-4cb6-bf83-0a0b2e856f08",
      "source_id": "b78ac52e-93ca-4ed2-9b68-32f6cf1f4617",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "3D Model Assisted Image Segmentation",
      "prompt": "Write the abstract for the academic paper titled \"3D Model Assisted Image Segmentation\".",
      "generation": "3D model-assisted image segmentation is a powerful tool for extracting meaningful information from medical images to aid in the diagnosis and treatment of various medical conditions. This paper discusses the application of 3D models in image segmentation and explores the benefits and challenges of this approach.\n\nThe use of 3D models offers several advantages over traditional image segmentation techniques. First, it allows for more accurate and precise segmentation, as the 3D model can be fitted to the image data more accurately than traditional techniques. This is particularly important for medical images, which often contain complex and intricate structures that can be difficult to segment using traditional methods.\n\nIn addition, 3D models can be used to extract additional information from medical images, such as the size and shape of structures, which can be used to aid in the diagnosis and treatment of medical conditions. This information can be used to improve the accuracy and efficiency of medical imaging analysis and can potentially lead to better patient outcomes.\n\nHowever, there are also challenges associated with the use of 3D models in image segmentation. One challenge is the complexity of the 3D models, which can be difficult to create and fit to the image data. In addition, the use of 3D models can be computationally expensive, which can be a challenge in the context of medical imaging, where large amounts of data need to be processed in a timely manner.\n\nIn conclusion, 3D model-assisted image segmentation is a powerful tool for extracting meaningful information from medical images to aid in the diagnosis and treatment of various medical conditions. While there are challenges associated with the use of 3D models, the benefits of improved accuracy and efficiency make this approach a valuable tool for medical imaging analysis."
    },
    {
      "id": "9715b49c-8aed-4966-ade9-be0d85080fdb",
      "adv_source_id": "9715b49c-8aed-4966-ade9-be0d85080fdb",
      "source_id": "2dca91ca-3683-4870-92c8-e9172f837fa9",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A linear framework for region-based image segmentation and inpainting involving curvature penalization",
      "prompt": "Write the abstract for the academic paper titled \"A linear framework for region-based image segmentation and inpainting involving curvature penalization\".",
      "generation": "An innovative linear framework for image segmentation and inpainting involving curvature penalization is proposed in this paper, providing a new approach to handle difficult regions, such as object boundaries and noisy areas, effectively. This framework utilizes a region-based segmentation algorithm that incorporates curvature within the energy function. The energy function minimization process is achieved through a linear programming approach, which allows for efficient computation and real-time processing. Additionally, an inpainting algorithm is presented to effectively restore damaged or missing regions within an image, utilizing the proposed segmentation framework. The experimental results demonstrate the superiority of the proposed framework compared to state-of-the-art techniques in terms of segmentation quality and computational efficiency. The combination of curvature penalization and linear programming provides a robust and efficient solution for image segmentation and inpainting tasks."
    },
    {
      "id": "9eb5391c-4956-4fac-b26d-d7d94865d5ba",
      "adv_source_id": "9eb5391c-4956-4fac-b26d-d7d94865d5ba",
      "source_id": "2dca91ca-3683-4870-92c8-e9172f837fa9",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A linear framework for region-based image segmentation and inpainting involving curvature penalization",
      "prompt": "Write the abstract for the academic paper titled \"A linear framework for region-based image segmentation and inpainting involving curvature penalization\".",
      "generation": "An innovative, effective, and efficient linear framework for image segmentation and inpainting involving curvature penalization is proposed in this paper to tackle the issue of pixel-wise labeling of objects in images. The framework focuses on extracting the foreground and background regions in images using a fully-linearized optimization approach. This approach allows for the simultaneous segmentation and inpainting of images while incorporating curvature penalization to preserve the boundaries of objects. The proposed method utilizes a graph-based representation of the image, where the nodes represent the pixels, and the edges represent the similarities between the pixels. The pixel-wise labeling is achieved by minimizing an energy function that enforces the smoothness of the foreground and background regions, as well as the curvature penalization of the object boundaries. The proposed method has been evaluated on a variety of images, and the results show that it is a promising approach for image segmentation and inpainting involving curvature penalization. The evaluation metrics used to assess the performance of the proposed method include the well-known metrics of Mean Absolute Error, Mean Squared Error, and Structural Similarity Index."
    },
    {
      "id": "40c5aa8f-56e9-4a80-877b-6358e9800875",
      "adv_source_id": "40c5aa8f-56e9-4a80-877b-6358e9800875",
      "source_id": "8c9cafc4-fe7b-4b95-9ed7-ac1e5f2cafea",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-environment model estimation for motility analysis of Caenorhabditis Elegans",
      "prompt": "Write the abstract for the academic paper titled \"Multi-environment model estimation for motility analysis of Caenorhabditis Elegans\".",
      "generation": "Abstract:\nTraditional modeling techniques for studying Caenorhabditis Elegans (C. Elegans) motility data from a single environment can produce inaccurate results due to the inability of the model to capture the complete behavioral repertoire of the organism. Such techniques often rely on manually curated features that fail to capture the full range of behaviors exhibited by the organism. This paper presents a novel multi-environment model estimation approach for motility analysis of C. Elegans. Data-driven deep learning is used to automatically estimate a low-dimensional behavioral space from videos of C. Elegans in multiple environments. This behavioral space can then be used to infer a model of the organism's behavior in a new environment. The approach was tested on a new dataset of C. Elegans videos in different environments and the results show that the proposed method improves the accuracy of motility analysis compared to traditional modeling techniques."
    },
    {
      "id": "9d77aa03-5621-46de-8f47-f52f10ae4a01",
      "adv_source_id": "9d77aa03-5621-46de-8f47-f52f10ae4a01",
      "source_id": "8c9cafc4-fe7b-4b95-9ed7-ac1e5f2cafea",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "Multi-environment model estimation for motility analysis of Caenorhabditis Elegans",
      "prompt": "Write the abstract for the academic paper titled \"Multi-environment model estimation for motility analysis of Caenorhabditis Elegans\".",
      "generation": "The paper presents a multi-environment model estimation (MMEM) method for the analysis of the motility patterns of Caenorhabditis Elegans. The MMEM method uses a supervised machine learning approach to estimate the parameters of a biological model from experimental data. The data is collected in multiple environments, and the method uses the information from these multiple environments to estimate the model parameters. The estimated parameters can then be used to predict the behavior of the organism in new environments. The MMEM method is demonstrated on experimental data from C. Elegans, and the results show that the method can accurately estimate the model parameters and predict the behavior of the organism in new environments. The MMEM method offers a powerful new approach for the analysis of the motility patterns of C. Elegans and other organisms."
    },
    {
      "id": "6d039f78-ff2a-4ce6-98ff-c955ac1ee7e1",
      "adv_source_id": "6d039f78-ff2a-4ce6-98ff-c955ac1ee7e1",
      "source_id": "4c4676d5-c143-448c-b058-9c360b04d900",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Topological derivative based image segmentation for sign language recognition system using isotropic filter",
      "prompt": "Write the abstract for the academic paper titled \"A Topological derivative based image segmentation for sign language recognition system using isotropic filter\".",
      "generation": "In this paper, we present a topological derivative-based image segmentation approach for sign language recognition. The segmentation is achieved through the isotropic filter, which reduces the complexity of the image and preserves the edge information. The topological derivative is computed through the eigenvalues and eigenvectors of the Laplacian operator. The proposed method is evaluated on a sign language recognition dataset, and the results show that the proposed method achieves a higher accuracy compared to the existing methods. The experiment results show that the proposed method achieves a segmentation accuracy of 95.9%, which is higher than the existing methods. The proposed method also achieves a recognition accuracy of 92.5%, which is higher than the existing methods.\n\nThe proposed method is based on the isotropic filter, which reduces the complexity of the image and preserves the edge information. This allows for a more accurate segmentation of the sign language images. The topological derivative is computed through the eigenvalues and eigenvectors of the Laplacian operator. This allows for a more efficient segmentation of the sign language images. \n\nThe proposed method is capable of accurately segmenting sign language images, which is an active area of research. The method utilizes the isotropic filter and topological derivative to achieve accurate segmentation, enabling subsequent recognition of sign language."
    },
    {
      "id": "2b44c2e7-beed-49c5-af55-cfcbba7f26a3",
      "adv_source_id": "2b44c2e7-beed-49c5-af55-cfcbba7f26a3",
      "source_id": "4c4676d5-c143-448c-b058-9c360b04d900",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "A Topological derivative based image segmentation for sign language recognition system using isotropic filter",
      "prompt": "Write the abstract for the academic paper titled \"A Topological derivative based image segmentation for sign language recognition system using isotropic filter\".",
      "generation": "In this paper, we present a topological derivative-based image segmentation approach for recognizing sign language gestures in real-time. The proposed method utilizes an isotropic filter to capture the spatial distribution of pixels in the image, enabling efficient and effective segmentation of the gesture from the background. The topological derivative measures the gradient of the image intensity function with respect to the image domain, resulting in a powerful tool for image analysis and processing. The segmentation process is based on thresholding the topological derivative output, enabling the extraction of the gesture region from the image. The proposed method has been evaluated on a sign language recognition system and achieved promising results in terms of accuracy and efficiency."
    },
    {
      "id": "2abb1d9d-f58d-41d7-923f-9c767e9e9ba4",
      "adv_source_id": "2abb1d9d-f58d-41d7-923f-9c767e9e9ba4",
      "source_id": "dfae1272-4190-42eb-bc08-706538d93519",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An information-theoretic derivation of min-cut based clustering",
      "prompt": "Write the abstract for the academic paper titled \"An information-theoretic derivation of min-cut based clustering\".",
      "generation": "This paper derives a new family of clustering algorithms based on the min-cut algorithm, a graph-based algorithm commonly used in computer science to solve the minimal cut problem in both weighted and unweighted graphs. In this work, we consider the problem of clustering, where the goal is to partition a given set of data points into distinct clusters based on a measure of similarity or distance between data points. We propose a new family of clustering algorithms that use the min-cut algorithm as a basis for clustering, and show that these algorithms are able to produce high-quality clusters in a variety of data sets. Furthermore, we provide an information-theoretic derivation of these algorithms, showing that they are able to produce clusters that optimize a certain information-theoretic measure of cluster quality."
    },
    {
      "id": "d9cbe148-af9d-434a-adf1-33858407672f",
      "adv_source_id": "d9cbe148-af9d-434a-adf1-33858407672f",
      "source_id": "dfae1272-4190-42eb-bc08-706538d93519",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "An information-theoretic derivation of min-cut based clustering",
      "prompt": "Write the abstract for the academic paper titled \"An information-theoretic derivation of min-cut based clustering\".",
      "generation": "This paper derives a min-cut based clustering algorithm using an information-theoretic approach. The algorithm is applied to a dataset of 100,000 randomly generated points and the min-cut between the clusters are calculated. The algorithm is compared to k-means, a widely used clustering algorithm, and it is shown to have a lower error rate and to be more efficient in terms of computational cost. The min-cut based clustering algorithm is also shown to be more effective at identifying clusters in datasets with complex structures and noise. The experimental results demonstrate the effectiveness of the proposed algorithm and suggest that it may be a useful alternative to k-means and other clustering algorithms."
    },
    {
      "id": "43d26516-477f-44bb-bdc9-67fa3413245c",
      "adv_source_id": "43d26516-477f-44bb-bdc9-67fa3413245c",
      "source_id": "106a77e0-cf20-44a6-bfa9-6ffd08551882",
      "model": "cohere-chat",
      "decoding": "greedy",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients",
      "prompt": "Write the abstract for the academic paper titled \"$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients\".",
      "generation": "This paper addresses the $L^2$ well-posedness of boundary value problems for parabolic systems with measurable coefficients. Well-posedness in $L^2$ means that for every $L^2$-boundary value problem for the given parabolic system, there is a unique solution $u \\in L^2$ of class $W^{1,2}_{0,\\text{loc}}$.\n\nThe main results of this paper are Theorems 1.1 and 1.2. The former provides a sufficient condition for $L^2$ well-posedness in terms of the coercivity and continuity of the associated linear system. The latter presents a necessary and sufficient condition for $L^2$-well posedness in terms of the coercivity and continuity of the nonlinear system. These results generalize and complement corresponding results in the literature for parabolic systems with constant coefficients.\n\nThe paper concludes with an example that illustrates the applicability of Theorems 1.1 and 1.2."
    },
    {
      "id": "f0642b3d-f4d3-4a0f-8931-2c3e58c6bf9e",
      "adv_source_id": "f0642b3d-f4d3-4a0f-8931-2c3e58c6bf9e",
      "source_id": "106a77e0-cf20-44a6-bfa9-6ffd08551882",
      "model": "cohere-chat",
      "decoding": "sampling",
      "repetition_penalty": "no",
      "attack": "none",
      "domain": "abstracts",
      "title": "$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients",
      "prompt": "Write the abstract for the academic paper titled \"$L^2$ well-posedness of boundary value problems for parabolic systems\n  with measurable coefficients\".",
      "generation": "This paper addresses the $L^2$ well-posedness of boundary value problems for parabolic systems with measurable coefficients. Well-posedness in $L^2$ means that for every $L^2$ boundary value problem for the given parabolic system, there is a unique solution $u \\in L^2$ of that problem. To prove well-posedness, we show that the associated integral operator is invertible on $L^2$.\nParabolic systems with measurable coefficients that are considered in this paper are given by nonlinear boundary value problems of the form:\n$$\\begin{aligned}\n    \\partial_t u - \\mathcal A u &= f \\quad \\text{for} \\ t>0, \\\\\n    u(0) = u_0, \\quad \\partial_t u(0) &= u_1,\n  \\end{aligned}$$\nwhere $u = (u_1,\\dots,u_d)^T$ is an $L^2$-function on $\\mathbb R^d$, $\\mathcal A$ is a non-densely defined, closed, and symmetric operator in $L^2$ with compact inverse, and $f \\in L^2$.\nThe main results of this paper are Theorems 1.1 and 1.2. The former implies that the operator $\\mathcal A$ is invertible on $L^2$ under the assumption that its spectrum is not the whole interval $[0,1]$. The latter asserts the well-posedness of boundary value problems for the heat equation with measurable coefficients."
    }
  ]
}