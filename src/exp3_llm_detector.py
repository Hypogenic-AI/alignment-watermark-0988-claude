"""
Experiment 3: LLM-as-Detector
Uses GPT-4.1 to classify texts as human vs. AI-generated.
Tests the hypothesis that aligned models are more detectable by other AIs.
"""
import json
import os
import sys
import time
import numpy as np
import pandas as pd
from openai import OpenAI
from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix
from scipy import stats
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import seaborn as sns

SEED = 42
np.random.seed(SEED)

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
RESULTS_DIR = os.path.join(BASE_DIR, "results", "data")
PLOTS_DIR = os.path.join(BASE_DIR, "results", "plots")
os.makedirs(RESULTS_DIR, exist_ok=True)
os.makedirs(PLOTS_DIR, exist_ok=True)

SAMPLES_PER_CATEGORY = 80  # Balance cost vs. statistical power
MAX_TEXT_LENGTH = 1500  # Truncate very long texts

DETECTION_PROMPT = """You are an AI text detection system. Your task is to determine whether the following text was written by a human or generated by an AI language model.

Analyze the text carefully for:
- Writing style, vocabulary diversity, and naturalness
- Sentence structure and variation
- Logical coherence and depth
- Presence of formulaic patterns typical of AI

TEXT TO ANALYZE:
---
{text}
---

Respond with ONLY a JSON object in this exact format (no other text):
{{"classification": "human" or "ai", "confidence": 0.0 to 1.0}}"""


def load_raid_data():
    """Load RAID samples organized by category."""
    data_path = os.path.join(BASE_DIR, "datasets", "raid", "raid_clean_samples.json")
    with open(data_path) as f:
        samples = json.load(f)

    by_model = {}
    for s in samples:
        model = s['model']
        if model not in by_model:
            by_model[model] = []
        by_model[model].append(s)

    categories = {
        'human': by_model.get('human', []),
        'base_mistral': by_model.get('mistral', []),
        'aligned_mistral': by_model.get('mistral-chat', []),
        'base_mpt': by_model.get('mpt', []),
        'aligned_mpt': by_model.get('mpt-chat', []),
        'base_cohere': by_model.get('cohere', []),
        'aligned_cohere': by_model.get('cohere-chat', []),
    }
    return categories


def sample_texts(categories, n_per_cat=SAMPLES_PER_CATEGORY):
    """Sample n texts from each category for API evaluation."""
    rng = np.random.RandomState(SEED)
    sampled = {}
    for cat, texts in categories.items():
        indices = rng.choice(len(texts), min(n_per_cat, len(texts)), replace=False)
        sampled[cat] = [texts[i] for i in indices]
    return sampled


def classify_text(client, text, model="gpt-4.1"):
    """Use GPT-4.1 to classify a text as human or AI."""
    truncated = text[:MAX_TEXT_LENGTH]
    prompt = DETECTION_PROMPT.format(text=truncated)

    try:
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=100,
        )
        result_text = response.choices[0].message.content.strip()

        # Parse JSON response
        # Handle possible markdown wrapping
        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]
            result_text = result_text.strip()

        result = json.loads(result_text)
        classification = result.get('classification', '').lower()
        confidence = float(result.get('confidence', 0.5))

        return {
            'classification': classification,
            'confidence': confidence,
            'ai_score': confidence if classification == 'ai' else 1 - confidence,
            'raw_response': result_text,
        }
    except Exception as e:
        print(f"  API error: {e}")
        return {
            'classification': 'error',
            'confidence': 0.5,
            'ai_score': 0.5,
            'raw_response': str(e),
        }


def run_experiment():
    print("=" * 70)
    print("EXPERIMENT 3: LLM-as-Detector (GPT-4.1)")
    print("=" * 70)

    client = OpenAI()

    categories = load_raid_data()
    sampled = sample_texts(categories)

    total_texts = sum(len(v) for v in sampled.values())
    print(f"Total texts to classify: {total_texts}")
    print(f"Estimated cost: ~${total_texts * 0.003:.2f} (GPT-4.1)")

    # Check for cached results
    cache_path = os.path.join(RESULTS_DIR, "exp3_raw_classifications.json")
    if os.path.exists(cache_path):
        print(f"Loading cached results from {cache_path}")
        with open(cache_path) as f:
            all_results = json.load(f)
        print(f"Loaded {len(all_results)} cached classifications")
    else:
        all_results = []

    # Build set of already-classified texts
    classified_ids = set()
    for r in all_results:
        classified_ids.add(r.get('text_id', ''))

    # Classify each text
    new_count = 0
    for cat_name, texts in sampled.items():
        print(f"\nClassifying {cat_name} ({len(texts)} texts)...")
        for i, t in enumerate(texts):
            text_id = f"{cat_name}_{t['id']}"
            if text_id in classified_ids:
                continue

            result = classify_text(client, t['generation'])
            result['category'] = cat_name
            result['text_id'] = text_id
            result['true_label'] = 'human' if cat_name == 'human' else 'ai'

            def alignment_type(cat):
                if cat == 'human':
                    return 'human'
                elif cat.startswith('base_'):
                    return 'base'
                elif cat.startswith('aligned_'):
                    return 'aligned'
                return 'other'

            result['alignment'] = alignment_type(cat_name)
            all_results.append(result)
            new_count += 1

            if new_count % 20 == 0:
                # Save intermediate results
                with open(cache_path, 'w') as f:
                    json.dump(all_results, f, indent=2)
                print(f"  Progress: {new_count} new classifications saved")

            # Rate limiting
            time.sleep(0.1)

    # Save final results
    with open(cache_path, 'w') as f:
        json.dump(all_results, f, indent=2)
    print(f"\nTotal classifications: {len(all_results)} ({new_count} new)")

    # Convert to DataFrame
    df = pd.DataFrame(all_results)
    df = df[df['classification'] != 'error']  # Remove errors

    # ---- ANALYSIS ----
    print("\n" + "=" * 70)
    print("DETECTION ACCURACY BY CATEGORY")
    print("=" * 70)

    accuracy_results = []
    for cat in sorted(df['category'].unique()):
        cat_df = df[df['category'] == cat]
        true_ai = cat != 'human'
        predicted_ai = cat_df['classification'] == 'ai'

        if true_ai:
            accuracy = predicted_ai.mean()  # TPR for AI text
        else:
            accuracy = (~predicted_ai).mean()  # TNR for human text

        mean_confidence = cat_df['confidence'].mean()
        mean_ai_score = cat_df['ai_score'].mean()

        accuracy_results.append({
            'category': cat,
            'alignment': cat_df['alignment'].iloc[0],
            'n_samples': len(cat_df),
            'correct_rate': accuracy,
            'mean_confidence': mean_confidence,
            'mean_ai_score': mean_ai_score,
        })
        print(f"  {cat:20s} | Correct: {accuracy:.3f} | AI Score: {mean_ai_score:.3f} | n={len(cat_df)}")

    acc_df = pd.DataFrame(accuracy_results)
    acc_df.to_csv(os.path.join(RESULTS_DIR, "exp3_accuracy_by_category.csv"), index=False)

    # ---- AUROC: Human vs each category ----
    print("\n" + "=" * 70)
    print("AUROC: Human vs. AI Categories")
    print("=" * 70)

    human_df = df[df['category'] == 'human']
    auroc_results = []

    for cat in sorted(df['category'].unique()):
        if cat == 'human':
            continue
        cat_df = df[df['category'] == cat]

        y_true = np.array([1] * len(cat_df) + [0] * len(human_df))
        y_scores = np.concatenate([cat_df['ai_score'].values, human_df['ai_score'].values])

        if len(np.unique(y_true)) < 2:
            continue

        auroc = roc_auc_score(y_true, y_scores)

        auroc_results.append({
            'category': cat,
            'alignment': cat_df['alignment'].iloc[0],
            'auroc': auroc,
            'n_ai': len(cat_df),
            'n_human': len(human_df),
        })
        print(f"  {cat:20s} | AUROC={auroc:.4f}")

    auroc_df = pd.DataFrame(auroc_results)
    auroc_df.to_csv(os.path.join(RESULTS_DIR, "exp3_auroc_results.csv"), index=False)

    # ---- COMPARISON: Base vs Aligned ----
    print("\n" + "=" * 70)
    print("BASE vs ALIGNED: Detection by GPT-4.1")
    print("=" * 70)

    families = ['mistral', 'mpt', 'cohere']
    comparison_results = []

    for family in families:
        base_df = df[df['category'] == f'base_{family}']
        aligned_df = df[df['category'] == f'aligned_{family}']

        base_tpr = (base_df['classification'] == 'ai').mean()
        aligned_tpr = (aligned_df['classification'] == 'ai').mean()

        base_ai_score = base_df['ai_score'].mean()
        aligned_ai_score = aligned_df['ai_score'].mean()

        # Statistical test: proportions test
        n_base = len(base_df)
        n_aligned = len(aligned_df)
        base_detected = (base_df['classification'] == 'ai').sum()
        aligned_detected = (aligned_df['classification'] == 'ai').sum()

        # Two-proportion z-test
        p_pool = (base_detected + aligned_detected) / (n_base + n_aligned)
        if p_pool > 0 and p_pool < 1:
            se = np.sqrt(p_pool * (1 - p_pool) * (1/n_base + 1/n_aligned))
            z_stat = (aligned_tpr - base_tpr) / se if se > 0 else 0
            p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))
        else:
            z_stat = 0
            p_value = 1.0

        comparison_results.append({
            'family': family,
            'base_tpr': base_tpr,
            'aligned_tpr': aligned_tpr,
            'delta_tpr': aligned_tpr - base_tpr,
            'base_mean_ai_score': base_ai_score,
            'aligned_mean_ai_score': aligned_ai_score,
            'z_stat': z_stat,
            'p_value': p_value,
        })

        sig = "***" if p_value < 0.001 else "**" if p_value < 0.01 else "*" if p_value < 0.05 else "ns"
        print(f"  {family:10s} | Base TPR={base_tpr:.3f}, Aligned TPR={aligned_tpr:.3f}, "
              f"Delta={aligned_tpr - base_tpr:+.3f} (p={p_value:.4f} {sig})")

    comp_df = pd.DataFrame(comparison_results)
    comp_df.to_csv(os.path.join(RESULTS_DIR, "exp3_base_vs_aligned.csv"), index=False)

    # ---- VISUALIZATIONS ----
    print("\nGenerating plots...")

    # Plot 1: Detection accuracy by category
    fig, ax = plt.subplots(figsize=(12, 6))
    cats = acc_df.sort_values('correct_rate')
    colors = {'human': '#2ecc71', 'base': '#3498db', 'aligned': '#e74c3c'}
    bar_colors = [colors[a] for a in cats['alignment']]
    ax.barh(range(len(cats)), cats['correct_rate'], color=bar_colors, alpha=0.85)
    ax.set_yticks(range(len(cats)))
    ax.set_yticklabels(cats['category'])
    ax.set_xlabel('Classification Accuracy')
    ax.set_title('GPT-4.1 Detection Accuracy by Source Category', fontsize=14, fontweight='bold')
    ax.axvline(x=0.5, color='gray', linestyle='--', alpha=0.5, label='Random')

    # Add custom legend
    from matplotlib.patches import Patch
    legend_elements = [Patch(facecolor=colors['human'], label='Human'),
                       Patch(facecolor=colors['base'], label='Base'),
                       Patch(facecolor=colors['aligned'], label='Aligned')]
    ax.legend(handles=legend_elements)

    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, "exp3_accuracy_by_category.png"), dpi=150, bbox_inches='tight')
    plt.close()

    # Plot 2: AI score distribution
    fig, ax = plt.subplots(figsize=(10, 6))
    for atype, color in [('human', '#2ecc71'), ('base', '#3498db'), ('aligned', '#e74c3c')]:
        vals = df[df['alignment'] == atype]['ai_score'].values
        ax.hist(vals, bins=20, alpha=0.5, color=color, label=atype, density=True)
    ax.set_xlabel('AI Score (from GPT-4.1)')
    ax.set_ylabel('Density')
    ax.set_title('AI Score Distributions: Human vs Base vs Aligned', fontsize=14, fontweight='bold')
    ax.legend()
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, "exp3_ai_score_distributions.png"), dpi=150, bbox_inches='tight')
    plt.close()

    # Plot 3: Base vs Aligned comparison per family
    fig, ax = plt.subplots(figsize=(10, 6))
    x = np.arange(len(families))
    width = 0.35
    base_vals = comp_df['base_tpr'].values
    aligned_vals = comp_df['aligned_tpr'].values

    ax.bar(x - width/2, base_vals, width, label='Base', color='#3498db', alpha=0.85)
    ax.bar(x + width/2, aligned_vals, width, label='Aligned', color='#e74c3c', alpha=0.85)

    # Add significance markers
    for i, row in comp_df.iterrows():
        if row['p_value'] < 0.05:
            y = max(row['base_tpr'], row['aligned_tpr']) + 0.03
            sig = "***" if row['p_value'] < 0.001 else "**" if row['p_value'] < 0.01 else "*"
            ax.text(i, y, sig, ha='center', fontsize=14, fontweight='bold')

    ax.set_xticks(x)
    ax.set_xticklabels([f.capitalize() for f in families])
    ax.set_ylabel('True Positive Rate (AI detected as AI)')
    ax.set_title('GPT-4.1 Detection: Base vs Aligned Models', fontsize=14, fontweight='bold')
    ax.set_ylim(0, 1.15)
    ax.legend()
    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3)
    plt.tight_layout()
    plt.savefig(os.path.join(PLOTS_DIR, "exp3_base_vs_aligned.png"), dpi=150, bbox_inches='tight')
    plt.close()

    print(f"\nResults saved to {RESULTS_DIR}")
    print(f"Plots saved to {PLOTS_DIR}")
    print("Experiment 3 complete.")

    return df, acc_df, auroc_df, comp_df


if __name__ == "__main__":
    run_experiment()
