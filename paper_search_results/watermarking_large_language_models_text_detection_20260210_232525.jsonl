{"title": "A Watermark for Large Language Models", "year": 2023, "authors": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, T. Goldstein", "url": "https://www.semanticscholar.org/paper/cb5b71a622aff47014d4f28a958679629a8b6363", "relevance": 3, "abstract": "Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of\"green\"tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.", "citations": 754}
{"title": "Scalable watermarking for identifying large language model outputs", "year": 2024, "authors": "Sumanth Dathathri, Abigail See, Sumedh Ghaisas, Po-Sen Huang, Rob McAdam, Johannes Welbl, Vandana Bachani, Alex Kaskasoli, Robert Stanforth, Tatiana Matejovicova, Jamie Hayes, Nidhi Vyas, Majd Al Merey, Jonah Brown-Cohen, Rudy Bunel, Borja Balle, taylan. cemgil, Zahra Ahmed, Kitty Stacpoole, Ilia Shumailov, Cip Baetu, Sven Gowal, D. Hassabis, Pushmeet Kohli", "url": "https://www.semanticscholar.org/paper/89bd8efe0b9c0427cb7814d7b8c2b0190d2ffa9e", "relevance": 3, "abstract": "Large language models (LLMs) have enabled the generation of high-quality synthetic text, often indistinguishable from human-written content, at a scale that can markedly affect the nature of the information ecosystem1\u20133. Watermarking can help identify synthetic text and limit accidental or deliberate misuse4, but has not been adopted in production systems owing to stringent quality, detectability and computational efficiency requirements. Here we describe SynthID-Text, a production-ready text watermarking scheme that preserves text quality and enables high detection accuracy, with minimal latency overhead. SynthID-Text does not affect LLM training and modifies only the sampling procedure; watermark detection is computationally efficient, without using the underlying LLM. To enable watermarking at scale, we develop an algorithm integrating watermarking with speculative sampling, an efficiency technique frequently used in production systems5. Evaluations across multiple LLMs empirically show that SynthID-Text provides improved detectability over comparable methods, and standard benchmarks and human side-by-side ratings indicate no change in LLM capabilities. To demonstrate the feasibility of watermarking in large-scale-production systems, we conducted a live experiment that assessed feedback from nearly 20\u2009million Gemini6 responses, again confirming the preservation of text quality. We hope that the availability of SynthID-Text7 will facilitate further development of watermarking and responsible use of LLM systems. A scheme for watermarking the text generated by large language models shows high text quality preservation and detection accuracy and low latency, and is feasible in large-scale-production settings.", "citations": 181}
{"title": "On the Reliability of Watermarks for Large Language Models", "year": 2023, "authors": "John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, T. Goldstein", "url": "https://www.semanticscholar.org/paper/00c85c78a67dceb33621e36adef08b5d05c5251d", "relevance": 3, "abstract": "As LLMs become commonplace, machine-generated text has the potential to flood the internet with spam, social media bots, and valueless content. Watermarking is a simple and effective strategy for mitigating such harms by enabling the detection and documentation of LLM-generated text. Yet a crucial question remains: How reliable is watermarking in realistic settings in the wild? There, watermarked text may be modified to suit a user's needs, or entirely rewritten to avoid detection. We study the robustness of watermarked text after it is re-written by humans, paraphrased by a non-watermarked LLM, or mixed into a longer hand-written document. We find that watermarks remain detectable even after human and machine paraphrasing. While these attacks dilute the strength of the watermark, paraphrases are statistically likely to leak n-grams or even longer fragments of the original text, resulting in high-confidence detections when enough tokens are observed. For example, after strong human paraphrasing the watermark is detectable after observing 800 tokens on average, when setting a 1e-5 false positive rate. We also consider a range of new detection schemes that are sensitive to short spans of watermarked text embedded inside a large document, and we compare the robustness of watermarking to other kinds of detectors.", "citations": 185}
{"title": "Three Bricks to Consolidate Watermarks for Large Language Models", "year": 2023, "authors": "Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, T. Furon", "url": "https://www.semanticscholar.org/paper/98032f95e274db30570727fb7196c15e325fb35a", "relevance": 3, "abstract": "Discerning between generated and natural texts is increasingly challenging. In this context, watermarking emerges as a promising technique for ascribing text to a specific generative model. It alters the sampling generation process to leave an invisible trace in the output, facilitating later detection. This research consolidates watermarks for large language models based on three theoretical and empirical considerations. First, we introduce new statistical tests that offer robust theoretical guarantees which remain valid even at low false-positive rates (less than 10\u22126). Second, we compare the effectiveness of watermarks using classical benchmarks in the field of natural language processing, gaining insights into their real-world applicability. Third, we develop advanced detection schemes for scenarios where access to the LLM is available, as well as multi-bit watermarking.", "citations": 79}
{"title": "REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models", "year": 2023, "authors": "Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, F. Koushanfar", "url": "https://www.semanticscholar.org/paper/8ff1dd6e15408581592f91434be870acc5f1bdd4", "relevance": 3, "abstract": "We present REMARK-LLM, a novel efficient, and robust watermarking framework designed for texts generated by large language models (LLMs). Synthesizing human-like content using LLMs necessitates vast computational resources and extensive datasets, encapsulating critical intellectual property (IP). However, the generated content is prone to malicious exploitation, including spamming and plagiarism. To address the challenges, REMARK-LLM proposes three new components: (i) a learning-based message encoding module to infuse binary signatures into LLM-generated texts; (ii) a reparameterization module to transform the dense distributions from the message encoding to the sparse distribution of the watermarked textual tokens; (iii) a decoding module dedicated for signature extraction; Furthermore, we introduce an optimized beam search algorithm to guarantee the coherence and consistency of the generated content. REMARK-LLM is rigorously trained to encourage the preservation of semantic integrity in watermarked content, while ensuring effective watermark retrieval. Extensive evaluations on multiple unseen datasets highlight REMARK-LLM proficiency and transferability in inserting 2 times more signature bits into the same texts when compared to prior art, all while maintaining semantic integrity. Furthermore, REMARK-LLM exhibits better resilience against a spectrum of watermark detection and removal attacks.", "citations": 76}
{"title": "Publicly Detectable Watermarking for Language Models", "year": 2023, "authors": "Jaiden Fairoze, Sanjam Garg, Somesh Jha, Saeed Mahloujifar, Mohammad Mahmoody, Mingyuan Wang", "url": "https://www.semanticscholar.org/paper/702e727976e4f08c1ddc3a08cf76cd689dab2f63", "relevance": 3, "abstract": "We present a publicly-detectable watermarking scheme for LMs: the detection algorithm contains no secret information, and it is executable by anyone. We embed a publicly-verifiable cryptographic signature into LM output using rejection sampling and prove that this produces unforgeable and distortion-free (i.e., undetectable without access to the public key) text output. We make use of error-correction to overcome periods of low entropy, a barrier for all prior watermarking schemes. We implement our scheme and find that our formal claims are met in practice.", "citations": 74}
{"title": "An Unforgeable Publicly Verifiable Watermark for Large Language Models", "year": 2023, "authors": "Aiwei Liu, Leyi Pan, Xuming Hu, Shuang Li, Lijie Wen, Irwin King, Philip S. Yu", "url": "https://www.semanticscholar.org/paper/0d24a88f29dd87efb90dc8b33c1f41c25f63e574", "relevance": 3, "abstract": "Recently, text watermarking algorithms for large language models (LLMs) have been proposed to mitigate the potential harms of text generated by LLMs, including fake news and copyright issues. However, current watermark detection algorithms require the secret key used in the watermark generation process, making them susceptible to security breaches and counterfeiting during public detection. To address this limitation, we propose an unforgeable publicly verifiable watermark algorithm named UPV that uses two different neural networks for watermark generation and detection, instead of using the same key at both stages. Meanwhile, the token embedding parameters are shared between the generation and detection networks, which makes the detection network achieve a high accuracy very efficiently. Experiments demonstrate that our algorithm attains high detection accuracy and computational efficiency through neural networks. Subsequent analysis confirms the high complexity involved in forging the watermark from the detection network. Our code is available at \\href{https://github.com/THU-BPM/unforgeable_watermark}{https://github.com/THU-BPM/unforgeable\\_watermark}. Additionally, our algorithm could also be accessed through MarkLLM \\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.", "citations": 57}
{"title": "WaterMax: breaking the LLM watermark detectability-robustness-quality trade-off", "year": 2024, "authors": "Eva Giboulot, Teddy Furon", "url": "https://www.semanticscholar.org/paper/c199b7ee0609a560a7607ea84add32b6f819ecf7", "relevance": 3, "abstract": "Watermarking is a technical means to dissuade malfeasant usage of Large Language Models. This paper proposes a novel watermarking scheme, so-called WaterMax, that enjoys high detectability while sustaining the quality of the generated text of the original LLM. Its new design leaves the LLM untouched (no modification of the weights, logits, temperature, or sampling technique). WaterMax balances robustness and complexity contrary to the watermarking techniques of the literature inherently provoking a trade-off between quality and robustness. Its performance is both theoretically proven and experimentally validated. It outperforms all the SotA techniques under the most complete benchmark suite. Code available at https://github.com/eva-giboulot/WaterMax.", "citations": 49}
{"title": "Towards Codable Watermarking for Injecting Multi-Bits Information to LLMs", "year": 2023, "authors": "Lean Wang, Wenkai Yang, Deli Chen, Hao Zhou, Yankai Lin, Fandong Meng, Jie Zhou, Xu Sun", "url": "https://www.semanticscholar.org/paper/aeaf588d79671267cb19629ec4966744b6eccffa", "relevance": 3, "abstract": "As large language models (LLMs) generate texts with increasing fluency and realism, there is a growing need to identify the source of texts to prevent the abuse of LLMs. Text watermarking techniques have proven reliable in distinguishing whether a text is generated by LLMs by injecting hidden patterns. However, we argue that existing LLM watermarking methods are encoding-inefficient and cannot flexibly meet the diverse information encoding needs (such as encoding model version, generation time, user id, etc.). In this work, we conduct the first systematic study on the topic of Codable Text Watermarking for LLMs (CTWL) that allows text watermarks to carry multi-bit customizable information. First of all, we study the taxonomy of LLM watermarking technologies and give a mathematical formulation for CTWL. Additionally, we provide a comprehensive evaluation system for CTWL: (1) watermarking success rate, (2) robustness against various corruptions, (3) coding rate of payload information, (4) encoding and decoding efficiency, (5) impacts on the quality of the generated text. To meet the requirements of these non-Pareto-improving metrics, we follow the most prominent vocabulary partition-based watermarking direction, and devise an advanced CTWL method named Balance-Marking. The core idea of our method is to use a proxy language model to split the vocabulary into probability-balanced parts, thereby effectively maintaining the quality of the watermarked text. Our code is available at https://github.com/lancopku/codable-watermarking-for-llm.", "citations": 47}
{"title": "A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules", "year": 2024, "authors": "Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su", "url": "https://www.semanticscholar.org/paper/23032d660c0cc05d3822928ea67430e58923088a", "relevance": 3, "abstract": "Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key-provided by the LLM to the verifier-to control the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of incorrectly classifying LLM-generated text as human-written). Our framework further reduces the problem of determining the optimal detection rule to solving a minimax optimization program. We apply this framework to two representative watermarks-one of which has been internally implemented at OpenAI-and obtain several findings that can be instrumental in guiding the practice of implementing watermarks. In particular, we derive optimal detection rules for these watermarks under our framework. These theoretically derived detection rules are demonstrated to be competitive and sometimes enjoy a higher power than existing detection approaches through numerical experiments.", "citations": 40}
{"title": "Advancing Beyond Identification: Multi-bit Watermark for Large Language Models", "year": 2023, "authors": "Kiyoon Yoo, Wonhyuk Ahn, N. Kwak", "url": "https://www.semanticscholar.org/paper/4053dc56d31a5f0082f93e030b0dba7efc2cba01", "relevance": 3, "abstract": "We show the viability of tackling misuses of large language models beyond the identification of machine-generated text. While existing zero-bit watermark methods focus on detection only, some malicious misuses demand tracing the adversary user for counteracting them. To address this, we propose Multi-bit Watermark via Position Allocation, embedding traceable multi-bit information during language model generation. Through allocating tokens onto different parts of the messages, we embed longer messages in high corruption settings without added latency. By independently embedding sub-units of messages, the proposed method outperforms the existing works in terms of robustness and latency. Leveraging the benefits of zero-bit watermarking, our method enables robust extraction of the watermark without any model access, embedding and extraction of long messages (\\geq 32-bit) without finetuning, and maintaining text quality, while allowing zero-bit detection all at the same time.", "citations": 39}
{"title": "DeepTextMark: Deep Learning based Text Watermarking for Detection of Large Language Model Generated Text", "year": 2023, "authors": "Travis J. E. Munyer, Xin Zhong", "url": "https://www.semanticscholar.org/paper/3f358e5bedaae0eb49849dce98edb516f0731df5", "relevance": 3, "abstract": "", "citations": 38}
{"title": "DeepTextMark: A Deep Learning-Driven Text Watermarking Approach for Identifying Large Language Model Generated Text", "year": 2023, "authors": "Travis J. E. Munyer, A. Tanvir, A. Das, Xin Zhong", "url": "https://www.semanticscholar.org/paper/ec854a573cc540d49bd4fddda9af991cdbb10d92", "relevance": 3, "abstract": "The rapid advancement of Large Language Models (LLMs) has significantly enhanced the capabilities of text generators. With the potential for misuse escalating, the importance of discerning whether texts are human-authored or generated by LLMs has become paramount. Several preceding studies have ventured to address this challenge by employing binary classifiers to differentiate between human-written and LLM-generated text. Nevertheless, the reliability of these classifiers has been subject to question. Given that consequential decisions may hinge on the outcome of such classification, it is imperative that text source detection is of high caliber. In light of this, the present paper introduces DeepTextMark, a deep learning-driven text watermarking methodology devised for text source identification. By leveraging Word2Vec and Sentence Encoding for watermark insertion, alongside a transformer-based classifier for watermark detection, DeepTextMark epitomizes a blend of blindness, robustness, imperceptibility, and reliability. As elaborated within the paper, these attributes are crucial for universal text source detection, with a particular emphasis in this paper on text produced by LLMs. DeepTextMark offers a viable \u201cadd-on\u201d solution to prevailing text generation frameworks, requiring no direct access or alterations to the underlying text generation mechanism. Experimental evaluations underscore the high imperceptibility, elevated detection accuracy, augmented robustness, reliability, and swift execution of DeepTextMark.", "citations": 37}
{"title": "Token-Specific Watermarking with Enhanced Detectability and Semantic Coherence for Large Language Models", "year": 2024, "authors": "Mingjia Huo, Sai Ashish Somayajula, Youwei Liang, Ruisi Zhang, F. Koushanfar, Pengtao Xie", "url": "https://www.semanticscholar.org/paper/88ee6398a970acbb1cab592cbd07c944d8b59b4e", "relevance": 3, "abstract": "Large language models generate high-quality responses with potential misinformation, underscoring the need for regulation by distinguishing AI-generated and human-written texts. Watermarking is pivotal in this context, which involves embedding hidden markers in texts during the LLM inference phase, which is imperceptible to humans. Achieving both the detectability of inserted watermarks and the semantic quality of generated texts is challenging. While current watermarking algorithms have made promising progress in this direction, there remains significant scope for improvement. To address these challenges, we introduce a novel multi-objective optimization (MOO) approach for watermarking that utilizes lightweight networks to generate token-specific watermarking logits and splitting ratios. By leveraging MOO to optimize for both detection and semantic objective functions, our method simultaneously achieves detectability and semantic integrity. Experimental results show that our method outperforms current watermarking techniques in enhancing the detectability of texts generated by LLMs while maintaining their semantic coherence. Our code is available at https://github.com/mignonjia/TS_watermark.", "citations": 30}
{"title": "A Private Watermark for Large Language Models", "year": 2023, "authors": "Aiwei Liu, Leyi Pan, Xuming Hu, Shuang Li, Lijie Wen, Irwin King, Philip S. Yu", "url": "https://www.semanticscholar.org/paper/d994b38331d42fdaaab87c1f7578f05e99dd4204", "relevance": 3, "abstract": "", "citations": 29}
{"title": "Provably Robust Multi-bit Watermarking for AI-generated Text", "year": 2024, "authors": "Wenjie Qu, Wengrui Zheng, Tianyang Tao, Dong Yin, Yanze Jiang, Zhihua Tian, Wei Zou, Jinyuan Jia, Jiaheng Zhang", "url": "https://www.semanticscholar.org/paper/0221d531d38e4af2a842d51501908dff00437d46", "relevance": 3, "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities of generating texts resembling human language. However, they can be misused by criminals to create deceptive content, such as fake news and phishing emails, which raises ethical concerns. Watermarking is a key technique to address these concerns, which embeds a message (e.g., a bit string) into a text generated by an LLM. By embedding the user ID (represented as a bit string) into generated texts, we can trace generated texts to the user, known as content source tracing. The major limitation of existing watermarking techniques is that they achieve sub-optimal performance for content source tracing in real-world scenarios. The reason is that they cannot accurately or efficiently extract a long message from a generated text. We aim to address the limitations. In this work, we introduce a new watermarking method for LLM-generated text grounded in pseudo-random segment assignment. We also propose multiple techniques to further enhance the robustness of our watermarking algorithm. We conduct extensive experiments to evaluate our method. Our experimental results show that our method substantially outperforms existing baselines in both accuracy and robustness on benchmark datasets. For instance, when embedding a message of length 20 into a 200-token generated text, our method achieves a match rate of $97.6\\%$, while the state-of-the-art work Yoo et al. only achieves $49.2\\%$. Additionally, we prove that our watermark can tolerate edits within an edit distance of 17 on average for each paragraph under the same setting.", "citations": 22}
{"title": "Bileve: Securing Text Provenance in Large Language Models Against Spoofing with Bi-level Signature", "year": 2024, "authors": "Tong Zhou, Xuandong Zhao, Xiaolin Xu, Shaolei Ren", "url": "https://www.semanticscholar.org/paper/c3ea3a83a667f417d029fefc5bedc23be443f16d", "relevance": 3, "abstract": "Text watermarks for large language models (LLMs) have been commonly used to identify the origins of machine-generated content, which is promising for assessing liability when combating deepfake or harmful content. While existing watermarking techniques typically prioritize robustness against removal attacks, unfortunately, they are vulnerable to spoofing attacks: malicious actors can subtly alter the meanings of LLM-generated responses or even forge harmful content, potentially misattributing blame to the LLM developer. To overcome this, we introduce a bi-level signature scheme, Bileve, which embeds fine-grained signature bits for integrity checks (mitigating spoofing attacks) as well as a coarse-grained signal to trace text sources when the signature is invalid (enhancing detectability) via a novel rank-based sampling strategy. Compared to conventional watermark detectors that only output binary results, Bileve can differentiate 5 scenarios during detection, reliably tracing text provenance and regulating LLMs. The experiments conducted on OPT-1.3B and LLaMA-7B demonstrate the effectiveness of Bileve in defeating spoofing attacks with enhanced detectability. Code is available at https://github.com/Tongzhou0101/Bileve-official.", "citations": 19}
{"title": "Advancing Beyond Identification: Multi-bit Watermark for Language Models", "year": 2023, "authors": "Kiyoon Yoo, Wonhyuk Ahn, N. Kwak", "url": "https://www.semanticscholar.org/paper/cf046cee879e39c34dcb8057b9de06210e592536", "relevance": 3, "abstract": "", "citations": 33}
{"title": "Debiasing Watermarks for Large Language Models via Maximal Coupling", "year": 2024, "authors": "Yangxinyu Xie, Xiang Li, Tanwi Mallick, Weijie J. Su, Ruixun Zhang", "url": "https://www.semanticscholar.org/paper/5a3a043d4b673367b924576ecebfc56082ffd532", "relevance": 3, "abstract": "Abstract Watermarking language models is essential for distinguishing between human and machine-generated text and thus maintaining the integrity and trustworthiness of digital communication. We present a novel green/red list watermarking approach that partitions the token set into \u201cgreen\u201d and \u201cred\u201d lists, subtly increasing the generation probability for green tokens. To correct token distribution bias, our method employs maximal coupling, using a uniform coin flip to decide whether to apply bias correction, with the result embedded as a pseudorandom watermark signal. Theoretical analysis confirms this approach\u2019s unbiased nature and robust detection capabilities. Experimental results show that it outperforms prior techniques by preserving text quality while maintaining high detectability, and it demonstrates resilience to targeted modifications aimed at improving text quality. This research provides a promising watermarking solution for language models, balancing effective detection with minimal impact on text quality. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.", "citations": 13}
{"title": "Duwak: Dual Watermarks in Large Language Models", "year": 2024, "authors": "Chaoyi Zhu, Jeroen M. Galjaard, Pin-Yu Chen, Lydia Y. Chen", "url": "https://www.semanticscholar.org/paper/833299cbaa0793527951b2b78e1438daf6fe8cb6", "relevance": 3, "abstract": "As large language models (LLM) are increasingly used for text generation tasks, it is critical to audit their usages, govern their applications, and mitigate their potential harms. Existing watermark techniques are shown effective in embedding single human-imperceptible and machine-detectable patterns without significantly affecting generated text quality and semantics. However, the efficiency in detecting watermarks, i.e., the minimum number of tokens required to assert detection with significance and robustness against post-editing, is still debatable. In this paper, we propose, Duwak, to fundamentally enhance the efficiency and quality of watermarking by embedding dual secret patterns in both token probability distribution and sampling schemes. To mitigate expression degradation caused by biasing toward certain tokens, we design a contrastive search to watermark the sampling scheme, which minimizes the token repetition and enhances the diversity. We theoretically explain the interdependency of the two watermarks within Duwak. We evaluate Duwak extensively on Llama2 under various post-editing attacks, against four state-of-the-art watermarking techniques and combinations of them. Our results show that Duwak marked text achieves the highest watermarked text quality at the lowest required token count for detection, up to 70% tokens less than existing approaches, especially under post paraphrasing.", "citations": 12}
{"title": "Theoretically Grounded Framework for LLM Watermarking: A Distribution-Adaptive Approach", "year": 2024, "authors": "Haiyun He, Yepeng Liu, Ziqiao Wang, Yongyi Mao, Yuheng Bu", "url": "https://www.semanticscholar.org/paper/dfaf500928b40f705572bc4344e39d366a986902", "relevance": 3, "abstract": "Watermarking has emerged as a crucial method to distinguish AI-generated text from human-created text. Current watermarking approaches often lack formal optimality guarantees or address the scheme and detector design separately. In this paper, we introduce a novel, unified theoretical framework for watermarking Large Language Models (LLMs) that jointly optimizes both the watermarking scheme and detector. Our approach aims to maximize detection performance while maintaining control over the worst-case false positive rate (FPR) and distortion on text quality. We derive closed-form optimal solutions for this joint design and characterize the fundamental trade-off between watermark detectability and distortion. Notably, we reveal that the optimal watermarking schemes should be adaptive to the LLM's generative distribution. Building on our theoretical insights, we propose a distortion-free, distribution-adaptive watermarking algorithm (DAWA) that leverages a surrogate model for model-agnosticism and efficiency. Experiments on Llama2-13B and Mistral-8$\\times$7B models confirm the effectiveness of our approach, particularly at ultra-low FPRs. Our code is available at https://github.com/yepengliu/DAWA.", "citations": 12}
{"title": "A Watermark for Black-Box Language Models", "year": 2024, "authors": "Dara Bahri, J. Michael Wieting, Dana Alon, Donald Metzler", "url": "https://www.semanticscholar.org/paper/976aa5e91b25d345d747952e17a67f53a04deb18", "relevance": 3, "abstract": "Watermarking has recently emerged as an effective strategy for detecting the outputs of large language models (LLMs). Most existing schemes require white-box access to the model's next-token probability distribution, which is typically not accessible to downstream users of an LLM API. In this work, we propose a principled watermarking scheme that requires only the ability to sample sequences from the LLM (i.e. black-box access), boasts a distortion-free property, and can be chained or nested using multiple secret keys. We provide performance guarantees, demonstrate how it can be leveraged when white-box access is available, and show when it can outperform existing white-box schemes via comprehensive experiments.", "citations": 11}
{"title": "Watermarking Language Models for Many Adaptive Users", "year": 2024, "authors": "Aloni Cohen, Alexander Hoover, Gabe Schoenbach", "url": "https://www.semanticscholar.org/paper/a5eaf891edd07c3a3faa7e56dd19be40a0cc0261", "relevance": 3, "abstract": "We study watermarking schemes for language models with provable guarantees. As we show, prior works offer no robustness guarantees against adaptive prompting: when a user queries a language model more than once, as even benign users do. And with just a single exception [1], prior works are restricted to zero-bit watermarking: machine-generated text can be detected as such, but no additional information can be extracted from the watermark. Unfortunately, merely detecting AI-generated text may not prevent future abuses. We introduce multi-user watermarks, which allow tracing model-generated text to individual users or to groups of colluding users, even in the face of adaptive prompting. We construct multi-user watermarking schemes from undetectable, adaptively robust, zero-bit watermarking schemes (and prove that the undetectable zero-bit scheme of [2] is adaptively robust). Importantly, our scheme provides both zero-bit and multi-user assurances at the same time. It detects shorter snippets just as well as the original scheme, and traces longer excerpts to individuals. The main technical component is a construction of message-embedding watermarks from zero-bit watermarks. Ours is the first generic reduction between watermarking schemes for language models. A challenge for such reductions is the lack of a unified abstraction for robustness - that marked text is detectable even after edits. We introduce a new unifying abstraction called AEB-robustness. AEB-robustness provides that the watermark is detectable whenever the edited text \u201capproximates enough blocks\u201d of model-generated output.", "citations": 11}
{"title": "Multi-Bit Distortion-Free Watermarking for Large Language Models", "year": 2024, "authors": "Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, Brian L. Mark", "url": "https://www.semanticscholar.org/paper/a7059d44991cc98c75e86a915390dd3abd4fe5fb", "relevance": 3, "abstract": "Methods for watermarking large language models have been proposed that distinguish AI-generated text from human-generated text by slightly altering the model output distribution, but they also distort the quality of the text, exposing the watermark to adversarial detection. More recently, distortion-free watermarking methods were proposed that require a secret key to detect the watermark. The prior methods generally embed zero-bit watermarks that do not provide additional information beyond tagging a text as being AI-generated. We extend an existing zero-bit distortion-free watermarking method by embedding multiple bits of meta-information as part of the watermark. We also develop a computationally efficient decoder that extracts the embedded information from the watermark with low bit error rate.", "citations": 10}
{"title": "GaussMark: A Practical Approach for Structural Watermarking of Language Models", "year": 2025, "authors": "Adam Block, Ayush Sekhari, Alexander Rakhlin", "url": "https://www.semanticscholar.org/paper/3ac62ade187e9dda40efdccb9e76f2c2493ed868", "relevance": 3, "abstract": "Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks, but their ability to generate human-quality text raises significant ethical and operational concerns in settings where it is important to recognize whether or not a given text was generated by a human. Thus, recent work has focused on developing techniques for watermarking LLM-generated text, i.e., introducing an almost imperceptible signal that allows a provider equipped with a secret key to determine if given text was generated by their model. Current watermarking techniques are often not practical due to concerns with generation latency, detection time, degradation in text quality, or robustness. Many of these drawbacks come from the focus on token-level watermarking, which ignores the inherent structure of text. In this work, we introduce a new scheme, GaussMark, that is simple and efficient to implement, has formal statistical guarantees on its efficacy, comes at no cost in generation latency, and embeds the watermark into the weights of the model itself, providing a structural watermark. Our approach is based on Gaussian independence testing and is motivated by recent empirical observations that minor additive corruptions to LLM weights can result in models of identical (or even improved) quality. We show that by adding a small amount of Gaussian noise to the weights of a given LLM, we can watermark the model in a way that is statistically detectable by a provider who retains the secret key. We provide formal statistical bounds on the validity and power of our procedure. Through an extensive suite of experiments, we demonstrate that GaussMark is reliable, efficient, and relatively robust to corruptions such as insertions, deletions, substitutions, and roundtrip translations and can be instantiated with essentially no loss in model quality.", "citations": 8}
{"title": "In-Context Watermarks for Large Language Models", "year": 2025, "authors": "Yepeng Liu, Xuandong Zhao, Christopher Kruegel, D. Song, Yuheng Bu", "url": "https://www.semanticscholar.org/paper/5063f41336549b08240f14420b9000c2281aab19", "relevance": 3, "abstract": "The growing use of large language models (LLMs) for sensitive applications has highlighted the need for effective watermarking techniques to ensure the provenance and accountability of AI-generated text. However, most existing watermarking methods require access to the decoding process, limiting their applicability in real-world settings. One illustrative example is the use of LLMs by dishonest reviewers in the context of academic peer review, where conference organizers have no access to the model used but still need to detect AI-generated reviews. Motivated by this gap, we introduce In-Context Watermarking (ICW), which embeds watermarks into generated text solely through prompt engineering, leveraging LLMs' in-context learning and instruction-following abilities. We investigate four ICW strategies at different levels of granularity, each paired with a tailored detection method. We further examine the Indirect Prompt Injection (IPI) setting as a specific case study, in which watermarking is covertly triggered by modifying input documents such as academic manuscripts. Our experiments validate the feasibility of ICW as a model-agnostic, practical watermarking approach. Moreover, our findings suggest that as LLMs become more capable, ICW offers a promising direction for scalable and accessible content attribution.", "citations": 7}
{"title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models", "year": 2025, "authors": "Xiaoyan Feng, He Zhang, Yanjun Zhang, L. Zhang, Shirui Pan", "url": "https://www.semanticscholar.org/paper/f1aeb16a3d150133f269fd735f68798a9fe97efe", "relevance": 3, "abstract": "Recent advances in Large Language Models (LLMs) have raised urgent concerns about LLM-generated text authenticity, prompting regulatory demands for reliable identification mechanisms. Although watermarking offers a promising solution, existing approaches struggle to simultaneously achieve three critical requirements: text quality preservation, model-agnostic detection, and message embedding capacity, which are crucial for practical implementation. To achieve these goals, the key challenge lies in balancing the trade-off between text quality preservation and message embedding capacity. To address this challenge, we propose BiMark, a novel watermarking framework that achieves these requirements through three key innovations: (1) a bit-flip unbiased reweighting mechanism enabling model-agnostic detection, (2) a multilayer architecture enhancing detectability without compromising generation quality, and (3) an information encoding approach supporting multi-bit watermarking. Through theoretical analysis and extensive experiments, we validate that, compared to state-of-the-art multi-bit watermarking methods, BiMark achieves up to 30% higher extraction rates for short texts while maintaining text quality indicated by lower perplexity, and performs comparably to non-watermarked text on downstream tasks such as summarization and translation.", "citations": 5}
{"title": "Post-Hoc Watermarking for Robust Detection in Text Generated by Large Language Models", "year": 2025, "authors": "Jifei Hao, Jipeng Qiang, Yi Zhu, Yun Li, Yunhao Yuan, Xiaoye Ouyang", "url": "https://www.semanticscholar.org/paper/f2864081e3220eb932bfbbc4d69814599d7162be", "relevance": 3, "abstract": "", "citations": 5}
{"title": "Optimized Couplings for Watermarking Large Language Models", "year": 2025, "authors": "Dor Tsur, Carol Xuan Long, C. M. Verdun, Hsiang Hsu, H. Permuter, F. Calmon", "url": "https://www.semanticscholar.org/paper/8327d23037bd3d11c530229b59f4cf8f5e924b42", "relevance": 3, "abstract": "Large-language models (LLMs) are now able to produce text that is indistinguishable from human-generated content. This has fueled the development of watermarks that imprint a \u201csignal\u201d in LLM-generated text with minimal perturbation of an LLM's output. This paper provides an analysis of text watermarking in a one-shot setting. Through the lens of hypothesis testing with side information, we formulate and analyze the fundamental trade-off between watermark detection power and distortion in generated textual quality. We argue that a key component in watermark design is generating a coupling between the side information shared with the watermark detector and a random partition of the LLM vocabulary. Our analysis identifies the optimal coupling and randomization strategy under the worst-case LLM next-token distribution that satisfies a minentropy constraint. We provide a closed-form expression of the resulting detection rate under the proposed scheme and quantify the cost in a max-min sense. Finally, we numerically compare the proposed scheme with the theoretical optimum.", "citations": 5}
{"title": "StealthInk: A Multi-bit and Stealthy Watermark for Large Language Models", "year": 2025, "authors": "Ya Jiang, Chuxiong Wu, Massieh Kordi Boroujeny, Brian L. Mark, Kai Zeng", "url": "https://www.semanticscholar.org/paper/ace97b945a407479fd1925db5481ebe21bd1831d", "relevance": 3, "abstract": "Watermarking for large language models (LLMs) offers a promising approach to identifying AI-generated text. Existing approaches, however, either compromise the distribution of original generated text by LLMs or are limited to embedding zero-bit information that only allows for watermark detection but ignores identification. We present StealthInk, a stealthy multi-bit watermarking scheme that preserves the original text distribution while enabling the embedding of provenance data, such as userID, TimeStamp, and modelID, within LLM-generated text. This enhances fast traceability without requiring access to the language model's API or prompts. We derive a lower bound on the number of tokens necessary for watermark detection at a fixed equal error rate, which provides insights on how to enhance the capacity. Comprehensive empirical evaluations across diverse tasks highlight the stealthiness, detectability, and resilience of StealthInk, establishing it as an effective solution for LLM watermarking applications.", "citations": 4}
{"title": "Adaptive Robust Watermarking for Large Language Models via Dynamic Token Embedding Perturbation", "year": 2026, "authors": "Ziyang Zeng, Han Lin, Shuxin Zhang, Boyuan Wang", "url": "https://www.semanticscholar.org/paper/c20dc09bd5f22fa5776929ff3d3855aefaba4408", "relevance": 3, "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in generating high-quality text, raising significant concerns regarding copyright protection and content provenance verification. However, most existing watermarking techniques rely on uniform perturbation or rule-based token biasing schemes, which exhibit critical vulnerabilities under adversarial attacks such as paraphrasing, translation, and content truncation, often failing to maintain detection reliability in real-world deployment scenarios. To address these challenges, this paper introduces a novel context-aware robust watermarking framework that dynamically adjusts watermark embedding strength according to contextual semantic characteristics during text generation. The proposed approach incorporates a token-level semantic modulation mechanism that strategically intensifies watermark signals in copyright-sensitive segments while minimizing perturbations in semantically neutral regions, achieving an improved balance between imperceptibility and robustness. Furthermore, an adaptive threshold estimation algorithm is developed for watermark detection, which automatically calibrates detection boundaries based on noise statistics, significantly enhancing resilience against diverse attack vectors. Extensive experiments on the WaterBench benchmark demonstrate superior performance over state-of-the-art baselines, maintaining high detection accuracy with a 95.3% true positive rate (TPR) under clean conditions and strong robustness under severe perturbations, including paraphrasing attacks (82.7% TPR), translation attacks (78.4% TPR), and content truncation (88.9% TPR at 50% retention). Meanwhile, the proposed method reduces false positive rates by 43.2% compared with existing approaches while preserving text quality with negligible perplexity increase (1.8%). These results establish a new paradigm for practical and scalable LLM watermarking in real-world copyright-sensitive deployment scenarios.", "citations": 0}
{"title": "BanglaLorica: Design and Evaluation of a Robust Watermarking Algorithm for Large Language Models in Bangla Text Generation", "year": 2026, "authors": "Amit Bin Tariqul, A. N. M. Z. H. Milkan, Sahab-Al-Chowdhury, Syed Rifat Raiyan, Hasan Mahmud, Md. Kamrul Hasan", "url": "https://www.semanticscholar.org/paper/40aa6b41190ef27d23c05d6c76d241f67adefad7", "relevance": 3, "abstract": "As large language models (LLMs) are increasingly deployed for text generation, watermarking has become essential for authorship attribution, intellectual property protection, and misuse detection. While existing watermarking methods perform well in high-resource languages, their robustness in low-resource languages remains underexplored. This work presents the first systematic evaluation of state-of-the-art text watermarking methods: KGW, Exponential Sampling (EXP), and Waterfall, for Bangla LLM text generation under cross-lingual round-trip translation (RTT) attacks. Under benign conditions, KGW and EXP achieve high detection accuracy (>88%) with negligible perplexity and ROUGE degradation. However, RTT causes detection accuracy to collapse below RTT causes detection accuracy to collapse to 9-13%, indicating a fundamental failure of token-level watermarking. To address this, we propose a layered watermarking strategy that combines embedding-time and post-generation watermarks. Experimental results show that layered watermarking improves post-RTT detection accuracy by 25-35%, achieving 40-50% accuracy, representing a 3$\\times$ to 4$\\times$ relative improvement over single-layer methods, at the cost of controlled semantic degradation. Our findings quantify the robustness-quality trade-off in multilingual watermarking and establish layered watermarking as a practical, training-free solution for low-resource languages such as Bangla. Our code and data will be made public.", "citations": 0}
{"title": "WorldCup Sampling for Multi-bit LLM Watermarking", "year": 2026, "authors": "Yidan Wang, Yubing Ren, Yanan Cao, Li Guo", "url": "https://www.semanticscholar.org/paper/611e8cfa16e6a4fda941b8753c4f5fffbc8d9a79", "relevance": 3, "abstract": "As large language models (LLMs) generate increasingly human-like text, watermarking offers a promising solution for reliable attribution beyond mere detection. While multi-bit watermarking enables richer provenance encoding, existing methods largely extend zero-bit schemes through seed-driven steering, leading to indirect information flow, limited effective capacity, and suboptimal decoding. In this paper, we propose WorldCup, a multi-bit watermarking framework for LLMs that treats sampling as a natural communication channel and embeds message bits directly into token selection via a hierarchical competition mechanism guided by complementary signals. Moreover, WorldCup further adopts entropy-aware modulation to preserve generation quality and supports robust message recovery through confidence-aware decoding. Comprehensive experiments show that WorldCup achieves a strong balance across capacity, detectability, robustness, text quality, and decoding efficiency, consistently outperforming prior baselines and laying a solid foundation for future LLM watermarking studies.", "citations": 0}
{"title": "MirrorMark: A Distortion-Free Multi-Bit Watermark for Large Language Models", "year": 2026, "authors": "Ya Jiang, Massieh Kordi Boroujeny, Surender Suresh Kumar, Kai Zeng", "url": "https://www.semanticscholar.org/paper/a813dbbbf3dc152bc885ff23e6eef0dfb34fe5c9", "relevance": 3, "abstract": "As large language models (LLMs) become integral to applications such as question answering and content creation, reliable content attribution has become increasingly important. Watermarking is a promising approach, but existing methods either provide only binary signals or distort the sampling distribution, degrading text quality; distortion-free approaches, in turn, often suffer from weak detectability or robustness. We propose MirrorMark, a multi-bit and distortion-free watermark for LLMs. By mirroring sampling randomness in a measure-preserving manner, MirrorMark embeds multi-bit messages without altering the token probability distribution, preserving text quality by design. To improve robustness, we introduce a context-based scheduler that balances token assignments across message positions while remaining resilient to insertions and deletions. We further provide a theoretical analysis of the equal error rate to interpret empirical performance. Experiments show that MirrorMark matches the text quality of non-watermarked generation while achieving substantially stronger detectability: with 54 bits embedded in 300 tokens, it improves bit accuracy by 8-12% and correctly identifies up to 11% more watermarked texts at 1% false positive rate.", "citations": 0}
{"title": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking", "year": 2025, "authors": "Jingqi Zhang, Ruibo Chen, Yingqing Yang, Peihua Mai, Heng Huang, Yan Pang", "url": "https://www.semanticscholar.org/paper/6bb65dc8330fa65653b961710e24a414b684c9e6", "relevance": 3, "abstract": "Large Language Models (LLMs) are increasingly fine-tuned on smaller, domain-specific datasets to improve downstream performance. These datasets often contain proprietary or copyrighted material, raising the need for reliable safeguards against unauthorized use. Existing membership inference attacks (MIAs) and dataset-inference methods typically require access to internal signals such as logits, while current black-box approaches often rely on handcrafted prompts or a clean reference dataset for calibration, both of which limit practical applicability. Watermarking is a promising alternative, but prior techniques can degrade text quality or reduce task performance. We propose TRACE, a practical framework for fully black-box detection of copyrighted dataset usage in LLM fine-tuning. \\texttt{TRACE} rewrites datasets with distortion-free watermarks guided by a private key, ensuring both text quality and downstream utility. At detection time, we exploit the radioactivity effect of fine-tuning on watermarked data and introduce an entropy-gated procedure that selectively scores high-uncertainty tokens, substantially amplifying detection power. Across diverse datasets and model families, TRACE consistently achieves significant detections (p<0.05), often with extremely strong statistical evidence. Furthermore, it supports multi-dataset attribution and remains robust even after continued pretraining on large non-watermarked corpora. These results establish TRACE as a practical route to reliable black-box verification of copyrighted dataset usage. We will make our code available at: https://github.com/NusIoraPrivacy/TRACE.", "citations": 4}
{"title": "Watermarking Language Models with Error Correcting Codes", "year": 2024, "authors": "Patrick Chao, Edgar Dobriban, Hamed Hassani", "url": "https://www.semanticscholar.org/paper/5e00009d18c686b2fadc91aab05378303be7b7d9", "relevance": 3, "abstract": "Recent progress in large language models enables the creation of realistic machine-generated content. Watermarking is a promising approach to distinguish machine-generated text from human text, embedding statistical signals in the output that are ideally undetectable to humans. We propose a watermarking framework that encodes such signals through an error correcting code. Our method, termed robust binary code (RBC) watermark, introduces no noticeable degradation in quality. We evaluate our watermark on base and instruction fine-tuned models and find that our watermark is robust to edits, deletions, and translations. We provide an information-theoretic perspective on watermarking, a powerful statistical test for detection and for generating $p$-values, and theoretical guarantees. Our empirical findings suggest our watermark is fast, powerful, and robust, comparing favorably to the state-of-the-art.", "citations": 6}
{"title": "Improve the Trade-off Between Watermark Strength and Speculative Sampling Efficiency for Language Models", "year": 2026, "authors": "Weiqing He, Xiang Li, Li Shen, Weijie J. Su, Qi Long", "url": "https://www.semanticscholar.org/paper/276ea85fea72e611148a3f3456960990ac58b822", "relevance": 3, "abstract": "Watermarking is a principled approach for tracing the provenance of large language model (LLM) outputs, but its deployment in practice is hindered by inference inefficiency. Speculative sampling accelerates inference, with efficiency improving as the acceptance rate between draft and target models increases. Yet recent work reveals a fundamental trade-off: higher watermark strength reduces acceptance, preventing their simultaneous achievement. We revisit this trade-off and show it is not absolute. We introduce a quantitative measure of watermark strength that governs statistical detectability and is maximized when tokens are deterministic functions of pseudorandom numbers. Using this measure, we fully characterize the trade-off as a constrained optimization problem and derive explicit Pareto curves for two existing watermarking schemes. Finally, we introduce a principled mechanism that injects pseudorandomness into draft-token acceptance, ensuring maximal watermark strength while maintaining speculative sampling efficiency. Experiments further show that this approach improves detectability without sacrificing efficiency. Our findings uncover a principle that unites speculative sampling and watermarking, paving the way for their efficient and practical deployment.", "citations": 0}
{"title": "An End-to-End Model For Logits Based Large Language Models Watermarking", "year": 2025, "authors": "Kahim Wong, Jicheng Zhou, Jiantao Zhou, Yain-Whar Si", "url": "https://www.semanticscholar.org/paper/1b951e64ef26e3102d4302c1dd865a91b6902ec7", "relevance": 3, "abstract": "The rise of LLMs has increased concerns over source tracing and copyright protection for AIGC, highlighting the need for advanced detection technologies. Passive detection methods usually face high false positives, while active watermarking techniques using logits or sampling manipulation offer more effective protection. Existing LLM watermarking methods, though effective on unaltered content, suffer significant performance drops when the text is modified and could introduce biases that degrade LLM performance in downstream tasks. These methods fail to achieve an optimal tradeoff between text quality and robustness, particularly due to the lack of end-to-end optimization of the encoder and decoder. In this paper, we introduce a novel end-to-end logits perturbation method for watermarking LLM-generated text. By jointly optimization, our approach achieves a better balance between quality and robustness. To address non-differentiable operations in the end-to-end training pipeline, we introduce an online prompting technique that leverages the on-the-fly LLM as a differentiable surrogate. Our method achieves superior robustness, outperforming distortion-free methods by 37-39% under paraphrasing and 17.2% on average, while maintaining text quality on par with these distortion-free methods in terms of text perplexity and downstream tasks. Our method can be easily generalized to different LLMs. Code is available at https://github.com/KAHIMWONG/E2E_LLM_WM.", "citations": 3}
{"title": "BiMarker: Enhancing Text Watermark Detection for Large Language Models with Bipolar Watermarks", "year": 2025, "authors": "Zhuang Li", "url": "https://www.semanticscholar.org/paper/49a6fbf03a372aca6173985316e52d8ba768a12c", "relevance": 3, "abstract": "The rapid growth of Large Language Models (LLMs) raises concerns about distinguishing AI-generated text from human content. Existing watermarking techniques, like \\kgw, struggle with low watermark strength and stringent false-positive requirements. Our analysis reveals that current methods rely on coarse estimates of non-watermarked text, limiting watermark detectability. To address this, we propose Bipolar Watermark (\\tool), which splits generated text into positive and negative poles, enhancing detection without requiring additional computational resources or knowledge of the prompt. Theoretical analysis and experimental results demonstrate \\tool's effectiveness and compatibility with existing optimization techniques, providing a new optimization dimension for watermarking in LLM-generated content.", "citations": 2}
{"title": "A Nested Watermark for Large Language Models", "year": 2025, "authors": "Koichi Nagatsuka, Terufumi Morishita, Yasuhiro Sogawa", "url": "https://www.semanticscholar.org/paper/4f0d02307b8722c18cbed42888799688dd0bc088", "relevance": 3, "abstract": "The rapid advancement of large language models (LLMs) has raised concerns regarding their potential misuse, particularly in generating fake news and misinformation. To address these risks, watermarking techniques for autoregressive language models have emerged as a promising means for detecting LLM-generated text. Existing methods typically embed a watermark by increasing the probabilities of tokens within a group selected according to a single secret key. However, this approach suffers from a critical limitation: if the key is leaked, it becomes impossible to trace the text's provenance or attribute authorship. To overcome this vulnerability, we propose a novel nested watermarking scheme that embeds two distinct watermarks into the generated text using two independent keys. This design enables reliable authorship identification even in the event that one key is compromised. Experimental results demonstrate that our method achieves high detection accuracy for both watermarks while maintaining the fluency and overall quality of the generated text.", "citations": 0}
{"title": "Black-Box Text Watermarking Algorithm towards Large Language Models", "year": 2025, "authors": "\u5955\u8fb0 \u59da", "url": "https://www.semanticscholar.org/paper/44ed96ad930ad61d83c2bc49905ca52b10a7fc99", "relevance": 3, "abstract": "To address the detection issue of AI-generated text content arising from the rapid development of large language models and the daily usage of their API interfaces, this paper proposes a black-box text watermarking algorithm for text generated from the API of large language model. The algo-rithm employs reinforcement learning techniques to train text watermark embedding through three parts: watermark embedding statement, watermark embedding agent, and watermark embedding reward, which enables the good performance in watermark detectability, text semantic", "citations": 0}
{"title": "Position: LLM Watermarking Should Align Stakeholders' Incentives for Practical Adoption", "year": 2025, "authors": "Yepeng Liu, Xuandong Zhao, D. Song, Gregory W. Wornell, Yuheng Bu", "url": "https://www.semanticscholar.org/paper/8430b9bda8d2a02a8b6b1e975550cd4853a17c8c", "relevance": 3, "abstract": "Despite progress in watermarking algorithms for large language models (LLMs), real-world deployment remains limited. We argue that this gap stems from misaligned incentives among LLM providers, platforms, and end users, which manifest as four key barriers: competitive risk, detection-tool governance, robustness concerns and attribution issues. We revisit three classes of watermarking through this lens. \\emph{Model watermarking} naturally aligns with LLM provider interests, yet faces new challenges in open-source ecosystems. \\emph{LLM text watermarking} offers modest provider benefit when framed solely as an anti-misuse tool, but can gain traction in narrowly scoped settings such as dataset de-contamination or user-controlled provenance. \\emph{In-context watermarking} (ICW) is tailored for trusted parties, such as conference organizers or educators, who embed hidden watermarking instructions into documents. If a dishonest reviewer or student submits this text to an LLM, the output carries a detectable watermark indicating misuse. This setup aligns incentives: users experience no quality loss, trusted parties gain a detection tool, and LLM providers remain neutral by simply following watermark instructions. We advocate for a broader exploration of incentive-aligned methods, with ICW as an example, in domains where trusted parties need reliable tools to detect misuse. More broadly, we distill design principles for incentive-aligned, domain-specific watermarking and outline future research directions. Our position is that the practical adoption of LLM watermarking requires aligning stakeholder incentives in targeted application domains and fostering active community engagement.", "citations": 2}
{"title": "CEFW: A Comprehensive Evaluation Framework for Watermark in Large Language Models", "year": 2025, "authors": "Shuhao Zhang, Bo Cheng, Jiale Han, Yuli Chen, Zhixuan Wu, Changbao Li, Pingli Gu", "url": "https://www.semanticscholar.org/paper/2c0a2c68b5a122e22750afa45a4c2a9b670fe6e7", "relevance": 3, "abstract": "Text watermarking provides an effective solution for identifying synthetic text generated by large language models. However, existing techniques often focus on satisfying specific criteria while ignoring other key aspects, lacking a unified evaluation. To fill this gap, we propose the Comprehensive Evaluation Framework for Watermark (CEFW), a unified framework that comprehensively evaluates watermarking methods across five key dimensions: ease of detection, fidelity of text quality, minimal embedding cost, robustness to adversarial attacks, and imperceptibility to prevent imitation or forgery. By assessing watermarks according to all these key criteria, CEFW offers a thorough evaluation of their practicality and effectiveness. Moreover, we introduce a simple and effective watermarking method called Balanced Watermark (BW), which guarantees robustness and imperceptibility through balancing the way watermark information is added. Extensive experiments show that BW outperforms existing methods in overall performance across all evaluation dimensions. We release our code to the community for future research 1.", "citations": 0}
{"title": "Signature vs. Substance: Evaluating the Balance of Adversarial Resistance and Linguistic Quality in Watermarking Large Language Models", "year": 2025, "authors": "William Guo, Adaku Uchendu, Ana Smith", "url": "https://www.semanticscholar.org/paper/07ca80fca004f02c0f2ac4b1aeead6eef9c8edae", "relevance": 3, "abstract": "To mitigate the potential harms of Large Language Models (LLMs)generated text, researchers have proposed watermarking, a process of embedding detectable signals within text. With watermarking, we can always accurately detect LLM-generated texts. However, recent findings suggest that these techniques often negatively affect the quality of the generated texts, and adversarial attacks can strip the watermarking signals, causing the texts to possibly evade detection. These findings have created resistance in the wide adoption of watermarking by LLM creators. Finally, to encourage adoption, we evaluate the robustness of several watermarking techniques to adversarial attacks by comparing paraphrasing and back translation (i.e., English $\\to$ another language $\\to$ English) attacks; and their ability to preserve quality and writing style of the unwatermarked texts by using linguistic metrics to capture quality and writing style of texts. Our results suggest that these watermarking techniques preserve semantics, deviate from the writing style of the unwatermarked texts, and are susceptible to adversarial attacks, especially for the back translation attack.", "citations": 0}
{"title": "DMark: Order-Agnostic Watermarking for Diffusion Large Language Models", "year": 2025, "authors": "Linyu Wu, Linhao Zhong, Wenjie Qu, Yuexin Li, Yue Liu, Shengfang Zhai, Chunhua Shen, Jiaheng Zhang", "url": "https://www.semanticscholar.org/paper/efff5ddc9aae3b2bd67e7fa40a7144a16f1823a1", "relevance": 3, "abstract": "Diffusion large language models (dLLMs) offer faster generation than autoregressive models while maintaining comparable quality, but existing watermarking methods fail on them due to their non-sequential decoding. Unlike autoregressive models that generate tokens left-to-right, dLLMs can finalize tokens in arbitrary order, breaking the causal design underlying traditional watermarks. We present DMark, the first watermarking framework designed specifically for dLLMs. DMark introduces three complementary strategies to restore watermark detectability: predictive watermarking uses model-predicted tokens when actual context is unavailable; bidirectional watermarking exploits both forward and backward dependencies unique to diffusion decoding; and predictive-bidirectional watermarking combines both approaches to maximize detection strength. Experiments across multiple dLLMs show that DMark achieves 92.0-99.5% detection rates at 1% false positive rate while maintaining text quality, compared to only 49.6-71.2% for naive adaptations of existing methods. DMark also demonstrates robustness against text manipulations, establishing that effective watermarking is feasible for non-autoregressive language models.", "citations": 0}
{"title": "HATS: High-Accuracy Triple-Set Watermarking for Large Language Models", "year": 2025, "authors": "Zhiqing Hu, Chenxu Zhao, Jiazhong Lu, Xiaolei Liu", "url": "https://www.semanticscholar.org/paper/f2ee3b0ea99a0aa7b9d64c4d64ead5327d2a36a2", "relevance": 3, "abstract": "Misuse of LLM-generated text can be curbed by watermarking techniques that embed implicit signals into the output. We propose a watermark that partitions the vocabulary at each decoding step into three sets (Green/Yellow/Red) with fixed ratios and restricts sampling to the Green and Yellow sets. At detection time, we replay the same partitions, compute Green-enrichment and Red-depletion statistics, convert them to one-sided z-scores, and aggregate their p-values via Fisher's method to decide whether a passage is watermarked. We implement generation, detection, and testing on Llama 2 7B, and evaluate true-positive rate, false-positive rate, and text quality. Results show that the triple-partition scheme achieves high detection accuracy at fixed FPR while preserving readability.", "citations": 0}
{"title": "A Contrastive Semantic Watermarking Framework for Large Language Models", "year": 2025, "authors": "Jianxin Wang, Xiangze Chang, Chaoen Xiao, Lei Zhang", "url": "https://www.semanticscholar.org/paper/9421c649fec086c741fc9e8ad556a637ee61dddd", "relevance": 3, "abstract": "The widespread deployment of large language models (LLMs) has raised urgent demands for verifiable content attribution and misuse mitigation. Existing text watermarking techniques often struggle in black-box or sampling-based scenarios due to limitations in robustness, imperceptibility, and detection generality. These challenges are particularly critical in open-access settings, where model internals and generation logits are unavailable for attribution. To address these limitations, we propose CWS (Contrastive Watermarking with Semantic Modeling)\u2014a novel keyless watermarking framework that integrates contrastive semantic token selection and shared embedding space alignment. CWS enables context-aware, fluent watermark embedding while supporting robust detection via a dual-branch mechanism: a lightweight z-score statistical test for public verification and a GRU-based semantic decoder for black-box adversarial robustness. Experiments on GPT-2, OPT-1.3B, and LLaMA-7B over C4 and DBpedia datasets demonstrate that CWS achieves F1 scores up to 99.9% and maintains F1 \u2265 93% under semantic rewriting, token substitution, and lossy compression (\u03b5 \u2264 0.25, \u03b4 \u2264 0.2). The GRU-based detector offers a superior speed\u2013accuracy trade-off (0.42 s/sample) over LSTM and Transformer baselines. These results highlight CWS as a lightweight, black-box-compatible, and semantically robust watermarking method suitable for practical content attribution across LLM architectures and decoding strategies. Furthermore, CWS maintains a symmetrical architecture between embedding and detection stages via shared semantic representations, ensuring structural consistency and robustness. This semantic symmetry helps preserve detection reliability across diverse decoding strategies and adversarial conditions.", "citations": 0}
{"title": "AdaptiveWatermark: Dynamic Watermarking for Large Language Models with Agent", "year": 2025, "authors": "Qingze Peng, Daifeng Li", "url": "https://www.semanticscholar.org/paper/7a74123dae03e29ce091ccbed9a5584f6260e9fd", "relevance": 3, "abstract": "Text watermarking embeds invisible yet detectable markers in generated content to prevent misuse of Large Language Models (LLMs). Existing methods either require fine-tuning or randomly select tokens from the vocabulary to form a green list, which increases generation likelihood. The former is computationally expensive, while the latter may degrade content quality. To address this, we propose Reinforcement Learning- based Adaptive WaterMarking (RLAWM), a novel framework using Reinforcement Learning (RL) to dynamically generate watermarks without fine-tuning. The watermark agent selects green-list tokens based on content semantics, while the detection agent identifies watermarks by analyzing green word frequency and provides feedback to improve concealment. To preserve quality, Maximum Mean Discrepancy (MMD) measures semantic differences between original and watermarked texts. Direct Preference Optimization (DPO) enables end-to-end training and system stability. On LLaMA2-7B, RLAWM improved TPR@1% by 2.48%, best F1 by 1.25%, and reduced perplexity by 3.45%.", "citations": 0}
{"title": "TWT-LLM: A Universal and Robust Tagged Watermark for Large Language Models", "year": 2025, "authors": "Jibin Zheng, Li Ma, Wenyin Yang, Fen Liu, Yongqiang Li, Zhengbin Liu", "url": "https://www.semanticscholar.org/paper/3a9ea681ce33994c64c38a0dad9d4a19cb43b1bf", "relevance": 3, "abstract": "Large Language Models (LLMs) generate realistic and coherent text, boosting efficiency and decision-making in various fields. However, their generative capabilities pose risks of intellectual property abuse. Watermarking technology offers a solution for information hiding in LLMs, with large model watermarking gaining attention for its unique methods. A critical challenge is embedding watermarks with minimal impact on text quality while ensuring rapid detection, making it an urgent issue to address. In this paper, to address these challenge, we propose a universal and robust tagged watermark technology for LLMs (TWT-LLM). Firstly, the method of TWT embeds a certain amount of watermark information in the sampling process while generating subsequent word text based on the prompt. Then, to enhance the quality of the generated text, we have proposed a group-based local watermark embedding method, which significantly reduces the impact on text quality. The method involves tagging tokens within each group, only the tokens that have been tagged will embed the watermark information. Moreover, to detect the watermark information in the generated text, we have designed a detection method specifically for this watermark embedding technique. Finally, we conducted experiments using the C4 and HC3 datasets, demonstrating that TWT-LLM achieves a lower False Negative Rate and is lighter compared to state-of-the-art methods.", "citations": 0}
{"title": "HeavyWater and SimplexWater: Distortion-Free LLM Watermarks for Low-Entropy Next-Token Predictions", "year": 2025, "authors": "Dor Tsur, Carol Xuan Long, C. M. Verdun, Hsiang Hsu, Chen-Fu Chen, H. Permuter, Sajani Vithana, F. Calmon", "url": "https://www.semanticscholar.org/paper/b30b59d508ba651c1343915722008ba069e45c4f", "relevance": 3, "abstract": "Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater", "citations": 0}
{"title": "Subtle Signatures, Strong Shields: Advancing Robust and Imperceptible Watermarking in Large Language Models", "year": 2024, "authors": "Yubing Ren, Ping Guo, Yanan Cao, Wei Ma", "url": "https://www.semanticscholar.org/paper/ac09419ff605ccf52ab272c443ba0598212f3d47", "relevance": 3, "abstract": "The widespread adoption of Large Language Models (LLMs) has led to an increase in AI-generated text on the Internet, presenting a crucial challenge to differentiate AI-created content from human-written text. This challenge is critical to prevent issues of authenticity, trust, and potential copyright violations. Current research focuses on watermarking LLM-generated text, but traditional techniques struggle to balance robustness with text quality. We introduce a novel watermarking approach, Ro-bust and Imperceptible Watermarking (R IW ) for LLMs, which leverages token prior probabilities to improve detectability and maintain watermark imperceptibility. R IW methodically embeds watermarks by partitioning selected tokens into two distinct groups based on their prior probabilities and employing tailored strategies for each group. In the detection stage, R IW method employs the \u2018 voted z-test \u2019 to provide a statistically robust framework to identify the presence of a watermark accurately. The effectiveness of R IW is evaluated across three key dimensions: success rate, text quality, and robustness against removal attacks. Our experimental results on various LLMs, including GPT2-XL, OPT-1.3B, and LLaMA2-7B, indicate that R IW surpasses existing models, and also exhibits increased robustness against various attacks and good imperceptibility, thus promoting the responsible use of LLMs. Our code is available at https://github.com", "citations": 3}
{"title": "Signal Watermark on Large Language Models", "year": 2024, "authors": "Zhenyu Xu, Victor S. Sheng", "url": "https://www.semanticscholar.org/paper/6fefabadfca976a9a55fc2134220394e27930ce1", "relevance": 3, "abstract": "As Large Language Models (LLMs) become increasingly sophisticated, they raise significant security concerns, including the creation of fake news and academic misuse. Most detectors for identifying model-generated text are limited by their reliance on variance in perplexity and burstiness, and they require substantial computational resources. In this paper, we proposed a watermarking method embedding a specific watermark into the text during its generation by LLMs, based on a pre-defined signal pattern. This technique not only ensures the watermark's invisibility to humans but also maintains the quality and grammatical integrity of model-generated text. We utilize LLMs and Fast Fourier Transform (FFT) for token probability computation and detection of the signal watermark. The unique application of signal processing principles within the realm of text generation by LLMs allows for subtle yet effective embedding of watermarks, which do not compromise the quality or coherence of the generated text. Our method has been empirically validated across multiple LLMs, consistently maintaining high detection accuracy, even with variations in temperature settings during text generation. In the experiment of distinguishing between human-written and watermarked text, our method achieved an AUROC score of 0.97, significantly outperforming existing methods like GPTZero, which scored 0.64. The watermark's resilience to various attacking scenarios further confirms its robustness, addressing significant challenges in model-generated text authentication.", "citations": 2}
{"title": "How Good is Post-Hoc Watermarking With Language Model Rephrasing?", "year": 2025, "authors": "Pierre Fernandez, Tom Sander, Hady Elsahar, Hongyan Chang, Tom\u00e1s Soucek, Valeriu Lacatusu, Tuan Tran, Sylvestre-Alvise Rebuffi, Alex Mourachko", "url": "https://www.semanticscholar.org/paper/6ea989a77ba0d11cff88d204c8c7f3d184f91e88", "relevance": 3, "abstract": "Generation-time text watermarking embeds statistical signals into text for traceability of AI-generated content. We explore *post-hoc watermarking* where an LLM rewrites existing text while applying generation-time watermarking, to protect copyrighted documents, or detect their use in training or RAG via watermark radioactivity. Unlike generation-time approaches, which is constrained by how LLMs are served, this setting offers additional degrees of freedom for both generation and detection. We investigate how allocating compute (through larger rephrasing models, beam search, multi-candidate generation, or entropy filtering at detection) affects the quality-detectability trade-off. Our strategies achieve strong detectability and semantic fidelity on open-ended text such as books. Among our findings, the simple Gumbel-max scheme surprisingly outperforms more recent alternatives under nucleus sampling, and most methods benefit significantly from beam search. However, most approaches struggle when watermarking verifiable text such as code, where we counterintuitively find that smaller models outperform larger ones. This study reveals both the potential and limitations of post-hoc watermarking, laying groundwork for practical applications and future research.", "citations": 0}
{"title": "Learning to Watermark LLM-generated Text via Reinforcement Learning", "year": 2024, "authors": "Xiaojun Xu, Yuanshun Yao, Yang Liu", "url": "https://www.semanticscholar.org/paper/5a2d52a4f03e26707710a40af96e997bd49e1544", "relevance": 3, "abstract": "We study how to watermark LLM outputs, i.e. embedding algorithmically detectable signals into LLM-generated text to track misuse. Unlike the current mainstream methods that work with a fixed LLM, we expand the watermark design space by including the LLM tuning stage in the watermark pipeline. While prior works focus on token-level watermark that embeds signals into the output, we design a model-level watermark that embeds signals into the LLM weights, and such signals can be detected by a paired detector. We propose a co-training framework based on reinforcement learning that iteratively (1) trains a detector to detect the generated watermarked text and (2) tunes the LLM to generate text easily detectable by the detector while keeping its normal utility. We empirically show that our watermarks are more accurate, robust, and adaptable (to new attacks). It also allows watermarked model open-sourcing. In addition, if used together with alignment, the extra overhead introduced is low - only training an extra reward model (i.e. our detector). We hope our work can bring more effort into studying a broader watermark design that is not limited to working with a fixed LLM. We open-source the code: https://github.com/xiaojunxu/learning-to-watermark-llm .", "citations": 23}
{"title": "ArcMark: Multi-bit LLM Watermark via Optimal Transport", "year": 2026, "authors": "Atefeh Gilani, Carol Xuan Long, Sajani Vithana, O. Kosut, L. Sankar, F. Calmon", "url": "https://www.semanticscholar.org/paper/a4497eb42db1cbbab4d5d98d109d212f1470bf92", "relevance": 3, "abstract": "Watermarking is an important tool for promoting the responsible use of language models (LMs). Existing watermarks insert a signal into generated tokens that either flags LM-generated text (zero-bit watermarking) or encodes more complex messages (multi-bit watermarking). Though a number of recent multi-bit watermarks insert several bits into text without perturbing average next-token predictions, they largely extend design principles from the zero-bit setting, such as encoding a single bit per token. Notably, the information-theoretic capacity of multi-bit watermarking -- the maximum number of bits per token that can be inserted and detected without changing average next-token predictions -- has remained unknown. We address this gap by deriving the first capacity characterization of multi-bit watermarks. Our results inform the design of ArcMark: a new watermark construction based on coding-theoretic principles that, under certain assumptions, achieves the capacity of the multi-bit watermark channel. In practice, ArcMark outperforms competing multi-bit watermarks in terms of bit rate per token and detection accuracy. Our work demonstrates that LM watermarking is fundamentally a channel coding problem, paving the way for principled coding-theoretic approaches to watermark design.", "citations": 0}
{"title": "AI-Generated Text Detection: A Comprehensive Review of Active and Passive Approaches", "year": 2025, "authors": "Lingyun Xiang, Nian Li, Yuling Liu, Jiayong Hu", "url": "https://www.semanticscholar.org/paper/a1f3225208427cd40e6f89319467dfa0aa185a34", "relevance": 3, "abstract": ": The rapid advancement of large language models (LLMs) has driven the pervasive adoption of AI-generated content (AIGC), while also raising concerns about misinformation, academic misconduct, biased or harmful content, and other risks. Detecting AI-generated text has thus become essential to safeguard the authenticity and reliability of digital information. This survey reviews recent progress in detection methods, categorizing approaches into passive and active categories based on their reliance on intrinsic textual features or embedded signals. Passive detection is further divided into surface linguistic feature-based and language model-based methods, whereas active detection encompasses watermarking-based and semantic retrieval-based approaches. This taxonomy enables systematic comparison of methodological differences in model dependency, applicability, and robustness. A key challenge for AI-generated text detection is that existing detectors are highly vulnerable to adversarial attacks, particularly paraphrasing, which substantially compromises their effectiveness. Addressing this gap highlights the need for future research on enhancing robustness and cross-domain generalization. By synthesizing current advances and limitations, this survey provides a structured reference for the field and outlines pathways toward more reliable and scalable detection solutions.", "citations": 0}
{"title": "Analyzing and Evaluating Unbiased Language Model Watermark", "year": 2025, "authors": "Yihan Wu, Xuehao Cui, Ruibo Chen, Heng Huang", "url": "https://www.semanticscholar.org/paper/85fcf16cbee318d1b1a0c842b6743d1a9e2dee21", "relevance": 3, "abstract": "Verifying the authenticity of AI-generated text has become increasingly important with the rapid advancement of large language models, and unbiased watermarking has emerged as a promising approach due to its ability to preserve output distribution without degrading quality. However, recent work reveals that unbiased watermarks can accumulate distributional bias over multiple generations and that existing robustness evaluations are inconsistent across studies. To address these issues, we introduce UWbench, the first open-source benchmark dedicated to the principled evaluation of unbiased watermarking methods. Our framework combines theoretical and empirical contributions: we propose a statistical metric to quantify multi-batch distribution drift, prove an impossibility result showing that no unbiased watermark can perfectly preserve the distribution under infinite queries, and develop a formal analysis of robustness against token-level modification attacks. Complementing this theory, we establish a three-axis evaluation protocol: unbiasedness, detectability, and robustness, and show that token modification attacks provide more stable robustness assessments than paraphrasing-based methods. Together, UWbench offers the community a standardized and reproducible platform for advancing the design and evaluation of unbiased watermarking algorithms.", "citations": 2}
{"title": "SoK: Are Watermarks in LLMs Ready for Deployment?", "year": 2025, "authors": "Kieu Dang, Phung Lai, Nhathai Phan, Yelong Shen, Ruoming Jin, Abdallah Khreishah, My T. Thai", "url": "https://www.semanticscholar.org/paper/f9b4f91ceca3254f882789b330b7c9044bf408a8", "relevance": 3, "abstract": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs. To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.", "citations": 1}
{"title": "Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning", "year": 2025, "authors": "Li An, Yujian Liu, Yepeng Liu, Yang Zhang, Yuheng Bu, Shiyu Chang", "url": "https://www.semanticscholar.org/paper/3c3ffcd0af8297f096c5e907e038c601e2d5314e", "relevance": 3, "abstract": "Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria: high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text-transforming it into hate speech-while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/contrastive-watermark.", "citations": 8}
{"title": "Two Halves Make a Whole: How to Reconcile Soundness and Robustness in Watermarking for Large Language Models", "year": 2024, "authors": "Lei Fan, Chenhao Tang, Weicheng Yang, Hong-Sheng Zhou", "url": "https://www.semanticscholar.org/paper/9cda9426fd34cf52258f90f87a39b7477b32bb9f", "relevance": 3, "abstract": "", "citations": 0}
{"title": "A Unified Framework for LLM Watermarks", "year": 2026, "authors": "Thibaud Gloaguen, Robin Staab, Nikola Jovanovi'c, Martin T. Vechev", "url": "https://www.semanticscholar.org/paper/8d99cc287356eafbf6266d67862abf0b41804b03", "relevance": 3, "abstract": "LLM watermarks allow tracing AI-generated texts by inserting a detectable signal into their generated content. Recent works have proposed a wide range of watermarking algorithms, each with distinct designs, usually built using a bottom-up approach. Crucially, there is no general and principled formulation for LLM watermarking. In this work, we show that most existing and widely used watermarking schemes can in fact be derived from a principled constrained optimization problem. Our formulation unifies existing watermarking methods and explicitly reveals the constraints that each method optimizes. In particular, it highlights an understudied quality-diversity-power trade-off. At the same time, our framework also provides a principled approach for designing novel watermarking schemes tailored to specific requirements. For instance, it allows us to directly use perplexity as a proxy for quality, and derive new schemes that are optimal with respect to this constraint. Our experimental evaluation validates our framework: watermarking schemes derived from a given constraint consistently maximize detection power with respect to that constraint.", "citations": 0}
{"title": "HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions", "year": 2025, "authors": "Dor Tsur, Carol Xuan Long, C. M. Verdun, Hsiang Hsu, Chen-Fu Chen, H. Permuter, Sajani Vithana, F. Calmon", "url": "https://www.semanticscholar.org/paper/f78ef14f389c7b92c83e1a074542309ad9076bb4", "relevance": 3, "abstract": "", "citations": 1}
{"title": "A Semantic Invariant Robust Watermark for Large Language Models", "year": 2023, "authors": "Aiwei Liu, Leyi Pan, Xuming Hu, Shiao Meng, Lijie Wen", "url": "https://www.semanticscholar.org/paper/893bc4c9d3be06319db7f31b6491110483db5fd1", "relevance": 2, "abstract": "Watermark algorithms for large language models (LLMs) have achieved extremely high accuracy in detecting text generated by LLMs. Such algorithms typically involve adding extra watermark logits to the LLM's logits at each generation step. However, prior algorithms face a trade-off between attack robustness and security robustness. This is because the watermark logits for a token are determined by a certain number of preceding tokens; a small number leads to low security robustness, while a large number results in insufficient attack robustness. In this work, we propose a semantic invariant watermarking method for LLMs that provides both attack robustness and security robustness. The watermark logits in our work are determined by the semantics of all preceding tokens. Specifically, we utilize another embedding LLM to generate semantic embeddings for all preceding tokens, and then these semantic embeddings are transformed into the watermark logits through our trained watermark model. Subsequent analyses and experiments demonstrated the attack robustness of our method in semantically invariant settings: synonym substitution and text paraphrasing settings. Finally, we also show that our watermark possesses adequate security robustness. Our code and data are available at \\href{https://github.com/THU-BPM/Robust_Watermark}{https://github.com/THU-BPM/Robust\\_Watermark}. Additionally, our algorithm could also be accessed through MarkLLM \\citep{pan2024markllm} \\footnote{https://github.com/THU-BPM/MarkLLM}.", "citations": 96}
{"title": "SoK: Watermarking for AI-Generated Content", "year": 2024, "authors": "Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tram\u00e8r, Somesh Jha, Lei Li, Yu-Xiang Wang, D. Song", "url": "https://www.semanticscholar.org/paper/ad200cf8c1e4659e42510aa339d71299a1abc9d9", "relevance": 2, "abstract": "As the outputs of generative AI (GenAl) techniques improve in quality, it becomes increasingly challenging to distinguish them from human-created content. Watermarking schemes are a promising approach to address the problem of distinguishing between AI and human-generated content. These schemes embed hidden signals within AI -generated content to enable reliable detection. While watermarking is not a silver bullet for addressing all risks associated with GenAl, it can play a crucial role in enhancing AI safety and trustworthiness by combating misinformation and deception. This paper presents a comprehensive overview of water-marking techniques for GenAl, beginning with the need for watermarking from historical and regulatory perspectives. We formalize the definitions and desired properties of watermarking schemes and examine the key objectives and threat models for existing approaches. Practical evaluation strategies are also explored, providing insights into the development of robust watermarking techniques capable of resisting various attacks. Additionally, we review recent representative works, highlight open challenges, and discuss potential directions for this emerging field. By offering a thorough understanding of watermarking in GenAl, this work aims to guide researchers in advancing watermarking methods and applications, and support policymakers in addressing the broader implications of GenAl.", "citations": 55}
{"title": "Adaptive Text Watermark for Large Language Models", "year": 2024, "authors": "Yepeng Liu, Yuheng Bu", "url": "https://www.semanticscholar.org/paper/f68a35c0c3b1949236ceb1d51c442cca0d0606da", "relevance": 2, "abstract": "The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining strong security, robustness, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive watermarking strategy to address this problem. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured using an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits in proportion based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of \\emph{un-watermarked} LLMs while maintaining security even under various attacks.", "citations": 55}
{"title": "Context-aware Watermark with Semantic Balanced Green-red Lists for Large Language Models", "year": 2024, "authors": "Yuxuan Guo, Zhiliang Tian, Yiping Song, Tianlun Liu, Liang Ding, Dongsheng Li", "url": "https://www.semanticscholar.org/paper/f56e016c69ca40e9ec1cced00caaa0fa7e00cf9a", "relevance": 2, "abstract": "Watermarking enables people to determine whether the text is generated by a specific model. It injects a unique signature based on the \u201cgreen-red\u201d list that can be tracked during detection, where the words in green lists are encouraged to be generated. Recent researchers propose to fix the green/red lists or increase the proportion of green tokens to defend against paraphrasing attacks. However, these methods cause degradation of text quality due to semantic disparities between the watermarked text and the unwatermarked text. In this paper, we propose a semantic-aware watermark method that considers contexts to generate a semantic-aware key to split a semantically balanced green/red list for watermark injection. The semantic balanced list reduces the performance drop due to adding bias on green lists. To defend against paraphrasing attacks, we generate the watermark key considering the semantics of contexts via locally sensitive hashing. To improve the text quality, we propose to split green/red lists considering semantics to enable the green list to cover almost all semantics. We also dynamically adapt the bias to balance text quality and robustness. The experiments show our advantages in both robustness and text quality comparable to existing baselines.", "citations": 4}
{"title": "A Sentence-Semantic-Based Watermark for Large Language Models Against Paraphrasing Attacks", "year": 2025, "authors": "Lei Zhao, Han Wang, Tao Zhang, Limin Xu", "url": "https://www.semanticscholar.org/paper/3a0a2ca0fda97e2e5959b7f29239916f591c57f9", "relevance": 2, "abstract": "In light of rapid advancements in the performance of Large Language Models (LLMs), their potential misuse to generate harmful and malicious text has begun to pose a significant threat to Internet security and the integrity of digital content. Text watermarking has emerged as a potent tool to ascertain authorship, effectively distinguishing between human-authored text and that generated by LLMs. However, current text watermarking methods, which rely exclusively on assessing the hash values of preceding tokens to predict the subsequent token, remain vulnerable to paraphrasing attacks and cracking. To address these vulnerabilities, we propose a sentence-semantic-based text watermarking method to counteract performance degradation caused by paraphrasing attacks while preserving watermark security. Specifically, our approach involves extracting semantic embeddings from the generated sentences and projecting them into a vocabulary-sized vector space. By analyzing the magnitude of elements within the generated semantic vector, we establish a word preference mechanism for subsequent token prediction. This preferential selection, guided by ranked magnitudes, ensures that certain words are more likely to be chosen during token generation. The resilience of our method to paraphrasing attacks is corroborated by experimental results, demonstrating a marked improvement over existing watermarking methods.", "citations": 0}
{"title": "Inevitable Trade-off between Watermark Strength and Speculative Sampling Efficiency for Language Models", "year": 2024, "authors": "Zhengmian Hu, Heng Huang", "url": "https://www.semanticscholar.org/paper/0d19cb8a04601929be86ab7d64f21ced130557ff", "relevance": 2, "abstract": "Large language models are probabilistic models, and the process of generating content is essentially sampling from the output distribution of the language model. Existing watermarking techniques inject watermarks into the generated content without altering the output quality. On the other hand, existing acceleration techniques, specifically speculative sampling, leverage a draft model to speed up the sampling process while preserving the output distribution. However, there is no known method to simultaneously accelerate the sampling process and inject watermarks into the generated content. In this paper, we investigate this direction and find that the integration of watermarking and acceleration is non-trivial. We prove a no-go theorem, which states that it is impossible to simultaneously maintain the highest watermark strength and the highest sampling efficiency. Furthermore, we propose two methods that maintain either the sampling efficiency or the watermark strength, but not both. Our work provides a rigorous theoretical foundation for understanding the inherent trade-off between watermark strength and sampling efficiency in accelerating the generation of watermarked tokens for large language models. We also conduct numerical experiments to validate our theoretical findings and demonstrate the effectiveness of the proposed methods.", "citations": 8}
{"title": "Excuse me, sir? Your language model is leaking (information)", "year": 2024, "authors": "Or Zamir", "url": "https://www.semanticscholar.org/paper/069f3eec8b80c64a2f4b6e4a955af4372da27a4b", "relevance": 2, "abstract": "We introduce a cryptographic method to hide an arbitrary secret payload in the response of a Large Language Model (LLM). A secret key is required to extract the payload from the model's response, and without the key it is provably impossible to distinguish between the responses of the original LLM and the LLM that hides a payload. In particular, the quality of generated text is not affected by the payload. Our approach extends a recent result of Christ, Gunn and Zamir (2023) who introduced an undetectable watermarking scheme for LLMs.", "citations": 11}
{"title": "An Entropy-based Text Watermarking Detection Method", "year": 2024, "authors": "Yijian Lu, Aiwei Liu, Dianzhi Yu, Jingjing Li, Irwin King", "url": "https://www.semanticscholar.org/paper/ff4ac308b02331c75c431b4bf01c0485b91761c8", "relevance": 1, "abstract": "Text watermarking algorithms for large language models (LLMs) can effectively identify machine-generated texts by embedding and detecting hidden features in the text. Although the current text watermarking algorithms perform well in most high-entropy scenarios, its performance in low-entropy scenarios still needs to be improved. In this work, we opine that the influence of token entropy should be fully considered in the watermark detection process, $i.e.$, the weight of each token during watermark detection should be customized according to its entropy, rather than setting the weights of all tokens to the same value as in previous methods. Specifically, we propose \\textbf{E}ntropy-based Text \\textbf{W}atermarking \\textbf{D}etection (\\textbf{EWD}) that gives higher-entropy tokens higher influence weights during watermark detection, so as to better reflect the degree of watermarking. Furthermore, the proposed detection process is training-free and fully automated. From the experiments, we demonstrate that our EWD can achieve better detection performance in low-entropy scenarios, and our method is also general and can be applied to texts with different entropy distributions. Our code and data is available\\footnote{\\url{https://github.com/luyijian3/EWD}}. Additionally, our algorithm could be accessed through MarkLLM \\cite{pan2024markllm}\\footnote{\\url{https://github.com/THU-BPM/MarkLLM}}.", "citations": 77}
{"title": "Robust Detection of Watermarks for Large Language Models Under Human Edits", "year": 2024, "authors": "Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su", "url": "https://www.semanticscholar.org/paper/bb41aa5b3a26bd9690cdb7b279ea52bb578a58f4", "relevance": 1, "abstract": "Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality adaptively as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman-Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.", "citations": 17}
{"title": "Adaptive Testing for Segmenting Watermarked Texts From Language Models", "year": 2025, "authors": "Xingchi Li, Xiaochi Liu, Guanxun Li", "url": "https://www.semanticscholar.org/paper/0086ae38097093e422b6de4a81f5515917c08e93", "relevance": 1, "abstract": "The rapid adoption of large language models (LLMs), such as GPT\u20104 and Claude 3.5, underscores the need to distinguish LLM\u2010generated text from human\u2010written content to mitigate the spread of misinformation and misuse in education. One promising approach to address this issue is the watermark technique, which embeds subtle statistical signals into LLM\u2010generated text to enable reliable identification. In this paper, we first generalize the likelihood\u2010based LLM detection method by introducing a flexible weighted formulation and further adapt this approach to the inverse transform sampling method. Moving beyond watermark detection, we extend this adaptive detection strategy to tackle the more challenging problem of segmenting a given text into watermarked and non\u2010watermarked substrings. In contrast to the approach in a previous study, which relies on accurate estimation of next\u2010token probabilities that are highly sensitive to prompt estimation, our proposed framework removes the need for precise prompt estimation. Extensive numerical experiments demonstrate that the proposed methodology is both effective and robust in accurately segmenting texts containing a mixture of watermarked and non\u2010watermarked content.", "citations": 0}
{"title": "WISER: Segmenting watermarked region - an epidemic change-point perspective", "year": 2025, "authors": "Soham Bonnerjee, Sayar Karmakar, Subhrajyoty Roy", "url": "https://www.semanticscholar.org/paper/0910d37321642b95f2ca3637538c47a44d2e4184", "relevance": 1, "abstract": "With the increasing popularity of large language models, concerns over content authenticity have led to the development of myriad watermarking schemes. These schemes can be used to detect a machine-generated text via an appropriate key, while being imperceptible to readers with no such keys. The corresponding detection mechanisms usually take the form of statistical hypothesis testing for the existence of watermarks, spurring extensive research in this direction. However, the finer-grained problem of identifying which segments of a mixed-source text are actually watermarked, is much less explored; the existing approaches either lack scalability or theoretical guarantees robust to paraphrase and post-editing. In this work, we introduce a unique perspective to such watermark segmentation problems through the lens of epidemic change-points. By highlighting the similarities as well as differences of these two problems, we motivate and propose WISER: a novel, computationally efficient, watermark segmentation algorithm. We theoretically validate our algorithm by deriving finite sample error-bounds, and establishing its consistency in detecting multiple watermarked segments in a single text. Complementing these theoretical results, our extensive numerical experiments show that WISER outperforms state-of-the-art baseline methods, both in terms of computational speed as well as accuracy, on various benchmark datasets embedded with diverse watermarking schemes. Our theoretical and empirical findings establish WISER as an effective tool for watermark localization in most settings. It also shows how insights from a classical statistical problem can lead to a theoretically valid and computationally efficient solution of a modern and pertinent problem.", "citations": 0}
{"title": "Segmenting Watermarked Texts From Language Models", "year": 2024, "authors": "Xingchi Li, Guanxun Li, Xianyang Zhang", "url": "https://www.semanticscholar.org/paper/82afcfad1c3bbefbbe40b29777e9a94e4070c27b", "relevance": 1, "abstract": "Watermarking is a technique that involves embedding nearly unnoticeable statistical signals within generated content to help trace its source. This work focuses on a scenario where an untrusted third-party user sends prompts to a trusted language model (LLM) provider, who then generates a text from their LLM with a watermark. This setup makes it possible for a detector to later identify the source of the text if the user publishes it. The user can modify the generated text by substitutions, insertions, or deletions. Our objective is to develop a statistical method to detect if a published text is LLM-generated from the perspective of a detector. We further propose a methodology to segment the published text into watermarked and non-watermarked sub-strings. The proposed approach is built upon randomization tests and change point detection techniques. We demonstrate that our method ensures Type I and Type II error control and can accurately identify watermarked sub-strings by finding the corresponding change point locations. To validate our technique, we apply it to texts generated by several language models with prompts extracted from Google's C4 dataset and obtain encouraging numerical results. We release all code publicly at https://github.com/doccstat/llm-watermark-cpd.", "citations": 3}
{"title": "Discovering Spoofing Attempts on Language Model Watermarks", "year": 2024, "authors": "Thibaud Gloaguen, Nikola Jovanovi'c, Robin Staab, Martin T. Vechev", "url": "https://www.semanticscholar.org/paper/4fdc32c1f7012f14b8a9748ec5dc6830a5826874", "relevance": 1, "abstract": "LLM watermarks stand out as a promising way to attribute ownership of LLM-generated text. One threat to watermark credibility comes from spoofing attacks, where an unauthorized third party forges the watermark, enabling it to falsely attribute arbitrary texts to a particular LLM. Despite recent work demonstrating that state-of-the-art schemes are, in fact, vulnerable to spoofing, no prior work has focused on post-hoc methods to discover spoofing attempts. In this work, we for the first time propose a reliable statistical method to distinguish spoofed from genuinely watermarked text, suggesting that current spoofing attacks are less effective than previously thought. In particular, we show that regardless of their underlying approach, all current learning-based spoofing methods consistently leave observable artifacts in spoofed texts, indicative of watermark forgery. We build upon these findings to propose rigorous statistical tests that reliably reveal the presence of such artifacts and thus demonstrate that a watermark has been spoofed. Our experimental evaluation shows high test power across all learning-based spoofing methods, providing insights into their fundamental limitations and suggesting a way to mitigate this threat. We make all our code available at https://github.com/eth-sri/watermark-spoofing-detection .", "citations": 1}
{"title": "A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions", "year": 2025, "authors": "Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Lidia S. Chao, Derek F. Wong", "url": "https://www.semanticscholar.org/paper/a1a43a1eed2f33f59a0c3c86ae2a703bd13f7ca7", "relevance": 1, "abstract": "\n The remarkable ability of large language models (LLMs) to comprehend, interpret, and generate complex language has rapidly integrated LLM-generated text into various aspects of daily life, where users increasingly accept it. However, the growing reliance on LLMs underscores the urgent need for effective detection mechanisms to identify LLM-generated text. Such mechanisms are critical to mitigating misuse and safeguarding domains like artistic expression and social networks from potential negative consequences. LLM-generated text detection, conceptualised as a binary classification task, seeks to determine whether an LLM produced a given text. Recent advances in this field stem from innovations in watermarking techniques, statistics-based detectors, and neural-based detectors. Human- Assisted methods also play a crucial role. In this survey, we consolidate recent research breakthroughs in this field, emphasising the urgent need to strengthen detector research. Additionally, we review existing datasets, highlighting their limitations and developmental requirements. Furthermore, we examine various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and ineffective evaluation frameworks. Finally, we outline intriguing directions for future research in LLM-generated text detection to advance responsible artificial intelligence (AI). This survey aims to provide a clear and comprehensive introduction for newcomers while offering seasoned researchers valuable updates in the field.", "citations": 64}
{"title": "Towards Optimal Statistical Watermarking", "year": 2023, "authors": "Baihe Huang, Banghua Zhu, Hanlin Zhu, Jason D. Lee, Jiantao Jiao, Michael I. Jordan", "url": "https://www.semanticscholar.org/paper/7a9a7610dbc5e406961e666f4e47c49604c38e22", "relevance": 1, "abstract": "We study statistical watermarking by formulating it as a hypothesis testing problem, a general framework which subsumes all previous statistical watermarking methods. Key to our formulation is a coupling of the output tokens and the rejection region, realized by pseudo-random generators in practice, that allows non-trivial trade-offs between the Type I error and Type II error. We characterize the Uniformly Most Powerful (UMP) watermark in the general hypothesis testing setting and the minimax Type II error in the model-agnostic setting. In the common scenario where the output is a sequence of $n$ tokens, we establish nearly matching upper and lower bounds on the number of i.i.d. tokens required to guarantee small Type I and Type II errors. Our rate of $\\Theta(h^{-1} \\log (1/h))$ with respect to the average entropy per token $h$ highlights potentials for improvement from the rate of $h^{-2}$ in the previous works. Moreover, we formulate the robust watermarking problem where the user is allowed to perform a class of perturbations on the generated texts, and characterize the optimal Type II error of robust UMP tests via a linear programming problem. To the best of our knowledge, this is the first systematic statistical treatment on the watermarking problem with near-optimal rates in the i.i.d. setting, which might be of interest for future works.", "citations": 24}
{"title": "Securing Large Language Models: Addressing Bias, Misinformation, and Prompt Attacks", "year": 2024, "authors": "Benji Peng, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Junyu Liu, Qian Niu", "url": "https://api.semanticscholar.org/CorpusId:272599907", "relevance": 1, "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities across various fields, yet their increasing use raises critical security concerns. This article reviews recent literature addressing key issues in LLM security, with a focus on accuracy, bias, content detection, and vulnerability to attacks. Issues related to inaccurate or misleading outputs from LLMs is discussed, with emphasis on the implementation from fact-checking methodologies to enhance response reliability. Inherent biases within LLMs are critically examined through diverse evaluation techniques, including controlled input studies and red teaming exercises. A comprehensive analysis of bias mitigation strategies is presented, including approaches from pre-processing interventions to in-training adjustments and post-processing refinements. The article also probes the complexity of distinguishing LLM-generated content from human-produced text, introducing detection mechanisms like DetectGPT and watermarking techniques while noting the limitations of machine learning enabled classifiers under intricate circumstances. Moreover, LLM vulnerabilities, including jailbreak attacks and prompt injection exploits, are analyzed by looking into different case studies and large-scale competitions like HackAPrompt. This review is concluded by retrospecting defense mechanisms to safeguard LLMs, accentuating the need for more extensive research into the LLM security field.", "citations": 31}
{"title": "Watermark Smoothing Attacks against Language Models", "year": 2024, "authors": "Hongyan Chang, Hamed Hassani, Reza Shokri", "url": "https://www.semanticscholar.org/paper/ebe7099fb2f0844695a90b2fabd4ac4901bd1a8d", "relevance": 1, "abstract": "Watermarking is a key technique for detecting AI-generated text. In this work, we study its vulnerabilities and introduce the Smoothing Attack, a novel watermark removal method. By leveraging the relationship between the model's confidence and watermark detectability, our attack selectively smoothes the watermarked content, erasing watermark traces while preserving text quality. We validate our attack on open-source models ranging from $1.3$B to $30$B parameters on $10$ different watermarks, demonstrating its effectiveness. Our findings expose critical weaknesses in existing watermarking schemes and highlight the need for stronger defenses.", "citations": 6}
{"title": "Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation", "year": 2025, "authors": "Xin Yi, Shunfan Zheng, Linlin Wang, Xiaoling Wang, Liang He", "url": "https://www.semanticscholar.org/paper/f0a678a8b3ca1db2409494066982c13ac5a495ec", "relevance": 1, "abstract": "Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.", "citations": 2}
{"title": "Detection of Large Language Model-Generated Text: A Comprehensive Review", "year": 2025, "authors": "Doaa Mostafa, Sally S. Ismail, Mostafa Aref", "url": "https://www.semanticscholar.org/paper/1624206fea3c09927cfccba62e28c61c617f3e0d", "relevance": 1, "abstract": "With the rapid advancement of large language models (LLMs), extremely complex texts that are identical to human-written ones have been generated. However, this also raises risks such as misinformation and academic dishonesty. As the responsible usage of LLMs becomes increasingly critical, identifying content generated by LLMs has become an essential task. Despite various proposed detection methods, a thorough understanding of their successes and limitations is still lacking. This paper presents a review of the literature on detecting texts generated by LLMs. LLM-generated text detection approaches are neural-based, feature-based, watermarking, and humanassisted; this paper highlights these approaches. This paper highlights significant challenges of existing detection approaches, such as robustness against text perturbation, generalization across domains and models, lack of interpretability, reliance on model access constraints (black-box vs. white-box), scalability limitations, computational costs, sensitivity to text length, and data scarcity for training. Furthermore, the quick development of LLMs continues to pose evolving challenges to detection techniques.", "citations": 0}
{"title": "Stealing Watermarks of Large Language Models via Mixed Integer Programming", "year": 2024, "authors": "Zhaoxi Zhang, Xiaomei Zhang, Yanjun Zhang, Leo Yu Zhang, Chao Chen, Shengshan Hu, Asif Gill, Shirui Pan", "url": "https://www.semanticscholar.org/paper/a6854f91dbad0b39a0289872ac838545a9c18225", "relevance": 1, "abstract": "The Large Language Model (LLM) watermark is a newly emerging technique that shows promise in addressing concerns surrounding LLM copyright, monitoring AI-generated text, and preventing its misuse. The LLM watermark scheme commonly includes generating secret keys to partition the vocabulary into green and red lists, applying a perturbation to the logits of tokens in the green list to increase their sampling likelihood, thus facilitating watermark detection to identify AI-generated text if the proportion of green tokens exceeds a threshold. However, recent research indicates that watermarking methods using numerous keys are susceptible to removal attacks, such as token editing, synonym substitution, and paraphrasing, with robustness declining as the number of keys increases. Therefore, the state-of-the-art watermark schemes that employ fewer or single keys have been demonstrated to be more robust against text editing and paraphrasing. In this paper, we propose a novel green list stealing attack against the state-of-the-art LLM watermark scheme and systematically examine its vulnerability to this attack. We formalize the attack as a mixed integer programming problem with constraints. We evaluate our attack under a comprehensive threat model, including an extreme scenario where the attacker has no prior knowledge, lacks access to the watermark detector API, and possesses no information about the LLM\u2019s parameter settings or watermark injection/detection scheme. Extensive experiments on LLMs, such as OPT and LLaMA, demonstrate that our attack can successfully steal the green list and remove the watermark across all settings.", "citations": 3}
{"title": "Large Language Models and Forensic Linguistics: Navigating Opportunities and Threats in the Age of Generative AI", "year": 2025, "authors": "George Mikros", "url": "https://www.semanticscholar.org/paper/996f04913c68dc4de8509564fe5091241c70f1a5", "relevance": 1, "abstract": "Large language models (LLMs) present a dual challenge for forensic linguistics. They serve as powerful analytical tools enabling scalable corpus analysis and embedding-based authorship attribution, while simultaneously destabilising foundational assumptions about idiolect through style mimicry, authorship obfuscation, and the proliferation of synthetic texts. Recent stylometric research indicates that LLMs can approximate surface stylistic features yet exhibit detectable differences from human writers, a tension with significant forensic implications. However, current AI-text detection techniques, whether classifier-based, stylometric, or watermarking approaches, face substantial limitations: high false positive rates for non-native English writers and vulnerability to adversarial strategies such as homoglyph substitution. These uncertainties raise concerns under legal admissibility standards, particularly the Daubert and Kumho Tire frameworks. The article concludes that forensic linguistics requires methodological reconfiguration to remain scientifically credible and legally admissible. Proposed adaptations include hybrid human-AI workflows, explainable detection paradigms beyond binary classification, and validation regimes measuring error and bias across diverse populations. The discipline's core insight, i.e., that language reveals information about its producer, remains valid but must accommodate increasingly complex chains of human and machine authorship.", "citations": 0}
{"title": "A Survey on AI-Generated Text Detection: Methods, Challenges, and Future Directions", "year": 2025, "authors": "Ahmed Ismail, Cherry A. Ezzat, Abeer M. El-Korany", "url": "https://www.semanticscholar.org/paper/8efe77cb890b156f81496a0c6610e11436333220", "relevance": 1, "abstract": "Recent advancements in large language models (LLMs) have unlocked unprecedented capabilities in generating human-like text and introducing new challenges and opportunities, especially when distinguishing between AIgenerated text (AIGT) and human-authored content. Existing solutions, including watermarking, statistical analysis, stylistic detection, and machine learning-based approaches are promising for detecting AIGT. Still, When it comes to combating text obfuscation techniques such as adversarial attacks and other strategies intended to mislead text origin, these methods often encounter significant obstacles. This survey focuses on the effectiveness of current detection methods in the presence of attacks such as paraphrasing. We investigate the limitations of existing detectors, their robustness against adversarial strategies and the changing field of detection methods. We also explore the role of datasets in developing robust detectors and the challenges they face in simulating real-world scenarios. From recent research work, we aim to provide insights to improve detection methods on post-attack text, identify gaps in the current state-of-the-art, and offer actionable recommendations for future work. Our study highlights the need to advance detection solutions to ensure trust and responsibility in AI-generated text systems.", "citations": 0}
{"title": "Watermarking Vision-Language Models", "year": 2024, "authors": "Shan Wan, Wu Liu, Yijun Liu, Feiniu Yuan, Chunli Meng", "url": "https://www.semanticscholar.org/paper/62ee2210dee60aa185bbd88ce796419f18abba75", "relevance": 1, "abstract": "As the use of visual-language models (VLMs) increases, it is important to prevent the spread of AI-generated fake news. Additionally, due to concerns over model theft and abuse, the demand for intellectual property protection of VLMs is on the rise. Watermarking presents a viable solution to address these concerns. However, previous watermarking methods for multimodal models have limitations, either being constrained by specific model structures or application scenarios, or relying on backdoor techniques prone to false activations, and often require access to the model, leading to inefficiencies and making it difficult to trace suspicious content solely based on output text. In this paper, we propose a new watermarking strategy specifically designed for large-scale VLMs. Our method embeds watermarks into multimodal models without requiring retraining and offers strong generalizability, and enabling detection without accessing the model itself. It starts by partitioning the sampling vocabulary using a key generated by an private key generator. Then, by altering the output sampling, we embed a watermark into the text that is easily detectable. Extensive experiments demonstrate that our method achieves over 90% watermark detection rate eliminating false positives, minimizes performance impact, and remains robust against random attacks.", "citations": 0}
{"title": "From Text to Source: Results in Detecting Large Language Model-Generated Content", "year": 2023, "authors": "Wissam Antoun, Beno\u00eet Sagot, Djam\u00e9 Seddah", "url": "https://www.semanticscholar.org/paper/a0012cf12186fde248dde4cccdccb74e83624ac3", "relevance": 1, "abstract": "The widespread use of Large Language Models (LLMs), celebrated for their ability to generate human-like text, has raised concerns about misinformation and ethical implications. Addressing these concerns necessitates the development of robust methods to detect and attribute text generated by LLMs. This paper investigates \u201cCross-Model Detection,\u201d by evaluating whether a classifier trained to distinguish between source LLM-generated and human-written text can also detect text from a target LLM without further training. The study comprehensively explores various LLM sizes and families and assesses the impact of conversational fine-tuning techniques, quantization, and watermarking on classifier generalization. The research also explores Model Attribution, encompassing source model identification, model family, and model size classification, in addition to quantization and watermarking detection. Our results reveal several key findings: a clear inverse relationship between classifier effectiveness and model size, with larger LLMs being more challenging to detect, especially when the classifier is trained on data from smaller models. Training on data from similarly sized LLMs can improve detection performance from larger models but may lead to decreased performance when dealing with smaller models. Additionally, model attribution experiments show promising results in identifying source models and model families, highlighting detectable signatures in LLM-generated text, with particularly remarkable outcomes in watermarking detection, while no detectable signatures of quantization were observed. Overall, our study contributes valuable insights into the interplay of model size, family, and training data in LLM detection and attribution.", "citations": 20}
