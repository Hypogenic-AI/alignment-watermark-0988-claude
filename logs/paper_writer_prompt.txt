You are an academic paper writer. Generate a complete NEURIPS style paper
based on the experiment results provided.

════════════════════════════════════════════════════════════════════════════════
                         IMPORTANT: BEFORE YOU START
════════════════════════════════════════════════════════════════════════════════

Before writing any content, you MUST complete these steps:

1. READ THE SKILL: Review the paper-writer skill at templates/skills/paper-writer/SKILL.md
2. READ THE STYLE GUIDE: Study templates/paper_writing/lab_style_guide.md carefully
3. REVIEW EXAMPLES: Browse paper_examples/ for formatting and language patterns:
   - Look at sections/1.introduction.tex for language style
   - Look at tables/*.tex for table formatting
   - Look at commands/*.tex for macro usage
4. USE COMMAND TEMPLATES: Copy templates/paper_writing/commands/ to paper_draft/commands/

CRITICAL: Reference example papers for FORMATTING and LANGUAGE STYLE only.
Do NOT copy content, phrasing, or narrative structure from the example papers.
The examples are in a different research domain - focus only on presentation style.

════════════════════════════════════════════════════════════════════════════════
                            EXPERIMENT REPORT
════════════════════════════════════════════════════════════════════════════════

# Alignment is the Watermark: Research Report

## 1. Executive Summary

**Research question:** Does alignment training create an inherent, detectable distributional signature—an implicit watermark—that makes aligned LLM outputs more AI-detectable than base model outputs?

**Key finding:** Yes. Across three model families (Mistral, MPT, Cohere), three detection paradigms (distributional features, statistical zero-shot detection, and LLM-as-detector), and 12 independent comparisons, aligned models were more detectable than their base counterparts in 11 of 12 cases (sign test p=0.006). The strongest effect was observed with zero-shot statistical detection, where alignment increased AUROC by 10-35 percentage points.

**Practical implications:** Alignment training—the very process that makes LLMs helpful, harmless, and honest—simultaneously creates a robust, implicit watermark. This means that detection of aligned AI text is not a temporary arms race but a fundamental consequence of making AI useful. Policymakers and the detection community should recognize that alignment itself is the most durable form of AI text watermarking.

---

## 2. Goal

### Hypothesis
Base language models trained solely to match data distributions are not easily AI-detectable, as they approximate the human text distribution. Aligned models—regardless of the specific alignment method—are likely to be AI-detectable because their characteristic language choices, honesty, and competence create measurable distributional shifts from human text. Therefore, while base models may become increasingly undetectable, aligned models will remain inherently detectable, at least by other AIs.

### Why This Matters
1. **AI policy:** If alignment = implicit watermark, mandatory explicit watermarking may be redundant for aligned models
2. **Detection research:** Understanding *why* detection works (alignment signatures) is more valuable than building better detectors
3. **AI safety:** The detectability of aligned models provides a natural accountability mechanism
4. **Fundamental insight:** There is an inherent tension between making AI useful (alignment) and making AI undetectable

### Expected Impact
This work provides empirical evidence for a conceptual reframing: alignment training is not merely a target for detection—it *is* the watermark. This shifts the paradigm from &#34;can we detect AI text?&#34; to &#34;alignment guarantees detectability.&#34;

---

## 3. Data Construction

### Dataset Description
We used the **RAID benchmark** (Dugan et al., ACL 2024), the largest publicly available AI text detection dataset with 6.2M+ generations across 11 LLMs.

**Source:** HuggingFace (`liamdugan/raid`)
**Domain:** Scientific abstracts
**Attack status:** Clean only (`attack=&#39;none&#39;`) to avoid confounding adversarial modifications with alignment signals

### Key Feature: Base/Chat Model Pairs
RAID uniquely provides texts from the same model architecture at different alignment stages:

| Base Model | Aligned Variant | Texts per Variant |
|------------|----------------|-------------------|
| Mistral-7B | Mistral-7B Chat | 500 each |
| MPT-30B | MPT-30B Chat | 500 each |
| Cohere Command | Cohere Chat | 500 each |

**Human baseline:** 500 human-written scientific abstracts.

### Example Samples

**Human text (first 200 chars):**
&gt; &#34;[Typical scientific abstract with varied sentence structure, domain-specific jargon, and natural flow]&#34;

**Base model text (Mistral, first 200 chars):**
&gt; &#34;Artificial intelligence (AI) is increasingly being used in medical imaging to improve diagnostic accuracy...&#34;

**Aligned model text (Mistral Chat, first 200 chars):**
&gt; &#34;The development of trustworthy artificial intelligence (AI) is crucial for the future of medical imaging...&#34;

Note the aligned model&#39;s characteristic framing language (&#34;trustworthy,&#34; &#34;crucial for the future&#34;).

### Data Quality
- All 6,000 samples passed validation (500 per model × 12 models)
- No missing values in text or metadata
- Each model contributes exactly 500 texts (250 greedy + 250 sampling decoding)
- Texts range from ~50 to ~500 tokens

### Preprocessing
1. Loaded all 6,000 clean samples from `raid_clean_samples.json`
2. Organized by model category (human, base_X, aligned_X)
3. No text modification—raw generations used as-is for ecological validity

---

## 4. Experiment Description

### Methodology

#### High-Level Approach
We employed three complementary detection paradigms to test the alignment watermark hypothesis:

1. **Distributional Feature Analysis (Experiment 1):** Measures *how* alignment changes text properties
2. **Zero-Shot Statistical Detection (Experiment 2):** Tests *whether* these changes enable detection
3. **LLM-as-Detector (Experiment 3):** Tests whether aligned models are detectable &#34;by other AIs&#34;
4. **Cross-Family Generalization (Experiment 4):** Tests *consistency* across model families

#### Why This Multi-Method Approach?
- Single methods can be confounded by artifacts
- Different detection paradigms test different aspects of the hypothesis
- Convergent evidence across methods strengthens conclusions
- The specific claim about AI-to-AI detection requires testing with actual LLMs

### Implementation Details

#### Tools and Libraries
| Library | Version | Purpose |
|---------|---------|---------|
| Python | 3.12.8 | Runtime |
| PyTorch | 2.10.0+cu128 | GPU computation |
| Transformers | 5.1.0 | Model loading |
| scikit-learn | 1.8.0 | AUROC, metrics |
| SciPy | 1.17.0 | Statistical tests |
| OpenAI | 2.20.0 | GPT-4.1 API |

#### Hardware
- 4× NVIDIA RTX A6000 (48GB each)
- GPU 0 used for Experiment 2 (GPT-2 Large inference)

#### Hyperparameters
| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Random seed | 42 | Reproducibility |
| Max tokens (Exp 2) | 512 | Balance coverage vs. speed |
| Reference model (Exp 2) | GPT-2 Large (774M) | Widely-used detection reference model |
| LLM detector (Exp 3) | GPT-4.1 | State-of-the-art reasoning model |
| Samples per category (Exp 3) | 80 | Balance cost vs. statistical power |
| Temperature (Exp 3) | 0.0 | Deterministic classification |
| Bootstrap iterations | 1,000 | Confidence interval estimation |
| Significance level | α=0.05 | Standard; Bonferroni-corrected where applicable |

---

### Experiment 1: Distributional Feature Analysis

**Purpose:** Quantify how alignment changes text distributions.

**Features computed per text:**
- Type-token ratio (vocabulary richness)
- Distinct 1/2/3-gram ratios (lexical diversity)
- Mean and std sentence length
- Mean word length
- Hapax legomena ratio (proportion of words appearing once)
- Token count

**Statistical tests:** Mann-Whitney U (non-parametric) with Bonferroni correction; Cohen&#39;s d effect sizes.

### Experiment 2: Zero-Shot Statistical Detection

**Purpose:** Test whether alignment increases detectability by standard zero-shot methods.

**Method:** GPT-2 Large computes per-token log-probabilities for each text. Three detection signals are extracted:
- **Mean log-probability:** Higher values = text more expected by the model = more AI-like
- **Mean log-rank:** Lower values = tokens are highly ranked = more AI-like
- **Mean entropy:** Lower values = model is more certain = more AI-like

**Evaluation:** AUROC for distinguishing human text from each AI source, with bootstrap 95% CIs.

### Experiment 3: LLM-as-Detector

**Purpose:** Test the specific hypothesis claim that aligned models are &#34;detectable by other AIs.&#34;

**Method:** GPT-4.1 classifies 80 texts per category (560 total) as &#34;human&#34; or &#34;AI-generated&#34; with a confidence score.

**Prompt design:** Zero-shot classification prompt asking GPT-4.1 to analyze writing style, vocabulary diversity, naturalness, and formulaic patterns.

**Evaluation:** True positive rate (correctly identifying AI text as AI), AUROC, and two-proportion z-test comparing base vs. aligned detection rates.

---

### Raw Results

#### Experiment 1: Distributional Features (Base vs. Aligned, Cohen&#39;s d)

| Feature | Mistral | MPT | Cohere | Direction |
|---------|---------|-----|--------|-----------|
| Type-token ratio | +0.57 | +0.64 | -0.29 | Mixed |
| Distinct 2-gram | +0.92 | +1.13 | -0.28 | Mixed |
| Distinct 3-gram | +1.02 | +1.21 | -0.26 | Mixed |
| Mean sent. length | -0.11 | -0.52 | +0.08 | Mixed |
| Std sent. length | **-0.66** | **-0.64** | **-0.45** | **Consistent ↓** |
| Mean word length | +0.56 | +0.01 | +0.19 | Mostly ↑ |
| Hapax ratio | +0.68 | +0.75 | -0.27 | Mixed |
| Num tokens | **-1.57** | **-2.25** | **-0.22** | **Consistent ↓** |

**Key finding:** Alignment consistently reduces sentence length variability (std_sent_length d=-0.45 to -0.66) and text length across all families. The diversity measures (distinct n-grams) show alignment *increases* diversity for Mistral/MPT but *decreases* it for Cohere, suggesting family-specific effects.

**Aligned vs. Human comparison:**
- Aligned text has significantly lower sentence length variation than human text (d=-0.65, p&lt;10⁻¹⁴⁴)
- Aligned text is significantly shorter (d=-1.29, p&lt;10⁻¹³⁷)
- Aligned text has higher type-token ratio (d=+0.66, p&lt;10⁻³³) — more varied vocabulary per unit text
- Base text is closer to human text on most diversity metrics (smaller effect sizes)

#### Experiment 2: Zero-Shot Statistical Detection (AUROC)

| Family | Metric | Base AUROC | Aligned AUROC | Delta |
|--------|--------|-----------|---------------|-------|
| **Mistral** | Mean log-prob | 0.514 | **0.831** | **+0.317** |
| **Mistral** | Mean log-rank | 0.535 | **0.881** | **+0.346** |
| Mistral | Mean entropy | 0.635 | 0.724 | +0.088 |
| **MPT** | Mean log-prob | 0.504 | **0.711** | **+0.206** |
| **MPT** | Mean log-rank | 0.506 | **0.740** | **+0.234** |
| MPT | Mean entropy | 0.587 | 0.573 | -0.014 |
| **Cohere** | Mean log-prob | 0.861 | **0.954** | **+0.092** |
| **Cohere** | Mean log-rank | 0.840 | **0.949** | **+0.109** |
| Cohere | Mean entropy | 0.620 | 0.731 | +0.110 |

**Key finding:** Aligned models are dramatically more detectable than base models across ALL three families for the primary detection metrics (mean log-prob and mean log-rank). The base Mistral and MPT models are near-random (AUROC ~0.50-0.54), while their aligned variants reach AUROC 0.71-0.88. Even Cohere, which has a higher baseline detectability (0.84-0.86), shows a further 9-11 point AUROC increase with alignment.

#### Experiment 3: GPT-4.1 as Detector

| Category | True Positive Rate | Mean AI Score |
|----------|-------------------|---------------|
| Human (TNR) | 0.562 | 0.467 |
| Base Mistral | 0.935 | 0.896 |
| **Aligned Mistral** | **1.000** | **0.936** |
| Base MPT | 0.850 | 0.833 |
| **Aligned MPT** | **1.000** | **0.943** |
| Base Cohere | 0.963 | 0.899 |
| **Aligned Cohere** | **0.975** | **0.919** |

| Family | Base TPR | Aligned TPR | Delta | p-value |
|--------|----------|-------------|-------|---------|
| Mistral | 0.935 | 1.000 | +0.065 | 0.021* |
| MPT | 0.850 | 1.000 | +0.150 | 0.0003*** |
| Cohere | 0.963 | 0.975 | +0.013 | 0.650 ns |

**Key finding:** GPT-4.1 detects aligned model text with near-perfect accuracy (97.5-100% TPR) compared to base model text (85-96.3% TPR). The improvement is statistically significant for Mistral (p=0.021) and MPT (p=0.0003). Even base models are highly detectable by GPT-4.1, but aligned models push detection rates to ceiling.

**Important nuance:** GPT-4.1&#39;s TNR on human text is only 56.2%, meaning it frequently labels human scientific abstracts as AI-generated. This suggests the AUROC of ~0.77-0.80 is driven more by separating confidence levels than by perfect binary classification.

#### Experiment 4: Cross-Family Consistency

**Sign test:** 11 of 12 detection comparisons show aligned &gt; base (p=0.006, binomial test).

**Consistent across all families:**
- Mean log-prob AUROC: aligned &gt; base (all 3 families)
- Mean log-rank AUROC: aligned &gt; base (all 3 families)
- GPT-4.1 TPR: aligned ≥ base (all 3 families)

**Not consistent (Cohere diverges):**
- Lexical diversity features (distinct n-grams): Mistral/MPT show increased diversity with alignment, Cohere shows decreased diversity

#### Output Locations
- Results CSV files: `results/data/`
- Visualization plots: `results/plots/`
- Raw GPT-4.1 classifications: `results/data/exp3_raw_classifications.json`
- Configuration: `results/data/exp2_detection_features.csv`

---

## 5. Result Analysis

### Key Findings

1. **Alignment dramatically increases statistical detectability.** The mean log-rank AUROC for detecting AI text (vs. human) increases by +0.11 to +0.35 with alignment, representing the single largest effect in our study. Base Mistral and MPT are essentially undetectable by this metric (AUROC ~0.50), while their aligned versions reach AUROC 0.74-0.88.

2. **Alignment reduces textual variability.** Aligned models consistently produce text with lower sentence length variation (Cohen&#39;s d = -0.45 to -0.66 across all families). This &#34;smoothing&#34; effect creates a detectable regularity—the alignment watermark manifests as predictable, well-structured prose.

3. **State-of-the-art LLMs detect aligned text near-perfectly.** GPT-4.1 achieves 97.5-100% TPR on aligned text vs. 85-96.3% on base text, confirming the hypothesis that aligned models are &#34;detectable by other AIs.&#34;

4. **The alignment watermark generalizes across model families.** The sign test confirms that 11/12 detection comparisons show aligned &gt; base (p=0.006). This is not an artifact of one model—it is a cross-architecture phenomenon.

5. **Base models are closer to human distributions.** Base Mistral and MPT have AUROC near 0.50 for log-probability-based detection, meaning they are statistically indistinguishable from human text by this method. This directly supports the hypothesis that &#34;the ideal base LM is not AI detectable because it is the distribution.&#34;

### Hypothesis Testing Results

| Hypothesis | Supported? | Evidence |
|-----------|-----------|---------|
| **H1: Alignment shifts distributions** | **Yes** | Consistent reduction in sentence length variability (d=-0.45 to -0.66); text length reduction (d=-0.22 to -2.25) |
| **H2: Aligned models more statistically detectable** | **Yes** | AUROC improvement of +0.09 to +0.35 across all families for primary metrics |
| **H3: Detectable by other AIs** | **Yes** | GPT-4.1 TPR: 97.5-100% (aligned) vs. 85-96.3% (base); significant for Mistral (p=0.021) and MPT (p=0.0003) |
| **H4: Cross-family generality** | **Yes** | Sign test: 11/12 positive, p=0.006 |

### Comparison to Prior Work

Our results are consistent with and extend the literature:

| Finding | Our Result | Prior Work |
|---------|-----------|------------|
| AUROC base→aligned | +0.09 to +0.35 | +0.23 (Xu &amp; Zubiaga, 2025; Fast-DetectGPT on Llama) |
| Diversity reduction | d=-0.45 to -0.66 (sent. length std) | 75-90% reduction (Kirk et al., 2024) |
| Reward model detection | N/A (used GPT-4.1 instead) | 95-99% AUROC (Lee et al., 2024) |
| Cross-model generalization | 3 families consistent | First systematic demonstration |

### Surprises and Insights

1. **Cohere base model is already quite detectable** (AUROC 0.84-0.86). This may be because Cohere&#39;s base model already has some instruction tuning or because its pretraining data differs substantially from human scientific abstracts. The alignment delta for Cohere is smaller but still positive.

2. **GPT-4.1 struggles with human text** (TNR = 56.2%). It over-detects, labeling many human scientific abstracts as AI-generated. This is consistent with the observation that modern scientific writing is increasingly formulaic, overlapping with AI text patterns.

3. **Vocabulary diversity is not uniformly reduced by alignment.** Contrary to Kirk et al.&#39;s findings of universal diversity collapse, we found that Mistral-Chat and MPT-Chat actually have *higher* distinct n-gram ratios than their base counterparts, while Cohere-Chat has lower ratios. The consistent alignment watermark is in *structural regularity* (sentence length variation), not necessarily vocabulary narrowing.

4. **Mean entropy is the weakest detection signal.** While log-probability and log-rank show strong alignment effects, entropy barely discriminates between base and aligned models. This suggests the alignment watermark is more about token-level predictability than overall uncertainty.

### Error Analysis

**GPT-4.1 errors on human text:** The 43.8% misclassification of human text as AI reveals that scientific abstracts—a domain with inherent formulaic structure—present a challenging detection scenario. The boundary between &#34;human writing that sounds AI-like&#34; and &#34;AI writing that sounds human-like&#34; is blurrier in formal academic domains.

**Cohere&#39;s different diversity pattern:** Cohere&#39;s base model already has relatively high vocabulary diversity (distinct 2-gram = 0.89) and sentence structure regularity, making the alignment effect smaller. This suggests the alignment watermark strength depends on how far the base model already is from the &#34;aligned distribution.&#34;

### Limitations

1. **Single domain.** All texts are scientific abstracts. Results may differ for creative writing, conversation, or code. Prior work (Xu &amp; Zubiaga, 2025) found that code-mixed text shows reversed effects.

2. **Limited model families.** Three families (Mistral, MPT, Cohere) provide convergent evidence, but testing on Llama, GPT, and Gemini families would strengthen generalizability claims.

3. **Alignment method not controlled.** We compare base vs. aligned but cannot distinguish RLHF vs. SFT vs. DPO effects, as the RAID dataset doesn&#39;t provide intermediate alignment stages.

4. **GPT-2 Large as reference model.** Using a more modern reference model might produce different detection patterns. However, GPT-2 is the standard in the detection literature.

5. **GPT-4.1 as detector has its own biases.** The LLM-as-detector experiment tests whether *one specific* AI can detect alignment, not whether all AIs can. Different detector models might perform differently.

6. **Decoding strategy confound.** The RAID dataset includes both greedy and sampling decoding; we did not separate these. However, since both base and aligned variants use the same decoding mix, this is balanced.

---

## 6. Conclusions

### Summary
Alignment training creates a robust, measurable distributional shift that serves as an implicit watermark for AI-generated text. Across three model families and three complementary detection methods, aligned models are consistently and significantly more detectable than their base counterparts. The alignment watermark manifests primarily as increased structural regularity and higher token-level predictability, making aligned text cluster in distinctive regions of probability space.

### Implications

**Theoretical:** The hypothesis &#34;alignment is the watermark&#34; is strongly supported. The ideal base model approximates the human distribution and approaches undetectability (AUROC ~0.50 for Mistral/MPT). The moment alignment is applied, detectability jumps dramatically. This is not a bug—it is the fundamental consequence of optimizing for human preferences, which necessarily shifts the output distribution away from the natural human distribution.

**Practical:** Explicit watermarking (green/red list, SynthID) may be unnecessary for aligned models, which already carry an inherent signature. Detection efforts should focus on exploiting the alignment watermark rather than imposing external marks that can be removed.

**Policy:** Regulations requiring AI text watermarking should recognize that alignment itself provides a durable, cross-family detection signal. The real detection challenge is base models, not aligned models.

### Confidence in Findings
**High confidence** in the direction of the effect: alignment increases detectability. This is supported by convergent evidence across methods, families, and metrics, and is consistent with the prior literature.

**Moderate confidence** in the magnitude: specific AUROC values depend on domain, reference model, and text length. The absolute numbers should be interpreted with caution.

**Lower confidence** in universality: we tested one domain and three model families. More diverse testing is needed.

---

## 7. Next Steps

### Immediate Follow-ups
1. **Multi-domain testing:** Run the same experiments on creative writing, conversational, and code domains to test domain generality.
2. **Alignment stage decomposition:** Test base → SFT → RLHF/DPO progression on Llama-3 to isolate which alignment stage contributes most.
3. **Adversarial robustness:** Test whether paraphrasing, back-translation, or prompt engineering can erase the alignment watermark.

### Alternative Approaches
- **Probing classifiers:** Train lightweight classifiers on activation patterns of base vs. aligned models to understand *where* in the model the alignment watermark resides.
- **Reward model as detector:** Implement the ReMoDetect approach (Lee et al., 2024) and compare with our zero-shot methods.
- **Information-theoretic measurement:** Estimate the total variation distance between base and aligned model distributions to provide a formal bound on detectability.

### Broader Extensions
- Extend to multimodal models (image generation, audio)
- Study whether alignment watermarks in different languages show consistent patterns
- Investigate whether fine-tuning for specific tasks (code, math) creates similar watermarks

### Open Questions
1. **Is there a minimum alignment strength below which the watermark disappears?** Can light alignment be undetectable?
2. **Can adversarial training remove the alignment watermark while preserving helpfulness?** Is there a fundamental trade-off?
3. **Do different alignment objectives (helpful vs. harmless vs. honest) create different watermark signatures?**
4. **As base models improve, will the alignment watermark shrink or remain constant?**

---

## References

1. Xu, B. &amp; Zubiaga, A. (2025). Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts. arXiv:2503.17965.
2. Kirk, R. et al. (2024). Understanding the Effects of RLHF on LLM Generalisation and Diversity. ICLR 2024. arXiv:2310.06452.
3. Lee, J., Tack, J., &amp; Shin, J. (2024). ReMoDetect: Reward Models Recognize Aligned LLM&#39;s Generations.
4. Chakraborty, M. et al. (2023). On the Possibilities of AI-Generated Text Detection. arXiv:2304.04736.
5. Mitchell, E. et al. (2023). DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. ICML 2023.
6. Bao, G. et al. (2024). Fast-DetectGPT: Efficient Zero-Shot Detection via Conditional Probability Curvature. ICLR 2024.
7. Dugan, L. et al. (2024). RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors. ACL 2024.
8. Kirchenbauer, J. et al. (2023). A Watermark for Large Language Models. ICML 2023.
9. Dathathri, S. et al. (2024). Scalable Watermarking for Identifying Large Language Model Outputs. Nature.
10. Ouyang, L. et al. (2022). Training Language Models to Follow Instructions with Human Feedback. NeurIPS 2022.

---

## Appendix: Reproducibility Information

- **Python:** 3.12.8
- **Hardware:** 4× NVIDIA RTX A6000 (48GB)
- **GPU used:** GPU 0 (Experiment 2)
- **Random seed:** 42
- **Total execution time:** ~15 minutes (Exp 1: 30s, Exp 2: 4m, Exp 3: 8m, Exp 4: 5s)
- **API costs:** ~$2 (560 GPT-4.1 calls)
- **Dataset:** RAID benchmark (liamdugan/raid), 6,000 clean samples


════════════════════════════════════════════════════════════════════════════════
                            RESEARCH PLAN
════════════════════════════════════════════════════════════════════════════════

# Research Plan: Alignment is the Watermark

## Motivation &amp; Novelty Assessment

### Why This Research Matters
AI-generated text detection is increasingly critical for content integrity, education, and trust. Current detection methods are designed as external tools, but this research reframes the problem: alignment training itself—the process that makes LLMs helpful, harmless, and honest—inevitably creates a detectable distributional shift. If true, this means detection of aligned AI text is not merely a temporary arms race but a fundamental consequence of making AI useful. This insight has profound implications for AI policy, watermarking research, and the detection community.

### Gap in Existing Work
Based on the literature review, while individual papers have shown:
- RLHF increases detectability (Xu &amp; Zubiaga, 2025)
- RLHF collapses diversity (Kirk et al., 2024)
- Reward models can serve as detectors (Lee et al., 2024)
- Detection is theoretically possible when TV &gt; 0 (Chakraborty et al., 2023)

No paper has:
1. **Systematically tested the thesis that alignment = implicit watermark** across multiple model families using multiple detection methods
2. **Quantified the distributional gap** between base and aligned models using information-theoretic measures
3. **Used state-of-the-art LLMs as AI detectors** to test whether aligned models are more detectable by other AIs (as the hypothesis specifically states &#34;at least by other AIs&#34;)
4. **Connected text-level statistical features** (vocabulary diversity, perplexity, burstiness) to detection performance in a unified framework

### Our Novel Contribution
We conduct a multi-method, multi-model empirical study that:
1. Compares base vs. aligned model texts on distributional features (linguistic fingerprints)
2. Tests multiple detection approaches on base vs. aligned model pairs from the RAID benchmark
3. Uses a state-of-the-art LLM (GPT-4.1) as an AI detector to test the &#34;detectable by other AIs&#34; claim
4. Quantifies the alignment watermark strength across model families

### Experiment Justification
- **Experiment 1 (Distributional Analysis):** Measures HOW alignment changes text distributions — vocabulary diversity, sentence structure, perplexity. Establishes that alignment creates a measurable distributional shift.
- **Experiment 2 (Statistical Detection):** Tests WHETHER this distributional shift translates to actual detectability differences. Uses zero-shot statistical detectors (perplexity, entropy, log-rank) on base vs. aligned pairs.
- **Experiment 3 (AI-as-Detector):** Tests the specific claim that aligned models are &#34;detectable by other AIs.&#34; Uses GPT-4.1 as a classifier on base vs. aligned vs. human texts.
- **Experiment 4 (Cross-Family Generalization):** Tests whether the alignment watermark generalizes across model families (Mistral, MPT, Cohere) or is family-specific.

---

## Research Question
Does alignment training (RLHF, instruction tuning) create an inherent, detectable distributional shift—an implicit watermark—that makes aligned LLM outputs more AI-detectable than base model outputs, and is this detectable by other AIs?

## Hypothesis Decomposition

**H1 (Distributional Shift):** Aligned models produce text with lower vocabulary diversity, lower perplexity variance, and more formulaic structure compared to their base counterparts.

**H2 (Increased Detectability):** Aligned model outputs are significantly more detectable than base model outputs by zero-shot statistical detection methods.

**H3 (AI Detection):** A state-of-the-art LLM can distinguish aligned model text from human text with significantly higher accuracy than it can distinguish base model text from human text.

**H4 (Cross-Family Generality):** The alignment watermark phenomenon is consistent across different model families (not specific to one architecture).

## Proposed Methodology

### Approach
We use the RAID benchmark&#39;s base/chat model pairs (Mistral, MPT, Cohere) which provide a controlled comparison: same architecture, same prompts, different training. We apply three complementary detection paradigms:

1. **Linguistic feature analysis** — measure textual properties that differ between base/aligned/human text
2. **Statistical detection metrics** — apply established zero-shot detection signals (perplexity, entropy, log-rank)
3. **LLM-as-judge detection** — use GPT-4.1 to classify texts as human vs. AI

### Experimental Steps

1. **Data Preparation**
   - Load RAID base/chat pairs (Mistral, MPT, Cohere) — 500 samples each
   - Load human baseline (500 samples)
   - Compute text statistics (length, tokens, etc.)

2. **Experiment 1: Distributional Feature Analysis**
   - Compute per-text features: distinct n-gram ratios (1,2,3-gram), vocabulary richness (type-token ratio), mean word length, sentence length distribution, perplexity (using a small reference model)
   - Compare distributions across: human, base, aligned
   - Statistical tests: Mann-Whitney U for each feature between base and aligned

3. **Experiment 2: Zero-Shot Statistical Detection**
   - Use a local reference model (e.g., GPT-2 or small Llama) to compute per-token log-probabilities for each text
   - Compute detection signals: mean log-prob, log-rank, entropy
   - Evaluate AUROC for distinguishing human vs. base and human vs. aligned
   - Compare: is AUROC(human vs aligned) &gt; AUROC(human vs base)?

4. **Experiment 3: LLM-as-Detector**
   - Sample 100 texts from each category (human, base-Mistral, chat-Mistral, base-MPT, chat-MPT, base-Cohere, chat-Cohere)
   - Prompt GPT-4.1 to classify each text as &#34;human&#34; or &#34;AI-generated&#34; with confidence
   - Compute accuracy, AUROC for each source category
   - Test: is detection accuracy higher for aligned vs. base?

5. **Experiment 4: Cross-Family Analysis**
   - Aggregate results from Experiments 1-3 across all three model families
   - Test consistency with paired comparisons
   - Identify family-specific vs. universal alignment signatures

### Baselines
- **Human text** (ground truth, should be least detectable)
- **Base model text** (no alignment, our &#34;control&#34;)
- **Random baseline** (AUROC = 0.5, accuracy = 50%)

### Evaluation Metrics
- **AUROC** — primary metric, threshold-independent discrimination
- **Accuracy at 50% threshold** — practical binary classification
- **Cohen&#39;s d** — effect size for feature differences
- **Distinct n-gram ratios** — diversity measure (from Kirk et al.)
- **Type-token ratio** — vocabulary richness

### Statistical Analysis Plan
- **Mann-Whitney U test** for comparing feature distributions (non-parametric, handles non-normal data)
- **Bootstrap 95% confidence intervals** for AUROC estimates
- **Bonferroni correction** for multiple comparisons
- **Alpha = 0.05** significance level
- **Effect sizes** reported for all comparisons

## Expected Outcomes
- **H1 supported:** Aligned models show 30-60% lower vocabulary diversity than base models (per Kirk et al. findings)
- **H2 supported:** AUROC for detecting aligned models is 10-20 points higher than for base models (per Xu &amp; Zubiaga)
- **H3 supported:** GPT-4.1 achieves &gt;80% accuracy on aligned text vs. ~60% on base text
- **H4 supported:** Pattern is consistent across Mistral, MPT, and Cohere families

## Timeline and Milestones
1. Environment setup &amp; data loading: 15 min
2. Experiment 1 (distributional analysis): 30 min
3. Experiment 2 (statistical detection with local model): 45 min
4. Experiment 3 (LLM-as-detector): 30 min
5. Experiment 4 (cross-family analysis): 15 min
6. Analysis &amp; visualization: 30 min
7. Documentation: 30 min

## Potential Challenges
- **GPU memory:** We have 4x A6000s — ample for running local models
- **API costs:** Experiment 3 uses ~700 API calls to GPT-4.1; estimated $5-15
- **Base model quality:** RAID base model texts may be low quality, making the comparison partially about quality vs. alignment specifically

## Success Criteria
1. Clear, statistically significant difference in distributional features between base and aligned texts
2. Higher AUROC for detecting aligned vs. base model text across multiple detection methods
3. GPT-4.1 demonstrates higher classification accuracy on aligned text than base text
4. Results replicate across at least 2 of 3 model families


════════════════════════════════════════════════════════════════════════════════
                          LITERATURE REVIEW
════════════════════════════════════════════════════════════════════════════════

# Literature Review: Alignment is the Watermark

## Research Hypothesis

&gt; Base language models trained solely to match data distributions are not AI detectable, as they replicate the distribution itself. In contrast, aligned models -- regardless of the specific definition of alignment -- are likely to be AI detectable due to their characteristic choices in language, honesty, and competence.

This review synthesizes findings from 26 papers spanning AI-generated text detection, alignment training (RLHF/DPO/SFT), text watermarking, and distributional analysis of LLM outputs. The literature is organized around the central question: **does alignment training create an inherent, detectable signature in LLM outputs?**

---

## 1. Core Evidence: Alignment Increases Detectability

### 1.1 Xu &amp; Zubiaga (2025) -- Direct Evidence

**Paper:** &#34;Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts&#34; (arXiv:2503.17965)

This is the most directly relevant paper, testing the exact hypothesis. Using Llama-7B at three alignment stages (base, SFT, PPO), the authors measure both output quality and detectability.

**Key results for instruction following (general text):**

| Detector | Llama (base) | Llama_SFT | Llama_PPO |
|----------|-------------|-----------|-----------|
| Fast-DetectGPT (AUROC) | 0.68 | 0.80 | **0.91** |
| GPTZero (AUROC) | 0.59 | 0.70 | **0.81** |

The monotonic increase from base to PPO -- a 23-point AUROC improvement for Fast-DetectGPT -- provides direct evidence that alignment progressively increases detectability.

**Diversity collapse as mechanism:**
- Distinct n-gram scores: base=0.42, SFT=0.24, PPO=**0.18** (57% reduction)
- RLHF produces &#34;lengthier and more repetitive&#34; outputs with &#34;decreased syntactic and semantic diversity&#34;
- The model &#34;internalizes and reproduces features inherently associated with LLM generation&#34;

**Quality-detectability trade-off:** PPO achieves 20.62% AlpacaEval win rate (vs. 1.14% for base) while simultaneously becoming far more detectable. Better alignment = better quality = more detectable.

**Caveat:** For question answering with mixed code/text (StackExchange), detectability *decreases* with alignment (Fast-DetectGPT AUROC: 0.91 base -&gt; 0.84 PPO), suggesting the effect is strongest for natural language generation.

### 1.2 Kirk et al. (2024) -- RLHF Diversity Collapse

**Paper:** &#34;Understanding the Effects of RLHF on LLM Generalisation and Diversity&#34; (ICLR 2024, arXiv:2310.06452)

This paper provides the most rigorous empirical evidence that RLHF systematically narrows the output distribution. While not focused on detection, its findings have profound implications.

**Per-input diversity collapse (LLaMA 7B, summarization):**
- EAD (distinct n-grams): RLHF ~0.2 vs. SFT ~0.8 (**75% reduction**)
- SentenceBERT diversity: RLHF ~0.05 vs. SFT ~0.5 (**90% reduction**)

**Across-input mode collapse (first rigorous demonstration):**
- Even for different inputs, RLHF outputs converge to a consistent &#34;mode&#34;
- EAD: RLHF ~0.83 vs. SFT ~0.89
- The authors state this is &#34;the first rigorous empirical demonstration of across-input mode collapse emerging from RLHF training&#34;

**Critical findings:**
1. The KL penalty (standard RLHF countermeasure) **fails to recover diversity** -- increasing it actually reduces per-input diversity further
2. Best-of-N sampling preserves diversity while improving quality, localizing the collapse to PPO optimization specifically
3. Effects are consistent across model scales (LLaMA 7B and OPT at multiple sizes)

**Implication for detection:** Reduced per-input diversity means more predictable patterns. Across-input mode collapse creates a consistent stylistic fingerprint. Both are exploitable detection signals.

### 1.3 Lee, Tack &amp; Shin (2024) -- ReMoDetect

**Paper:** &#34;ReMoDetect: Reward Models Recognize Aligned LLM&#39;s Generations&#34;

This paper demonstrates that reward models -- the very models trained during RLHF -- can serve as highly effective AI text detectors.

**Core insight:** Alignment training pushes LLM outputs into high-reward regions of text space. A reward model can detect this by simply thresholding on reward score: aligned outputs cluster at high values, human text does not.

**Detection performance:**
- On ChatGPT outputs: ~95-99% AUROC
- On LLaMA-2-Chat outputs: ~92-98% AUROC
- On base (unaligned) models: **~60-75% AUROC** (much weaker)

**Key implications:**
- Cross-model transfer works: a reward model from one family detects outputs from differently-aligned LLMs, suggesting alignment methods converge on similar distributional signatures
- Stronger alignment = stronger detection signal (monotonic relationship)
- The reward model serves as the natural &#34;detection key&#34; for the implicit alignment watermark
- Detection is robust to paraphrasing attacks

---

## 2. Theoretical Foundations

### 2.1 Chakraborty et al. (2023) -- Information-Theoretic Possibility

**Paper:** &#34;On the Possibilities of AI-Generated Text Detection&#34; (arXiv:2304.04736)

This paper provides the theoretical framework for when detection is possible vs. impossible.

**Central theorem:** Detection is possible if and only if TV(m, h) &gt; 0 (the total variation distance between machine and human distributions is non-zero). The only case where detection is truly impossible is when the distributions are **identical** across their entire support.

**Multi-sample detection:** For any non-zero TV distance delta:
- Sample complexity: n = Omega(1/delta^2 * log(1/(1-epsilon)))
- Single-sample AUROC upper bound: 1/2 + TV - TV^2/2
- With n samples, TV(m^n, h^n) -&gt; 1 exponentially

**Quantitative scaling (Figure 1, for TV=0.1):**
- n=1: AUROC ~0.6 (near random)
- n=30: AUROC ~0.7
- n=100: AUROC ~0.85
- n=500: AUROC ~0.98 (near perfect)

**Connection to alignment hypothesis:** The paper observes that &#34;it would be quite difficult to make LLMs exactly equal to human distributions due to the vast diversity within the human population.&#34; Alignment makes this not just difficult but *intentionally impossible* -- alignment IS the mechanism ensuring the distributional gap persists. A hypothetical perfect base model (TV=0) would be undetectable but also unaligned; the moment you align it, you guarantee TV&gt;0 and thus detectability.

### 2.2 Mitchell et al. (2023) -- DetectGPT

**Paper:** &#34;DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature&#34; (ICML 2023, arXiv:2301.11305)

DetectGPT introduces the perturbation discrepancy as a detection signal: machine-generated text sits in negative-curvature regions of the model&#39;s log probability function.

**The implicit watermark insight:** The paper explicitly states: &#34;LLMs that do not perfectly imitate human writing essentially watermark themselves implicitly.&#34; This is the foundational observation for our hypothesis.

**Mathematical basis:**
- Perturbation discrepancy: d(x, p_theta, q) = log p_theta(x) - E[log p_theta(x_tilde)]
- Approximates the negative trace of the Hessian of log p_theta (curvature measure)
- Machine text sits near local maxima; perturbations move downhill
- Human text sits in flatter regions; perturbations move in mixed directions

**Results (AUROC, averaged across 5 base models):**

| Method | XSum | SQuAD | WritingPrompts |
|--------|------|-------|----------------|
| Log p(x) | 0.83 | 0.82 | 0.95 |
| DetectGPT | **0.97** | **0.92** | **0.97** |

**Gap in the paper:** All models tested are base models only. The authors speculate that RLHF &#34;may exacerbate&#34; the disconnect between model and human distributions but do not test this. Our hypothesis predicts that aligned models would produce even sharper curvature signatures.

### 2.3 Bao et al. (2024) -- Fast-DetectGPT

**Paper:** &#34;Fast-DetectGPT: Efficient Zero-Shot Detection of Machine-Generated Text via Conditional Probability Curvature&#34; (ICLR 2024, arXiv:2310.05130)

Fast-DetectGPT replaces DetectGPT&#39;s perturbation-based curvature estimation with conditional probability curvature, achieving 340x speedup while improving accuracy. Uses sampling/scoring model pairs (e.g., Llama3-8B/Llama3-8B-Instruct).

This is the detector that shows the strongest alignment-dependent improvement in Xu &amp; Zubiaga (2025): AUROC 0.68 -&gt; 0.91 from base to PPO on instruction following.

---

## 3. Detection Benchmarks and Empirical Landscape

### 3.1 Dugan et al. (2024) -- RAID Benchmark

**Paper:** &#34;RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors&#34; (ACL 2024, arXiv:2405.07940)

RAID is the largest detection benchmark: 6.2M+ generations across 11 LLMs, 8 domains, 4 decoding strategies, and 11 adversarial attacks.

**Critical for our hypothesis -- base vs. chat model pairs:**

| Model Family | Base Variant | Chat Variant |
|-------------|-------------|--------------|
| Mistral-7B | Mistral-7B | Mistral-7B Chat |
| MPT-30B | MPT-30B | MPT-30B Chat |
| Cohere | Cohere (command) | Cohere Chat |

**Detection accuracy at FPR=5%:**
- Chat/aligned models are **consistently easier to detect** than base counterparts
- Base models with sampling + repetition penalty are hardest to detect (some detectors drop to ~0% accuracy)
- Repetition penalty decreases accuracy by up to 32 percentage points

**Self-BLEU statistics (lower = less repetitive):**
- Mistral base: 19.1 vs. Mistral Chat: 9.16
- MPT base: 22.1 vs. MPT Chat: 5.39

**Key finding:** &#34;Even strong detectors can catastrophically fail&#34; -- simply changing the generator, decoding strategy, or applying repetition penalty introduces up to 95%+ error rates.

### 3.2 He et al. (2023) -- MGTBench

**Paper:** &#34;MGTBench: Benchmarking Machine-Generated Text Detection&#34; (ACM CCS 2024, arXiv:2303.14822)

Comprehensive benchmark with 13 detection methods (metric-based and model-based) across multiple generators and datasets.

### 3.3 Li et al. (2024) -- MAGE

**Paper:** &#34;MAGE: Machine-Generated Text Detection in the Wild&#34; (ACL 2024, arXiv:2305.13242)

Largest-scale multi-generator detection study with 447,674 texts from 27 LLMs across 10 domains and 8 systematic testbeds of increasing difficulty.

### 3.4 Guo et al. (2023) -- HC3

**Paper:** &#34;How Close is ChatGPT to Human Experts?&#34; (arXiv:2301.07597)

Human ChatGPT Comparison Corpus with parallel human and ChatGPT answers, useful for direct base vs. aligned comparison studies.

---

## 4. Alignment Training Methods and Their Effects

### 4.1 Ouyang et al. (2022) -- InstructGPT

**Paper:** &#34;Training Language Models to Follow Instructions with Human Feedback&#34; (NeurIPS 2022, arXiv:2203.02155)

The foundational RLHF paper establishing the SFT -&gt; Reward Modeling -&gt; PPO pipeline. Demonstrates that alignment dramatically changes output characteristics: instruction-following, safety, format compliance. Each stage represents a measurable distributional shift from the base model.

### 4.2 Kirchenbauer et al. (2023a, 2023b) -- LLM Watermarking

**Papers:**
- &#34;A Watermark for Large Language Models&#34; (ICML 2023, arXiv:2301.10226)
- &#34;On the Reliability of Watermarks for Large Language Models&#34; (ICLR 2024, arXiv:2306.04634)

These papers on explicit watermarking provide an instructive contrast to our implicit watermark hypothesis:
- **Explicit watermarks** modify the generation process (green/red list token partitioning, logit bias delta=2.0)
- **Implicit alignment watermark** emerges naturally from the RLHF training objective
- Both create detectable distributional shifts; the key difference is intentionality
- Explicit watermarks are vulnerable to paraphrasing; the alignment watermark may be more robust because it is embedded in the model&#39;s learned preferences, not just the token selection mechanism

### 4.3 Dathathri et al. (2024) -- SynthID

**Paper:** &#34;Scalable Watermarking for Identifying Large Language Model Outputs&#34; (Nature 2024)

Google&#39;s production watermarking system using tournament sampling. Demonstrates that practical watermarking at scale is possible, but requires control over the generation process. Our hypothesis suggests aligned models already carry a natural version of this signal.

---

## 5. Additional Supporting Evidence

### 5.1 Rivera-Soto et al. (2025) -- Distinct Style

**Paper:** &#34;AI-generated Text has a Distinct Style Which is Influenced by the Prompt&#34;

Demonstrates that AI-generated text has measurable stylistic properties that differ from human text, and that these properties are influenced by but not fully determined by the prompt.

### 5.2 McGovern et al. (2024) -- Fingerprints

**Paper:** &#34;Fingerprints of AI-Generated Text&#34;

Identifies specific linguistic fingerprints in AI-generated text that persist across models and domains.

### 5.3 Fraser et al. (2024) -- Factors of Detectability

**Paper:** &#34;Factors Affecting the Detectability of AI-Generated Text&#34;

Systematic study of what makes some AI text more detectable than others. Factors include model size, decoding strategy, domain, and -- crucially -- alignment training.

### 5.4 Panickssery et al. (2024) -- Self-Recognition

**Paper:** &#34;LLM Evaluators Recognize and Favor Their Own Generations&#34;

LLMs can recognize their own outputs, suggesting alignment creates model-specific signatures that are inherently discriminable.

### 5.5 Li et al. (2024) -- Predicting vs. Acting

**Paper:** &#34;Predicting vs. Acting: A Trade-off Between World Knowledge and Language Generation&#34;

Explores the tension between a model&#39;s world knowledge (base model capability) and its generation patterns (alignment-influenced), relevant to understanding why alignment shifts distributions.

### 5.6 Bhattacharjee et al. (2023) -- ChatGPT Detector

**Paper:** &#34;Fighting Fire with Fire: Can ChatGPT Be Used to Detect ChatGPT-Generated Text?&#34;

Early work on using LLMs themselves as detectors, finding that ChatGPT can detect its own outputs with reasonable accuracy.

---

## 6. Detection Methods Taxonomy

### 6.1 Zero-Shot Statistical Methods
- **DetectGPT** (Mitchell et al., 2023): Probability curvature via perturbation
- **Fast-DetectGPT** (Bao et al., 2024): Conditional probability curvature (340x faster)
- **DNA-GPT** (Yang et al., 2023): Divergence-based detection
- **GLTR** (Gehrmann et al., 2019): Statistical token analysis and visualization
- **Binoculars** (Hans et al., 2024): Cross-model perplexity comparison

### 6.2 Trained Classifiers
- **RoBERTa-based detectors** (OpenAI): Fine-tuned on labeled human/machine data
- **RADAR** (Hu et al., 2023): Adversarially-trained robust detector
- **ReMoDetect** (Lee et al., 2024): Reward model as detector

### 6.3 Proactive Methods
- **Green/Red List Watermarking** (Kirchenbauer et al., 2023)
- **SynthID** (Dathathri et al., 2024): Tournament sampling watermark
- Our hypothesis: **Alignment itself** as a natural watermark

### 6.4 Benchmark Frameworks
- **RAID** (Dugan et al., 2024): 11 models, 8 domains, adversarial attacks
- **MGTBench** (He et al., 2023): 13 detection methods
- **MAGE** (Li et al., 2024): 27 generators, 10 domains

---

## 7. Key Methodological Insights for Experiments

### 7.1 Recommended Evaluation Metrics
- **AUROC** as primary metric (used across all surveyed papers)
- **Accuracy at FPR=5%** for practical deployment scenarios (RAID standard)
- **TPR at fixed FPR** (1%, 5%, 10%) for threshold sensitivity
- **Distinct n-gram scores** and **SentenceBERT similarity** for diversity measurement

### 7.2 Critical Experimental Controls
1. **Same model family, different alignment stages:** Llama base -&gt; SFT -&gt; PPO (Xu &amp; Zubiaga design)
2. **Base vs. chat pairs:** Mistral/Mistral-Chat, MPT/MPT-Chat, Cohere/Cohere-Chat (RAID design)
3. **Multiple detectors:** At minimum Fast-DetectGPT (zero-shot) + RoBERTa (supervised) + reward model approach
4. **Multiple domains:** News, creative writing, QA, academic text
5. **Decoding strategy control:** Greedy, sampling, with/without repetition penalty

### 7.3 Known Confounds
- **Text length:** GPTZero requires 150+ characters; detection improves with length (Xu &amp; Zubiaga)
- **Code-mixed text:** Detection degrades on mixed code/NL content (QA results in Xu &amp; Zubiaga)
- **Repetition penalty:** Dramatically reduces detectability (up to 32 percentage points in RAID)
- **Domain shift:** Detectors trained on one domain may fail on others (RAID, MAGE)

---

## 8. Gaps in the Literature and Experimental Opportunities

### 8.1 Gaps Our Research Can Address

1. **No systematic study of alignment type vs. detectability.** Papers test SFT+RLHF or just RLHF. No study compares RLHF vs. DPO vs. Constitutional AI vs. RLHF+CAI on the same base model.

2. **No theory connecting alignment strength to TV distance.** Chakraborty et al. prove detection requires TV&gt;0; no paper quantifies how alignment magnitude relates to TV distance.

3. **No cross-family alignment comparison.** Do Llama-RLHF and GPT-RLHF produce similar alignment signatures? ReMoDetect&#39;s cross-model transfer suggests yes, but this hasn&#39;t been systematically studied.

4. **Limited base model testing.** DetectGPT only tested base models; most detection papers only test aligned models. Direct comparison on the same architecture is rare (Xu &amp; Zubiaga is one of the few).

5. **No formal framing of alignment as implicit watermark.** The phrase appears in DetectGPT&#39;s discussion but hasn&#39;t been developed into a formal framework.

### 8.2 Proposed Experimental Program

1. **Phase 1 (Distributionial analysis):** Compare token probability distributions between base and aligned model pairs (using RAID data) to quantify TV distance at the empirical level.

2. **Phase 2 (Detection spectrum):** Run DetectGPT, Fast-DetectGPT, and reward model detection across base/SFT/RLHF variants of multiple model families.

3. **Phase 3 (Alignment type comparison):** Compare detectability of different alignment methods (RLHF, DPO, SFT-only, Constitutional) applied to the same base model.

4. **Phase 4 (Formal framework):** Develop the theoretical framework connecting alignment objectives to distributional shifts that enable detection.

---

## 9. Summary of Evidence Chain

The literature supports the following logical chain:

1. **RLHF collapses output diversity** by 57-90% across lexical, semantic, and syntactic measures (Kirk et al., Xu &amp; Zubiaga)

2. **This creates a consistent stylistic mode** -- across-input mode collapse means aligned models produce similar text regardless of prompt (Kirk et al.)

3. **The distributional shift is detectable** by zero-shot methods (Fast-DetectGPT AUROC improves from 0.68 to 0.91 with alignment, Xu &amp; Zubiaga)

4. **Reward models serve as natural detectors** because they encode the very preferences that alignment was optimized toward (ReMoDetect, AUROC 95-99% on aligned vs. 60-75% on base models)

5. **Detection is theoretically guaranteed** for any non-zero distributional shift (Chakraborty et al.), and alignment ensures this shift is non-zero

6. **LLMs &#34;watermark themselves implicitly&#34;** through their learned probability landscapes (DetectGPT), and alignment strengthens this watermark

**Conclusion:** The evidence strongly supports the hypothesis that alignment is the watermark. The alignment training process -- by design -- moves the model&#39;s output distribution away from the human text distribution, creating an inherent, detectable signature. This signature is not an artifact or a bug; it is a fundamental consequence of what alignment does: it makes the model&#39;s outputs systematically different from human text in ways that serve human preferences but simultaneously enable detection.

---

## References

1. Xu, B. &amp; Zubiaga, A. (2025). Understanding the Effects of RLHF on the Quality and Detectability of LLM-Generated Texts. arXiv:2503.17965.
2. Kirk, R. et al. (2024). Understanding the Effects of RLHF on LLM Generalisation and Diversity. ICLR 2024. arXiv:2310.06452.
3. Lee, J., Tack, J., &amp; Shin, J. (2024). ReMoDetect: Reward Models Recognize Aligned LLM&#39;s Generations.
4. Chakraborty, M. et al. (2023). On the Possibilities of AI-Generated Text Detection. arXiv:2304.04736.
5. Mitchell, E. et al. (2023). DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature. ICML 2023. arXiv:2301.11305.
6. Bao, G. et al. (2024). Fast-DetectGPT: Efficient Zero-Shot Detection via Conditional Probability Curvature. ICLR 2024. arXiv:2310.05130.
7. Dugan, L. et al. (2024). RAID: A Shared Benchmark for Robust Evaluation of Machine-Generated Text Detectors. ACL 2024. arXiv:2405.07940.
8. He, X. et al. (2023). MGTBench: Benchmarking Machine-Generated Text Detection. ACM CCS 2024. arXiv:2303.14822.
9. Li, Y. et al. (2024). MAGE: Machine-Generated Text Detection in the Wild. ACL 2024. arXiv:2305.13242.
10. Guo, B. et al. (2023). How Close is ChatGPT to Human Experts? arXiv:2301.07597.
11. Ouyang, L. et al. (2022). Training Language Models to Follow Instructions with Human Feedback. NeurIPS 2022. arXiv:2203.02155.
12. Kirchenbauer, J. et al. (2023). A Watermark for Large Language Models. ICML 2023. arXiv:2301.10226.
13. Kirchenbauer, J. et al. (2024). On the Reliability of Watermarks for Large Language Models. ICLR 2024. arXiv:2306.04634.
14. Dathathri, S. et al. (2024). Scalable Watermarking for Identifying Large Language Model Outputs. Nature.
15. Gehrmann, S. et al. (2019). GLTR: Statistical Detection and Visualization of Generated Text. ACL 2019. arXiv:1906.04043.
16. Ippolito, D. et al. (2020). Automatic Detection of Generated Text is Easiest When Humans are Fooled. ACL 2020. arXiv:1911.00650.
17. Yang, X. et al. (2023). DNA-GPT: Divergent N-Gram Analysis for Training-Free Detection of GPT-Generated Text. arXiv:2305.17359.
18. Tulchinskii, E. et al. (2023). Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. arXiv:2306.04723.
19. Sadasivan, V. et al. (2023). Can AI-Generated Text be Reliably Detected? arXiv:2303.11156.
20. Rivera-Soto, R. et al. (2025). AI-generated Text has a Distinct Style.
21. McGovern, H. et al. (2024). Fingerprints of AI-Generated Text.
22. Fraser, K. et al. (2024). Factors Affecting the Detectability of AI-Generated Text.
23. Panickssery, A. et al. (2024). LLM Evaluators Recognize and Favor Their Own Generations.
24. Li, Y. et al. (2024). Predicting vs. Acting: A Trade-off.
25. Bhattacharjee, A. et al. (2023). Fighting Fire with Fire: Can ChatGPT Detect Its Own Text?
26. Hans, A. et al. (2024). Spotting LLMs with Binoculars.


════════════════════════════════════════════════════════════════════════════════
                          PAPER REQUIREMENTS
════════════════════════════════════════════════════════════════════════════════

Generate a complete academic paper with the following structure:

1. TITLE
   - Clear, specific, informative
   - Should convey main finding or contribution

2. ABSTRACT (150-250 words)
   - Problem statement
   - Approach
   - Key results
   - Significance

3. INTRODUCTION
   - Research problem and motivation
   - Gap in existing work
   - Our contribution (be specific)
   - Paper organization

4. RELATED WORK
   - Organized by theme/approach
   - Position our work relative to prior work
   - Cite papers from literature review

5. METHODOLOGY
   - Clear description of approach
   - Experimental setup
   - Datasets used
   - Evaluation metrics
   - Baselines

6. RESULTS
   - Present results with tables and figures
   - Statistical analysis
   - Comparison to baselines
   - Ablation studies (if applicable)

7. DISCUSSION
   - Interpretation of results
   - Limitations
   - Broader implications

8. CONCLUSION
   - Summary of contributions
   - Key findings
   - Future work

9. REFERENCES
   - BibTeX format
   - All cited papers

════════════════════════════════════════════════════════════════════════════════
                          OUTPUT FORMAT
════════════════════════════════════════════════════════════════════════════════

Create a MODULAR LaTeX project with the following directory structure:

paper_draft/
├── main.tex              # Main file that imports all sections
├── references.bib        # BibTeX references
├── sections/
│   ├── abstract.tex      # Abstract content
│   ├── introduction.tex  # Introduction section
│   ├── related_work.tex  # Related work section
│   ├── methodology.tex   # Methodology section
│   ├── results.tex       # Results section
│   ├── discussion.tex    # Discussion section
│   └── conclusion.tex    # Conclusion section
├── figures/              # Directory for any generated figures
├── tables/               # Directory for complex standalone tables
└── appendix/             # Directory for appendix sections (if needed)

INSTRUCTIONS:
1. First, create the directory structure above (mkdir -p paper_draft/sections paper_draft/figures paper_draft/tables paper_draft/appendix)
2. Write main.tex using the EXACT preamble for NEURIPS:

   \documentclass{article}
   \usepackage[final]{neurips_2025}  % NEURIPS style (neurips_2025.sty is in paper_draft/)
   \usepackage[hidelinks]{hyperref}  % REQUIRED: clickable links
   \usepackage{booktabs}  % REQUIRED: professional tables
   \usepackage{graphicx}
   \usepackage{amsmath,amssymb}

   % Import command files
   \input{commands/math}
   \input{commands/general}
   \input{commands/macros}

   % Use this bibliography style:
   \bibliographystyle{plainnat}

   - Use \input{sections/...} to include each section
   - Use \bibliography{references} for references
3. Write each section file with COMPLETE content (no placeholders)
4. Each section file should include its \section{} command
5. Write references.bib with all citations in BibTeX format
6. After writing all files, compile the paper:
   cd paper_draft && pdflatex -interaction=nonstopmode main.tex && bibtex main && pdflatex -interaction=nonstopmode main.tex && pdflatex -interaction=nonstopmode main.tex

This modular structure allows humans to easily:
- Edit individual sections without navigating a large file
- Track changes per section
- Reuse sections across different paper versions

════════════════════════════════════════════════════════════════════════════════
                          QUALITY REQUIREMENTS
════════════════════════════════════════════════════════════════════════════════

- Academic tone throughout
- All claims must be supported by data from the experiment report
- Proper citations using \cite{} commands
- Clear figures and tables with proper captions
- NO placeholder text - every section must have real content
- The paper MUST compile without errors
- If compilation fails, debug and fix the LaTeX errors

════════════════════════════════════════════════════════════════════════════════
                          LAB WRITING STYLE
════════════════════════════════════════════════════════════════════════════════

Follow these lab-specific conventions to match our paper style:

1. LANGUAGE STYLE:
   - Use active voice: "We propose", "We examine", "We focus on"
   - Be direct and confident: "Our main question is...", "We hypothesize that..."
   - State things clearly and simply - prefer plain language over jargon
   - Use bold questions as paragraph organizers: {\bf what is X?}
   - Include specific quantitative claims: "8.97% over baseline"
   - Avoid fancy wording: "utilize" → "use", "facilitate" → "help"

2. INTRODUCTION STRUCTURE:
   - Engaging hook (get to the point quickly)
   - Problem importance
   - Gap identification
   - Your approach with method figure reference
   - Quantitative preview of results
   - Contribution bullets (3-4 items, action verbs)

3. CONTRIBUTION LISTS:
   \begin{itemize}[leftmargin=*,itemsep=0pt,topsep=0pt]
       \item We propose...
       \item We conduct...
       \item We complement...
   \end{itemize}

4. MODULAR COMMANDS STRUCTURE:
   Create paper_draft/commands/ directory with:
   - math.tex: Math notation macros (copy from templates/paper_writing/commands/)
   - general.tex: Formatting macros (copy from templates/paper_writing/commands/)
   - macros.tex: Project-specific term definitions

   In main.tex, include:
   \input{commands/math}
   \input{commands/general}
   \input{commands/macros}

5. REFERENCE CONVENTIONS:
   Use reference macros from math.tex:
   - \figref{fig:name} for "figure 1" (lowercase, in-sentence)
   - \Figref{fig:name} for "Figure 1" (capitalized, start of sentence)
   - \secref{sec:name} for "section 2"

6. TEXT FORMATTING:
   - Use \para{Header text} for bold paragraph headers
   - Define method/dataset names with \textsc and \xspace:
     \newcommand{\methodname}{\textsc{MethodName}\xspace}

7. TABLE FORMATTING:
   - Use booktabs package (no vertical lines)
   - Use \resizebox{\textwidth}{!}{...} for wide tables
   - Use @{} to remove padding at table edges
   - Use \cmidrule(lr){x-y} for sub-headers
   - Use \textsc{} for dataset/method names in headers
   - Bold best results with {\bf ...}

8. FIGURE FORMATTING:
   - Use 0.32\textwidth for 3-column subfigures
   - Use 0.95\linewidth for full-width figures
   - Use \input{figures/legend} for shared legends
   - Write self-contained captions explaining key observations

9. RESULTS PRESENTATION:
   - Define \increase and \decrease for colored arrows (green up, red down)
   - Bold best results in tables
   - Report confidence intervals when available

10. ALGORITHM STYLING:
    - Use algpseudocode with [noend]
    - Use \triangleright for comments

11. HYPERLINKS (REQUIRED):
    - Always use \usepackage[hidelinks]{hyperref} or with colored links
    - All citations, section refs, figure refs, table refs must be clickable
    - This is essential for reader navigation

════════════════════════════════════════════════════════════════════════════════
                          WORKFLOW: REVIEW AND REFLECT
════════════════════════════════════════════════════════════════════════════════

Before calling finish, you MUST complete these review steps:

1. REVIEW RESOURCES (at the start):
   - Read templates/skills/paper-writer/SKILL.md for detailed guidance
   - Study templates/paper_writing/lab_style_guide.md for style conventions
   - Browse paper_examples/ for formatting and language patterns

2. SELF-REFLECTION (before finishing):
   After writing all sections, review your work against these criteria:

   LANGUAGE CHECK:
   - [ ] Is the writing clear and jargon-free?
   - [ ] Are claims specific with quantitative support?
   - [ ] Is active voice used throughout?

   FORMATTING CHECK:
   - [ ] Does main.tex include \input{commands/math}, \input{commands/general}, \input{commands/macros}?
   - [ ] Is hyperref package included for clickable references?
   - [ ] Do tables use booktabs (no vertical lines)?
   - [ ] Are best results bolded in tables?
   - [ ] Are figures/tables properly captioned?

   STRUCTURE CHECK:
   - [ ] Does introduction follow: hook → importance → gap → approach → preview → contributions?
   - [ ] Are contribution bullets specific with action verbs?
   - [ ] Does the paper compile without errors?

3. FIX ISSUES:
   - Address any issues found in the self-reflection
   - Re-compile and verify the PDF looks correct

Only after completing this review should you consider the paper finished.